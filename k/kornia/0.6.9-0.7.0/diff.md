# Comparing `tmp/kornia-0.6.9.tar.gz` & `tmp/kornia-0.7.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/kornia-0.6.9.tar", last modified: Wed Dec 21 21:44:56 2022, max compression
+gzip compressed data, was "kornia-0.7.0.tar", last modified: Wed Aug  2 10:18:16 2023, max compression
```

## Comparing `kornia-0.6.9.tar` & `kornia-0.7.0.tar`

### file list

```diff
@@ -1,348 +1,449 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/
--rw-r--r--   0 runner    (1001) docker     (122)    10173 2022-12-21 21:44:52.000000 kornia-0.6.9/LICENSE
--rw-r--r--   0 runner    (1001) docker     (122)    11168 2022-12-21 21:44:56.000000 kornia-0.6.9/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)     9763 2022-12-21 21:44:52.000000 kornia-0.6.9/README.md
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/
--rw-r--r--   0 runner    (1001) docker     (122)      887 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/
--rw-r--r--   0 runner    (1001) docker     (122)      138 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2288 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/
--rw-r--r--   0 runner    (1001) docker     (122)      972 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6490 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/affine.py
--rw-r--r--   0 runner    (1001) docker     (122)     4924 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     5987 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/center_crop.py
--rw-r--r--   0 runner    (1001) docker     (122)    13339 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/crop.py
--rw-r--r--   0 runner    (1001) docker     (122)     3977 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/elastic_transform.py
--rw-r--r--   0 runner    (1001) docker     (122)     3892 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/fisheye.py
--rw-r--r--   0 runner    (1001) docker     (122)     3235 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/horizontal_flip.py
--rw-r--r--   0 runner    (1001) docker     (122)     3112 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/pad.py
--rw-r--r--   0 runner    (1001) docker     (122)     4375 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/perspective.py
--rw-r--r--   0 runner    (1001) docker     (122)     5173 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/resize.py
--rw-r--r--   0 runner    (1001) docker     (122)     6730 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/resized_crop.py
--rw-r--r--   0 runner    (1001) docker     (122)     4672 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/rotation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2911 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/thin_plate_spline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2686 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/vertical_flip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/
--rw-r--r--   0 runner    (1001) docker     (122)     1778 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1140 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     2389 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/box_blur.py
--rw-r--r--   0 runner    (1001) docker     (122)     3124 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/brightness.py
--rw-r--r--   0 runner    (1001) docker     (122)     2014 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/channel_shuffle.py
--rw-r--r--   0 runner    (1001) docker     (122)     3796 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jiggle.py
--rw-r--r--   0 runner    (1001) docker     (122)     4661 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jitter.py
--rw-r--r--   0 runner    (1001) docker     (122)     3061 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/contrast.py
--rw-r--r--   0 runner    (1001) docker     (122)     2243 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/denormalize.py
--rw-r--r--   0 runner    (1001) docker     (122)     2083 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/equalize.py
--rw-r--r--   0 runner    (1001) docker     (122)     3668 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/erasing.py
--rw-r--r--   0 runner    (1001) docker     (122)     2952 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/gamma.py
--rw-r--r--   0 runner    (1001) docker     (122)     2807 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_blur.py
--rw-r--r--   0 runner    (1001) docker     (122)     2560 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_noise.py
--rw-r--r--   0 runner    (1001) docker     (122)     2574 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/grayscale.py
--rw-r--r--   0 runner    (1001) docker     (122)     2887 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/hue.py
--rw-r--r--   0 runner    (1001) docker     (122)     2302 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/invert.py
--rw-r--r--   0 runner    (1001) docker     (122)     5372 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (122)     2152 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/normalize.py
--rw-r--r--   0 runner    (1001) docker     (122)     6725 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/planckian_jitter.py
--rw-r--r--   0 runner    (1001) docker     (122)     7316 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/plasma.py
--rw-r--r--   0 runner    (1001) docker     (122)     2620 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/posterize.py
--rw-r--r--   0 runner    (1001) docker     (122)     4418 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/random_rgb_shift.py
--rw-r--r--   0 runner    (1001) docker     (122)     2968 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/saturation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2478 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/sharpness.py
--rw-r--r--   0 runner    (1001) docker     (122)     3067 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/solarize.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/
--rw-r--r--   0 runner    (1001) docker     (122)      242 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9014 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     5984 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/cutmix.py
--rw-r--r--   0 runner    (1001) docker     (122)     3234 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/jigsaw.py
--rw-r--r--   0 runner    (1001) docker     (122)     5235 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/mixup.py
--rw-r--r--   0 runner    (1001) docker     (122)    10043 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/mosaic.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_3d/
--rw-r--r--   0 runner    (1001) docker     (122)       96 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2171 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/
--rw-r--r--   0 runner    (1001) docker     (122)      602 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7401 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/affine.py
--rw-r--r--   0 runner    (1001) docker     (122)     4835 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/center_crop.py
--rw-r--r--   0 runner    (1001) docker     (122)     7223 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/crop.py
--rw-r--r--   0 runner    (1001) docker     (122)     3349 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/depthical_flip.py
--rw-r--r--   0 runner    (1001) docker     (122)     2808 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/horizontal_flip.py
--rw-r--r--   0 runner    (1001) docker     (122)     4007 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/perspective.py
--rw-r--r--   0 runner    (1001) docker     (122)     5503 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/rotation.py
--rw-r--r--   0 runner    (1001) docker     (122)     3333 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/vertical_flip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/
--rw-r--r--   0 runner    (1001) docker     (122)      149 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2574 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/equalize.py
--rw-r--r--   0 runner    (1001) docker     (122)     5763 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (122)     3413 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11554 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/container/
--rw-r--r--   0 runner    (1001) docker     (122)      382 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    19658 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/augment.py
--rw-r--r--   0 runner    (1001) docker     (122)     6424 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     4195 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/dispatcher.py
--rw-r--r--   0 runner    (1001) docker     (122)    18586 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/image.py
--rw-r--r--   0 runner    (1001) docker     (122)    18999 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/patch.py
--rw-r--r--   0 runner    (1001) docker     (122)    26301 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    12955 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/video.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/random_generator/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/
--rw-r--r--   0 runner    (1001) docker     (122)     1046 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9345 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/affine.py
--rw-r--r--   0 runner    (1001) docker     (122)     4695 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jiggle.py
--rw-r--r--   0 runner    (1001) docker     (122)     4913 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jitter.py
--rw-r--r--   0 runner    (1001) docker     (122)    13710 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/crop.py
--rw-r--r--   0 runner    (1001) docker     (122)    11941 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/cutmix.py
--rw-r--r--   0 runner    (1001) docker     (122)     2128 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/jigsaw.py
--rw-r--r--   0 runner    (1001) docker     (122)     5504 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/mixup.py
--rw-r--r--   0 runner    (1001) docker     (122)     4570 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/mosaic.py
--rw-r--r--   0 runner    (1001) docker     (122)     4610 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (122)     4145 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/perspective.py
--rw-r--r--   0 runner    (1001) docker     (122)     3708 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/plain_uniform.py
--rw-r--r--   0 runner    (1001) docker     (122)     1158 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/planckian_jitter.py
--rw-r--r--   0 runner    (1001) docker     (122)     2368 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/posterize.py
--rw-r--r--   0 runner    (1001) docker     (122)     2917 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/probability.py
--rw-r--r--   0 runner    (1001) docker     (122)     6013 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/rectangle_earase.py
--rw-r--r--   0 runner    (1001) docker     (122)     3663 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/resize.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/
--rw-r--r--   0 runner    (1001) docker     (122)      320 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11810 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/affine.py
--rw-r--r--   0 runner    (1001) docker     (122)     9803 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/crop.py
--rw-r--r--   0 runner    (1001) docker     (122)     5173 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (122)     3638 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/perspective.py
--rw-r--r--   0 runner    (1001) docker     (122)     3204 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/rotation.py
--rw-r--r--   0 runner    (1001) docker     (122)      152 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3728 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      395 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)     1204 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10609 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (122)     7403 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/utils/param_validation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/color/
--rw-r--r--   0 runner    (1001) docker     (122)     2373 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5452 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/gray.py
--rw-r--r--   0 runner    (1001) docker     (122)     5439 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/hls.py
--rw-r--r--   0 runner    (1001) docker     (122)     4392 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/hsv.py
--rw-r--r--   0 runner    (1001) docker     (122)     5625 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/lab.py
--rw-r--r--   0 runner    (1001) docker     (122)     5428 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/luv.py
--rw-r--r--   0 runner    (1001) docker     (122)     9624 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/raw.py
--rw-r--r--   0 runner    (1001) docker     (122)    12923 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/rgb.py
--rw-r--r--   0 runner    (1001) docker     (122)     2440 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/sepia.py
--rw-r--r--   0 runner    (1001) docker     (122)     3391 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/xyz.py
--rw-r--r--   0 runner    (1001) docker     (122)     4016 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/ycbcr.py
--rw-r--r--   0 runner    (1001) docker     (122)    12970 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/yuv.py
--rw-r--r--   0 runner    (1001) docker     (122)     3344 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/contrib/
--rw-r--r--   0 runner    (1001) docker     (122)     1071 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      836 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     2007 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/connected_components.py
--rw-r--r--   0 runner    (1001) docker     (122)     8743 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/diamond_square.py
--rw-r--r--   0 runner    (1001) docker     (122)     3512 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/distance_transform.py
--rw-r--r--   0 runner    (1001) docker     (122)     1206 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/edge_detection.py
--rw-r--r--   0 runner    (1001) docker     (122)    13646 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/extract_patches.py
--rw-r--r--   0 runner    (1001) docker     (122)    15450 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/face_detection.py
--rw-r--r--   0 runner    (1001) docker     (122)     2891 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/histogram_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)     6057 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/image_stitching.py
--rw-r--r--   0 runner    (1001) docker     (122)      814 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/lambda_module.py
--rw-r--r--   0 runner    (1001) docker     (122)     8179 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/vit.py
--rw-r--r--   0 runner    (1001) docker     (122)     9460 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/vit_mobile.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/core/
--rw-r--r--   0 runner    (1001) docker     (122)      473 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      568 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/core/_backend.py
--rw-r--r--   0 runner    (1001) docker     (122)     4267 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/core/tensor_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/enhance/
--rw-r--r--   0 runner    (1001) docker     (122)     2053 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    52178 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/adjust.py
--rw-r--r--   0 runner    (1001) docker     (122)     3422 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/core.py
--rw-r--r--   0 runner    (1001) docker     (122)    15520 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/equalization.py
--rw-r--r--   0 runner    (1001) docker     (122)     9847 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/histogram.py
--rw-r--r--   0 runner    (1001) docker     (122)     9733 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/normalize.py
--rw-r--r--   0 runner    (1001) docker     (122)      613 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/shift_rgb.py
--rw-r--r--   0 runner    (1001) docker     (122)    13321 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/zca.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/
--rw-r--r--   0 runner    (1001) docker     (122)     3348 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/adalam/
--rw-r--r--   0 runner    (1001) docker     (122)       74 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11891 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/adalam.py
--rw-r--r--   0 runner    (1001) docker     (122)    18566 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/core.py
--rw-r--r--   0 runner    (1001) docker     (122)     7014 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/ransac.py
--rw-r--r--   0 runner    (1001) docker     (122)     3986 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    10285 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/affine_shape.py
--rw-r--r--   0 runner    (1001) docker     (122)    11407 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/defmo.py
--rw-r--r--   0 runner    (1001) docker     (122)     7534 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/hardnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     7372 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/hynet.py
--rw-r--r--   0 runner    (1001) docker     (122)    14250 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/integrated.py
--rw-r--r--   0 runner    (1001) docker     (122)    12366 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/keynet.py
--rw-r--r--   0 runner    (1001) docker     (122)    19385 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/laf.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/
--rw-r--r--   0 runner    (1001) docker     (122)       25 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      429 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6858 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/backbone/resnet_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     8010 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/
--rw-r--r--   0 runner    (1001) docker     (122)       93 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2922 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/fine_preprocess.py
--rw-r--r--   0 runner    (1001) docker     (122)     2991 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/linear_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     3757 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10506 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/coarse_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)     2893 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/fine_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)     2084 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/geometry.py
--rw-r--r--   0 runner    (1001) docker     (122)     2995 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)     5601 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/supervision.py
--rw-r--r--   0 runner    (1001) docker     (122)    16164 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/matching.py
--rw-r--r--   0 runner    (1001) docker     (122)    23687 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/mkd.py
--rw-r--r--   0 runner    (1001) docker     (122)     9588 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/orientation.py
--rw-r--r--   0 runner    (1001) docker     (122)    11814 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/responses.py
--rw-r--r--   0 runner    (1001) docker     (122)    10328 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/scale_space_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)    10773 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/siftdesc.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/sold2/
--rw-r--r--   0 runner    (1001) docker     (122)       68 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13823 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/backbones.py
--rw-r--r--   0 runner    (1001) docker     (122)    15530 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/sold2.py
--rw-r--r--   0 runner    (1001) docker     (122)    26202 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/sold2_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     2746 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sosnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     2213 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/tfeat.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/filters/
--rw-r--r--   0 runner    (1001) docker     (122)     2242 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3548 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/blur.py
--rw-r--r--   0 runner    (1001) docker     (122)    10053 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/blur_pool.py
--rw-r--r--   0 runner    (1001) docker     (122)     8434 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/canny.py
--rw-r--r--   0 runner    (1001) docker     (122)    10332 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/dexined.py
--rw-r--r--   0 runner    (1001) docker     (122)    11037 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/filter.py
--rw-r--r--   0 runner    (1001) docker     (122)     3816 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/gaussian.py
--rw-r--r--   0 runner    (1001) docker     (122)    24573 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/kernels.py
--rw-r--r--   0 runner    (1001) docker     (122)     8136 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/kernels_geometry.py
--rw-r--r--   0 runner    (1001) docker     (122)     3035 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/laplacian.py
--rw-r--r--   0 runner    (1001) docker     (122)     2568 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/median.py
--rw-r--r--   0 runner    (1001) docker     (122)     8302 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/motion.py
--rw-r--r--   0 runner    (1001) docker     (122)     9127 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/sobel.py
--rw-r--r--   0 runner    (1001) docker     (122)     2553 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/unsharp.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/
--rw-r--r--   0 runner    (1001) docker     (122)      302 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    23306 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/bbox.py
--rw-r--r--   0 runner    (1001) docker     (122)    41542 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/boxes.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/calibration/
--rw-r--r--   0 runner    (1001) docker     (122)      246 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5976 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/distort.py
--rw-r--r--   0 runner    (1001) docker     (122)    12366 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/pnp.py
--rw-r--r--   0 runner    (1001) docker     (122)     7525 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/undistort.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/camera/
--rw-r--r--   0 runner    (1001) docker     (122)      257 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2927 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/perspective.py
--rw-r--r--   0 runner    (1001) docker     (122)    26425 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/pinhole.py
--rw-r--r--   0 runner    (1001) docker     (122)    11674 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/stereo.py
--rw-r--r--   0 runner    (1001) docker     (122)    55879 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/conversions.py
--rw-r--r--   0 runner    (1001) docker     (122)    16394 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/depth.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/epipolar/
--rw-r--r--   0 runner    (1001) docker     (122)     1839 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7355 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/_metrics.py
--rw-r--r--   0 runner    (1001) docker     (122)    11190 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/essential.py
--rw-r--r--   0 runner    (1001) docker     (122)    11544 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/fundamental.py
--rw-r--r--   0 runner    (1001) docker     (122)      839 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/numeric.py
--rw-r--r--   0 runner    (1001) docker     (122)     7021 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/projection.py
--rw-r--r--   0 runner    (1001) docker     (122)     1618 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/scene.py
--rw-r--r--   0 runner    (1001) docker     (122)     2904 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/triangulation.py
--rw-r--r--   0 runner    (1001) docker     (122)    14513 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/homography.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/liegroup/
--rw-r--r--   0 runner    (1001) docker     (122)      124 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2547 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    10895 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/se2.py
--rw-r--r--   0 runner    (1001) docker     (122)    12634 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/se3.py
--rw-r--r--   0 runner    (1001) docker     (122)     8562 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/so2.py
--rw-r--r--   0 runner    (1001) docker     (122)    10126 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/so3.py
--rw-r--r--   0 runner    (1001) docker     (122)    10487 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/linalg.py
--rw-r--r--   0 runner    (1001) docker     (122)     6705 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/line.py
--rw-r--r--   0 runner    (1001) docker     (122)     4888 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/plane.py
--rw-r--r--   0 runner    (1001) docker     (122)    12797 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/quaternion.py
--rw-r--r--   0 runner    (1001) docker     (122)    10553 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/ransac.py
--rw-r--r--   0 runner    (1001) docker     (122)      173 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/ray.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/subpix/
--rw-r--r--   0 runner    (1001) docker     (122)      769 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5871 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/dsnt.py
--rw-r--r--   0 runner    (1001) docker     (122)     6458 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/nms.py
--rw-r--r--   0 runner    (1001) docker     (122)    24479 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/spatial_soft_argmax.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/transform/
--rw-r--r--   0 runner    (1001) docker     (122)      266 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    37259 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/affwarp.py
--rw-r--r--   0 runner    (1001) docker     (122)    13706 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/crop2d.py
--rw-r--r--   0 runner    (1001) docker     (122)    13750 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/crop3d.py
--rw-r--r--   0 runner    (1001) docker     (122)     4134 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/elastic_transform.py
--rw-r--r--   0 runner    (1001) docker     (122)     3545 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/flips.py
--rw-r--r--   0 runner    (1001) docker     (122)     4873 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/homography_warper.py
--rw-r--r--   0 runner    (1001) docker     (122)    10130 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/image_registrator.py
--rw-r--r--   0 runner    (1001) docker     (122)    49748 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/imgwarp.py
--rw-r--r--   0 runner    (1001) docker     (122)    14565 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/pyramid.py
--rw-r--r--   0 runner    (1001) docker     (122)    10879 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/thin_plate_spline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3429 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/vector.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/grad_estimator/
--rw-r--r--   0 runner    (1001) docker     (122)       55 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/grad_estimator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5054 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/grad_estimator/ste.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/io/
--rw-r--r--   0 runner    (1001) docker     (122)       42 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4287 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/io/io.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/losses/
--rw-r--r--   0 runner    (1001) docker     (122)     1264 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3709 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/depth_smooth.py
--rw-r--r--   0 runner    (1001) docker     (122)     3950 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/dice.py
--rw-r--r--   0 runner    (1001) docker     (122)     2560 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/divergence.py
--rw-r--r--   0 runner    (1001) docker     (122)    10219 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/focal.py
--rw-r--r--   0 runner    (1001) docker     (122)    10217 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/hausdorff.py
--rw-r--r--   0 runner    (1001) docker     (122)     4646 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/lovasz_hinge.py
--rw-r--r--   0 runner    (1001) docker     (122)     5155 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/lovasz_softmax.py
--rw-r--r--   0 runner    (1001) docker     (122)     6536 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/ms_ssim.py
--rw-r--r--   0 runner    (1001) docker     (122)     1748 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/psnr.py
--rw-r--r--   0 runner    (1001) docker     (122)     3791 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/ssim.py
--rw-r--r--   0 runner    (1001) docker     (122)     2733 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/total_variation.py
--rw-r--r--   0 runner    (1001) docker     (122)     4715 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/tversky.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/metrics/
--rw-r--r--   0 runner    (1001) docker     (122)      453 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      857 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/accuracy.py
--rw-r--r--   0 runner    (1001) docker     (122)      865 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/average_meter.py
--rw-r--r--   0 runner    (1001) docker     (122)     3106 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/confusion_matrix.py
--rw-r--r--   0 runner    (1001) docker     (122)     8344 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/mean_average_precision.py
--rw-r--r--   0 runner    (1001) docker     (122)     4697 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/mean_iou.py
--rw-r--r--   0 runner    (1001) docker     (122)     1634 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/psnr.py
--rw-r--r--   0 runner    (1001) docker     (122)     6013 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/ssim.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/morphology/
--rw-r--r--   0 runner    (1001) docker     (122)       26 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/morphology/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    22676 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/morphology/morphology.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/py.typed
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/testing/
--rw-r--r--   0 runner    (1001) docker     (122)    17342 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/testing/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/tracking/
--rw-r--r--   0 runner    (1001) docker     (122)       79 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/tracking/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6073 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/tracking/planar_tracker.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/utils/
--rw-r--r--   0 runner    (1001) docker     (122)     1098 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1988 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/_compat.py
--rw-r--r--   0 runner    (1001) docker     (122)    12277 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/draw.py
--rw-r--r--   0 runner    (1001) docker     (122)     4344 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/grid.py
--rw-r--r--   0 runner    (1001) docker     (122)     9216 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (122)     8586 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/image.py
--rw-r--r--   0 runner    (1001) docker     (122)     1792 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/memory.py
--rw-r--r--   0 runner    (1001) docker     (122)     2450 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (122)     1695 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/one_hot.py
--rw-r--r--   0 runner    (1001) docker     (122)     3245 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/pointcloud_io.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/x/
--rw-r--r--   0 runner    (1001) docker     (122)      236 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3212 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/callbacks.py
--rw-r--r--   0 runner    (1001) docker     (122)     7851 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4560 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/trainers.py
--rw-r--r--   0 runner    (1001) docker     (122)     2894 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)    11168 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)    10650 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)      442 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)        7 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/zip-safe
--rw-r--r--   0 runner    (1001) docker     (122)      315 2022-12-21 21:44:52.000000 kornia-0.6.9/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (122)     2812 2022-12-21 21:44:56.000000 kornia-0.6.9/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (122)      316 2022-12-21 21:44:52.000000 kornia-0.6.9/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/test/
--rw-r--r--   0 runner    (1001) docker     (122)    30427 2022-12-21 21:44:52.000000 kornia-0.6.9/test/test_contrib.py
--rw-r--r--   0 runner    (1001) docker     (122)    10407 2022-12-21 21:44:52.000000 kornia-0.6.9/test/test_metrics.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.825537 kornia-0.7.0/
+-rw-r--r--   0 runner    (1001) docker     (122)    10173 2023-08-02 10:17:59.000000 kornia-0.7.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (122)    11699 2023-08-02 10:18:16.825537 kornia-0.7.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    10125 2023-08-02 10:17:59.000000 kornia-0.7.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.761537 kornia-0.7.0/kornia/
+-rw-r--r--   0 runner    (1001) docker     (122)      681 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.761537 kornia-0.7.0/kornia/augmentation/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.765537 kornia-0.7.0/kornia/augmentation/_2d/
+-rw-r--r--   0 runner    (1001) docker     (122)      138 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5516 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.765537 kornia-0.7.0/kornia/augmentation/_2d/geometric/
+-rw-r--r--   0 runner    (1001) docker     (122)     1108 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6629 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11162 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6099 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/center_crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14274 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4673 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/elastic_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3554 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/fisheye.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3391 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/horizontal_flip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2985 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/pad.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4432 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4894 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/resize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6379 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/resized_crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4729 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/rotation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5001 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/shear.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2674 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/thin_plate_spline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4920 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/translate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2849 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/geometric/vertical_flip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.769537 kornia-0.7.0/kornia/augmentation/_2d/intensity/
+-rw-r--r--   0 runner    (1001) docker     (122)     2070 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1374 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/auto_contrast.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2791 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2131 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/box_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3038 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/brightness.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1886 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/channel_shuffle.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3711 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/color_jiggle.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4216 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/color_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2895 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/contrast.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2161 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/denormalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1960 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/equalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4212 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/erasing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2784 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/gamma.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3743 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/gaussian_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2433 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/gaussian_noise.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/grayscale.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2683 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/hue.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2195 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/invert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/median_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5295 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2109 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/normalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6537 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/planckian_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6998 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/plasma.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2584 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/posterize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4041 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/random_rain.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4334 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/random_rgb_shift.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3278 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/random_snow.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2797 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/saturation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2386 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/sharpness.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2983 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/intensity/solarize.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/_2d/mix/
+-rw-r--r--   0 runner    (1001) docker     (122)      242 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/mix/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9476 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/mix/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5986 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/mix/cutmix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3323 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/mix/jigsaw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5237 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/mix/mixup.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10058 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_2d/mix/mosaic.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/_3d/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4962 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/_3d/geometric/
+-rw-r--r--   0 runner    (1001) docker     (122)      602 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7232 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)      148 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4577 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/center_crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6954 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3254 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/depthical_flip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/horizontal_flip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3953 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5449 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/rotation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3238 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/geometric/vertical_flip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/_3d/intensity/
+-rw-r--r--   0 runner    (1001) docker     (122)      149 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/intensity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      148 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/intensity/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2479 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/intensity/equalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5687 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/_3d/intensity/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4125 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/auto/
+-rw-r--r--   0 runner    (1001) docker     (122)      297 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/auto/autoaugment/
+-rw-r--r--   0 runner    (1001) docker     (122)       37 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/autoaugment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6801 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/autoaugment/autoaugment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3556 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/autoaugment/ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4364 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/auto/operations/
+-rw-r--r--   0 runner    (1001) docker     (122)       88 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/operations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7540 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/operations/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22026 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/operations/ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5124 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/operations/policy.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.773537 kornia-0.7.0/kornia/augmentation/auto/rand_augment/
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/rand_augment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3204 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/rand_augment/ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4470 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/rand_augment/rand_augment.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.777537 kornia-0.7.0/kornia/augmentation/auto/trivial_augment/
+-rw-r--r--   0 runner    (1001) docker     (122)       44 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/trivial_augment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3102 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/auto/trivial_augment/trivial_augment.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20083 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.777537 kornia-0.7.0/kornia/augmentation/container/
+-rw-r--r--   0 runner    (1001) docker     (122)      449 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18892 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/augment.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13865 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4161 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/dispatcher.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14223 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18848 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)      323 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/params.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18473 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/patch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15201 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/container/video.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.777537 kornia-0.7.0/kornia/augmentation/random_generator/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.781537 kornia-0.7.0/kornia/augmentation/random_generator/_2d/
+-rw-r--r--   0 runner    (1001) docker     (122)     1308 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9565 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4747 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/color_jiggle.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4965 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/color_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13838 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6257 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/cutmix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2258 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/gaussian_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2163 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/jigsaw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2952 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/mixup.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4614 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/mosaic.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4655 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4195 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3927 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/plain_uniform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/planckian_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2456 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/posterize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2964 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/probability.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3556 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/random_rain.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6068 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/rectangle_earase.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3716 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/resize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4015 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/shear.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3975 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_2d/translate.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.781537 kornia-0.7.0/kornia/augmentation/random_generator/_3d/
+-rw-r--r--   0 runner    (1001) docker     (122)      320 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_3d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11841 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_3d/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9830 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_3d/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5183 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_3d/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3652 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_3d/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3274 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/_3d/rotation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      152 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3888 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      431 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/random_generator/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.781537 kornia-0.7.0/kornia/augmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14069 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7463 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/augmentation/utils/param_validation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.781537 kornia-0.7.0/kornia/color/
+-rw-r--r--   0 runner    (1001) docker     (122)     2542 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6657 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/colormap.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5545 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/gray.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5413 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/hls.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4391 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/hsv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5826 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/lab.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5427 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/luv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9623 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/raw.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12920 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/rgb.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2440 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/sepia.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3390 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/xyz.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3998 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/ycbcr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12962 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/color/yuv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3363 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.785537 kornia-0.7.0/kornia/contrib/
+-rw-r--r--   0 runner    (1001) docker     (122)     1190 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      836 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/connected_components.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8763 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/diamond_square.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3519 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/distance_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/edge_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13646 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/extract_patches.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15488 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/face_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2890 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/histogram_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15764 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/image_prompter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5951 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/image_stitching.py
+-rw-r--r--   0 runner    (1001) docker     (122)      825 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/lambda_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.785537 kornia-0.7.0/kornia/contrib/models/
+-rw-r--r--   0 runner    (1001) docker     (122)       74 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1763 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5290 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/common.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.785537 kornia-0.7.0/kornia/contrib/models/rt_detr/
+-rw-r--r--   0 runner    (1001) docker     (122)      161 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.785537 kornia-0.7.0/kornia/contrib/models/rt_detr/architecture/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/architecture/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5017 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/architecture/hgnetv2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7946 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/architecture/hybrid_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4399 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/architecture/resnet_d.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10921 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/architecture/rtdetr_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7135 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2495 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/rt_detr/post_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.785537 kornia-0.7.0/kornia/contrib/models/sam/
+-rw-r--r--   0 runner    (1001) docker     (122)       73 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.785537 kornia-0.7.0/kornia/contrib/models/sam/architecture/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/architecture/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      869 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/architecture/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11808 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/architecture/image_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5468 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/architecture/mask_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8110 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/architecture/prompt_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8081 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/architecture/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13189 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/sam/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4320 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/structures.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20851 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/models/tiny_vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3337 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/object_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15485 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/visual_prompter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8290 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9829 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/contrib/vit_mobile.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.789537 kornia-0.7.0/kornia/core/
+-rw-r--r--   0 runner    (1001) docker     (122)      796 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      838 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/core/_backend.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14017 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/core/check.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4714 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/core/tensor_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.789537 kornia-0.7.0/kornia/enhance/
+-rw-r--r--   0 runner    (1001) docker     (122)     2226 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    52226 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/adjust.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3491 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/core.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15556 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/equalization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9845 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/histogram.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3820 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/integral.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9760 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/normalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)      615 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/shift_rgb.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13249 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/enhance/zca.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.789537 kornia-0.7.0/kornia/feature/
+-rw-r--r--   0 runner    (1001) docker     (122)     3797 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.793537 kornia-0.7.0/kornia/feature/adalam/
+-rw-r--r--   0 runner    (1001) docker     (122)       74 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/adalam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13167 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/adalam/adalam.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18716 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/adalam/core.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7489 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/adalam/ransac.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4261 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/adalam/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10192 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/affine_shape.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11274 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/defmo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.793537 kornia-0.7.0/kornia/feature/disk/
+-rw-r--r--   0 runner    (1001) docker     (122)       57 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/disk/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.793537 kornia-0.7.0/kornia/feature/disk/_unets/
+-rw-r--r--   0 runner    (1001) docker     (122)       23 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/disk/_unets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2048 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/disk/_unets/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2226 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/disk/_unets/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1903 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/disk/detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5336 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/disk/disk.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2800 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/disk/structs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7586 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/hardnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7409 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/hynet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19081 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/integrated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6391 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/keynet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19609 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/laf.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19378 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/lightglue.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.793537 kornia-0.7.0/kornia/feature/loftr/
+-rw-r--r--   0 runner    (1001) docker     (122)       61 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.793537 kornia-0.7.0/kornia/feature/loftr/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      558 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7266 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/backbone/resnet_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8081 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/loftr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.793537 kornia-0.7.0/kornia/feature/loftr/loftr_module/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/loftr_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3114 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/loftr_module/fine_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3198 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/loftr_module/linear_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3807 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/loftr_module/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.793537 kornia-0.7.0/kornia/feature/loftr/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11027 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/utils/coarse_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3046 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/utils/fine_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2101 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/utils/geometry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3130 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5904 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/loftr/utils/supervision.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16295 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22841 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/mkd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9757 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/orientation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13740 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/responses.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17816 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/scale_space_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10348 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/siftdesc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.797537 kornia-0.7.0/kornia/feature/sold2/
+-rw-r--r--   0 runner    (1001) docker     (122)       68 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/sold2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13731 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/sold2/backbones.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15723 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/sold2/sold2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26536 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/sold2/sold2_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2733 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/sosnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2215 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/feature/tfeat.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.801537 kornia-0.7.0/kornia/filters/
+-rw-r--r--   0 runner    (1001) docker     (122)     2954 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10822 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/bilateral.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4373 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10709 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/blur_pool.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7919 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/canny.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10623 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/dexined.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10794 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4396 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/gaussian.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8087 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/guided.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36148 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/kernels.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7928 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/kernels_geometry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3008 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/laplacian.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2419 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/median.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8433 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/motion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8591 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/sobel.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2534 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/filters/unsharp.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.801537 kornia-0.7.0/kornia/geometry/
+-rw-r--r--   0 runner    (1001) docker     (122)      325 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23251 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/bbox.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45045 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/boxes.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.805537 kornia-0.7.0/kornia/geometry/calibration/
+-rw-r--r--   0 runner    (1001) docker     (122)      246 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/calibration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5976 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/calibration/distort.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12366 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/calibration/pnp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7529 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/calibration/undistort.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.805537 kornia-0.7.0/kornia/geometry/camera/
+-rw-r--r--   0 runner    (1001) docker     (122)      257 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/camera/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2927 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/camera/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26748 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/camera/pinhole.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11747 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/camera/stereo.py
+-rw-r--r--   0 runner    (1001) docker     (122)    51277 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17893 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/depth.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.805537 kornia-0.7.0/kornia/geometry/epipolar/
+-rw-r--r--   0 runner    (1001) docker     (122)     1839 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7381 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11184 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/essential.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16495 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/fundamental.py
+-rw-r--r--   0 runner    (1001) docker     (122)      839 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/numeric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6891 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/projection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1632 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/scene.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2904 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/epipolar/triangulation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14518 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/homography.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10961 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/keypoints.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.809537 kornia-0.7.0/kornia/geometry/liegroup/
+-rw-r--r--   0 runner    (1001) docker     (122)      124 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/liegroup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2567 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/liegroup/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12701 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/liegroup/se2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15017 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/liegroup/se3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8961 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/liegroup/so2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11233 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/liegroup/so3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9886 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/linalg.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6759 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/line.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4955 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/plane.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12807 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/quaternion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10572 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/ransac.py
+-rw-r--r--   0 runner    (1001) docker     (122)      173 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/ray.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.809537 kornia-0.7.0/kornia/geometry/solvers/
+-rw-r--r--   0 runner    (1001) docker     (122)      106 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/solvers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7080 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/solvers/polynomial_solver.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.809537 kornia-0.7.0/kornia/geometry/subpix/
+-rw-r--r--   0 runner    (1001) docker     (122)      805 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/subpix/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5686 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/subpix/dsnt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6805 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/subpix/nms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23926 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/subpix/spatial_soft_argmax.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.809537 kornia-0.7.0/kornia/geometry/transform/
+-rw-r--r--   0 runner    (1001) docker     (122)      266 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    37258 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/affwarp.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13706 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/crop2d.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13750 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/crop3d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4239 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/elastic_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3447 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/flips.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5259 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/homography_warper.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10504 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/image_registrator.py
+-rw-r--r--   0 runner    (1001) docker     (122)    50363 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/imgwarp.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15184 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/pyramid.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10884 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/transform/thin_plate_spline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5400 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/geometry/vector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.813537 kornia-0.7.0/kornia/grad_estimator/
+-rw-r--r--   0 runner    (1001) docker     (122)       55 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/grad_estimator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5051 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/grad_estimator/ste.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.813537 kornia-0.7.0/kornia/image/
+-rw-r--r--   0 runner    (1001) docker     (122)      175 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/image/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1831 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/image/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10143 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/image/image.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.813537 kornia-0.7.0/kornia/io/
+-rw-r--r--   0 runner    (1001) docker     (122)      113 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5916 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/io/io.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.817537 kornia-0.7.0/kornia/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)     1763 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3476 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/cauchy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4001 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/charbonnier.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3744 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/depth_smooth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5060 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/dice.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2576 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/divergence.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12295 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/focal.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3610 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/geman_mcclure.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9829 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/hausdorff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4669 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/lovasz_hinge.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5149 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/lovasz_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6517 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/ms_ssim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1776 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/psnr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3820 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/ssim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3766 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/ssim3d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2780 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/total_variation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4746 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/tversky.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3615 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/losses/welsch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.817537 kornia-0.7.0/kornia/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)      516 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      883 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/accuracy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1024 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/average_meter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3106 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/confusion_matrix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8344 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/mean_average_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4697 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/mean_iou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1634 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/psnr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6115 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/ssim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5847 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/metrics/ssim3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.817537 kornia-0.7.0/kornia/morphology/
+-rw-r--r--   0 runner    (1001) docker     (122)       26 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/morphology/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22730 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/morphology/morphology.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.817537 kornia-0.7.0/kornia/sensors/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/sensors/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.821537 kornia-0.7.0/kornia/sensors/camera/
+-rw-r--r--   0 runner    (1001) docker     (122)      337 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/sensors/camera/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11694 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/sensors/camera/camera_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2163 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/sensors/camera/distortion_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1844 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/sensors/camera/projection_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.821537 kornia-0.7.0/kornia/testing/
+-rw-r--r--   0 runner    (1001) docker     (122)    10101 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/testing/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.821537 kornia-0.7.0/kornia/tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)       79 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/tracking/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6073 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/tracking/planar_tracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.821537 kornia-0.7.0/kornia/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1466 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1857 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/_compat.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14767 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/draw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4344 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/grid.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11180 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8468 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9670 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/image_print.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1856 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/memory.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2460 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1695 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/one_hot.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3245 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/utils/pointcloud_io.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.825537 kornia-0.7.0/kornia/x/
+-rw-r--r--   0 runner    (1001) docker     (122)      236 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/x/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3336 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/x/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7945 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/x/trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4606 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/x/trainers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2987 2023-08-02 10:17:59.000000 kornia-0.7.0/kornia/x/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.761537 kornia-0.7.0/kornia.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)    11699 2023-08-02 10:18:16.000000 kornia-0.7.0/kornia.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    14036 2023-08-02 10:18:16.000000 kornia-0.7.0/kornia.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-08-02 10:18:16.000000 kornia-0.7.0/kornia.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      411 2023-08-02 10:18:16.000000 kornia-0.7.0/kornia.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        7 2023-08-02 10:18:16.000000 kornia-0.7.0/kornia.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-08-02 10:18:16.000000 kornia-0.7.0/kornia.egg-info/zip-safe
+-rw-r--r--   0 runner    (1001) docker     (122)     5945 2023-08-02 10:17:59.000000 kornia-0.7.0/pyproject.toml
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.825537 kornia-0.7.0/requirements/
+-rw-r--r--   0 runner    (1001) docker     (122)      119 2023-08-02 10:17:59.000000 kornia-0.7.0/requirements/requirements-dev.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      230 2023-08-02 10:17:59.000000 kornia-0.7.0/requirements/requirements-docs.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       19 2023-08-02 10:17:59.000000 kornia-0.7.0/requirements/requirements-x.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       23 2023-08-02 10:17:59.000000 kornia-0.7.0/requirements/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-08-02 10:18:16.825537 kornia-0.7.0/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-08-02 10:18:16.825537 kornia-0.7.0/test/
+-rw-r--r--   0 runner    (1001) docker     (122)    15443 2023-08-02 10:17:59.000000 kornia-0.7.0/test/test_metrics.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `kornia-0.6.9/LICENSE` & `kornia-0.7.0/LICENSE`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/PKG-INFO` & `kornia-0.7.0/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,36 +1,38 @@
 Metadata-Version: 2.1
 Name: kornia
-Version: 0.6.9
+Version: 0.7.0
 Summary: Open Source Differentiable Computer Vision Library for PyTorch
-Home-page: https://www.kornia.org
-Download-URL: https://github.com/kornia/kornia
-Author: Edgar Riba
-Author-email: edgar@kornia.org
+Author-email: Edgar Riba <edgar@kornia.org>
 License: Apache-2.0
 Project-URL: Bug Tracker, https://github.com/kornia/kornia/issues
 Project-URL: Documentation, https://kornia.readthedocs.io/en/latest
+Project-URL: Download, https://github.com/kornia/kornia
+Project-URL: Homepage, https://www.kornia.org
 Project-URL: Source Code, https://github.com/kornia/kornia
 Keywords: computer vision,deep learning,pytorch
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
 Classifier: Environment :: GPU
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Natural Language :: English
 Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Processing
 Classifier: Topic :: Software Development :: Libraries
-Requires-Python: >=3.7
+Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Provides-Extra: dev
 Provides-Extra: docs
 Provides-Extra: x
 License-File: LICENSE
 
 <div align="center">
@@ -54,16 +56,18 @@
 [![PyPI python](https://img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia)
 [![PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/kornia)
 [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/project/kornia)
 [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENCE)
 [![Slack](https://img.shields.io/badge/Slack-4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)
 [![Twitter](https://img.shields.io/twitter/follow/kornia_foss?style=social)](https://twitter.com/kornia_foss)
 
-[![tests-cpu](https://github.com/kornia/kornia/actions/workflows/tests_cpu.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cpu.yml)
+[![tests-cpu](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml)
+[![tests-cpu-nightly](https://github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml)
 [![tests-cuda](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml)
+[![tests-cpu-float16](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml)
 [![codecov](https://codecov.io/gh/kornia/kornia/branch/master/graph/badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/kornia)
 [![Documentation Status](https://readthedocs.org/projects/kornia/badge/?version=latest)](https://kornia.readthedocs.io/en/latest/?badge=latest)
 [![pre-commit.ci status](https://results.pre-commit.ci/badge/github/kornia/kornia/master.svg)](https://results.pre-commit.ci/latest/github/kornia/kornia/master)
 
 <a href="https://www.producthunt.com/posts/kornia?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-kornia" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=306439&theme=light" alt="Kornia - Computer vision library for deep learning | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /></a>
 
 </p>
@@ -146,28 +150,28 @@
 
 :triangular_flag_on_post: **Updates**
 - :white_check_mark: [Image Matching](https://kornia.readthedocs.io/en/latest/applications/image_matching.html) Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR).
 - :white_check_mark: [Face Detection](https://kornia.readthedocs.io/en/latest/applications/face_detection.html) Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/frapochetti/blurry-faces).
 
 ## Cite
 
-If you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](https://github.com/kornia/kornia/blob/master/CITATION.md).
+If you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](./CITATION.md).
 
   ```bibtex
   @inproceedings{eriba2019kornia,
     author    = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},
     title     = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},
     booktitle = {Winter Conference on Applications of Computer Vision},
     year      = {2020},
     url       = {https://arxiv.org/pdf/1910.02190.pdf}
   }
   ```
 
 ## Contributing
-We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](https://github.com/kornia/kornia/blob/master/CONTRIBUTING.rst) notes. The participation in this open source project is subject to [Code of Conduct](https://github.com/kornia/kornia/blob/master/CODE_OF_CONDUCT.md).
+We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes. The participation in this open source project is subject to [Code of Conduct](./CODE_OF_CONDUCT.md).
 
 
 ## Community
 - **Forums:** discuss implementations, research, etc. [GitHub Forums](https://github.com/kornia/kornia/discussions)
 - **GitHub Issues:** bug reports, feature requests, install issues, RFCs, thoughts, etc. [OPEN](https://github.com/kornia/kornia/issues/new/choose)
 - **Slack:** Join our workspace to keep in touch with our core contributors and be part of our community. [JOIN HERE](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)
 - For general information, please visit our website at www.kornia.org
```

#### html2text {}

```diff
@@ -1,44 +1,53 @@
-Metadata-Version: 2.1 Name: kornia Version: 0.6.9 Summary: Open Source
-Differentiable Computer Vision Library for PyTorch Home-page: https://
-www.kornia.org Download-URL: https://github.com/kornia/kornia Author: Edgar
-Riba Author-email: edgar@kornia.org License: Apache-2.0 Project-URL: Bug
-Tracker, https://github.com/kornia/kornia/issues Project-URL: Documentation,
-https://kornia.readthedocs.io/en/latest Project-URL: Source Code, https://
-github.com/kornia/kornia Keywords: computer vision,deep learning,pytorch
-Classifier: Development Status :: 4 - Beta Classifier: Environment :: Console
-Classifier: Environment :: GPU Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Education Classifier: Intended Audience ::
-Information Technology Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: Apache Software License Classifier:
-Natural Language :: English Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3 Classifier: Programming
-Language :: Python :: 3 :: Only Classifier: Topic :: Scientific/Engineering ::
-Artificial Intelligence Classifier: Topic :: Scientific/Engineering :: Image
-Processing Classifier: Topic :: Software Development :: Libraries Requires-
-Python: >=3.7 Description-Content-Type: text/markdown Provides-Extra: dev
-Provides-Extra: docs Provides-Extra: x License-File: LICENSE
+Metadata-Version: 2.1 Name: kornia Version: 0.7.0 Summary: Open Source
+Differentiable Computer Vision Library for PyTorch Author-email: Edgar Riba
+kornia.org> License: Apache-2.0 Project-URL: Bug Tracker, https://github.com/
+kornia/kornia/issues Project-URL: Documentation, https://kornia.readthedocs.io/
+en/latest Project-URL: Download, https://github.com/kornia/kornia Project-URL:
+Homepage, https://www.kornia.org Project-URL: Source Code, https://github.com/
+kornia/kornia Keywords: computer vision,deep learning,pytorch Classifier:
+Development Status :: 4 - Beta Classifier: Environment :: Console Classifier:
+Environment :: GPU Classifier: Intended Audience :: Developers Classifier:
+Intended Audience :: Education Classifier: Intended Audience :: Information
+Technology Classifier: Intended Audience :: Science/Research Classifier:
+License :: OSI Approved :: Apache Software License Classifier: Natural Language
+:: English Classifier: Operating System :: OS Independent Classifier:
+Programming Language :: Python :: 3 :: Only Classifier: Programming Language ::
+Python :: 3.8 Classifier: Programming Language :: Python :: 3.9 Classifier:
+Programming Language :: Python :: 3.10 Classifier: Programming Language ::
+Python :: 3.11 Classifier: Topic :: Scientific/Engineering :: Artificial
+Intelligence Classifier: Topic :: Scientific/Engineering :: Image Processing
+Classifier: Topic :: Software Development :: Libraries Requires-Python: >=3.8
+Description-Content-Type: text/markdown Provides-Extra: dev Provides-Extra:
+docs Provides-Extra: x License-File: LICENSE
        [https://github.com/kornia/data/raw/main/kornia_banner_pixie.png]
 --- English | [](README_zh-CN.md)  Website  Docs  Try_it_Now
    Tutorials  Examples  Blog  Community [![PyPI python](https://
   img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia) [!
 [PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/
    kornia) [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/
 project/kornia) [![License](https://img.shields.io/badge/License-Apache%202.0-
        blue.svg)](LICENCE) [![Slack](https://img.shields.io/badge/Slack-
      4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/
     shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA) [![Twitter](https://
  img.shields.io/twitter/follow/kornia_foss?style=social)](https://twitter.com/
 kornia_foss) [![tests-cpu](https://github.com/kornia/kornia/actions/workflows/
- tests_cpu.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/
-    tests_cpu.yml) [![tests-cuda](https://github.com/kornia/kornia/actions/
-workflows/tests_cuda.yml/badge.svg)](https://github.com/kornia/kornia/actions/
-  workflows/tests_cuda.yml) [![codecov](https://codecov.io/gh/kornia/kornia/
-branch/master/graph/badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/
-kornia) [![Documentation Status](https://readthedocs.org/projects/kornia/badge/
+   scheduled_test_cpu.yml/badge.svg?event=schedule&&branch=master)](https://
+ github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml) [![tests-
+       cpu-nightly](https://github.com/kornia/kornia/actions/workflows/
+ scheduled_test_nightly.yml/badge.svg?event=schedule&&branch=master)](https://
+   github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml) [!
+[tests-cuda](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml/
+badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml)
+   [![tests-cpu-float16](https://github.com/kornia/kornia/actions/workflows/
+scheduled_test_cpu_half.yml/badge.svg?event=schedule&&branch=master)](https://
+  github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml) [!
+      [codecov](https://codecov.io/gh/kornia/kornia/branch/master/graph/
+     badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/kornia) [!
+     [Documentation Status](https://readthedocs.org/projects/kornia/badge/
   ?version=latest)](https://kornia.readthedocs.io/en/latest/?badge=latest) [!
    [pre-commit.ci status](https://results.pre-commit.ci/badge/github/kornia/
 kornia/master.svg)](https://results.pre-commit.ci/latest/github/kornia/kornia/
   master) [Kornia_-_Computer_vision_library_for_deep_learning_|_Product_Hunt]
 *Kornia* is a differentiable computer vision library for [PyTorch](https://
 pytorch.org). It consists of a set of routines and differentiable modules to
 solve generic computer vision problems. At its core, the package uses *PyTorch*
@@ -90,28 +99,26 @@
 Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio
 Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR). - :
 white_check_mark: [Face Detection](https://kornia.readthedocs.io/en/latest/
 applications/face_detection.html) Integrated to [Huggingface Spaces](https://
 huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/
 frapochetti/blurry-faces). ## Cite If you are using kornia in your research-
 related documents, it is recommended that you cite the paper. See more in
-[CITATION](https://github.com/kornia/kornia/blob/master/CITATION.md). ```bibtex
-@inproceedings{eriba2019kornia, author = {E. Riba, D. Mishkin, D. Ponsa, E.
-Rublee and G. Bradski}, title = {Kornia: an Open Source Differentiable Computer
-Vision Library for PyTorch}, booktitle = {Winter Conference on Applications of
-Computer Vision}, year = {2020}, url = {https://arxiv.org/pdf/1910.02190.pdf} }
-``` ## Contributing We appreciate all contributions. If you are planning to
-contribute back bug-fixes, please do so without any further discussion. If you
-plan to contribute new features, utility functions or extensions, please first
-open an issue and discuss the feature with us. Please, consider reading the
-[CONTRIBUTING](https://github.com/kornia/kornia/blob/master/CONTRIBUTING.rst)
-notes. The participation in this open source project is subject to [Code of
-Conduct](https://github.com/kornia/kornia/blob/master/CODE_OF_CONDUCT.md). ##
-Community - **Forums:** discuss implementations, research, etc. [GitHub Forums]
-(https://github.com/kornia/kornia/discussions) - **GitHub Issues:** bug
-reports, feature requests, install issues, RFCs, thoughts, etc. [OPEN](https://
-github.com/kornia/kornia/issues/new/choose) - **Slack:** Join our workspace to
-keep in touch with our core contributors and be part of our community. [JOIN
-HERE](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-
-2AQRi~X9Uu6PLMuUZdvfjA) - For general information, please visit our website at
-www.kornia.org [https://contrib.rocks/image?repo=Kornia/kornia] Made with
-[contrib.rocks](https://contrib.rocks).
+[CITATION](./CITATION.md). ```bibtex @inproceedings{eriba2019kornia, author =
+{E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski}, title = {Kornia: an
+Open Source Differentiable Computer Vision Library for PyTorch}, booktitle =
+{Winter Conference on Applications of Computer Vision}, year = {2020}, url =
+{https://arxiv.org/pdf/1910.02190.pdf} } ``` ## Contributing We appreciate all
+contributions. If you are planning to contribute back bug-fixes, please do so
+without any further discussion. If you plan to contribute new features, utility
+functions or extensions, please first open an issue and discuss the feature
+with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes.
+The participation in this open source project is subject to [Code of Conduct]
+(./CODE_OF_CONDUCT.md). ## Community - **Forums:** discuss implementations,
+research, etc. [GitHub Forums](https://github.com/kornia/kornia/discussions) -
+**GitHub Issues:** bug reports, feature requests, install issues, RFCs,
+thoughts, etc. [OPEN](https://github.com/kornia/kornia/issues/new/choose) -
+**Slack:** Join our workspace to keep in touch with our core contributors and
+be part of our community. [JOIN HERE](https://join.slack.com/t/kornia/
+shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA) - For general information,
+please visit our website at www.kornia.org [https://contrib.rocks/
+image?repo=Kornia/kornia] Made with [contrib.rocks](https://contrib.rocks).
```

### Comparing `kornia-0.6.9/README.md` & `kornia-0.7.0/README.md`

 * *Files 10% similar despite different names*

```diff
@@ -19,16 +19,18 @@
 [![PyPI python](https://img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia)
 [![PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/kornia)
 [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/project/kornia)
 [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENCE)
 [![Slack](https://img.shields.io/badge/Slack-4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)
 [![Twitter](https://img.shields.io/twitter/follow/kornia_foss?style=social)](https://twitter.com/kornia_foss)
 
-[![tests-cpu](https://github.com/kornia/kornia/actions/workflows/tests_cpu.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cpu.yml)
+[![tests-cpu](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml)
+[![tests-cpu-nightly](https://github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml)
 [![tests-cuda](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml)
+[![tests-cpu-float16](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml)
 [![codecov](https://codecov.io/gh/kornia/kornia/branch/master/graph/badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/kornia)
 [![Documentation Status](https://readthedocs.org/projects/kornia/badge/?version=latest)](https://kornia.readthedocs.io/en/latest/?badge=latest)
 [![pre-commit.ci status](https://results.pre-commit.ci/badge/github/kornia/kornia/master.svg)](https://results.pre-commit.ci/latest/github/kornia/kornia/master)
 
 <a href="https://www.producthunt.com/posts/kornia?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-kornia" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=306439&theme=light" alt="Kornia - Computer vision library for deep learning | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /></a>
 
 </p>
@@ -111,28 +113,28 @@
 
 :triangular_flag_on_post: **Updates**
 - :white_check_mark: [Image Matching](https://kornia.readthedocs.io/en/latest/applications/image_matching.html) Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR).
 - :white_check_mark: [Face Detection](https://kornia.readthedocs.io/en/latest/applications/face_detection.html) Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/frapochetti/blurry-faces).
 
 ## Cite
 
-If you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](https://github.com/kornia/kornia/blob/master/CITATION.md).
+If you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](./CITATION.md).
 
   ```bibtex
   @inproceedings{eriba2019kornia,
     author    = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},
     title     = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},
     booktitle = {Winter Conference on Applications of Computer Vision},
     year      = {2020},
     url       = {https://arxiv.org/pdf/1910.02190.pdf}
   }
   ```
 
 ## Contributing
-We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](https://github.com/kornia/kornia/blob/master/CONTRIBUTING.rst) notes. The participation in this open source project is subject to [Code of Conduct](https://github.com/kornia/kornia/blob/master/CODE_OF_CONDUCT.md).
+We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes. The participation in this open source project is subject to [Code of Conduct](./CODE_OF_CONDUCT.md).
 
 
 ## Community
 - **Forums:** discuss implementations, research, etc. [GitHub Forums](https://github.com/kornia/kornia/discussions)
 - **GitHub Issues:** bug reports, feature requests, install issues, RFCs, thoughts, etc. [OPEN](https://github.com/kornia/kornia/issues/new/choose)
 - **Slack:** Join our workspace to keep in touch with our core contributors and be part of our community. [JOIN HERE](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)
 - For general information, please visit our website at www.kornia.org
```

#### html2text {}

```diff
@@ -6,20 +6,27 @@
    kornia) [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/
 project/kornia) [![License](https://img.shields.io/badge/License-Apache%202.0-
        blue.svg)](LICENCE) [![Slack](https://img.shields.io/badge/Slack-
      4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/
     shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA) [![Twitter](https://
  img.shields.io/twitter/follow/kornia_foss?style=social)](https://twitter.com/
 kornia_foss) [![tests-cpu](https://github.com/kornia/kornia/actions/workflows/
- tests_cpu.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/
-    tests_cpu.yml) [![tests-cuda](https://github.com/kornia/kornia/actions/
-workflows/tests_cuda.yml/badge.svg)](https://github.com/kornia/kornia/actions/
-  workflows/tests_cuda.yml) [![codecov](https://codecov.io/gh/kornia/kornia/
-branch/master/graph/badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/
-kornia) [![Documentation Status](https://readthedocs.org/projects/kornia/badge/
+   scheduled_test_cpu.yml/badge.svg?event=schedule&&branch=master)](https://
+ github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml) [![tests-
+       cpu-nightly](https://github.com/kornia/kornia/actions/workflows/
+ scheduled_test_nightly.yml/badge.svg?event=schedule&&branch=master)](https://
+   github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml) [!
+[tests-cuda](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml/
+badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml)
+   [![tests-cpu-float16](https://github.com/kornia/kornia/actions/workflows/
+scheduled_test_cpu_half.yml/badge.svg?event=schedule&&branch=master)](https://
+  github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml) [!
+      [codecov](https://codecov.io/gh/kornia/kornia/branch/master/graph/
+     badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/kornia) [!
+     [Documentation Status](https://readthedocs.org/projects/kornia/badge/
   ?version=latest)](https://kornia.readthedocs.io/en/latest/?badge=latest) [!
    [pre-commit.ci status](https://results.pre-commit.ci/badge/github/kornia/
 kornia/master.svg)](https://results.pre-commit.ci/latest/github/kornia/kornia/
   master) [Kornia_-_Computer_vision_library_for_deep_learning_|_Product_Hunt]
 *Kornia* is a differentiable computer vision library for [PyTorch](https://
 pytorch.org). It consists of a set of routines and differentiable modules to
 solve generic computer vision problems. At its core, the package uses *PyTorch*
@@ -71,28 +78,26 @@
 Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio
 Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR). - :
 white_check_mark: [Face Detection](https://kornia.readthedocs.io/en/latest/
 applications/face_detection.html) Integrated to [Huggingface Spaces](https://
 huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/
 frapochetti/blurry-faces). ## Cite If you are using kornia in your research-
 related documents, it is recommended that you cite the paper. See more in
-[CITATION](https://github.com/kornia/kornia/blob/master/CITATION.md). ```bibtex
-@inproceedings{eriba2019kornia, author = {E. Riba, D. Mishkin, D. Ponsa, E.
-Rublee and G. Bradski}, title = {Kornia: an Open Source Differentiable Computer
-Vision Library for PyTorch}, booktitle = {Winter Conference on Applications of
-Computer Vision}, year = {2020}, url = {https://arxiv.org/pdf/1910.02190.pdf} }
-``` ## Contributing We appreciate all contributions. If you are planning to
-contribute back bug-fixes, please do so without any further discussion. If you
-plan to contribute new features, utility functions or extensions, please first
-open an issue and discuss the feature with us. Please, consider reading the
-[CONTRIBUTING](https://github.com/kornia/kornia/blob/master/CONTRIBUTING.rst)
-notes. The participation in this open source project is subject to [Code of
-Conduct](https://github.com/kornia/kornia/blob/master/CODE_OF_CONDUCT.md). ##
-Community - **Forums:** discuss implementations, research, etc. [GitHub Forums]
-(https://github.com/kornia/kornia/discussions) - **GitHub Issues:** bug
-reports, feature requests, install issues, RFCs, thoughts, etc. [OPEN](https://
-github.com/kornia/kornia/issues/new/choose) - **Slack:** Join our workspace to
-keep in touch with our core contributors and be part of our community. [JOIN
-HERE](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-
-2AQRi~X9Uu6PLMuUZdvfjA) - For general information, please visit our website at
-www.kornia.org [https://contrib.rocks/image?repo=Kornia/kornia] Made with
-[contrib.rocks](https://contrib.rocks).
+[CITATION](./CITATION.md). ```bibtex @inproceedings{eriba2019kornia, author =
+{E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski}, title = {Kornia: an
+Open Source Differentiable Computer Vision Library for PyTorch}, booktitle =
+{Winter Conference on Applications of Computer Vision}, year = {2020}, url =
+{https://arxiv.org/pdf/1910.02190.pdf} } ``` ## Contributing We appreciate all
+contributions. If you are planning to contribute back bug-fixes, please do so
+without any further discussion. If you plan to contribute new features, utility
+functions or extensions, please first open an issue and discuss the feature
+with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes.
+The participation in this open source project is subject to [Code of Conduct]
+(./CODE_OF_CONDUCT.md). ## Community - **Forums:** discuss implementations,
+research, etc. [GitHub Forums](https://github.com/kornia/kornia/discussions) -
+**GitHub Issues:** bug reports, feature requests, install issues, RFCs,
+thoughts, etc. [OPEN](https://github.com/kornia/kornia/issues/new/choose) -
+**Slack:** Join our workspace to keep in touch with our core contributors and
+be part of our community. [JOIN HERE](https://join.slack.com/t/kornia/
+shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA) - For general information,
+please visit our website at www.kornia.org [https://contrib.rocks/
+image?repo=Kornia/kornia] Made with [contrib.rocks](https://contrib.rocks).
```

### Comparing `kornia-0.6.9/kornia/__init__.py` & `kornia-0.7.0/kornia/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -9,15 +9,8 @@
 
 # NOTE: we are going to expose to top level very few things
 from kornia.constants import pi
 from kornia.testing import xla_is_available
 from kornia.utils import eye_like, vec_like, create_meshgrid, image_to_tensor, tensor_to_image
 
 # Version variable
-import sys
-
-if sys.version_info >= (3, 8):  # pragma: >=3.8 cover
-    import importlib.metadata as importlib_metadata
-else:  # pragma: <3.8 cover
-    import importlib_metadata
-
-__version__ = importlib_metadata.version('kornia')
+__version__ = "0.7.0"
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/__init__.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -5,9 +5,11 @@
 from kornia.augmentation._2d.geometric.fisheye import RandomFisheye
 from kornia.augmentation._2d.geometric.horizontal_flip import RandomHorizontalFlip
 from kornia.augmentation._2d.geometric.pad import PadTo
 from kornia.augmentation._2d.geometric.perspective import RandomPerspective
 from kornia.augmentation._2d.geometric.resize import LongestMaxSize, Resize, SmallestMaxSize
 from kornia.augmentation._2d.geometric.resized_crop import RandomResizedCrop
 from kornia.augmentation._2d.geometric.rotation import RandomRotation
+from kornia.augmentation._2d.geometric.shear import RandomShear
 from kornia.augmentation._2d.geometric.thin_plate_spline import RandomThinPlateSpline
+from kornia.augmentation._2d.geometric.translate import RandomTranslate
 from kornia.augmentation._2d.geometric.vertical_flip import RandomVerticalFlip
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/affine.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/affine.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-import torch
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.constants import Resample, SamplePadding
-from kornia.core import Tensor
+from kornia.core import Tensor, as_tensor
 from kornia.geometry.conversions import deg2rad
 from kornia.geometry.transform import get_affine_matrix2d, warp_affine
 
 
 class RandomAffine(GeometricAugmentationBase2D):
     r"""Apply a random 2D affine transformation to a tensor image.
 
@@ -84,37 +82,39 @@
         shear: Optional[Union[Tensor, float, Tuple[float, float]]] = None,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = False,
         padding_mode: Union[str, int, SamplePadding] = SamplePadding.ZEROS.name,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self._param_generator: rg.AffineGenerator = rg.AffineGenerator(degrees, translate, scale, shear)
-        self.flags = dict(
-            resample=Resample.get(resample), padding_mode=SamplePadding.get(padding_mode), align_corners=align_corners
-        )
+        self.flags = {
+            "resample": Resample.get(resample),
+            "padding_mode": SamplePadding.get(padding_mode),
+            "align_corners": align_corners,
+        }
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return get_affine_matrix2d(
-            torch.as_tensor(params["translations"], device=input.device, dtype=input.dtype),
-            torch.as_tensor(params["center"], device=input.device, dtype=input.dtype),
-            torch.as_tensor(params["scale"], device=input.device, dtype=input.dtype),
-            torch.as_tensor(params["angle"], device=input.device, dtype=input.dtype),
-            deg2rad(torch.as_tensor(params["sx"], device=input.device, dtype=input.dtype)),
-            deg2rad(torch.as_tensor(params["sy"], device=input.device, dtype=input.dtype)),
+            as_tensor(params["translations"], device=input.device, dtype=input.dtype),
+            as_tensor(params["center"], device=input.device, dtype=input.dtype),
+            as_tensor(params["scale"], device=input.device, dtype=input.dtype),
+            as_tensor(params["angle"], device=input.device, dtype=input.dtype),
+            deg2rad(as_tensor(params["shear_x"], device=input.device, dtype=input.dtype)),
+            deg2rad(as_tensor(params["shear_y"], device=input.device, dtype=input.dtype)),
         )
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         _, _, height, width = input.shape
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
 
         return warp_affine(
             input,
             transform[:, :2, :],
             (height, width),
             flags["resample"].name.lower(),
             align_corners=flags["align_corners"],
@@ -124,13 +124,15 @@
     def inverse_transform(
         self,
         input: Tensor,
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
         return self.apply_transform(
             input,
             params=self._params,
-            transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),
+            transform=as_tensor(transform, device=input.device, dtype=input.dtype),
             flags=flags,
         )
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/center_crop.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/center_crop.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-import torch
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.constants import Resample
+from kornia.core import Tensor
 from kornia.geometry.transform import crop_by_indices, crop_by_transform_mat, get_perspective_transform
 
 
 class CenterCrop(GeometricAugmentationBase2D):
     r"""Crop a given image tensor at the center.
 
     .. image:: _static/img/CenterCrop.png
@@ -66,49 +64,49 @@
         self,
         size: Union[int, Tuple[int, int]],
         align_corners: bool = True,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         p: float = 1.0,
         keepdim: bool = False,
         cropping_mode: str = "slice",
-        return_transform: Optional[bool] = None,
     ) -> None:
         # same_on_batch is always True for CenterCrop
         # Since PyTorch does not support ragged tensor. So cropping function happens batch-wisely.
-        super().__init__(p=1.0, return_transform=return_transform, same_on_batch=True, p_batch=p, keepdim=keepdim)
+        super().__init__(p=1.0, same_on_batch=True, p_batch=p, keepdim=keepdim)
         if isinstance(size, tuple):
             self.size = (size[0], size[1])
         elif isinstance(size, int):
             self.size = (size, size)
         else:
             raise Exception(f"Invalid size type. Expected (int, tuple(int, int). " f"Got: {type(size)}.")
 
-        self.flags = dict(
-            resample=Resample.get(resample),
-            cropping_mode=cropping_mode,
-            align_corners=align_corners,
-            size=self.size,
-            padding_mode="zeros",
-        )
+        self.flags = {
+            "resample": Resample.get(resample),
+            "cropping_mode": cropping_mode,
+            "align_corners": align_corners,
+            "size": self.size,
+            "padding_mode": "zeros",
+        }
 
-    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:
+    def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         return rg.center_crop_generator(batch_shape[0], batch_shape[-2], batch_shape[-1], self.size, self.device)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         if flags["cropping_mode"] in ("resample", "slice"):
             transform: Tensor = get_perspective_transform(params["src"].to(input), params["dst"].to(input))
             transform = transform.expand(input.shape[0], -1, -1)
             return transform
         raise NotImplementedError(f"Not supported type: {flags['cropping_mode']}.")
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         if flags["cropping_mode"] == "resample":  # uses bilinear interpolation to crop
-            transform = cast(Tensor, transform)
+            if not isinstance(transform, Tensor):
+                raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
 
             return crop_by_transform_mat(
                 input, transform[:, :2, :], self.size, flags["resample"].name.lower(), "zeros", flags["align_corners"]
             )
         if flags["cropping_mode"] == "slice":  # uses advanced slicing to crop
             return crop_by_indices(input, params["src"], flags["size"])
         raise NotImplementedError(f"Not supported type: {flags['cropping_mode']}.")
@@ -122,15 +120,16 @@
     ) -> Tensor:
         if flags["cropping_mode"] != "resample":
             raise NotImplementedError(
                 f"`inverse` is only applicable for resample cropping mode. Got {flags['cropping_mode']}."
             )
         if size is None:
             size = self.size
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
         return crop_by_transform_mat(
             input,
             transform[:, :2, :],
             size,
             flags["resample"].name.lower(),
             flags["padding_mode"],
             flags["align_corners"],
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/crop.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/crop.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,17 @@
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
-from kornia.augmentation.utils import _transform_input, _transform_output_shape, override_parameters
 from kornia.constants import Resample
 from kornia.core import Tensor, pad, tensor
+from kornia.geometry.boxes import Boxes
+from kornia.geometry.keypoints import Keypoints
 from kornia.geometry.transform import crop_by_indices, crop_by_transform_mat, get_perspective_transform
 
 
 class RandomCrop(GeometricAugmentationBase2D):
     r"""Crop random patches of a tensor image on a given size.
 
     .. image:: _static/img/RandomCrop.png
@@ -80,33 +81,30 @@
         padding_mode: str = "constant",
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = True,
         p: float = 1.0,
         keepdim: bool = False,
         cropping_mode: str = "slice",
-        return_transform: Optional[bool] = None,
     ) -> None:
         # Since PyTorch does not support ragged tensor. So cropping function happens batch-wisely.
-        super().__init__(
-            p=1.0, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim
-        )
+        super().__init__(p=1.0, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim)
         self._param_generator = rg.CropGenerator(size)
-        self.flags = dict(
-            size=size,
-            padding=padding,
-            pad_if_needed=pad_if_needed,
-            fill=fill,
-            padding_mode=padding_mode,
-            resample=Resample.get(resample),
-            align_corners=align_corners,
-            cropping_mode=cropping_mode,
-        )
+        self.flags = {
+            "size": size,
+            "padding": padding,
+            "pad_if_needed": pad_if_needed,
+            "fill": fill,
+            "padding_mode": padding_mode,
+            "resample": Resample.get(resample),
+            "align_corners": align_corners,
+            "cropping_mode": cropping_mode,
+        }
 
-    def compute_padding(self, shape: torch.Size, flags: Optional[Dict[str, Any]] = None) -> List[int]:
+    def compute_padding(self, shape: Tuple[int, ...], flags: Optional[Dict[str, Any]] = None) -> List[int]:
         flags = self.flags if flags is None else flags
         if len(shape) != 4:
             raise AssertionError(f"Expected BCHW. Got {shape}.")
         padding = [0, 0, 0, 0]  # left, right, top, bottom
         if flags["padding"] is not None:
             if isinstance(flags["padding"], int):
                 padding = [flags["padding"]] * 4
@@ -138,30 +136,55 @@
     def precrop_padding(
         self, input: Tensor, padding: Optional[List[int]] = None, flags: Optional[Dict[str, Any]] = None
     ) -> Tensor:
         flags = self.flags if flags is None else flags
         if padding is None:
             padding = self.compute_padding(input.shape)
 
-        input = pad(input, padding, value=flags["fill"], mode=flags["padding_mode"])
+        if any(padding):
+            input = pad(input, padding, value=flags["fill"], mode=flags["padding_mode"])
 
         return input
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         if flags["cropping_mode"] in ("resample", "slice"):
             transform: Tensor = get_perspective_transform(params["src"].to(input), params["dst"].to(input))
             return transform
         raise NotImplementedError(f"Not supported type: {flags['cropping_mode']}.")
 
+    def apply_transform_keypoint(
+        self, input: Keypoints, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
+    ) -> Keypoints:
+        """Process keypoints corresponding to the inputs that are no transformation applied."""
+        # For pad the keypoints properly.
+        padding_size = params["padding_size"].to(device=input.device)
+        input = input.pad(padding_size)
+        return super().apply_transform_keypoint(input=input, params=params, flags=flags, transform=transform)
+
+    def apply_transform_box(
+        self, input: Boxes, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
+    ) -> Boxes:
+        """Process keypoints corresponding to the inputs that are no transformation applied."""
+        # For pad the boxes properly.
+        padding_size = params["padding_size"]
+        input = input.pad(padding_size)
+        return super().apply_transform_box(input=input, params=params, flags=flags, transform=transform)
+
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
+        padding_size: Optional[List[int]] = None
+        if "padding_size" in params and isinstance(params["padding_size"], Tensor):
+            padding_size = params["padding_size"].unique(dim=0).cpu().squeeze().tolist()
+        input = self.precrop_padding(input, padding_size, flags)
+
         flags = self.flags if flags is None else flags
         if flags["cropping_mode"] == "resample":  # uses bilinear interpolation to crop
-            transform = cast(Tensor, transform)
+            if not isinstance(transform, Tensor):
+                raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
             # Fit the arg to F.pad
             if flags['padding_mode'] == "constant":
                 padding_mode = "zeros"
             elif flags['padding_mode'] == "replicate":
                 padding_mode = "border"
             elif flags['padding_mode'] == "reflect":
                 padding_mode = "reflection"
@@ -187,16 +210,18 @@
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
         if flags["cropping_mode"] != "resample":
             raise NotImplementedError(
                 f"`inverse` is only applicable for resample cropping mode. Got {flags['cropping_mode']}."
             )
-        size = cast(Tuple[int, int], size)
-        transform = cast(Tensor, transform)
+        if size is None:
+            raise RuntimeError("`size` has to be a tuple. Got None.")
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
         # Fit the arg to F.pad
         if flags['padding_mode'] == "constant":
             padding_mode = "zeros"
         elif flags['padding_mode'] == "replicate":
             padding_mode = "border"
         elif flags['padding_mode'] == "reflect":
             padding_mode = "reflection"
@@ -207,76 +232,76 @@
             transform[:, :2, :],
             size,
             flags["resample"].name.lower(),
             padding_mode=padding_mode,
             align_corners=flags["align_corners"],
         )
 
-    def inverse(
+    def inverse_inputs(
         self,
         input: Tensor,
-        params: Optional[Dict[str, Tensor]] = None,
-        size: Optional[Tuple[int, int]] = None,
-        **kwargs,
+        params: Dict[str, Tensor],
+        flags: Dict[str, Any],
+        transform: Optional[Tensor] = None,
+        **kwargs: Any,
     ) -> Tensor:
-        out = super().inverse(input, params, size, **kwargs)
-        if params is None:
-            params = self._params
-        if "padding_size" in params:
-            padding_size = params["padding_size"].unique(dim=0).cpu().squeeze().numpy().tolist()
-            padding_size = [-padding_size[0], -padding_size[1], -padding_size[2], -padding_size[3]]
-        else:
-            padding_size = [0, 0, 0, 0]
+        if flags["cropping_mode"] != "resample":
+            raise NotImplementedError(
+                f"`inverse` is only applicable for resample cropping mode. Got {flags['cropping_mode']}."
+            )
+        out = super().inverse_inputs(input, params, flags, transform, **kwargs)
+        if not params["batch_prob"].all():
+            return out
+        padding_size = params["padding_size"].unique(dim=0).cpu().squeeze().tolist()
+        padding_size = [-padding_size[0], -padding_size[1], -padding_size[2], -padding_size[3]]
         return self.precrop_padding(out, padding_size)
 
-    def forward_parameters_precrop(self, batch_shape) -> Dict[str, Tensor]:
+    def inverse_boxes(
+        self,
+        input: Boxes,
+        params: Dict[str, Tensor],
+        flags: Dict[str, Any],
+        transform: Optional[Tensor] = None,
+        **kwargs: Any,
+    ) -> Boxes:
+        if flags["cropping_mode"] != "resample":
+            raise NotImplementedError(
+                f"`inverse` is only applicable for resample cropping mode. Got {flags['cropping_mode']}."
+            )
+        output = super().inverse_boxes(input, params, flags, transform, **kwargs)
+        if not params["batch_prob"].all():
+            return output
+
+        return output.unpad(params["padding_size"])
+
+    def inverse_keypoints(
+        self,
+        input: Keypoints,
+        params: Dict[str, Tensor],
+        flags: Dict[str, Any],
+        transform: Optional[Tensor] = None,
+        **kwargs: Any,
+    ) -> Keypoints:
+        if flags["cropping_mode"] != "resample":
+            raise NotImplementedError(
+                f"`inverse` is only applicable for resample cropping mode. Got {flags['cropping_mode']}."
+            )
+        output = super().inverse_keypoints(input, params, flags, transform, **kwargs)
+        if not params["batch_prob"].all():
+            return output
+
+        return output.unpad(params["padding_size"].to(device=input.device))
+
+    # Override parameters for precrop
+    def forward_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         input_pad = self.compute_padding(batch_shape)
-        batch_shape_new = (
-            *batch_shape[:2],
-            batch_shape[2] + input_pad[2] + input_pad[3],  # original height + top + bottom padding
-            batch_shape[3] + input_pad[0] + input_pad[1],  # original width + left + right padding
+        batch_shape_new = torch.Size(
+            (
+                *batch_shape[:2],
+                batch_shape[2] + input_pad[2] + input_pad[3],  # original height + top + bottom padding
+                batch_shape[3] + input_pad[0] + input_pad[1],  # original width + left + right padding
+            )
         )
         padding_size = tensor(tuple(input_pad), dtype=torch.long).expand(batch_shape[0], -1)
         _params = super().forward_parameters(batch_shape_new)
         _params.update({"padding_size": padding_size})
         return _params
-
-    def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs) -> Tensor:
-        padding_size = params.get("padding_size") if params else None
-        if padding_size is not None:
-            input_pad = padding_size.unique(dim=0).cpu().squeeze().numpy().tolist()
-        else:
-            input_pad = None
-
-        flags = override_parameters(self.flags, kwargs, in_place=False)
-
-        if isinstance(input, (tuple, list)):
-            ori_shape = input[0].shape
-            input_temp = _transform_input(input[0])
-            input_pad = self.compute_padding(input[0].shape, flags) if input_pad is None else input_pad
-            _input = (self.precrop_padding(input_temp, input_pad, flags), input[1])
-            _input = _transform_output_shape(_input, ori_shape) if self.keepdim else _input
-        else:
-            ori_shape = input.shape
-            input_temp = _transform_input(input)
-            input_pad = self.compute_padding(input_temp.shape, flags) if input_pad is None else input_pad
-            _input = self.precrop_padding(input_temp, input_pad, flags)
-            _input = _transform_output_shape(_input, ori_shape) if self.keepdim else _input
-        if params is not None:
-            params, flags = self._process_kwargs_to_params_and_flags(params, self.flags, **kwargs)
-        out = super().forward(_input, params, **kwargs)
-
-        # Update the actual input size for inverse
-        if "padding_size" not in self._params:
-            _padding_size = tensor(tuple(input_pad), device=input_temp.device, dtype=torch.long).expand(
-                input_temp.size(0), -1
-            )
-            self._params.update({"padding_size": _padding_size})
-
-        if not self._params["batch_prob"].all():
-            # undo the pre-crop if nothing happened.
-            if isinstance(out, tuple) and isinstance(input, tuple):
-                return input[0], out[1]
-            if isinstance(out, tuple) and not isinstance(input, tuple):
-                return input, out[1]
-            return input
-        return out
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/elastic_transform.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/elastic_transform.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,32 +1,31 @@
 from typing import Any, Dict, Optional, Tuple, Union
 
 import torch
-from torch import Tensor
 
-from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
+from kornia.augmentation._2d.base import AugmentationBase2D
 from kornia.constants import Resample
+from kornia.core import Tensor
+from kornia.geometry.boxes import Boxes
 from kornia.geometry.transform import elastic_transform2d
 
 
-class RandomElasticTransform(GeometricAugmentationBase2D):
+class RandomElasticTransform(AugmentationBase2D):
     r"""Add random elastic transformation to a tensor image.
 
     .. image:: _static/img/RandomElasticTransform.png
 
     Args:
         kernel_size: the size of the Gaussian kernel.
         sigma: The standard deviation of the Gaussian in the y and x directions,
           respectively. Larger sigma results in smaller pixel displacements.
         alpha: The scaling factor that controls the intensity of the deformation
           in the y and x directions, respectively.
         align_corners: Interpolation flag used by `grid_sample`.
         resample: Interpolation mode used by `grid_sample`. Either 'nearest' (0) or 'bilinear' (1).
-        mode: Deprecated: Interpolation mode used by `grid_sample`. Either 'bilinear' or 'nearest'.
-          Please use the `resample` argument instead.
         padding_mode: The padding used by ```grid_sample```. Either 'zeros', 'border' or 'refection'.
         same_on_batch: apply the same transformation across the batch.
         p: probability of applying the transformation.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
             to the batch form (False).
 
     .. note::
@@ -53,47 +52,64 @@
         alpha: Tuple[float, float] = (1.0, 1.0),
         align_corners: bool = False,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         padding_mode: str = "zeros",
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
 
-        self.flags = dict(
-            kernel_size=kernel_size,
-            sigma=sigma,
-            alpha=alpha,
-            align_corners=align_corners,
-            resample=Resample.get(resample),
-            padding_mode=padding_mode,
-        )
+        self.flags = {
+            "kernel_size": kernel_size,
+            "sigma": sigma,
+            "alpha": alpha,
+            "align_corners": align_corners,
+            "resample": Resample.get(resample),
+            "padding_mode": padding_mode,
+        }
 
-    def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:
+    def generate_parameters(self, shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         B, _, H, W = shape
         if self.same_on_batch:
             noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).expand(B, 2, H, W)
         else:
             noise = torch.rand(B, 2, H, W, device=self.device, dtype=self.dtype)
-        return dict(noise=noise * 2 - 1)
-
-    # TODO: It is incorrect to return identity
-    def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        return self.identity_matrix(input)
+        return {"noise": noise * 2 - 1}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return elastic_transform2d(
             input,
             params["noise"].to(input),
             flags["kernel_size"],
             flags["sigma"],
             flags["alpha"],
             flags["align_corners"],
             flags["resample"].name.lower(),
             flags["padding_mode"],
         )
+
+    def apply_non_transform_mask(
+        self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
+    ) -> Tensor:
+        return input
+
+    def apply_transform_mask(
+        self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
+    ) -> Tensor:
+        """Process masks corresponding to the inputs that are transformed."""
+        return self.apply_transform(input, params=params, flags=flags, transform=transform)
+
+    def apply_transform_box(
+        self, input: Boxes, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
+    ) -> Boxes:
+        """Process masks corresponding to the inputs that are transformed."""
+        # We assume that boxes may not be affected too much by the deformation.
+        return input
+
+    def apply_transform_class(
+        self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
+    ) -> Tensor:
+        """Process class tags corresponding to the inputs that are transformed."""
+        return input
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/fisheye.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/fisheye.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 from typing import Any, Dict, Optional
 
 from kornia.augmentation import random_generator as rg
-from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
+from kornia.augmentation._2d.base import AugmentationBase2D
 from kornia.core import Tensor
 from kornia.geometry.transform import remap
 from kornia.utils import create_meshgrid
 
 
-class RandomFisheye(GeometricAugmentationBase2D):
+class RandomFisheye(AugmentationBase2D):
     r"""Add random camera radial distortion.
 
     .. image:: _static/img/RandomFisheye.png
 
     Args:
         center_x: Ranges to sample respect to x-coordinate center with shape (2,).
         center_y: Ranges to sample respect to y-coordinate center with shape (2,).
@@ -42,19 +42,16 @@
         self,
         center_x: Tensor,
         center_y: Tensor,
         gamma: Tensor,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
         self._check_tensor(center_x)
         self._check_tensor(center_y)
         self._check_tensor(gamma)
         self._param_generator = rg.PlainUniformGenerator(
             (center_x[:, None], "center_x", None, None),
             (center_y[:, None], "center_y", None, None),
             (gamma[:, None], "gamma", None, None),
@@ -63,18 +60,14 @@
     def _check_tensor(self, data: Tensor) -> None:
         if not isinstance(data, Tensor):
             raise TypeError(f"Invalid input type. Expected Tensor - got: {type(data)}")
 
         if len(data.shape) != 1 and data.shape[0] != 2:
             raise ValueError(f"Tensor must be of shape (2,). Got: {data.shape}.")
 
-    # TODO: It is incorrect to return identity
-    def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        return self.identity_matrix(input)
-
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         # create the initial sampling fields
         B, _, H, W = input.shape
         grid = create_meshgrid(H, W, normalized_coordinates=True)
         field_x = grid[..., 0].to(input)  # 1xHxW
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/horizontal_flip.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/horizontal_flip.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Any, Dict, Optional, Tuple
 
-import torch
 from torch import Tensor
 
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
+from kornia.core import as_tensor, tensor
 from kornia.geometry.transform import hflip
 
 
 class RandomHorizontalFlip(GeometricAugmentationBase2D):
     r"""Apply a random horizontal flip to a tensor image or a batch of tensor images with a given probability.
 
     .. image:: _static/img/RandomHorizontalFlip.png
@@ -51,15 +51,15 @@
         >>> seq = RandomHorizontalFlip(p=1.0)
         >>> (seq(input) == seq(input, params=seq._params)).all()
         tensor(True)
     """
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         w: int = int(params["forward_input_shape"][-1])
-        flip_mat: Tensor = torch.tensor([[-1, 0, w - 1], [0, 1, 0], [0, 0, 1]], device=input.device, dtype=input.dtype)
+        flip_mat: Tensor = tensor([[-1, 0, w - 1], [0, 1, 0], [0, 0, 1]], device=input.device, dtype=input.dtype)
 
         return flip_mat.expand(input.shape[0], 3, 3)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return hflip(input)
@@ -67,13 +67,15 @@
     def inverse_transform(
         self,
         input: Tensor,
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
         return self.apply_transform(
             input,
             params=self._params,
-            transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),
+            transform=as_tensor(transform, device=input.device, dtype=input.dtype),
             flags=flags,
         )
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/pad.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/posterize.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,82 +1,62 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
+from typing import Any, Dict, Optional, Tuple, Union
 
-import torch
-from torch import Tensor
+from kornia.augmentation import random_generator as rg
+from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
+from kornia.enhance import posterize
 
-from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 
+class RandomPosterize(IntensityAugmentationBase2D):
+    r"""Posterize given tensor image or a batch of tensor images randomly.
 
-class PadTo(GeometricAugmentationBase2D):
-    r"""Pad the given sample to a specific size.
+    .. image:: _static/img/RandomPosterize.png
 
     Args:
-        size: a tuple of ints in the format (height, width) that give the spatial
-            dimensions to pad inputs to.
-        pad_mode: the type of padding to perform on the image (valid values
-            are those accepted by torch.nn.functional.pad)
-        pad_value: fill value for 'constant' padding applied to the image
-        p: probability of the image being flipped.
+        p: probability of applying the transformation.
+        bits: Integer that ranged from (0, 8], in which 0 gives black image and 8 gives the original.
+            If int x, bits will be generated from (x, 8) then convert to int.
+            If tuple (x, y), bits will be generated from (x, y) then convert to int.
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
 
     Shape:
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
-        This function internally uses :func:`torch.nn.functional.pad`.
+        This function internally uses :func:`kornia.enhance.posterize`.
 
     Examples:
-        >>> import torch
-        >>> img = torch.tensor([[[[0., 0., 0.],
-        ...                       [0., 0., 0.],
-        ...                       [0., 0., 0.]]]])
-        >>> pad = PadTo((4, 5), pad_value=1.)
-        >>> out = pad(img)
-        >>> out
-        tensor([[[[0., 0., 0., 1., 1.],
-                  [0., 0., 0., 1., 1.],
-                  [0., 0., 0., 1., 1.],
-                  [1., 1., 1., 1., 1.]]]])
-        >>> pad.inverse(out)
-        tensor([[[[0., 0., 0.],
-                  [0., 0., 0.],
-                  [0., 0., 0.]]]])
+        >>> rng = torch.manual_seed(0)
+        >>> input = torch.rand(1, 1, 5, 5)
+        >>> posterize = RandomPosterize(3., p=1.)
+        >>> posterize(input)
+        tensor([[[[0.4863, 0.7529, 0.0784, 0.1255, 0.2980],
+                  [0.6275, 0.4863, 0.8941, 0.4549, 0.6275],
+                  [0.3451, 0.3922, 0.0157, 0.1569, 0.2824],
+                  [0.5176, 0.6902, 0.8000, 0.1569, 0.2667],
+                  [0.6745, 0.9098, 0.3922, 0.8627, 0.4078]]]])
+
+    To apply the exact augmenation again, you may take the advantage of the previous parameter state:
+        >>> input = torch.randn(1, 3, 32, 32)
+        >>> aug = RandomPosterize(3., p=1.)
+        >>> (aug(input) == aug(input, params=aug._params)).all()
+        tensor(True)
     """
 
     def __init__(
         self,
-        size: Tuple[int, int],
-        pad_mode: str = "constant",
-        pad_value: Union[int, float] = 0,
+        bits: Union[float, Tuple[float, float], Tensor] = 3,
+        same_on_batch: bool = False,
+        p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=1.0, return_transform=return_transform, same_on_batch=True, p_batch=1.0, keepdim=keepdim)
-        self.flags = dict(size=size, pad_mode=pad_mode, pad_value=pad_value)
-
-    # TODO: It is incorrect to return identity
-    # TODO: Having a resampled version with ``warp_affine``
-    def compute_transformation(self, image: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        return self.identity_matrix(image)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        # TODO: the generator should receive the device
+        self._param_generator = rg.PosterizeGenerator(bits)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        _, _, height, width = input.shape
-        height_pad: int = flags["size"][0] - height
-        width_pad: int = flags["size"][1] - width
-        return torch.nn.functional.pad(
-            input, [0, width_pad, 0, height_pad], mode=flags["pad_mode"], value=flags["pad_value"]
-        )
-
-    def inverse_transform(
-        self,
-        input: Tensor,
-        flags: Dict[str, Any],
-        transform: Optional[Tensor] = None,
-        size: Optional[Tuple[int, int]] = None,
-    ) -> Tensor:
-        size = cast(Tuple[int, int], size)
-        return input[..., : size[0], : size[1]]
+        return posterize(input, params["bits_factor"].to(input.device))
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/perspective.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/perspective.py`

 * *Files 6% similar despite different names*

```diff
@@ -60,41 +60,42 @@
         distortion_scale: Union[Tensor, float] = 0.5,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         sampling_method: str = "basic",
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self._param_generator = rg.PerspectiveGenerator(distortion_scale, sampling_method=sampling_method)
 
-        self.flags: Dict[str, Any] = dict(align_corners=align_corners, resample=Resample.get(resample))
+        self.flags: Dict[str, Any] = {"align_corners": align_corners, "resample": Resample.get(resample)}
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return get_perspective_transform(params["start_points"].to(input), params["end_points"].to(input))
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         _, _, height, width = input.shape
         if not isinstance(transform, Tensor):
-            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
 
         return warp_perspective(
             input, transform, (height, width), mode=flags["resample"].name.lower(), align_corners=flags["align_corners"]
         )
 
     def inverse_transform(
         self,
         input: Tensor,
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
         return self.apply_transform(
             input,
             params=self._params,
             transform=as_tensor(transform, device=input.device, dtype=input.dtype),
             flags=flags,
         )
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/resize.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/resize.py`

 * *Files 10% similar despite different names*

```diff
@@ -27,28 +27,33 @@
         self,
         size: Union[int, Tuple[int, int]],
         side: str = "short",
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         align_corners: bool = True,
         antialias: bool = False,
         p: float = 1.0,
-        return_transform: Optional[bool] = None,
         keepdim: bool = False,
     ) -> None:
-        super().__init__(p=1.0, return_transform=return_transform, same_on_batch=True, p_batch=p, keepdim=keepdim)
+        super().__init__(p=1.0, same_on_batch=True, p_batch=p, keepdim=keepdim)
         self._param_generator = rg.ResizeGenerator(resize_to=size, side=side)
-        self.flags = dict(
-            size=size, side=side, resample=Resample.get(resample), align_corners=align_corners, antialias=antialias
-        )
+        self.flags = {
+            "size": size,
+            "side": side,
+            "resample": Resample.get(resample),
+            "align_corners": align_corners,
+            "antialias": antialias,
+        }
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         if params["output_size"] == input.shape[-2:]:
             return eye_like(3, input)
 
-        transform: Tensor = get_perspective_transform(params["src"], params["dst"])
+        transform: Tensor = torch.as_tensor(
+            get_perspective_transform(params["src"], params["dst"]), dtype=input.dtype, device=input.device
+        )
         transform = transform.expand(input.shape[0], -1, -1)
         return transform
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         B, C, _, _ = input.shape
@@ -78,15 +83,15 @@
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
         if not isinstance(size, tuple):
             raise TypeError(f'Expected the size be a tuple. Gotcha {type(size)}')
 
         if not isinstance(transform, Tensor):
-            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
 
         return crop_by_transform_mat(
             input, transform[:, :2, :], size, flags["resample"].name.lower(), "zeros", flags["align_corners"]
         )
 
 
 class LongestMaxSize(Resize):
@@ -98,25 +103,17 @@
 
     def __init__(
         self,
         max_size: int,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         align_corners: bool = True,
         p: float = 1.0,
-        return_transform: Optional[bool] = None,
     ) -> None:
         # TODO: Support max_size list input to randomly select from
-        super().__init__(
-            size=max_size,
-            side="long",
-            resample=resample,
-            return_transform=return_transform,
-            align_corners=align_corners,
-            p=p,
-        )
+        super().__init__(size=max_size, side="long", resample=resample, align_corners=align_corners, p=p)
 
 
 class SmallestMaxSize(Resize):
     """Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.
 
     Args:
         max_size: maximum size of the image after the transformation.
@@ -124,18 +121,10 @@
 
     def __init__(
         self,
         max_size: int,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         align_corners: bool = True,
         p: float = 1.0,
-        return_transform: Optional[bool] = None,
     ) -> None:
         # TODO: Support max_size list input to randomly select from
-        super().__init__(
-            size=max_size,
-            side="short",
-            resample=resample,
-            return_transform=return_transform,
-            align_corners=align_corners,
-            p=p,
-        )
+        super().__init__(size=max_size, side="short", resample=resample, align_corners=align_corners, p=p)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/resized_crop.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/resized_crop.py`

 * *Files 10% similar despite different names*

```diff
@@ -14,17 +14,14 @@
 
     Args:
         size: Desired output size (out_h, out_w) of each edge.
             Must be Tuple[int, int], then out_h = size[0], out_w = size[1].
         scale: range of size of the origin size cropped.
         ratio: range of aspect ratio of the origin aspect ratio cropped.
         resample: the interpolation mode.
-        return_transform: if ``True`` return the matrix describing the transformation applied to each
-                          input tensor. If ``False`` and the input is a tuple the applied transformation
-                          won't be concatenated.
         same_on_batch: apply the same transformation across the batch.
         align_corners: interpolation flag.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                         to the batch form (False).
         cropping_mode: The used algorithm to crop. ``slice`` will use advanced slicing to extract the tensor based
                        on the sampled indices. ``resample`` will use `warp_affine` using the affine transformation
                        to extract and resize at once. Use `slice` for efficiency, or `resample` for proper
@@ -69,42 +66,39 @@
         ratio: Union[Tensor, Tuple[float, float]] = (3.0 / 4.0, 4.0 / 3.0),
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = True,
         p: float = 1.0,
         keepdim: bool = False,
         cropping_mode: str = "slice",
-        return_transform: Optional[bool] = None,
     ) -> None:
         # Since PyTorch does not support ragged tensor. So cropping function happens all the time.
-        super().__init__(
-            p=1.0, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim
-        )
+        super().__init__(p=1.0, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim)
         self._param_generator = rg.ResizedCropGenerator(size, scale, ratio)
-        self.flags = dict(
-            size=size,
-            resample=Resample.get(resample),
-            align_corners=align_corners,
-            cropping_mode=cropping_mode,
-            padding_mode="zeros",
-        )
+        self.flags = {
+            "size": size,
+            "resample": Resample.get(resample),
+            "align_corners": align_corners,
+            "cropping_mode": cropping_mode,
+            "padding_mode": "zeros",
+        }
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         if flags["cropping_mode"] in ("resample", "slice"):
             transform: Tensor = get_perspective_transform(params["src"].to(input), params["dst"].to(input))
             transform = transform.expand(input.shape[0], -1, -1)
             return transform
         raise NotImplementedError(f"Not supported type: {flags['cropping_mode']}.")
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         if flags["cropping_mode"] == "resample":  # uses bilinear interpolation to crop
             if not isinstance(transform, Tensor):
-                raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+                raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
 
             return crop_by_transform_mat(
                 input,
                 transform,
                 flags["size"],
                 mode=flags["resample"].name.lower(),
                 padding_mode="zeros",
@@ -131,15 +125,15 @@
             raise NotImplementedError(
                 f"`inverse` is only applicable for resample cropping mode. Got {flags['cropping_mode']}."
             )
         if not isinstance(size, tuple):
             raise TypeError(f'Expected the size be a tuple. Gotcha {type(size)}')
 
         if not isinstance(transform, Tensor):
-            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
 
         return crop_by_transform_mat(
             input,
             transform[:, :2, :],
             size,
             flags["resample"].name.lower(),
             flags["padding_mode"],
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/rotation.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/rotation.py`

 * *Files 7% similar despite different names*

```diff
@@ -63,20 +63,19 @@
         self,
         degrees: Union[Tensor, float, Tuple[float, float], List[float]],
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = True,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self._param_generator = rg.PlainUniformGenerator((degrees, "degrees", 0.0, (-360.0, 360.0)))
 
-        self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
+        self.flags = {"resample": Resample.get(resample), "align_corners": align_corners}
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         # TODO: Update to use `get_rotation_matrix2d`
         angles: Tensor = params["degrees"].to(input)
 
         center: Tensor = _compute_tensor_center(input)
         rotation_mat: Tensor = _compute_rotation_matrix(angles, center.expand(angles.shape[0], -1))
@@ -88,24 +87,26 @@
 
         return trans_mat
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         if not isinstance(transform, Tensor):
-            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
 
         return affine(input, transform[..., :2, :3], flags["resample"].name.lower(), "zeros", flags["align_corners"])
 
     def inverse_transform(
         self,
         input: Tensor,
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
         return self.apply_transform(
             input,
             params=self._params,
             transform=as_tensor(transform, device=input.device, dtype=input.dtype),
             flags=flags,
         )
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/thin_plate_spline.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/thin_plate_spline.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,17 +1,18 @@
-from typing import Any, Dict, Optional
+from typing import Any, Dict, Optional, Tuple
 
 import torch
-from torch import Tensor
 
-from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
+from kornia.augmentation._2d.base import AugmentationBase2D
+from kornia.core import Tensor, tensor
 from kornia.geometry.transform import get_tps_transform, warp_image_tps
 
 
-class RandomThinPlateSpline(GeometricAugmentationBase2D):
+# NOTE: This NEEDS to be updated. It is out of the random generator controller.
+class RandomThinPlateSpline(AugmentationBase2D):
     r"""Add random noise to the Thin Plate Spline algorithm.
 
     .. image:: _static/img/RandomThinPlateSpline.png
 
     Args:
         scale: the scale factor to apply to the destination points.
         align_corners: Interpolation flag used by ``grid_sample``.
@@ -39,31 +40,24 @@
     def __init__(
         self,
         scale: float = 0.2,
         align_corners: bool = False,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
-        self.flags = dict(align_corners=align_corners)
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
+        self.flags = {"align_corners": align_corners}
         self.dist = torch.distributions.Uniform(-scale, scale)
 
-    def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:
+    def generate_parameters(self, shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         B, _, _, _ = shape
-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2
+        src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2
         dst = src + self.dist.rsample(src.shape)
-        return dict(src=src, dst=dst)
-
-    # TODO: It is incorrect to return identity
-    def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        return self.identity_matrix(input)
+        return {"src": src, "dst": dst}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         src = params["src"].to(input)
         dst = params["dst"].to(input)
         # NOTE: warp_image_tps need to use inverse parameters
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/geometric/vertical_flip.py` & `kornia-0.7.0/kornia/augmentation/_2d/geometric/vertical_flip.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,11 @@
 from typing import Any, Dict, Optional, Tuple
 
-import torch
-from torch import Tensor
-
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
+from kornia.core import Tensor, as_tensor, tensor
 from kornia.geometry.transform import vflip
 
 
 class RandomVerticalFlip(GeometricAugmentationBase2D):
     r"""Apply a random vertical flip to a tensor image or a batch of tensor images with a given probability.
 
     .. image:: _static/img/RandomVerticalFlip.png
@@ -22,14 +20,15 @@
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
         This function internally uses :func:`kornia.geometry.transform.vflip`.
 
     Examples:
+        >>> import torch
         >>> input = torch.tensor([[[[0., 0., 0.],
         ...                         [0., 0., 0.],
         ...                         [0., 1., 1.]]]])
         >>> seq = RandomVerticalFlip(p=1.0)
         >>> seq(input), seq.transform_matrix
         (tensor([[[[0., 1., 1.],
                   [0., 0., 0.],
@@ -44,15 +43,15 @@
         >>> seq = RandomVerticalFlip(p=1.0)
         >>> (seq(input) == seq(input, params=seq._params)).all()
         tensor(True)
     """
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         h: int = int(params["forward_input_shape"][-2])
-        flip_mat: Tensor = torch.tensor([[1, 0, 0], [0, -1, h - 1], [0, 0, 1]], device=input.device, dtype=input.dtype)
+        flip_mat: Tensor = tensor([[1, 0, 0], [0, -1, h - 1], [0, 0, 1]], device=input.device, dtype=input.dtype)
 
         return flip_mat.expand(input.shape[0], 3, 3)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return vflip(input)
@@ -60,13 +59,15 @@
     def inverse_transform(
         self,
         input: Tensor,
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the `transform` be a Tensor. Got {type(transform)}.')
         return self.apply_transform(
             input,
             params=self._params,
-            transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),
+            transform=as_tensor(transform, device=input.device, dtype=input.dtype),
             flags=flags,
         )
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/__init__.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+from kornia.augmentation._2d.intensity.auto_contrast import RandomAutoContrast
 from kornia.augmentation._2d.intensity.box_blur import RandomBoxBlur
 from kornia.augmentation._2d.intensity.brightness import RandomBrightness
 from kornia.augmentation._2d.intensity.channel_shuffle import RandomChannelShuffle
 from kornia.augmentation._2d.intensity.color_jiggle import ColorJiggle
 from kornia.augmentation._2d.intensity.color_jitter import ColorJitter
 from kornia.augmentation._2d.intensity.contrast import RandomContrast
 from kornia.augmentation._2d.intensity.denormalize import Denormalize
@@ -9,16 +10,19 @@
 from kornia.augmentation._2d.intensity.erasing import RandomErasing
 from kornia.augmentation._2d.intensity.gamma import RandomGamma
 from kornia.augmentation._2d.intensity.gaussian_blur import RandomGaussianBlur
 from kornia.augmentation._2d.intensity.gaussian_noise import RandomGaussianNoise
 from kornia.augmentation._2d.intensity.grayscale import RandomGrayscale
 from kornia.augmentation._2d.intensity.hue import RandomHue
 from kornia.augmentation._2d.intensity.invert import RandomInvert
+from kornia.augmentation._2d.intensity.median_blur import RandomMedianBlur
 from kornia.augmentation._2d.intensity.motion_blur import RandomMotionBlur
 from kornia.augmentation._2d.intensity.normalize import Normalize
 from kornia.augmentation._2d.intensity.planckian_jitter import RandomPlanckianJitter
 from kornia.augmentation._2d.intensity.plasma import RandomPlasmaBrightness, RandomPlasmaContrast, RandomPlasmaShadow
 from kornia.augmentation._2d.intensity.posterize import RandomPosterize
+from kornia.augmentation._2d.intensity.random_rain import RandomRain
 from kornia.augmentation._2d.intensity.random_rgb_shift import RandomRGBShift
+from kornia.augmentation._2d.intensity.random_snow import RandomSnow
 from kornia.augmentation._2d.intensity.saturation import RandomSaturation
 from kornia.augmentation._2d.intensity.sharpness import RandomSharpness
 from kornia.augmentation._2d.intensity.solarize import RandomSolarize
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/box_blur.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/box_blur.py`

 * *Files 7% similar despite different names*

```diff
@@ -40,21 +40,15 @@
         self,
         kernel_size: Tuple[int, int] = (3, 3),
         border_type: str = "reflect",
         normalized: bool = True,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
-        self.flags = dict(kernel_size=kernel_size, border_type=border_type, normalized=normalized)
-
-    def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        return self.identity_matrix(input)
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
+        self.flags = {"kernel_size": kernel_size, "border_type": border_type, "normalized": normalized}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return box_blur(input, flags["kernel_size"], flags["border_type"], flags["normalized"])
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/brightness.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/gamma.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,75 +1,72 @@
 from typing import Any, Dict, Optional, Tuple
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
-from kornia.augmentation.utils import _range_bound
 from kornia.core import Tensor
-from kornia.enhance.adjust import adjust_brightness
+from kornia.enhance.adjust import adjust_gamma
 
 
-class RandomBrightness(IntensityAugmentationBase2D):
-    r"""Apply a random transformation to the brightness of a tensor image.
+class RandomGamma(IntensityAugmentationBase2D):
+    r"""Apply a random transformation to the gamma of a tensor image.
 
     This implementation aligns PIL. Hence, the output is close to TorchVision.
 
-    .. image:: _static/img/RandomBrighness.png
+    .. image:: _static/img/RandomGamma.png
 
     Args:
         p: probability of applying the transformation.
-        brightness: the brightness factor to apply
-        clip_output: if true clip output
-        silence_instantiation_warning: if True, silence the warning at instantiation.
+        gamma: the gamma factor to apply.
+        gain: the gain factor to apply.
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
     Shape:
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
-        This function internally uses :func:`kornia.enhance.adjust_brightness`
+        This function internally uses :func:`kornia.enhance.adjust_gamma`
 
-        Examples:
+    Examples:
         >>> rng = torch.manual_seed(0)
         >>> inputs = torch.rand(1, 3, 3, 3)
-        >>> aug = RandomBrightness(brightness = (0.5,2.),p=1.)
+        >>> aug = RandomGamma((0.5,2.),(1.5,1.5),p=1.)
         >>> aug(inputs)
-        tensor([[[[0.0505, 0.3225, 0.0000],
-                  [0.0000, 0.0000, 0.1883],
-                  [0.0443, 0.4507, 0.0099]],
+        tensor([[[[1.0000, 1.0000, 0.3912],
+                  [0.4883, 0.7801, 1.0000],
+                  [1.0000, 1.0000, 0.9702]],
         <BLANKLINE>
-                 [[0.1866, 0.0000, 0.0000],
-                  [0.0000, 0.0000, 0.0000],
-                  [0.0728, 0.2519, 0.3543]],
+                 [[1.0000, 0.8368, 0.9048],
+                  [0.1824, 0.5597, 0.7609],
+                  [1.0000, 1.0000, 1.0000]],
         <BLANKLINE>
-                 [[0.0000, 0.0000, 0.2359],
-                  [0.4694, 0.0000, 0.4284],
-                  [0.0000, 0.1072, 0.5070]]]])
+                 [[0.5452, 0.7441, 1.0000],
+                  [1.0000, 0.8990, 1.0000],
+                  [0.9267, 1.0000, 1.0000]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.rand(1, 3, 32, 32)
-        >>> aug = RandomBrightness((0.8,1.2), p=1.)
+        >>> aug = RandomGamma((0.8,1.2), p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
         self,
-        brightness: Tuple[float, float] = (1.0, 1.0),
-        clip_output: bool = True,
+        gamma: Tuple[float, float] = (1.0, 1.0),
+        gain: Tuple[float, float] = (1.0, 1.0),
         same_on_batch: bool = False,
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self.brightness: Tensor = _range_bound(brightness, 'brightness', center=1.0, bounds=(0.0, 2.0))
-        self._param_generator = rg.PlainUniformGenerator((self.brightness, "brightness_factor", None, None))
-
-        self.clip_output = clip_output
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        self._param_generator = rg.PlainUniformGenerator(
+            (gamma, "gamma_factor", None, None), (gain, "gain_factor", None, None)
+        )
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        brightness_factor = params["brightness_factor"].to(input)
-        return adjust_brightness(input, brightness_factor - 1, self.clip_output)
+        gamma_factor = params["gamma_factor"].to(input)
+        gain_factor = params["gain_factor"].to(input)
+        return adjust_gamma(input, gamma_factor, gain_factor)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/channel_shuffle.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/channel_shuffle.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-from typing import Any, Dict, Optional
+from typing import Any, Dict, Optional, Tuple
 
 import torch
-from torch import Tensor
 
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
 
 
 class RandomChannelShuffle(IntensityAugmentationBase2D):
     r"""Shuffle the channels of a batch of multi-dimensional images.
 
     .. image:: _static/img/RandomChannelShuffle.png
 
@@ -30,29 +30,21 @@
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.randn(1, 3, 32, 32)
         >>> aug = RandomChannelShuffle(p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
-    def __init__(
-        self,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
-    ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
+    def __init__(self, same_on_batch: bool = False, p: float = 0.5, keepdim: bool = False) -> None:
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
 
-    def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:
+    def generate_parameters(self, shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         B, C, _, _ = shape
         channels = torch.rand(B, C).argsort(dim=1)
-        return dict(channels=channels)
+        return {"channels": channels}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         out = torch.empty_like(input)
         for i in range(out.shape[0]):
             out[i] = input[i, params["channels"][i]]
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jiggle.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/color_jiggle.py`

 * *Files 4% similar despite different names*

```diff
@@ -59,27 +59,25 @@
         brightness: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         contrast: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         saturation: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         hue: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         same_on_batch: bool = False,
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self.brightness = brightness
         self.contrast = contrast
         self.saturation = saturation
         self.hue = hue
         self._param_generator = rg.ColorJiggleGenerator(brightness, contrast, saturation, hue)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-
         transforms = [
             lambda img: adjust_brightness(img, params["brightness_factor"] - 1),
             lambda img: adjust_contrast(img, params["contrast_factor"]),
             lambda img: adjust_saturation(img, params["saturation_factor"]),
             lambda img: adjust_hue(img, params["hue_factor"] * 2 * pi),
         ]
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jitter.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/color_jitter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-import warnings
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.constants import pi
 from kornia.core import Tensor
 from kornia.enhance import (
@@ -56,14 +55,15 @@
                   [0.9993, 0.9993, 0.9993]],
         <BLANKLINE>
                  [[0.9993, 0.9993, 0.9993],
                   [0.9993, 0.9993, 0.9993],
                   [0.9993, 0.9993, 0.9993]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
+
         >>> input = torch.randn(1, 3, 32, 32)
         >>> aug = ColorJitter(0.1, 0.1, 0.1, 0.1, p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
@@ -71,36 +71,26 @@
         brightness: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         contrast: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         saturation: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         hue: Union[Tensor, float, Tuple[float, float], List[float]] = 0.0,
         same_on_batch: bool = False,
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
-        silence_instantiation_warning: bool = False,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-
-        if not silence_instantiation_warning:
-            warnings.warn(
-                "`ColorJitter` is now following Torchvision implementation. Old "
-                "behavior can be retrieved by instantiating `ColorJiggle`.",
-                category=DeprecationWarning,
-            )
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
 
         self.brightness = brightness
         self.contrast = contrast
         self.saturation = saturation
         self.hue = hue
         self._param_generator = rg.ColorJitterGenerator(brightness, contrast, saturation, hue)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-
         transforms = [
             lambda img: adjust_brightness_accumulative(img, params["brightness_factor"]),
             lambda img: adjust_contrast_with_mean_subtraction(img, params["contrast_factor"]),
             lambda img: adjust_saturation_with_gray_subtraction(img, params["saturation_factor"]),
             lambda img: adjust_hue(img, params["hue_factor"] * 2 * pi),
         ]
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/contrast.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/contrast.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,61 +12,60 @@
 
     This implementation aligns PIL. Hence, the output is close to TorchVision.
 
     .. image:: _static/img/RandomContrast.png
 
     Args:
         p: probability of applying the transformation.
-        contrast: the contrast factor to apply
-        clip_output: if true clip output
-        silence_instantiation_warning: if True, silence the warning at instantiation.
+        contrast: the contrast factor to apply.
+        clip_output: if true clip output.
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
     Shape:
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
-        This function internally uses :func:`kornia.enhance.adjust_contrast
+        This function internally uses :func:`kornia.enhance.adjust_contrast`
 
-        Examples:
+    Examples:
         >>> rng = torch.manual_seed(0)
         >>> inputs = torch.rand(1, 3, 3, 3)
-        >>> aug = RandomContrast(contrast = (0.5,2.),p=1.)
+        >>> aug = RandomContrast(contrast = (0.5, 2.), p = 1.)
         >>> aug(inputs)
         tensor([[[[0.2750, 0.4258, 0.0490],
                   [0.0732, 0.1704, 0.3514],
                   [0.2716, 0.4969, 0.2525]],
         <BLANKLINE>
                  [[0.3505, 0.1934, 0.2227],
                   [0.0124, 0.0936, 0.1629],
                   [0.2874, 0.3867, 0.4434]],
         <BLANKLINE>
                  [[0.0893, 0.1564, 0.3778],
                   [0.5072, 0.2201, 0.4845],
                   [0.2325, 0.3064, 0.5281]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
+
         >>> input = torch.rand(1, 3, 32, 32)
         >>> aug = RandomContrast((0.8,1.2), p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
         self,
         contrast: Tuple[float, float] = (1.0, 1.0),
         clip_output: bool = True,
         same_on_batch: bool = False,
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self.contrast: Tensor = _range_bound(contrast, 'contrast', center=1.0)
         self._param_generator = rg.PlainUniformGenerator((self.contrast, "contrast_factor", None, None))
 
         self.clip_output = clip_output
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/denormalize.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/denormalize.py`

 * *Files 8% similar despite different names*

```diff
@@ -40,28 +40,27 @@
 
     def __init__(
         self,
         mean: Union[Tensor, Tuple[float], List[float], float],
         std: Union[Tensor, Tuple[float], List[float], float],
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=True, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=True, keepdim=keepdim)
         if isinstance(mean, float):
             mean = torch.tensor([mean])
 
         if isinstance(std, float):
             std = torch.tensor([std])
 
         if isinstance(mean, (tuple, list)):
             mean = torch.tensor(mean)
 
         if isinstance(std, (tuple, list)):
             std = torch.tensor(std)
 
-        self.flags = dict(mean=mean, std=std)
+        self.flags = {"mean": mean, "std": std}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return denormalize(input, flags["mean"], flags["std"])
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/equalize.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/equalize.py`

 * *Files 12% similar despite different names*

```diff
@@ -38,20 +38,14 @@
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.rand(1, 3, 32, 32)
         >>> aug = RandomEqualize(p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
-    def __init__(
-        self,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
-    ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+    def __init__(self, same_on_batch: bool = False, p: float = 0.5, keepdim: bool = False) -> None:
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return equalize(input)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/erasing.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/vertical_flip.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,82 +1,73 @@
-from typing import Any, Dict, Optional, Tuple, Union
+from typing import Any, Dict, Optional
 
-from kornia.augmentation import random_generator as rg
-from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
-from kornia.core import Tensor, where
-from kornia.geometry.bbox import bbox_generator, bbox_to_mask
+import torch
+from torch import Tensor
 
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 
-class RandomErasing(IntensityAugmentationBase2D):
-    r"""Erase a random rectangle of a tensor image according to a probability p value.
 
-    .. image:: _static/img/RandomErasing.png
+class RandomVerticalFlip3D(GeometricAugmentationBase3D):
+    r"""Apply random vertical flip to 3D volumes (5D tensor).
 
-    The operator removes image parts and fills them with zero values at a selected rectangle
-    for each of the images in the batch.
-
-    The rectangle will have an area equal to the original image area multiplied by a value uniformly
-    sampled between the range [scale[0], scale[1]) and an aspect ratio sampled
-    between [ratio[0], ratio[1])
+    Input should be a tensor of shape :math:`(C, D, H, W)` or a batch of tensors :math:`(*, C, D, H, W)`.
+    If Input is a tuple it is assumed that the first element contains the aforementioned tensors and the second,
+    the corresponding transformation matrix that has been applied to them. In this case the module
+    will Vertically flip the tensors and concatenate the corresponding transformation matrix to the
+    previous one. This is especially useful when using this functionality as part of an ``nn.Sequential`` module.
 
     Args:
-        scale: range of proportion of erased area against input image.
-        ratio: range of aspect ratio of erased area.
-        value: the value to fill the erased area.
+        p: probability of the image being flipped.
         same_on_batch: apply the same transformation across the batch.
-        p: probability that the random erasing operation will be performed.
-        keepdim: whether to keep the output shape the same as input (True) or broadcast it
-                        to the batch form (False).
+        keepdim: whether to keep the output shape the same as input ``True`` or broadcast it
+          to the batch form ``False``.
 
     Shape:
-        - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
-        - Output: :math:`(B, C, H, W)`
+        - Input: :math:`(C, D, H, W)` or :math:`(B, C, D, H, W)`, Optional: :math:`(B, 4, 4)`
+        - Output: :math:`(B, C, D, H, W)`
 
     Note:
         Input tensor must be float and normalized into [0, 1] for the best differentiability support.
-        Additionally, this function accepts another transformation tensor (:math:`(B, 3, 3)`), then the
+        Additionally, this function accepts another transformation tensor (:math:`(B, 4, 4)`), then the
         applied transformation will be merged int to the input transformation tensor and returned.
 
     Examples:
-        >>> rng = torch.manual_seed(0)
-        >>> inputs = torch.ones(1, 1, 3, 3)
-        >>> aug = RandomErasing((.4, .8), (.3, 1/.3), p=0.5)
-        >>> aug(inputs)
-        tensor([[[[1., 0., 0.],
-                  [1., 0., 0.],
-                  [1., 0., 0.]]]])
+        >>> import torch
+        >>> x = torch.eye(3).repeat(3, 1, 1)
+        >>> seq = RandomVerticalFlip3D(p=1.0)
+        >>> seq(x), seq.transform_matrix
+        (tensor([[[[[0., 0., 1.],
+                   [0., 1., 0.],
+                   [1., 0., 0.]],
+        <BLANKLINE>
+                  [[0., 0., 1.],
+                   [0., 1., 0.],
+                   [1., 0., 0.]],
+        <BLANKLINE>
+                  [[0., 0., 1.],
+                   [0., 1., 0.],
+                   [1., 0., 0.]]]]]), tensor([[[ 1.,  0.,  0.,  0.],
+                 [ 0., -1.,  0.,  2.],
+                 [ 0.,  0.,  1.,  0.],
+                 [ 0.,  0.,  0.,  1.]]]))
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
-        >>> input = torch.randn(1, 3, 32, 32)
-        >>> aug = RandomErasing((.4, .8), (.3, 1/.3), p=1.)
+        >>> input = torch.rand(1, 3, 32, 32, 32)
+        >>> aug = RandomVerticalFlip3D(p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
-    # Note: Extra params, inplace=False in Torchvision.
-    def __init__(
-        self,
-        scale: Union[Tensor, Tuple[float, float]] = (0.02, 0.33),
-        ratio: Union[Tensor, Tuple[float, float]] = (0.3, 3.3),
-        value: float = 0.0,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
-    ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self.scale = scale
-        self.ratio = ratio
-        self.value: float = float(value)
-        self._param_generator = rg.RectangleEraseGenerator(scale, ratio, float(value))
+    def __init__(self, same_on_batch: bool = False, p: float = 0.5, keepdim: bool = False) -> None:
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+
+    def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
+        h: int = input.shape[-2]
+        flip_mat: Tensor = torch.tensor(
+            [[1, 0, 0, 0], [0, -1, 0, h - 1], [0, 0, 1, 0], [0, 0, 0, 1]], device=input.device, dtype=input.dtype
+        )
+        return flip_mat.expand(input.shape[0], 4, 4)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        _, c, h, w = input.size()
-        values = params["values"].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, *input.shape[1:]).to(input)
-
-        bboxes = bbox_generator(params["xs"], params["ys"], params["widths"], params["heights"])
-        mask = bbox_to_mask(bboxes, w, h)  # Returns B, H, W
-        mask = mask.unsqueeze(1).repeat(1, c, 1, 1).to(input)  # Transform to B, c, H, W
-        transformed = where(mask == 1.0, values, input)
-        return transformed
+        return torch.flip(input, [-2])
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/gamma.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/hue.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,74 +1,67 @@
 from typing import Any, Dict, Optional, Tuple
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.augmentation.utils import _range_bound
+from kornia.constants import pi
 from kornia.core import Tensor
-from kornia.enhance.adjust import adjust_gamma
+from kornia.enhance.adjust import adjust_hue
 
 
-class RandomGamma(IntensityAugmentationBase2D):
-    r"""Apply a random transformation to the gamma of a tensor image.
+class RandomHue(IntensityAugmentationBase2D):
+    r"""Apply a random transformation to the hue of a tensor image.
 
     This implementation aligns PIL. Hence, the output is close to TorchVision.
 
-    .. image:: _static/img/RandomGamma.png
+    .. image:: _static/img/RandomHue.png
 
     Args:
         p: probability of applying the transformation.
-        gamma: the gamma factor to apply
-        gain: the gain factor to apply
-        silence_instantiation_warning: if True, silence the warning at instantiation.
+        hue: the saturation factor to apply.
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
     Shape:
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
-        This function internally uses :func:`kornia.enhance.adjust_gamma`
+        This function internally uses :func:`kornia.enhance.adjust_hue`
 
     Examples:
         >>> rng = torch.manual_seed(0)
         >>> inputs = torch.rand(1, 3, 3, 3)
-        >>> aug = RandomGamma((0.5,2.),(1.5,1.5),p=1.)
+        >>> aug = RandomHue(hue = (-0.5,0.5),p=1.)
         >>> aug(inputs)
-        tensor([[[[1.0000, 1.0000, 0.3912],
-                  [0.4883, 0.7801, 1.0000],
-                  [1.0000, 1.0000, 0.9702]],
+        tensor([[[[0.3993, 0.2823, 0.6816],
+                  [0.6117, 0.2090, 0.4081],
+                  [0.4693, 0.5529, 0.9527]],
         <BLANKLINE>
-                 [[1.0000, 0.8368, 0.9048],
-                  [0.1824, 0.5597, 0.7609],
-                  [1.0000, 1.0000, 1.0000]],
+                 [[0.1610, 0.5962, 0.4971],
+                  [0.9152, 0.3971, 0.8742],
+                  [0.4194, 0.6771, 0.7162]],
         <BLANKLINE>
-                 [[0.5452, 0.7441, 1.0000],
-                  [1.0000, 0.8990, 1.0000],
-                  [0.9267, 1.0000, 1.0000]]]])
+                 [[0.6323, 0.7682, 0.0885],
+                  [0.0223, 0.1689, 0.2939],
+                  [0.5185, 0.8964, 0.4556]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
+
         >>> input = torch.rand(1, 3, 32, 32)
-        >>> aug = RandomGamma((0.8,1.2), p=1.)
+        >>> aug = RandomHue((-0.2,0.2), p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
-        self,
-        gamma: Tuple[float, float] = (1.0, 1.0),
-        gain: Tuple[float, float] = (1.0, 1.0),
-        same_on_batch: bool = False,
-        p: float = 1.0,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
+        self, hue: Tuple[float, float] = (0.0, 0.0), same_on_batch: bool = False, p: float = 1.0, keepdim: bool = False
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = rg.PlainUniformGenerator(
-            (gamma, "gamma_factor", None, None), (gain, "gain_factor", None, None)
-        )
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        self.hue: Tensor = _range_bound(hue, 'hue', bounds=(-0.5, 0.5))
+        self._param_generator = rg.PlainUniformGenerator((self.hue, "hue_factor", None, None))
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        gamma_factor = params["gamma_factor"].to(input)
-        gain_factor = params["gain_factor"].to(input)
-        return adjust_gamma(input, gamma_factor, gain_factor)
+        hue_factor = params["hue_factor"].to(input)
+        return adjust_hue(input, hue_factor * 2 * pi)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_blur.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/gaussian_blur.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,68 +1,88 @@
-from typing import Any, Dict, Optional, Tuple
+import warnings
+from typing import Any, Dict, Optional, Tuple, Union
 
 from torch import Tensor
 
+from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.constants import BorderType
 from kornia.filters import gaussian_blur2d
 
 
 class RandomGaussianBlur(IntensityAugmentationBase2D):
-    r"""Apply gaussian blur given tensor image or a batch of tensor images randomly.
+    r"""Apply gaussian blur given tensor image or a batch of tensor images randomly. The standard deviation is
+    sampled for each instance.
 
     .. image:: _static/img/RandomGaussianBlur.png
 
     Args:
         kernel_size: the size of the kernel.
-        sigma: the standard deviation of the kernel.
+        sigma: the range for the standard deviation of the kernel.
         border_type: the padding mode to be applied before convolving.
           The expected modes are: ``constant``, ``reflect``, ``replicate`` or ``circular``.
+        separable: run as composition of two 1d-convolutions.
         same_on_batch: apply the same transformation across the batch.
         p: probability of applying the transformation.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
+        silence_instantiation_warning: if True, silence the warning at instantiation.
 
     Shape:
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
         This function internally uses :func:`kornia.filters.gaussian_blur2d`.
 
     Examples:
         >>> rng = torch.manual_seed(0)
         >>> input = torch.rand(1, 1, 5, 5)
         >>> blur = RandomGaussianBlur((3, 3), (0.1, 2.0), p=1.)
         >>> blur(input)
-        tensor([[[[0.6699, 0.4645, 0.3193, 0.1741, 0.1955],
-                  [0.5422, 0.6657, 0.6261, 0.6527, 0.5195],
-                  [0.3826, 0.2638, 0.1902, 0.1620, 0.2141],
-                  [0.6329, 0.6732, 0.5634, 0.4037, 0.2049],
-                  [0.8307, 0.6753, 0.7147, 0.5768, 0.7097]]]])
+        tensor([[[[0.5941, 0.5833, 0.5022, 0.4384, 0.3934],
+                  [0.5310, 0.4964, 0.4113, 0.3637, 0.3472],
+                  [0.4991, 0.4997, 0.4312, 0.3620, 0.3081],
+                  [0.6082, 0.5667, 0.4954, 0.3825, 0.3508],
+                  [0.7042, 0.6849, 0.6275, 0.4753, 0.4105]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.randn(1, 3, 32, 32)
         >>> aug = RandomGaussianBlur((3, 3), (0.1, 2.0), p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
         self,
-        kernel_size: Tuple[int, int],
-        sigma: Tuple[float, float],
+        kernel_size: Union[Tuple[int, int], int],
+        sigma: Union[Tuple[float, float], Tensor],
         border_type: str = "reflect",
+        separable: bool = True,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
+        silence_instantiation_warning: bool = False,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
-        self.flags = dict(kernel_size=kernel_size, sigma=sigma, border_type=BorderType.get(border_type))
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
+
+        if not silence_instantiation_warning:
+            warnings.warn(
+                "`RandomGaussianBlur` has changed its behavior and now randomly sample sigma for both axes. "
+                "To retrieve old behavior please consider using kornia.filters.GaussianBlur2d",
+                category=DeprecationWarning,
+            )
+
+        self.flags = {"kernel_size": kernel_size, "separable": separable, "border_type": BorderType.get(border_type)}
+        self._param_generator = rg.RandomGaussianBlurGenerator(sigma)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        return gaussian_blur2d(input, flags["kernel_size"], flags["sigma"], flags["border_type"].name.lower())
+        sigma = params["sigma"].to(device=input.device, dtype=input.dtype).unsqueeze(-1).expand(-1, 2)
+        return gaussian_blur2d(
+            input,
+            self.flags["kernel_size"],
+            sigma,
+            self.flags["border_type"].name.lower(),
+            separable=self.flags["separable"],
+        )
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_noise.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/gaussian_noise.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-from typing import Any, Dict, Optional
+from typing import Any, Dict, Optional, Tuple
 
 import torch
-from torch import Tensor
 
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
 
 
 def _randn_like(input: Tensor, mean: float, std: float) -> Tensor:
     x = torch.randn_like(input)  # Generating on GPU is fastest with `torch.randn_like(...)`
     if std != 1.0:  # `if` is cheaper than multiplication
         x *= std
     if mean != 0.0:  # `if` is cheaper than addition
@@ -38,28 +38,20 @@
         >>> input = torch.randn(1, 3, 32, 32)
         >>> aug = RandomGaussianNoise(mean=0., std=1., p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
-        self,
-        mean: float = 0.0,
-        std: float = 1.0,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
+        self, mean: float = 0.0, std: float = 1.0, same_on_batch: bool = False, p: float = 0.5, keepdim: bool = False
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
-        self.flags = dict(mean=mean, std=std)
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
+        self.flags = {"mean": mean, "std": std}
 
-    def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:
+    def generate_parameters(self, shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         return {}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         if "gaussian_noise" in params:
             gaussian_noise = params["gaussian_noise"]
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/grayscale.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/grayscale.py`

 * *Files 8% similar despite different names*

```diff
@@ -48,22 +48,19 @@
         >>> input = torch.randn(1, 3, 32, 32)
         >>> aug = RandomGrayscale(p=1.0)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
-        self,
-        same_on_batch: bool = False,
-        p: float = 0.1,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
+        self, rgb_weights: Optional[Tensor] = None, same_on_batch: bool = False, p: float = 0.1, keepdim: bool = False
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        self.rgb_weights = rgb_weights
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         # Make sure it returns (*, 3, H, W)
         grayscale = torch.ones_like(input)
-        grayscale[:] = rgb_to_grayscale(input)
+        grayscale[:] = rgb_to_grayscale(input, rgb_weights=self.rgb_weights)
         return grayscale
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/hue.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/saturation.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,72 +1,70 @@
 from typing import Any, Dict, Optional, Tuple
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.augmentation.utils import _range_bound
-from kornia.constants import pi
 from kornia.core import Tensor
-from kornia.enhance.adjust import adjust_hue
+from kornia.enhance.adjust import adjust_saturation
 
 
-class RandomHue(IntensityAugmentationBase2D):
-    r"""Apply a random transformation to the hue of a tensor image.
+class RandomSaturation(IntensityAugmentationBase2D):
+    r"""Apply a random transformation to the saturation of a tensor image.
 
     This implementation aligns PIL. Hence, the output is close to TorchVision.
 
-    .. image:: _static/img/RandomHue.png
+    .. image:: _static/img/RandomSaturation.png
 
     Args:
         p: probability of applying the transformation.
-        hue: the saturation factor to apply
-        silence_instantiation_warning: if True, silence the warning at instantiation.
+        saturation: the saturation factor to apply.
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
     Shape:
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
-        This function internally uses :func:`kornia.enhance.adjust_hue
+        This function internally uses :func:`kornia.enhance.adjust_saturation`
 
-        Examples:
+    Examples:
         >>> rng = torch.manual_seed(0)
         >>> inputs = torch.rand(1, 3, 3, 3)
-        >>> aug = RandomHue(hue = (-0.5,0.5),p=1.)
+        >>> aug = RandomSaturation(saturation = (0.5,2.),p=1.)
         >>> aug(inputs)
-        tensor([[[[0.3993, 0.2823, 0.6816],
-                  [0.6117, 0.2090, 0.4081],
-                  [0.4693, 0.5529, 0.9527]],
+        tensor([[[[0.5569, 0.7682, 0.3529],
+                  [0.4811, 0.3474, 0.7411],
+                  [0.5028, 0.8964, 0.6772]],
         <BLANKLINE>
-                 [[0.1610, 0.5962, 0.4971],
-                  [0.9152, 0.3971, 0.8742],
-                  [0.4194, 0.6771, 0.7162]],
+                 [[0.6323, 0.5358, 0.5265],
+                  [0.4203, 0.2706, 0.5525],
+                  [0.5185, 0.7863, 0.8681]],
         <BLANKLINE>
-                 [[0.6323, 0.7682, 0.0885],
-                  [0.0223, 0.1689, 0.2939],
-                  [0.5185, 0.8964, 0.4556]]]])
+                 [[0.3711, 0.4989, 0.6816],
+                  [0.9152, 0.3971, 0.8742],
+                  [0.4636, 0.7060, 0.9527]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
+
         >>> input = torch.rand(1, 3, 32, 32)
-        >>> aug = RandomHue((-0.2,0.2), p=1.)
+        >>> aug = RandomSaturation((0.8,1.2), p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
         self,
-        hue: Tuple[float, float] = (0.0, 0.0),
+        saturation: Tuple[float, float] = (1.0, 1.0),
         same_on_batch: bool = False,
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self.hue: Tensor = _range_bound(hue, 'hue', bounds=(-0.5, 0.5))
-        self._param_generator = rg.PlainUniformGenerator((self.hue, "hue_factor", None, None))
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        self.saturation: Tensor = _range_bound(saturation, 'saturation', center=1.0)
+        self._param_generator = rg.PlainUniformGenerator((self.saturation, "saturation_factor", None, None))
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        hue_factor = params["hue_factor"].to(input)
-        return adjust_hue(input, hue_factor * 2 * pi)
+        saturation_factor = params["saturation_factor"].to(input)
+        return adjust_saturation(input, saturation_factor)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/invert.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/invert.py`

 * *Files 5% similar despite different names*

```diff
@@ -42,18 +42,15 @@
 
     def __init__(
         self,
         max_val: Union[float, Tensor] = torch.tensor(1.0),
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
-        self.flags = dict(max_val=max_val)
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
+        self.flags = {"max_val": max_val}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return invert(input, torch.as_tensor(flags["max_val"], device=input.device, dtype=input.dtype))
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/motion_blur.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/motion_blur.py`

 * *Files 3% similar despite different names*

```diff
@@ -71,21 +71,20 @@
         angle: Union[Tensor, float, Tuple[float, float]],
         direction: Union[Tensor, float, Tuple[float, float]],
         border_type: Union[int, str, BorderType] = BorderType.CONSTANT.name,
         resample: Union[str, int, Resample] = Resample.NEAREST.name,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self._param_generator = rg.MotionBlurGenerator(kernel_size, angle, direction)
-        self.flags = dict(border_type=BorderType.get(border_type), resample=Resample.get(resample))
+        self.flags = {"border_type": BorderType.get(border_type), "resample": Resample.get(resample)}
 
-    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:
+    def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         params = super().generate_parameters(batch_shape)
         params["idx"] = tensor([0]) if batch_shape[0] == 0 else torch.randint(batch_shape[0], (1,))
         return params
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/normalize.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/normalize.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,10 @@
-from typing import Any, Dict, List, Optional, Tuple, Union
+from __future__ import annotations
+
+from typing import Any
 
 import torch
 from torch import Tensor
 
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.enhance import normalize
 
@@ -35,32 +37,31 @@
         >>> out = norm(x)
         >>> out.shape
         torch.Size([1, 4, 3, 3])
     """
 
     def __init__(
         self,
-        mean: Union[Tensor, Tuple[float], List[float], float],
-        std: Union[Tensor, Tuple[float], List[float], float],
+        mean: Tensor | tuple[float | int] | list[float | int] | float | int,
+        std: Tensor | tuple[float | int] | list[float | int] | float | int,
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=True, keepdim=keepdim)
-        if isinstance(mean, float):
+        super().__init__(p=p, same_on_batch=True, keepdim=keepdim)
+        if isinstance(mean, (int, float)):
             mean = torch.tensor([mean])
 
-        if isinstance(std, float):
+        if isinstance(std, (int, float)):
             std = torch.tensor([std])
 
         if isinstance(mean, (tuple, list)):
             mean = torch.tensor(mean)
 
         if isinstance(std, (tuple, list)):
             std = torch.tensor(std)
 
-        self.flags = dict(mean=mean, std=std)
+        self.flags = {"mean": mean, "std": std}
 
     def apply_transform(
-        self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
+        self, input: Tensor, params: dict[str, Tensor], flags: dict[str, Any], transform: Tensor | None = None
     ) -> Tensor:
         return normalize(input, flags["mean"], flags["std"])
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/planckian_jitter.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/planckian_jitter.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,88 +1,82 @@
 from typing import Any, Dict, List, Optional, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
-from kornia.core import Tensor, stack
+from kornia.core import Tensor, stack, tensor
+from kornia.utils import get_cuda_or_mps_device_if_available
 
-_planckian_coeffs = {
-    'blackbody': Tensor(
-        [
-            [0.6743, 0.4029, 0.0013],
-            [0.6281, 0.4241, 0.1665],
-            [0.5919, 0.4372, 0.2513],
-            [0.5623, 0.4457, 0.3154],
-            [0.5376, 0.4515, 0.3672],
-            [0.5163, 0.4555, 0.4103],
-            [0.4979, 0.4584, 0.4468],
-            [0.4816, 0.4604, 0.4782],
-            [0.4672, 0.4619, 0.5053],
-            [0.4542, 0.4630, 0.5289],
-            [0.4426, 0.4638, 0.5497],
-            [0.4320, 0.4644, 0.5681],
-            [0.4223, 0.4648, 0.5844],
-            [0.4135, 0.4651, 0.5990],
-            [0.4054, 0.4653, 0.6121],
-            [0.3980, 0.4654, 0.6239],
-            [0.3911, 0.4655, 0.6346],
-            [0.3847, 0.4656, 0.6444],
-            [0.3787, 0.4656, 0.6532],
-            [0.3732, 0.4656, 0.6613],
-            [0.3680, 0.4655, 0.6688],
-            [0.3632, 0.4655, 0.6756],
-            [0.3586, 0.4655, 0.6820],
-            [0.3544, 0.4654, 0.6878],
-            [0.3503, 0.4653, 0.6933],
-        ]
-    ),
-    'CIED': Tensor(
-        [
-            [0.5829, 0.4421, 0.2288],
-            [0.5510, 0.4514, 0.2948],
-            [0.5246, 0.4576, 0.3488],
-            [0.5021, 0.4618, 0.3941],
-            [0.4826, 0.4646, 0.4325],
-            [0.4654, 0.4667, 0.4654],
-            [0.4502, 0.4681, 0.4938],
-            [0.4364, 0.4692, 0.5186],
-            [0.4240, 0.4700, 0.5403],
-            [0.4127, 0.4705, 0.5594],
-            [0.4023, 0.4709, 0.5763],
-            [0.3928, 0.4713, 0.5914],
-            [0.3839, 0.4715, 0.6049],
-            [0.3757, 0.4716, 0.6171],
-            [0.3681, 0.4717, 0.6281],
-            [0.3609, 0.4718, 0.6380],
-            [0.3543, 0.4719, 0.6472],
-            [0.3480, 0.4719, 0.6555],
-            [0.3421, 0.4719, 0.6631],
-            [0.3365, 0.4719, 0.6702],
-            [0.3313, 0.4719, 0.6766],
-            [0.3263, 0.4719, 0.6826],
-            [0.3217, 0.4719, 0.6882],
-        ]
-    ),
-}
-
-_planckian_coeffs_ratio = {
-    'blackbody': stack(
-        (
-            _planckian_coeffs['blackbody'][:, 0] / _planckian_coeffs['blackbody'][:, 1],
-            _planckian_coeffs['blackbody'][:, 2] / _planckian_coeffs['blackbody'][:, 1],
-        ),
-        1,
-    ),
-    'CIED': stack(
-        (
-            _planckian_coeffs['CIED'][:, 0] / _planckian_coeffs['CIED'][:, 1],
-            _planckian_coeffs['CIED'][:, 2] / _planckian_coeffs['CIED'][:, 1],
-        ),
-        1,
-    ),
-}
+
+def get_planckian_coeffs(mode: str) -> Tensor:
+    default_device = get_cuda_or_mps_device_if_available()
+    if mode.lower() == 'blackbody':
+        coefs = tensor(
+            [
+                [0.6743, 0.4029, 0.0013],
+                [0.6281, 0.4241, 0.1665],
+                [0.5919, 0.4372, 0.2513],
+                [0.5623, 0.4457, 0.3154],
+                [0.5376, 0.4515, 0.3672],
+                [0.5163, 0.4555, 0.4103],
+                [0.4979, 0.4584, 0.4468],
+                [0.4816, 0.4604, 0.4782],
+                [0.4672, 0.4619, 0.5053],
+                [0.4542, 0.4630, 0.5289],
+                [0.4426, 0.4638, 0.5497],
+                [0.4320, 0.4644, 0.5681],
+                [0.4223, 0.4648, 0.5844],
+                [0.4135, 0.4651, 0.5990],
+                [0.4054, 0.4653, 0.6121],
+                [0.3980, 0.4654, 0.6239],
+                [0.3911, 0.4655, 0.6346],
+                [0.3847, 0.4656, 0.6444],
+                [0.3787, 0.4656, 0.6532],
+                [0.3732, 0.4656, 0.6613],
+                [0.3680, 0.4655, 0.6688],
+                [0.3632, 0.4655, 0.6756],
+                [0.3586, 0.4655, 0.6820],
+                [0.3544, 0.4654, 0.6878],
+                [0.3503, 0.4653, 0.6933],
+            ],
+            device=default_device,
+        )
+
+    elif mode.upper() == 'CIED':
+        coefs = tensor(
+            [
+                [0.5829, 0.4421, 0.2288],
+                [0.5510, 0.4514, 0.2948],
+                [0.5246, 0.4576, 0.3488],
+                [0.5021, 0.4618, 0.3941],
+                [0.4826, 0.4646, 0.4325],
+                [0.4654, 0.4667, 0.4654],
+                [0.4502, 0.4681, 0.4938],
+                [0.4364, 0.4692, 0.5186],
+                [0.4240, 0.4700, 0.5403],
+                [0.4127, 0.4705, 0.5594],
+                [0.4023, 0.4709, 0.5763],
+                [0.3928, 0.4713, 0.5914],
+                [0.3839, 0.4715, 0.6049],
+                [0.3757, 0.4716, 0.6171],
+                [0.3681, 0.4717, 0.6281],
+                [0.3609, 0.4718, 0.6380],
+                [0.3543, 0.4719, 0.6472],
+                [0.3480, 0.4719, 0.6555],
+                [0.3421, 0.4719, 0.6631],
+                [0.3365, 0.4719, 0.6702],
+                [0.3313, 0.4719, 0.6766],
+                [0.3263, 0.4719, 0.6826],
+                [0.3217, 0.4719, 0.6882],
+            ],
+            device=default_device,
+        )
+    else:
+        raise RuntimeError(f'Unexpected mode. Gotcha {mode}')
+
+    return stack((coefs[:, 0] / coefs[:, 1], coefs[:, 2] / coefs[:, 1]), 1)
 
 
 class RandomPlanckianJitter(IntensityAugmentationBase2D):
     r"""Apply planckian jitter transformation to input tensor.
 
     .. image:: _static/img/RandomPlanckianJitter.png
 
@@ -153,43 +147,39 @@
     def __init__(
         self,
         mode: str = "blackbody",
         select_from: Optional[Union[int, List[int]]] = None,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
 
         if isinstance(select_from, int):
             select_from = [select_from]
 
+        _pl = get_planckian_coeffs(mode)
         if select_from is not None:
-            self.register_buffer('pl', _planckian_coeffs_ratio[mode][select_from])
+            self.pl = _pl[select_from]
         else:
-            self.register_buffer('pl', _planckian_coeffs_ratio[mode])
-
-        if not isinstance(self.pl, Tensor):
-            raise TypeError(f'Expected the `pl` property be a Tensor. Gotcha {type(self.pl)}')
+            self.pl = _pl
 
         # the range of the sampling parameters
         _param_min: float = 0.0
         _param_max: float = float(self.pl.shape[0])
 
         self._param_generator = rg.PlanckianJitterGenerator([_param_min, _param_max])
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-
         list_idx = params['idx'].tolist()
 
-        if not isinstance(self.pl, Tensor):
-            raise TypeError(f'Expected the `pl` property be a Tensor. Gotcha {type(self.pl)}')
+        self.pl = self.pl.to(device=input.device)
+
         coeffs = self.pl[list_idx]
 
         r_w = coeffs[:, 0][..., None, None]
         b_w = coeffs[:, 1][..., None, None]
 
         r = input[..., 0, :, :] * r_w
         g = input[..., 1, :, :]
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/plasma.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/plasma.py`

 * *Files 6% similar despite different names*

```diff
@@ -37,19 +37,16 @@
     def __init__(
         self,
         roughness: Tuple[float, float] = (0.1, 0.7),
         intensity: Tuple[float, float] = (0.0, 1.0),
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
         self._param_generator = rg.PlainUniformGenerator(
             (roughness, "roughness", None, None), (intensity, "intensity", None, None)
         )
 
     def apply_transform(
         self, image: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
@@ -90,19 +87,16 @@
 
     def __init__(
         self,
         roughness: Tuple[float, float] = (0.1, 0.7),
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
         self._param_generator = rg.PlainUniformGenerator((roughness, "roughness", None, None))
 
     def apply_transform(
         self, image: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         B, C, H, W = image.shape
         roughness = params["roughness"].to(image)
@@ -143,19 +137,16 @@
         self,
         roughness: Tuple[float, float] = (0.1, 0.7),
         shade_intensity: Tuple[float, float] = (-1.0, 0.0),
         shade_quantity: Tuple[float, float] = (0.0, 1.0),
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
         self._param_generator = rg.PlainUniformGenerator(
             (roughness, "roughness", None, None),
             (shade_intensity, "shade_intensity", None, None),
             (shade_quantity, "shade_quantity", None, None),
         )
 
     def apply_transform(
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/posterize.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/solarize.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,63 +1,73 @@
-from typing import Any, Dict, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.core import Tensor
-from kornia.enhance import posterize
+from kornia.enhance import solarize
 
 
-class RandomPosterize(IntensityAugmentationBase2D):
-    r"""Posterize given tensor image or a batch of tensor images randomly.
+class RandomSolarize(IntensityAugmentationBase2D):
+    r"""Solarize given tensor image or a batch of tensor images randomly.
 
-    .. image:: _static/img/RandomPosterize.png
+    .. image:: _static/img/RandomSolarize.png
 
     Args:
         p: probability of applying the transformation.
-        bits: Integer that ranged from (0, 8], in which 0 gives black image and 8 gives the original.
-            If int x, bits will be generated from (x, 8).
-            If tuple (x, y), bits will be generated from (x, y).
+        thresholds:
+            If float x, threshold will be generated from (0.5 - x, 0.5 + x).
+            If tuple (x, y), threshold will be generated from (x, y).
+        additions:
+            If float x, addition will be generated from (-x, x).
+            If tuple (x, y), addition will be generated from (x, y).
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
 
     Shape:
         - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
         - Output: :math:`(B, C, H, W)`
 
     .. note::
-        This function internally uses :func:`kornia.enhance.posterize`.
+        This function internally uses :func:`kornia.enhance.solarize`.
 
     Examples:
         >>> rng = torch.manual_seed(0)
         >>> input = torch.rand(1, 1, 5, 5)
-        >>> posterize = RandomPosterize(3, p=1.)
-        >>> posterize(input)
-        tensor([[[[0.4706, 0.7529, 0.0627, 0.1255, 0.2824],
-                  [0.6275, 0.4706, 0.8784, 0.4392, 0.6275],
-                  [0.3451, 0.3765, 0.0000, 0.1569, 0.2824],
-                  [0.5020, 0.6902, 0.7843, 0.1569, 0.2510],
-                  [0.6588, 0.9098, 0.3765, 0.8471, 0.4078]]]])
+        >>> solarize = RandomSolarize(0.1, 0.1, p=1.)
+        >>> solarize(input)
+        tensor([[[[0.4132, 0.1412, 0.1790, 0.2226, 0.3980],
+                  [0.2754, 0.4194, 0.0130, 0.4538, 0.2771],
+                  [0.4394, 0.4923, 0.1129, 0.2594, 0.3844],
+                  [0.3909, 0.2118, 0.1094, 0.2516, 0.3728],
+                  [0.2278, 0.0000, 0.4876, 0.0353, 0.5100]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.randn(1, 3, 32, 32)
-        >>> aug = RandomPosterize(3, p=1.)
+        >>> aug = RandomSolarize(0.1, 0.1, p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
         self,
-        bits: Union[int, Tuple[int, int], Tensor] = 3,
+        thresholds: Union[Tensor, float, Tuple[float, float], List[float]] = 0.1,
+        additions: Union[Tensor, float, Tuple[float, float], List[float]] = 0.1,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        # TODO: the generator should receive the device
-        self._param_generator = rg.PosterizeGenerator(bits)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        self._param_generator = rg.PlainUniformGenerator(
+            (thresholds, "thresholds", 0.5, (0.0, 1.0)), (additions, "additions", 0.0, (-0.5, 0.5))
+        )
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        return posterize(input, params["bits_factor"].to(input.device))
+        thresholds = params["thresholds"]
+        additions: Optional[Tensor]
+        if "additions" in params:
+            additions = params["additions"]
+        else:
+            additions = None
+        return solarize(input, thresholds, additions)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/random_rgb_shift.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/random_rgb_shift.py`

 * *Files 6% similar despite different names*

```diff
@@ -77,17 +77,16 @@
         self,
         r_shift_limit: float = 0.5,
         g_shift_limit: float = 0.5,
         b_shift_limit: float = 0.5,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self._param_generator = rg.PlainUniformGenerator(
             (r_shift_limit, "r_shift", 0, (-r_shift_limit, r_shift_limit)),
             (g_shift_limit, "g_shift", 0, (-g_shift_limit, g_shift_limit)),
             (b_shift_limit, "b_shift", 0, (-b_shift_limit, b_shift_limit)),
         )
 
     def apply_transform(
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/saturation.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/median_blur.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,71 +1,51 @@
 from typing import Any, Dict, Optional, Tuple
 
-from kornia.augmentation import random_generator as rg
-from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
-from kornia.augmentation.utils import _range_bound
-from kornia.core import Tensor
-from kornia.enhance.adjust import adjust_saturation
+from torch import Tensor
 
+from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.filters import median_blur
 
-class RandomSaturation(IntensityAugmentationBase2D):
-    r"""Apply a random transformation to the saturation of a tensor image.
 
-    This implementation aligns PIL. Hence, the output is close to TorchVision.
+class RandomMedianBlur(IntensityAugmentationBase2D):
+    """Add random blur with a median filter to an image tensor.
 
-    .. image:: _static/img/RandomSaturation.png
+    .. image:: _static/img/RandomMedianBlur.png
 
     Args:
-        p: probability of applying the transformation.
-        saturation: the saturation factor to apply
-        silence_instantiation_warning: if True, silence the warning at instantiation.
+        kernel_size: the blurring kernel size.
         same_on_batch: apply the same transformation across the batch.
+        p: probability of applying the transformation.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
                  to the batch form (False).
-    Shape:
-        - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
-        - Output: :math:`(B, C, H, W)`
-
     .. note::
-        This function internally uses :func:`kornia.enhance.adjust_saturation
+        This function internally uses :func:`kornia.filters.median_blur`.
+
+    Examples:
 
-        Examples:
-        >>> rng = torch.manual_seed(0)
-        >>> inputs = torch.rand(1, 3, 3, 3)
-        >>> aug = RandomSaturation(saturation = (0.5,2.),p=1.)
-        >>> aug(inputs)
-        tensor([[[[0.5569, 0.7682, 0.3529],
-                  [0.4811, 0.3474, 0.7411],
-                  [0.5028, 0.8964, 0.6772]],
-        <BLANKLINE>
-                 [[0.6323, 0.5358, 0.5265],
-                  [0.4203, 0.2706, 0.5525],
-                  [0.5185, 0.7863, 0.8681]],
-        <BLANKLINE>
-                 [[0.3711, 0.4989, 0.6816],
-                  [0.9152, 0.3971, 0.8742],
-                  [0.4636, 0.7060, 0.9527]]]])
+        >>> img = torch.ones(1, 1, 4, 4)
+        >>> out = RandomMedianBlur((3, 3), p = 1)(img)
+        >>> out.shape
+        torch.Size([1, 1, 4, 4])
+        >>> out
+        tensor([[[[0., 1., 1., 0.],
+                  [1., 1., 1., 1.],
+                  [1., 1., 1., 1.],
+                  [0., 1., 1., 0.]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
-        >>> input = torch.rand(1, 3, 32, 32)
-        >>> aug = RandomSaturation((0.8,1.2), p=1.)
+        >>> input = torch.randn(1, 3, 32, 32)
+        >>> aug = RandomMedianBlur((7, 7), p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
-        self,
-        saturation: Tuple[float, float] = (1.0, 1.0),
-        same_on_batch: bool = False,
-        p: float = 1.0,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
+        self, kernel_size: Tuple[int, int] = (3, 3), same_on_batch: bool = False, p: float = 0.5, keepdim: bool = False
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self.saturation: Tensor = _range_bound(saturation, 'saturation', center=1.0)
-        self._param_generator = rg.PlainUniformGenerator((self.saturation, "saturation_factor", None, None))
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
+        self.flags = {"kernel_size": kernel_size}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        saturation_factor = params["saturation_factor"].to(input)
-        return adjust_saturation(input, saturation_factor)
+        return median_blur(input, flags["kernel_size"])
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/sharpness.py` & `kornia-0.7.0/kornia/augmentation/_2d/intensity/sharpness.py`

 * *Files 5% similar despite different names*

```diff
@@ -41,21 +41,20 @@
         >>> aug = RandomSharpness(1., p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
     def __init__(
         self,
-        sharpness: Union[Tensor, float, Tuple[float, float], Tensor] = 0.5,
+        sharpness: Union[Tensor, float, Tuple[float, float]] = 0.5,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self._param_generator = rg.PlainUniformGenerator((sharpness, "sharpness", 0.0, (0, float("inf"))))
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         factor = params["sharpness"]
         return sharpness(input, factor)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/intensity/solarize.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/horizontal_flip.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,74 +1,67 @@
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, Optional
 
-from kornia.augmentation import random_generator as rg
-from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
-from kornia.core import Tensor
-from kornia.enhance import solarize
+import torch
+from torch import Tensor
 
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 
-class RandomSolarize(IntensityAugmentationBase2D):
-    r"""Solarize given tensor image or a batch of tensor images randomly.
 
-    .. image:: _static/img/RandomSolarize.png
+class RandomHorizontalFlip3D(GeometricAugmentationBase3D):
+    r"""Apply random horizontal flip to 3D volumes (5D tensor).
 
     Args:
-        p: probability of applying the transformation.
-        thresholds:
-            If float x, threshold will be generated from (0.5 - x, 0.5 + x).
-            If tuple (x, y), threshold will be generated from (x, y).
-        additions:
-            If float x, addition will be generated from (-x, x).
-            If tuple (x, y), addition will be generated from (x, y).
+        p: probability of the image being flipped.
         same_on_batch: apply the same transformation across the batch.
-        keepdim: whether to keep the output shape the same as input (True) or broadcast it
-                 to the batch form (False).
+        keepdim: whether to keep the output shape the same as input ``True`` or broadcast it
+          to the batch form ``False``.
 
     Shape:
-        - Input: :math:`(C, H, W)` or :math:`(B, C, H, W)`, Optional: :math:`(B, 3, 3)`
-        - Output: :math:`(B, C, H, W)`
+        - Input: :math:`(C, D, H, W)` or :math:`(B, C, D, H, W)`, Optional: :math:`(B, 4, 4)`
+        - Output: :math:`(B, C, D, H, W)`
 
-    .. note::
-        This function internally uses :func:`kornia.enhance.solarize`.
+    Note:
+        Input tensor must be float and normalized into [0, 1] for the best differentiability support.
+        Additionally, this function accepts another transformation tensor (:math:`(B, 4, 4)`), then the
+        applied transformation will be merged int to the input transformation tensor and returned.
 
     Examples:
-        >>> rng = torch.manual_seed(0)
-        >>> input = torch.rand(1, 1, 5, 5)
-        >>> solarize = RandomSolarize(0.1, 0.1, p=1.)
-        >>> solarize(input)
-        tensor([[[[0.4132, 0.1412, 0.1790, 0.2226, 0.3980],
-                  [0.2754, 0.4194, 0.0130, 0.4538, 0.2771],
-                  [0.4394, 0.4923, 0.1129, 0.2594, 0.3844],
-                  [0.3909, 0.2118, 0.1094, 0.2516, 0.3728],
-                  [0.2278, 0.0000, 0.4876, 0.0353, 0.5100]]]])
+        >>> import torch
+        >>> x = torch.eye(3).repeat(3, 1, 1)
+        >>> seq = RandomHorizontalFlip3D(p=1.0)
+        >>> seq(x), seq.transform_matrix
+        (tensor([[[[[0., 0., 1.],
+                   [0., 1., 0.],
+                   [1., 0., 0.]],
+        <BLANKLINE>
+                  [[0., 0., 1.],
+                   [0., 1., 0.],
+                   [1., 0., 0.]],
+        <BLANKLINE>
+                  [[0., 0., 1.],
+                   [0., 1., 0.],
+                   [1., 0., 0.]]]]]), tensor([[[-1.,  0.,  0.,  2.],
+                 [ 0.,  1.,  0.,  0.],
+                 [ 0.,  0.,  1.,  0.],
+                 [ 0.,  0.,  0.,  1.]]]))
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
-        >>> input = torch.randn(1, 3, 32, 32)
-        >>> aug = RandomSolarize(0.1, 0.1, p=1.)
+        >>> input = torch.rand(1, 3, 32, 32, 32)
+        >>> aug = RandomHorizontalFlip3D(p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
-    def __init__(
-        self,
-        thresholds: Union[Tensor, float, Tuple[float, float], List[float]] = 0.1,
-        additions: Union[Tensor, float, Tuple[float, float], List[float]] = 0.1,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
-    ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = rg.PlainUniformGenerator(
-            (thresholds, "thresholds", 0.5, (0.0, 1.0)), (additions, "additions", 0.0, (-0.5, 0.5))
+    def __init__(self, same_on_batch: bool = False, p: float = 0.5, keepdim: bool = False) -> None:
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+
+    def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
+        w: int = input.shape[-1]
+        flip_mat: Tensor = torch.tensor(
+            [[-1, 0, 0, w - 1], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], device=input.device, dtype=input.dtype
         )
+        return flip_mat.expand(input.shape[0], 4, 4)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        thresholds = params["thresholds"]
-        additions: Optional[Tensor]
-        if "additions" in params:
-            additions = params["additions"]
-        else:
-            additions = None
-        return solarize(input, thresholds, additions)
+        return torch.flip(input, [-1])
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/mix/base.py` & `kornia-0.7.0/kornia/augmentation/_2d/mix/base.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,16 +2,16 @@
 
 import torch
 
 from kornia.augmentation.base import _BasicAugmentationBase
 from kornia.augmentation.utils import _transform_input, _transform_output_shape, _validate_input_dtype
 from kornia.constants import DataKey, DType
 from kornia.core import Tensor, tensor
+from kornia.core.check import KORNIA_UNWRAP
 from kornia.geometry.boxes import Boxes
-from kornia.testing import KORNIA_UNWRAP
 
 
 class MixAugmentationBaseV2(_BasicAugmentationBase):
     r"""MixAugmentationBase base class for customized mix augmentation implementations.
 
     For any augmentation, the implementation of "generate_parameters" and "apply_transform" are required.
     "apply_transform" will need to handle the probabilities internally.
@@ -20,15 +20,15 @@
         p: probability for applying an augmentation. This param controls if to apply the augmentation for the batch.
         p_batch: probability for applying an augmentation to a batch. This param controls the augmentation
           probabilities batch-wise.
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input ``True`` or broadcast it
           to the batch form ``False``.
         data_keys: the input type sequential for applying augmentations.
-            Accepts "input", "mask", "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
+            Accepts "input", "image", "mask", "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
     """
 
     def __init__(
         self,
         p: float,
         p_batch: float,
         same_on_batch: bool = False,
@@ -48,61 +48,66 @@
         raise NotImplementedError
 
     def apply_non_transform(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         # For the images where batch_prob == False.
         return input
 
     def transform_input(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        to_apply = params['batch_prob']
+        batch_prob = params['batch_prob']
+        to_apply = batch_prob > 0.5  # NOTE: in case of Relaxed Distributions.
         ori_shape = input.shape
         in_tensor = self.transform_tensor(input)
         output = in_tensor
         if sum(to_apply) != len(to_apply):
             output = self.apply_non_transform(in_tensor, params, flags)
         if sum(to_apply) != 0:
             applied = self.apply_transform(in_tensor, params, flags)
             output = self.apply_non_transform(in_tensor, params, flags)
             output = output.index_put((to_apply,), self.apply_non_transform(applied, params, flags))
         output = _transform_output_shape(output, ori_shape) if self.keepdim else output
         return output
 
     def transform_mask(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        to_apply = params['batch_prob']
+        batch_prob = params['batch_prob']
+        to_apply = batch_prob > 0.5  # NOTE: in case of Relaxed Distributions.
         output = input
         if sum(to_apply) != len(to_apply):
             output = self.apply_non_transform_mask(input, params, flags)
         if sum(to_apply) != 0:
             output = self.apply_transform_mask(input, params, flags)
         return output
 
     def transform_boxes(self, input: Union[Tensor, Boxes], params: Dict[str, Tensor], flags: Dict[str, Any]) -> Boxes:
         # input is BxNx4x2 or Boxes.
         if isinstance(input, Tensor):
             if not (len(input.shape) == 4 and input.shape[2:] == torch.Size([4, 2])):
                 raise RuntimeError(f"Only BxNx4x2 tensor is supported. Got {input.shape}.")
             input = Boxes(input, False, mode="vertices_plus")
-        to_apply = params['batch_prob']
+        batch_prob = params['batch_prob']
+        to_apply = batch_prob > 0.5  # NOTE: in case of Relaxed Distributions.
         output = input
         if sum(to_apply) != len(to_apply):
             output = self.apply_non_transform_boxes(input, params, flags)
         if sum(to_apply) != 0:
             output = self.apply_transform_boxes(output, params, flags)
         return output
 
     def transform_keypoint(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        to_apply = params['batch_prob']
+        batch_prob = params['batch_prob']
+        to_apply = batch_prob > 0.5  # NOTE: in case of Relaxed Distributions.
         output = input
         if sum(to_apply) != len(to_apply):
             output = self.apply_non_transform_keypoint(input, params, flags)
         if sum(to_apply) != 0:
             output = self.apply_transform_keypoint(input, params, flags)
         return output
 
     def transform_class(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        to_apply = params['batch_prob']
+        batch_prob = params['batch_prob']
+        to_apply = batch_prob > 0.5  # NOTE: in case of Relaxed Distributions.
         output = input
         if sum(to_apply) != len(to_apply):
             output = self.apply_non_transform_class(input, params, flags)
         if sum(to_apply) != 0:
             output = self.apply_transform_class(input, params, flags)
         return output
 
@@ -178,13 +183,13 @@
                 raise NotImplementedError
             outputs.append(output)
         if len(outputs) == 1:
             return outputs[0]
         return outputs
 
     @torch.jit.ignore
-    def inverse(self, **kwargs):
+    def inverse(self, **kwargs: Any) -> Optional[Tensor]:
         raise RuntimeError(f"Inverse for {self.__class__.__name__} is not supported.")
 
     @property
-    def transform_matrix(self):
+    def transform_matrix(self) -> Optional[Tensor]:
         raise RuntimeError(f"Transformation matrices for {self.__class__.__name__} is not supported.")
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/mix/cutmix.py` & `kornia-0.7.0/kornia/augmentation/_2d/mix/cutmix.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 from kornia.core import Tensor, stack, zeros
 from kornia.geometry.bbox import bbox_to_mask, infer_bbox_shape
 
 
 class RandomCutMixV2(MixAugmentationBaseV2):
     r"""Apply CutMix augmentation to a batch of tensor images.
 
-    .. image:: _static/img/RandomCutMix.png
+    .. image:: _static/img/RandomCutMixV2.png
 
     Implementation for `CutMix: Regularization Strategy to Train Strong Classifiers with
     Localizable Features` :cite:`yun2019cutmix`.
 
     The function returns (inputs, labels), in which the inputs is the tensor that contains the mixup images
     while the labels is a :math:`(\text{num_mixes}, B, 3)` tensor that contains (label_permuted_batch, lambda)
     for each cutmix.
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/mix/jigsaw.py` & `kornia-0.7.0/kornia/augmentation/_2d/mix/jigsaw.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 
     Args:
         grid: the Jigsaw puzzle grid. e.g. (2, 2) means
             each output will mix image patches in a 2x2 grid.
         ensure_perm: to ensure the nonidentical patch permutation generation against
             the original one.
         data_keys: the input type sequential for applying augmentations.
-            Accepts "input", "mask", "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
+            Accepts "input", "image", "mask", "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
         p: probability of applying the transformation for the whole batch.
         same_on_batch: apply the same transformation across the batch.
         keepdim: whether to keep the output shape the same as input ``True`` or broadcast it
             to the batch form ``False``.
 
     Examples:
         >>> jigsaw = RandomJigsaw((4, 4))
@@ -45,21 +45,22 @@
         p: float = 0.5,
         same_on_batch: bool = False,
         keepdim: bool = False,
         ensure_perm: bool = True,
     ) -> None:
         super().__init__(p=p, p_batch=1.0, same_on_batch=same_on_batch, keepdim=keepdim, data_keys=data_keys)
         self._param_generator = rg.JigsawGenerator(grid, ensure_perm)
-        self.flags = dict(grid=grid)
+        self.flags = {"grid": grid}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], maybe_flags: Optional[Dict[str, Any]] = None
     ) -> Tensor:
         # different from the Base class routine. This function will not refer to any non-transformation images.
-        to_apply = params['batch_prob']
+        batch_prob = params['batch_prob']
+        to_apply = batch_prob > 0.5  # NOTE: in case of Relaxed Distributions.
         input = input[to_apply].clone()
 
         b, c, h, w = input.shape
         perm = params["permutation"]
         piece_size_h, piece_size_w = input.shape[-2] // self.flags["grid"][0], input.shape[-1] // self.flags["grid"][1]
         # Convert to C BxN H' W'
         input = (
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/mix/mixup.py` & `kornia-0.7.0/kornia/augmentation/_2d/mix/mixup.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 from kornia.constants import DataKey, DType
 from kornia.core import Tensor, stack, zeros
 
 
 class RandomMixUpV2(MixAugmentationBaseV2):
     r"""Apply MixUp augmentation to a batch of tensor images.
 
-    .. image:: _static/img/RandomMixUp.png
+    .. image:: _static/img/RandomMixUpV2.png
 
     Implementation for `mixup: BEYOND EMPIRICAL RISK MINIMIZATION` :cite:`zhang2018mixup`.
 
     The function returns (inputs, labels), in which the inputs is the tensor that contains the mixup images
     while the labels is a :math:`(B, 3)` tensor that contains (label_batch, label_permuted_batch, lambda) for
     each image.
```

### Comparing `kornia-0.6.9/kornia/augmentation/_2d/mix/mosaic.py` & `kornia-0.7.0/kornia/augmentation/_2d/mix/mosaic.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,17 +2,17 @@
 
 import torch
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
 from kornia.constants import DataKey, Resample
 from kornia.core import Tensor, as_tensor, concatenate, pad, zeros
+from kornia.core.check import KORNIA_UNWRAP
 from kornia.geometry.boxes import Boxes
 from kornia.geometry.transform import crop_by_indices, crop_by_transform_mat, get_perspective_transform
-from kornia.testing import KORNIA_UNWRAP
 from kornia.utils import eye_like
 
 __all__ = ["RandomMosaic"]
 
 
 class RandomMosaic(MixAugmentationBaseV2):
     r"""Mosaic augmentation.
@@ -31,15 +31,15 @@
     Args:
         output_size: the output tensor width and height after mosaicing.
         start_ratio_range: top-left (x, y) position for cropping the mosaic images.
         mosaic_grid: the number of images and image arrangement. e.g. (2, 2) means
             each output will mix 4 images in a 2x2 grid.
         min_bbox_size: minimum area of bounding boxes. Default to 0.
         data_keys: the input type sequential for applying augmentations.
-            Accepts "input", "mask", "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
+            Accepts "input", "image", "mask", "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
         p: probability of applying the transformation for the whole batch.
         keepdim: whether to keep the output shape the same as input ``True`` or broadcast it
             to the batch form ``False``.
         padding_mode: Type of padding. Should be: constant, reflect, replicate.
         resample: the interpolation mode.
         align_corners: interpolation flag.
         cropping_mode: The used algorithm to crop. ``slice`` will use advanced slicing to extract the tensor based
@@ -73,52 +73,53 @@
         align_corners: bool = True,
         cropping_mode: str = "slice",
     ) -> None:
         super().__init__(p=p, p_batch=1.0, same_on_batch=False, keepdim=keepdim, data_keys=data_keys)
         self.start_ratio_range = start_ratio_range
         self._param_generator = rg.MosaicGenerator(output_size, mosaic_grid, start_ratio_range)
 
-        self.flags = dict(
-            mosaic_grid=mosaic_grid,
-            output_size=output_size,
-            min_bbox_size=min_bbox_size,
-            padding_mode=padding_mode,
-            resample=Resample.get(resample),
-            align_corners=align_corners,
-            cropping_mode=cropping_mode,
-        )
+        self.flags = {
+            "mosaic_grid": mosaic_grid,
+            "output_size": output_size,
+            "min_bbox_size": min_bbox_size,
+            "padding_mode": padding_mode,
+            "resample": Resample.get(resample),
+            "align_corners": align_corners,
+            "cropping_mode": cropping_mode,
+        }
 
     def apply_transform_mask(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         raise NotImplementedError
 
     @torch.no_grad()
     def apply_transform_boxes(self, input: Boxes, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Boxes:
+        to_apply = params["batch_prob"] > 0.5
         src_box = as_tensor(params["src"], device=input.device, dtype=input.dtype)
         dst_box = as_tensor(params["dst"], device=input.device, dtype=input.dtype)
         # Boxes is BxNx4x2 only.
         batch_shapes = as_tensor(params["batch_shapes"], device=input.device, dtype=input.dtype)
-        offset = zeros((len(params["batch_prob"]), 2), device=input.device, dtype=input.dtype)  # Bx2
+        offset = zeros((len(to_apply), 2), device=input.device, dtype=input.dtype)  # Bx2
         # NOTE: not a pretty good line I think.
         offset_end = dst_box[0, 2].repeat(input.data.shape[0], 1)
-        idx = torch.arange(0, input.data.shape[0], device=input.device, dtype=torch.long)[params["batch_prob"]]
+        idx = torch.arange(0, input.data.shape[0], device=input.device, dtype=torch.long)[to_apply]
 
         maybe_out_boxes: Optional[Boxes] = None
         for i in range(flags['mosaic_grid'][0]):
             for j in range(flags['mosaic_grid'][1]):
                 _offset = offset.clone()
                 _offset[idx, 0] = batch_shapes[:, -2] * i - src_box[:, 0, 0]
                 _offset[idx, 1] = batch_shapes[:, -1] * j - src_box[:, 0, 1]
                 _box = input.clone()
                 _idx = i * flags['mosaic_grid'][1] + j
                 _box._data[params["permutation"][:, 0]] = _box._data[params["permutation"][:, _idx]]
                 _box.translate(_offset, inplace=True)
                 # zero-out unrelated batch elements.
-                _box._data[~params["batch_prob"]] = 0
+                _box._data[~to_apply] = 0
                 if maybe_out_boxes is None:
-                    _box._data[~params["batch_prob"]] = input._data[~params["batch_prob"]]
+                    _box._data[~to_apply] = input._data[~to_apply]
                     maybe_out_boxes = _box
                 else:
                     KORNIA_UNWRAP(maybe_out_boxes, Boxes).merge(_box, inplace=True)
         out_boxes: Boxes = KORNIA_UNWRAP(maybe_out_boxes, Boxes)
         out_boxes.clamp(offset, offset_end, inplace=True)
         out_boxes.filter_boxes_by_area(flags["min_bbox_size"], inplace=True)
         return out_boxes
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/__init__.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/affine.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/affine.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 from kornia.constants import Resample
 from kornia.core import Tensor
 from kornia.geometry import deg2rad, get_affine_matrix3d, warp_affine3d
 
 
-class RandomAffine3D(AugmentationBase3D):
+class RandomAffine3D(GeometricAugmentationBase3D):
     r"""Apply affine transformation 3D volumes (5D tensor).
 
     The transformation is computed so that the center is kept invariant.
 
     Args:
         degrees: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
             If degrees is a number, then yaw, pitch, roll will be generated from the range of (-degrees, +degrees).
@@ -35,16 +35,14 @@
             If shear is a number, a shear to the 6 facets in the range (-shear, +shear) will be applied.
             If shear is a tuple of 2 values, a shear to the 6 facets in the range (shear[0], shear[1]) will be applied.
             If shear is a tuple of 6 values, a shear to the i-th facet in the range (-shear[i], shear[i])
             will be applied.
             If shear is a tuple of 6 tuples, a shear to the i-th facet in the range (-shear[i, 0], shear[i, 1])
             will be applied.
         resample: resample mode from "nearest" (0) or "bilinear" (1).
-        return_transform: if ``True`` return the matrix describing the transformation
-            applied to each.
         same_on_batch: apply the same transformation across the batch.
         align_corners: interpolation flag.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
           to the batch form (False). Default: False.
 
     Shape:
         - Input: :math:`(C, D, H, W)` or :math:`(B, C, D, H, W)`, Optional: :math:`(B, 4, 4)`
@@ -112,23 +110,22 @@
             ],
         ] = None,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
         self.degrees = degrees
         self.shears = shears
         self.translate = translate
         self.scale = scale
 
-        self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
+        self.flags = {"resample": Resample.get(resample), "align_corners": align_corners}
         self._param_generator = rg.AffineGenerator3D(degrees, translate, scale, shears)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         transform: Tensor = get_affine_matrix3d(
             params["translations"],
             params["center"],
             params["scale"],
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/center_crop.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/center_crop.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,25 @@
 from typing import Any, Dict, Optional, Tuple, Union, cast
 
-from torch import Size, Tensor
-
 from kornia.augmentation import random_generator as rg
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 from kornia.constants import Resample
+from kornia.core import Tensor
 from kornia.geometry import crop_by_transform_mat3d, get_perspective_transform3d
 
 
-class CenterCrop3D(AugmentationBase3D):
+class CenterCrop3D(GeometricAugmentationBase3D):
     r"""Apply center crop on 3D volumes (5D tensor).
 
     Args:
         p: probability of applying the transformation for the whole batch.
         size (Tuple[int, int, int] or int): Desired output size (out_d, out_h, out_w) of the crop.
             If integer, out_d = out_h = out_w = size.
             If Tuple[int, int, int], out_d = size[0], out_h = size[1], out_w = size[2].
         resample: resample mode from "nearest" (0) or "bilinear" (1).
-        return_transform: if ``True`` return the matrix describing the transformation applied to each
-          input tensor. If ``False`` and the input is a tuple the applied transformation won't be concatenated.
         align_corners: interpolation flag.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
           to the batch form (False).
 
     Shape:
         - Input: :math:`(C, D, H, W)` or :math:`(B, C, D, H, W)`, Optional: :math:`(B, 4, 4)`
         - Output: :math:`(B, C, out_d, out_h, out_w)`
@@ -64,28 +61,27 @@
     def __init__(
         self,
         size: Union[int, Tuple[int, int, int]],
         align_corners: bool = True,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         p: float = 1.0,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
         # same_on_batch is always True for CenterCrop
         # Since PyTorch does not support ragged tensor. So cropping function happens batch-wisely.
-        super().__init__(p=1.0, return_transform=return_transform, same_on_batch=True, p_batch=p, keepdim=keepdim)
+        super().__init__(p=1.0, same_on_batch=True, p_batch=p, keepdim=keepdim)
         if isinstance(size, tuple):
             self.size = (size[0], size[1], size[2])
         elif isinstance(size, int):
             self.size = (size, size, size)
         else:
             raise Exception(f"Invalid size type. Expected (int, tuple(int, int int). Got: {size}.")
-        self.flags = dict(align_corners=align_corners, resample=Resample.get(resample))
+        self.flags = {"align_corners": align_corners, "resample": Resample.get(resample)}
 
-    def generate_parameters(self, batch_shape: Size) -> Dict[str, Tensor]:
+    def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:
         return rg.center_crop_generator3d(
             batch_shape[0], batch_shape[-3], batch_shape[-2], batch_shape[-1], self.size, device=self.device
         )
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         transform: Tensor = get_perspective_transform3d(params["src"].to(input), params["dst"].to(input))
         transform = transform.expand(input.shape[0], -1, -1)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/crop.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/crop.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 from kornia.constants import Resample
 from kornia.core import Tensor, pad
 from kornia.geometry import crop_by_transform_mat3d, get_perspective_transform3d
 
 
-class RandomCrop3D(AugmentationBase3D):
+class RandomCrop3D(GeometricAugmentationBase3D):
     r"""Apply random crop on 3D volumes (5D tensor).
 
     Crops random sub-volumes on a given size.
 
     Args:
         p: probability of applying the transformation for the whole batch.
         size: Desired output size (out_d, out_h, out_w) of the crop.
@@ -25,16 +25,14 @@
             desired size to avoid raising an exception. Since cropping is done
             after padding, the padding seems to be done at a random offset.
         fill: Pixel fill value for constant fill. Default is 0. If a tuple of
             length 3, it is used to fill R, G, B channels respectively.
             This value is only used when the padding_mode is constant.
         padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.
         resample: resample mode from "nearest" (0) or "bilinear" (1).
-        return_transform: if ``True`` return the matrix describing the transformation applied to each
-          input tensor. If ``False`` and the input is a tuple the applied transformation won't be concatenated
         same_on_batch: apply the same transformation across the batch.
         align_corners: interpolation flag.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
           to the batch form (False).
 
     Shape:
         - Input: :math:`(C, D, H, W)` or :math:`(B, C, D, H, W)`, Optional: :math:`(B, 4, 4)`
@@ -68,33 +66,30 @@
         self,
         size: Tuple[int, int, int],
         padding: Optional[Union[int, Tuple[int, int, int], Tuple[int, int, int, int, int, int]]] = None,
         pad_if_needed: Optional[bool] = False,
         fill: int = 0,
         padding_mode: str = "constant",
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
-        return_transform: Optional[bool] = None,
         same_on_batch: bool = False,
         align_corners: bool = True,
         p: float = 1.0,
         keepdim: bool = False,
     ) -> None:
         # Since PyTorch does not support ragged tensor. So cropping function happens batch-wisely.
-        super().__init__(
-            p=1.0, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim
-        )
-        self.flags = dict(
-            size=size,
-            padding=padding,
-            pad_if_needed=pad_if_needed,
-            padding_mode=padding_mode,
-            fill=fill,
-            resample=Resample.get(resample),
-            align_corners=align_corners,
-        )
+        super().__init__(p=1.0, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim)
+        self.flags = {
+            "size": size,
+            "padding": padding,
+            "pad_if_needed": pad_if_needed,
+            "padding_mode": padding_mode,
+            "fill": fill,
+            "resample": Resample.get(resample),
+            "align_corners": align_corners,
+        }
         self._param_generator = rg.CropGenerator3D(size, None)
 
     def precrop_padding(self, input: Tensor, flags: Optional[Dict[str, Any]] = None) -> Tensor:
         flags = self.flags if flags is None else flags
         padding = flags["padding"]
         if padding is not None:
             if isinstance(padding, int):
@@ -132,11 +127,11 @@
         if not isinstance(transform, Tensor):
             raise TypeError(f'Expected the transform to be a Tensor. Gotcha {type(transform)}')
 
         return crop_by_transform_mat3d(
             input, transform, flags["size"], mode=flags["resample"].name.lower(), align_corners=flags["align_corners"]
         )
 
-    def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs) -> Tensor:
+    def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs: Any) -> Tensor:
         # TODO: need to align 2D implementations
         input = self.precrop_padding(input)
         return super().forward(input, params)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/depthical_flip.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/depthical_flip.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from typing import Any, Dict, Optional
 
 import torch
 from torch import Tensor
 
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 
 
-class RandomDepthicalFlip3D(AugmentationBase3D):
+class RandomDepthicalFlip3D(GeometricAugmentationBase3D):
     r"""Apply random flip along the depth axis of 3D volumes (5D tensor).
 
     Input should be a tensor of shape :math:`(C, D, H, W)` or a batch of tensors :math:`(*, C, D, H, W)`.
     If Input is a tuple it is assumed that the first element contains the aforementioned tensors and the second,
     the corresponding transformation matrix that has been applied to them. In this case the module
     will Depthically flip the tensors and concatenate the corresponding transformation matrix to the
     previous one. This is especially useful when using this functionality as part of an ``nn.Sequential`` module.
@@ -53,22 +53,16 @@
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.rand(1, 3, 32, 32, 32)
         >>> aug = RandomDepthicalFlip3D(p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
-    def __init__(
-        self,
-        return_transform: Optional[bool] = None,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-    ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+    def __init__(self, same_on_batch: bool = False, p: float = 0.5, keepdim: bool = False) -> None:
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         d: int = input.shape[-3]
         flip_mat: Tensor = torch.tensor(
             [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, -1, d - 1], [0, 0, 0, 1]], device=input.device, dtype=input.dtype
         )
         return flip_mat.expand(input.shape[0], 4, 4)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/perspective.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/perspective.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 from typing import Any, Dict, Optional, Union
 
 from kornia.augmentation import random_generator as rg
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 from kornia.constants import Resample
 from kornia.core import Tensor
 from kornia.geometry import get_perspective_transform3d, warp_perspective3d
 
 
-class RandomPerspective3D(AugmentationBase3D):
+class RandomPerspective3D(GeometricAugmentationBase3D):
     r"""Apply andom perspective transformation to 3D volumes (5D tensor).
 
     Args:
         p: probability of the image being perspectively transformed.
         distortion_scale: it controls the degree of distortion and ranges from 0 to 1.
         resample: resample mode from "nearest" (0) or "bilinear" (1).
         same_on_batch: apply the same transformation across the batch.
@@ -67,18 +67,17 @@
         self,
         distortion_scale: Union[Tensor, float] = 0.5,
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        self.flags = {"resample": Resample.get(resample), "align_corners": align_corners}
         self._param_generator = rg.PerspectiveGenerator3D(distortion_scale)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return get_perspective_transform3d(params["start_points"], params["end_points"]).to(input)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/rotation.py` & `kornia-0.7.0/kornia/augmentation/_3d/geometric/rotation.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 from typing import Any, Dict, Optional, Tuple, Union
 
 import kornia
 from kornia.augmentation import random_generator as rg
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
 from kornia.constants import Resample
 from kornia.core import Tensor
 from kornia.geometry import affine3d
 from kornia.geometry.transform.affwarp import _compute_rotation_matrix3d, _compute_tensor_center3d
 
 
-class RandomRotation3D(AugmentationBase3D):
+class RandomRotation3D(GeometricAugmentationBase3D):
     r"""Apply random rotations to 3D volumes (5D tensor).
 
     Input should be a tensor of shape (C, D, H, W) or a batch of tensors :math:`(B, C, D, H, W)`.
     If Input is a tuple it is assumed that the first element contains the aforementioned tensors and the second,
     the corresponding transformation matrix that has been applied to them. In this case the module
     will rotate the tensors and concatenate the corresponding transformation matrix to the
     previous one. This is especially useful when using this functionality as part of an ``nn.Sequential`` module.
@@ -79,18 +79,17 @@
             Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]],
         ],
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         same_on_batch: bool = False,
         align_corners: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
+        self.flags = {"resample": Resample.get(resample), "align_corners": align_corners}
         self._param_generator = rg.RotationGenerator3D(degrees)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         yaw: Tensor = params["yaw"].to(input)
         pitch: Tensor = params["pitch"].to(input)
         roll: Tensor = params["roll"].to(input)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/geometric/vertical_flip.py` & `kornia-0.7.0/kornia/augmentation/_3d/intensity/equalize.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,79 +1,61 @@
 from typing import Any, Dict, Optional
 
-import torch
 from torch import Tensor
 
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.intensity.base import IntensityAugmentationBase3D
+from kornia.enhance import equalize3d
 
 
-class RandomVerticalFlip3D(AugmentationBase3D):
-    r"""Apply random vertical flip to 3D volumes (5D tensor).
-
-    Input should be a tensor of shape :math:`(C, D, H, W)` or a batch of tensors :math:`(*, C, D, H, W)`.
-    If Input is a tuple it is assumed that the first element contains the aforementioned tensors and the second,
-    the corresponding transformation matrix that has been applied to them. In this case the module
-    will Vertically flip the tensors and concatenate the corresponding transformation matrix to the
-    previous one. This is especially useful when using this functionality as part of an ``nn.Sequential`` module.
+class RandomEqualize3D(IntensityAugmentationBase3D):
+    r"""Apply random equalization to 3D volumes (5D tensor).
 
     Args:
-        p: probability of the image being flipped.
-        same_on_batch: apply the same transformation across the batch.
-        keepdim: whether to keep the output shape the same as input ``True`` or broadcast it
-          to the batch form ``False``.
+        p: probability of the image being equalized.
+        same_on_batch): apply the same transformation across the batch.
+        keepdim: whether to keep the output shape the same as input (True) or broadcast it
+          to the batch form (False).
 
     Shape:
         - Input: :math:`(C, D, H, W)` or :math:`(B, C, D, H, W)`, Optional: :math:`(B, 4, 4)`
         - Output: :math:`(B, C, D, H, W)`
 
     Note:
         Input tensor must be float and normalized into [0, 1] for the best differentiability support.
         Additionally, this function accepts another transformation tensor (:math:`(B, 4, 4)`), then the
         applied transformation will be merged int to the input transformation tensor and returned.
 
     Examples:
         >>> import torch
-        >>> x = torch.eye(3).repeat(3, 1, 1)
-        >>> seq = RandomVerticalFlip3D(p=1.0)
-        >>> seq(x), seq.transform_matrix
-        (tensor([[[[[0., 0., 1.],
-                   [0., 1., 0.],
-                   [1., 0., 0.]],
+        >>> rng = torch.manual_seed(0)
+        >>> input = torch.rand(1, 1, 3, 3, 3)
+        >>> aug = RandomEqualize3D(p=1.0)
+        >>> aug(input)
+        tensor([[[[[0.4963, 0.7682, 0.0885],
+                   [0.1320, 0.3074, 0.6341],
+                   [0.4901, 0.8964, 0.4556]],
         <BLANKLINE>
-                  [[0., 0., 1.],
-                   [0., 1., 0.],
-                   [1., 0., 0.]],
+                  [[0.6323, 0.3489, 0.4017],
+                   [0.0223, 0.1689, 0.2939],
+                   [0.5185, 0.6977, 0.8000]],
         <BLANKLINE>
-                  [[0., 0., 1.],
-                   [0., 1., 0.],
-                   [1., 0., 0.]]]]]), tensor([[[ 1.,  0.,  0.,  0.],
-                 [ 0., -1.,  0.,  2.],
-                 [ 0.,  0.,  1.,  0.],
-                 [ 0.,  0.,  0.,  1.]]]))
+                  [[0.1610, 0.2823, 0.6816],
+                   [0.9152, 0.3971, 0.8742],
+                   [0.4194, 0.5529, 0.9527]]]]])
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.rand(1, 3, 32, 32, 32)
-        >>> aug = RandomVerticalFlip3D(p=1.)
+        >>> aug = RandomEqualize3D(p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
 
-    def __init__(
-        self,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-        return_transform: Optional[bool] = None,
-    ) -> None:
-        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
+    def __init__(self, p: float = 0.5, same_on_batch: bool = False, keepdim: bool = False) -> None:
+        super().__init__(p=p, same_on_batch=same_on_batch, keepdim=keepdim)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        h: int = input.shape[-2]
-        flip_mat: Tensor = torch.tensor(
-            [[1, 0, 0, 0], [0, -1, 0, h - 1], [0, 0, 1, 0], [0, 0, 0, 1]], device=input.device, dtype=input.dtype
-        )
-        return flip_mat.expand(input.shape[0], 4, 4)
+        return self.identity_matrix(input)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        return torch.flip(input, [-2])
+        return equalize3d(input)
```

### Comparing `kornia-0.6.9/kornia/augmentation/_3d/intensity/motion_blur.py` & `kornia-0.7.0/kornia/augmentation/_3d/intensity/motion_blur.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.intensity.base import IntensityAugmentationBase3D
 from kornia.constants import BorderType, Resample
 from kornia.core import Tensor
 from kornia.filters import motion_blur3d
 
 
-class RandomMotionBlur3D(AugmentationBase3D):
+class RandomMotionBlur3D(IntensityAugmentationBase3D):
     r"""Apply random motion blur on 3D volumes (5D tensor).
 
     Args:
         p: probability of applying the transformation.
         kernel_size: motion kernel size (odd and positive).
             If int, the kernel will have a fixed size.
             If Tuple[int, int], it will randomly generate the value from the range batch-wisely.
@@ -85,20 +85,17 @@
         ],
         direction: Union[Tensor, float, Tuple[float, float]],
         border_type: Union[int, str, BorderType] = BorderType.CONSTANT.name,
         resample: Union[str, int, Resample] = Resample.NEAREST.name,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
-        return_transform: Optional[bool] = None,
     ) -> None:
-        super().__init__(
-            p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
-        )
-        self.flags = dict(border_type=BorderType.get(border_type), resample=Resample.get(resample))
+        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim)
+        self.flags = {"border_type": BorderType.get(border_type), "resample": Resample.get(resample)}
         self._param_generator = rg.MotionBlurGenerator3D(kernel_size, angle, direction)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return self.identity_matrix(input)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
```

### Comparing `kornia-0.6.9/kornia/augmentation/__init__.py` & `kornia-0.7.0/kornia/augmentation/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,19 @@
+# Lazy loading auto module
+from kornia.augmentation import auto, container
 from kornia.augmentation._2d import (
     CenterCrop,
     ColorJiggle,
     ColorJitter,
     Denormalize,
     LongestMaxSize,
     Normalize,
     PadTo,
     RandomAffine,
+    RandomAutoContrast,
     RandomBoxBlur,
     RandomBrightness,
     RandomChannelShuffle,
     RandomContrast,
     RandomCrop,
     RandomCutMixV2,
     RandomElasticTransform,
@@ -21,35 +24,40 @@
     RandomGaussianBlur,
     RandomGaussianNoise,
     RandomGrayscale,
     RandomHorizontalFlip,
     RandomHue,
     RandomInvert,
     RandomJigsaw,
+    RandomMedianBlur,
     RandomMixUpV2,
     RandomMosaic,
     RandomMotionBlur,
     RandomPerspective,
     RandomPlanckianJitter,
     RandomPlasmaBrightness,
     RandomPlasmaContrast,
     RandomPlasmaShadow,
     RandomPosterize,
+    RandomRain,
     RandomResizedCrop,
     RandomRGBShift,
     RandomRotation,
     RandomSaturation,
     RandomSharpness,
+    RandomShear,
+    RandomSnow,
     RandomSolarize,
     RandomThinPlateSpline,
+    RandomTranslate,
     RandomVerticalFlip,
     Resize,
     SmallestMaxSize,
 )
-from kornia.augmentation._2d.base import AugmentationBase2D
+from kornia.augmentation._2d.base import AugmentationBase2D, RigidAffineAugmentationBase2D
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
 from kornia.augmentation._3d import (
     CenterCrop3D,
     RandomAffine3D,
     RandomCrop3D,
@@ -57,45 +65,54 @@
     RandomEqualize3D,
     RandomHorizontalFlip3D,
     RandomMotionBlur3D,
     RandomPerspective3D,
     RandomRotation3D,
     RandomVerticalFlip3D,
 )
-from kornia.augmentation._3d.base import AugmentationBase3D
+from kornia.augmentation._3d.base import AugmentationBase3D, RigidAffineAugmentationBase3D
+from kornia.augmentation._3d.geometric.base import GeometricAugmentationBase3D
+from kornia.augmentation._3d.intensity.base import IntensityAugmentationBase3D
 from kornia.augmentation.container import (
     AugmentationSequential,
     ImageSequential,
     ManyToManyAugmentationDispather,
     ManyToOneAugmentationDispather,
     PatchSequential,
     VideoSequential,
 )
 
 __all__ = [
+    "auto",
+    "container",
     "AugmentationBase2D",
+    "RigidAffineAugmentationBase2D",
     "GeometricAugmentationBase2D",
     "IntensityAugmentationBase2D",
     "MixAugmentationBaseV2",
     "CenterCrop",
     "ColorJiggle",
     "ColorJitter",
     "Normalize",
     "Denormalize",
     "LongestMaxSize",
     "PadTo",
     "RandomAffine",
+    "RandomShear",
+    "RandomTranslate",
     "RandomBoxBlur",
+    "RandomMedianBlur",
     "RandomBrightness",
     "RandomChannelShuffle",
     "RandomContrast",
     "RandomCrop",
     "RandomErasing",
     "RandomElasticTransform",
     "RandomFisheye",
+    "RandomAutoContrast",
     "RandomGamma",
     "RandomGrayscale",
     "RandomGaussianBlur",
     "RandomGaussianNoise",
     "RandomHorizontalFlip",
     "RandomHue",
     "RandomVerticalFlip",
@@ -106,26 +123,31 @@
     "RandomPlasmaContrast",
     "RandomResizedCrop",
     "RandomRotation",
     "RandomRGBShift",
     "RandomSaturation",
     "RandomSolarize",
     "RandomSharpness",
+    "RandomSnow",
+    "RandomRain",
     "RandomPosterize",
     "RandomEqualize",
     "RandomMotionBlur",
     "RandomInvert",
     "RandomThinPlateSpline",
     "RandomMixUpV2",
     "RandomCutMixV2",
     "RandomJigsaw",
     "RandomMosaic",
     "Resize",
     "SmallestMaxSize",
     "AugmentationBase3D",
+    "RigidAffineAugmentationBase3D",
+    "GeometricAugmentationBase3D",
+    "IntensityAugmentationBase3D",
     "CenterCrop3D",
     "RandomAffine3D",
     "RandomCrop3D",
     "RandomDepthicalFlip3D",
     "RandomVerticalFlip3D",
     "RandomHorizontalFlip3D",
     "RandomRotation3D",
```

### Comparing `kornia-0.6.9/kornia/augmentation/container/augment.py` & `kornia-0.7.0/kornia/augmentation/container/augment.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,60 +1,70 @@
 import warnings
-from itertools import zip_longest
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple, Union, cast
 
-from kornia.augmentation import (
-    AugmentationBase3D,
-    GeometricAugmentationBase2D,
-    IntensityAugmentationBase2D,
-    RandomErasing,
-)
-from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
+from kornia.augmentation._2d.base import RigidAffineAugmentationBase2D
+from kornia.augmentation._3d.base import AugmentationBase3D, RigidAffineAugmentationBase3D
 from kornia.augmentation.base import _AugmentationBase
-from kornia.augmentation.container.base import SequentialBase
-from kornia.augmentation.container.image import ImageSequential, ParamItem
-from kornia.augmentation.container.patch import PatchSequential
-from kornia.augmentation.container.utils import ApplyInverse
-from kornia.augmentation.container.video import VideoSequential
 from kornia.constants import DataKey, Resample
-from kornia.core import Tensor
-from kornia.geometry.boxes import Boxes
-from kornia.utils import eye_like
+from kornia.core import Module, Tensor
+from kornia.geometry.boxes import Boxes, VideoBoxes
+from kornia.geometry.keypoints import Keypoints, VideoKeypoints
+from kornia.utils import eye_like, is_autocast_enabled
+
+from .base import TransformMatrixMinIn
+from .image import ImageSequential
+from .ops import AugmentationSequentialOps, DataType
+from .params import ParamItem
+from .patch import PatchSequential
+from .video import VideoSequential
 
 __all__ = ["AugmentationSequential"]
 
+_BOXES_OPTIONS = {DataKey.BBOX, DataKey.BBOX_XYXY, DataKey.BBOX_XYWH}
+_KEYPOINTS_OPTIONS = {DataKey.KEYPOINTS}
+_IMG_MSK_OPTIONS = {DataKey.INPUT, DataKey.MASK}
 
-class AugmentationSequential(ImageSequential):
+
+class AugmentationSequential(TransformMatrixMinIn, ImageSequential):
     r"""AugmentationSequential for handling multiple input types like inputs, masks, keypoints at once.
 
-    .. image:: https://kornia-tutorials.readthedocs.io/en/latest/_images/data_augmentation_sequential_5_1.png
-        :width: 49 %
-    .. image:: https://kornia-tutorials.readthedocs.io/en/latest/_images/data_augmentation_sequential_7_0.png
-        :width: 49 %
+    .. image:: _static/img/AugmentationSequential.png
 
     Args:
         *args: a list of kornia augmentation modules.
-        data_keys: the input type sequential for applying augmentations.
-            Accepts "input", "mask", "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
-        same_on_batch: apply the same transformation across the batch.
-            If None, it will not overwrite the function-wise settings.
-        keepdim: whether to keep the output shape the same as input (True) or broadcast it
-            to the batch form (False). If None, it will not overwrite the function-wise settings.
-        random_apply: randomly select a sublist (order agnostic) of args to
-            apply transformation.
-            If int, a fixed number of transformations will be selected.
-            If (a,), x number of transformations (a <= x <= len(args)) will be selected.
-            If (a, b), x number of transformations (a <= x <= b) will be selected.
-            If True, the whole list of args will be processed as a sequence in a random order.
-            If False, the whole list of args will be processed as a sequence in original order.
-        extra_args: to control the behaviour for each datakeys. By default, masks are handled
-            by nearest interpolation strategies.
+
+        data_keys: the input type sequential for applying augmentations. Accepts "input", "image", "mask",
+                   "bbox", "bbox_xyxy", "bbox_xywh", "keypoints".
+
+        same_on_batch: apply the same transformation across the batch. If None, it will not overwrite the function-wise
+                       settings.
+
+        keepdim: whether to keep the output shape the same as input (True) or broadcast it to the batch form (False).
+                 If None, it will not overwrite the function-wise settings.
+
+        random_apply: randomly select a sublist (order agnostic) of args to apply transformation.
+                      If int, a fixed number of transformations will be selected.
+                      If (a,), x number of transformations (a <= x <= len(args)) will be selected.
+                      If (a, b), x number of transformations (a <= x <= b) will be selected.
+                      If True, the whole list of args will be processed as a sequence in a random order.
+                      If False, the whole list of args will be processed as a sequence in original order.
+
+        transformation_matrix_mode: computation mode for the chained transformation matrix, via `.transform_matrix`
+                                    attribute.
+                                    If `silent`, transformation matrix will be computed silently and the non-rigid
+                                    modules will be ignored as identity transformations.
+                                    If `rigid`, transformation matrix will be computed silently and the non-rigid
+                                    modules will trigger errors.
+                                    If `skip`, transformation matrix will be totally ignored.
+
+        extra_args: to control the behaviour for each datakeys. By default, masks are handled by nearest interpolation
+                    strategies.
 
     .. note::
-        Mix augmentations (e.g. RandomMixUp, RandomCutMix) can only be working with "input" data key.
+        Mix augmentations (e.g. RandomMixUp, RandomCutMix) can only be working with "input"/"image" data key.
         It is not clear how to deal with the conversions of masks, bounding boxes and keypoints.
 
     .. note::
         See a working example `here <https://kornia-tutorials.readthedocs.io/en/
         latest/data_augmentation_sequential.html>`__.
 
     Examples:
@@ -62,368 +72,352 @@
         >>> input = torch.randn(2, 3, 5, 6)
         >>> mask = torch.ones(2, 3, 5, 6)
         >>> bbox = torch.tensor([[
         ...     [1., 1.],
         ...     [2., 1.],
         ...     [2., 2.],
         ...     [1., 2.],
-        ... ]]).expand(2, -1, -1)
+        ... ]]).expand(2, 1, -1, -1)
         >>> points = torch.tensor([[[1., 1.]]]).expand(2, -1, -1)
         >>> aug_list = AugmentationSequential(
         ...     kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...     kornia.augmentation.RandomAffine(360, p=1.0),
         ...     data_keys=["input", "mask", "bbox", "keypoints"],
         ...     same_on_batch=False,
         ...     random_apply=10,
         ... )
         >>> out = aug_list(input, mask, bbox, points)
         >>> [o.shape for o in out]
-        [torch.Size([2, 3, 5, 6]), torch.Size([2, 3, 5, 6]), torch.Size([2, 4, 2]), torch.Size([2, 1, 2])]
+        [torch.Size([2, 3, 5, 6]), torch.Size([2, 3, 5, 6]), torch.Size([2, 1, 4, 2]), torch.Size([2, 1, 2])]
         >>> # apply the exact augmentation again.
         >>> out_rep = aug_list(input, mask, bbox, points, params=aug_list._params)
         >>> [(o == o_rep).all() for o, o_rep in zip(out, out_rep)]
         [tensor(True), tensor(True), tensor(True), tensor(True)]
         >>> # inverse the augmentations
         >>> out_inv = aug_list.inverse(*out)
         >>> [o.shape for o in out_inv]
-        [torch.Size([2, 3, 5, 6]), torch.Size([2, 3, 5, 6]), torch.Size([2, 4, 2]), torch.Size([2, 1, 2])]
+        [torch.Size([2, 3, 5, 6]), torch.Size([2, 3, 5, 6]), torch.Size([2, 1, 4, 2]), torch.Size([2, 1, 2])]
 
     This example demonstrates the integration of VideoSequential and AugmentationSequential.
 
         >>> import kornia
         >>> input = torch.randn(2, 3, 5, 6)[None]
         >>> mask = torch.ones(2, 3, 5, 6)[None]
         >>> bbox = torch.tensor([[
         ...     [1., 1.],
         ...     [2., 1.],
         ...     [2., 2.],
         ...     [1., 2.],
-        ... ]]).expand(2, -1, -1)[None]
+        ... ]]).expand(2, 1, -1, -1)[None]
         >>> points = torch.tensor([[[1., 1.]]]).expand(2, -1, -1)[None]
         >>> aug_list = AugmentationSequential(
         ...     VideoSequential(
         ...         kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...         kornia.augmentation.RandomAffine(360, p=1.0),
         ...     ),
         ...     data_keys=["input", "mask", "bbox", "keypoints"]
         ... )
         >>> out = aug_list(input, mask, bbox, points)
-        >>> [o.shape for o in out]
-        [torch.Size([1, 2, 3, 5, 6]), torch.Size([1, 2, 3, 5, 6]), torch.Size([1, 2, 4, 2]), torch.Size([1, 2, 1, 2])]
+        >>> [o.shape for o in out]  # doctest: +ELLIPSIS
+        [torch.Size([1, 2, 3, 5, 6]), torch.Size([1, 2, 3, 5, 6]), ...([1, 2, 1, 4, 2]), torch.Size([1, 2, 1, 2])]
 
-    Perform ``OneOf`` transformation with ``random_apply=1`` and ``random_apply_weights`` in ``AugmentationSequential``.
+    Perform ``OneOf`` transformation with ``random_apply=1`` and ``random_apply_weights``
+    in ``AugmentationSequential``.
 
         >>> import kornia
         >>> input = torch.randn(2, 3, 5, 6)[None]
         >>> mask = torch.ones(2, 3, 5, 6)[None]
         >>> bbox = torch.tensor([[
         ...     [1., 1.],
         ...     [2., 1.],
         ...     [2., 2.],
         ...     [1., 2.],
-        ... ]]).expand(2, -1, -1)[None]
+        ... ]]).expand(2, 1, -1, -1)[None]
         >>> points = torch.tensor([[[1., 1.]]]).expand(2, -1, -1)[None]
         >>> aug_list = AugmentationSequential(
         ...     VideoSequential(
         ...         kornia.augmentation.RandomAffine(360, p=1.0),
         ...     ),
         ...     VideoSequential(
         ...         kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...     ),
         ...     data_keys=["input", "mask", "bbox", "keypoints"],
         ...     random_apply=1,
         ...     random_apply_weights=[0.5, 0.3]
         ... )
         >>> out = aug_list(input, mask, bbox, points)
-        >>> [o.shape for o in out]
-        [torch.Size([1, 2, 3, 5, 6]), torch.Size([1, 2, 3, 5, 6]), torch.Size([1, 2, 4, 2]), torch.Size([1, 2, 1, 2])]
+        >>> [o.shape for o in out]  # doctest: +ELLIPSIS
+        [torch.Size([1, 2, 3, 5, 6]), torch.Size([1, 2, 3, 5, 6]), ...([1, 2, 1, 4, 2]), torch.Size([1, 2, 1, 2])]
     """
 
     def __init__(
         self,
         *args: Union[_AugmentationBase, ImageSequential],
-        data_keys: List[Union[str, int, DataKey]] = [DataKey.INPUT],
+        data_keys: Union[List[str], List[int], List[DataKey]] = [DataKey.INPUT],
         same_on_batch: Optional[bool] = None,
         keepdim: Optional[bool] = None,
         random_apply: Union[int, bool, Tuple[int, int]] = False,
         random_apply_weights: Optional[List[float]] = None,
-        extra_args: Dict[DataKey, Dict[str, Any]] = {DataKey.MASK: dict(resample=Resample.NEAREST, align_corners=True)},
+        transformation_matrix_mode: str = "silent",
+        extra_args: Dict[DataKey, Dict[str, Any]] = {
+            DataKey.MASK: {"resample": Resample.NEAREST, "align_corners": None}
+        },
     ) -> None:
+        self._transform_matrix: Optional[Tensor]
+        self._transform_matrices: List[Optional[Tensor]] = []
+
         super().__init__(
             *args,
             same_on_batch=same_on_batch,
             keepdim=keepdim,
             random_apply=random_apply,
             random_apply_weights=random_apply_weights,
         )
 
+        self._parse_transformation_matrix_mode(transformation_matrix_mode)
+
+        self._valid_ops_for_transform_computation: Tuple[Any, ...] = (
+            RigidAffineAugmentationBase2D,
+            RigidAffineAugmentationBase3D,
+            AugmentationSequential,
+        )
+
         self.data_keys = [DataKey.get(inp) for inp in data_keys]
 
         if not all(in_type in DataKey for in_type in self.data_keys):
-            raise AssertionError(f"`data_keys` must be in {DataKey}. Got {data_keys}.")
+            raise AssertionError(f"`data_keys` must be in {DataKey}. Got {self.data_keys}.")
 
         if self.data_keys[0] != DataKey.INPUT:
             raise NotImplementedError(f"The first input must be {DataKey.INPUT}.")
 
+        self.transform_op = AugmentationSequentialOps(self.data_keys)
+
         self.contains_video_sequential: bool = False
         self.contains_3d_augmentation: bool = False
         for arg in args:
             if isinstance(arg, PatchSequential) and not arg.is_intensity_only():
                 warnings.warn("Geometric transformation detected in PatchSeqeuntial, which would break bbox, mask.")
             if isinstance(arg, VideoSequential):
                 self.contains_video_sequential = True
             # NOTE: only for images are supported for 3D.
             if isinstance(arg, AugmentationBase3D):
                 self.contains_3d_augmentation = True
-        self._transform_matrix: Optional[Tensor] = None
+        self._transform_matrix = None
         self.extra_args = extra_args
 
+    def clear_state(self) -> None:
+        self._reset_transform_matrix_state()
+        return super().clear_state()
+
+    def _update_transform_matrix_for_valid_op(self, module: Module) -> None:
+        self._transform_matrices.append(module.transform_matrix)  # type: ignore
+
     def identity_matrix(self, input: Tensor) -> Tensor:
         """Return identity matrix."""
         if self.contains_3d_augmentation:
             return eye_like(4, input)
         else:
             return eye_like(3, input)
 
-    @property
-    def transform_matrix(self) -> Optional[Tensor]:
-        return self._transform_matrix
-
     def inverse(  # type: ignore[override]
         self,
-        *args: Tensor,
+        *args: DataType,
         params: Optional[List[ParamItem]] = None,
-        data_keys: Optional[List[Union[str, int, DataKey]]] = None,
-    ) -> Union[Tensor, List[Tensor]]:
+        data_keys: Optional[Union[List[str], List[int], List[DataKey]]] = None,
+    ) -> Union[DataType, List[DataType]]:
         """Reverse the transformation applied.
 
         Number of input tensors must align with the number of``data_keys``. If ``data_keys`` is not set, use
         ``self.data_keys`` by default.
         """
-        if data_keys is None:
-            _data_keys = self.data_keys
-        else:
-            _data_keys = [DataKey.get(inp) for inp in data_keys]
+        self.transform_op.data_keys = self.transform_op.preproc_datakeys(data_keys)
 
-        self._validate_args_datakeys(*args, data_keys=_data_keys)
+        self._validate_args_datakeys(*args, data_keys=self.transform_op.data_keys)
 
-        args = self._arguments_preproc(*args, data_keys=_data_keys)
+        in_args = self._arguments_preproc(*args, data_keys=self.transform_op.data_keys)
 
         if params is None:
             if self._params is None:
                 raise ValueError(
                     "No parameters available for inversing, please run a forward pass first "
                     "or passing valid params into this function."
                 )
             params = self._params
 
-        outputs: List[Optional[Tensor]] = [None] * len(_data_keys)
-        for idx, (arg, dcate) in enumerate(zip(args, _data_keys)):
-
-            if dcate in self.extra_args:
-                extra_args = self.extra_args[dcate]
-            else:
-                extra_args = {}
-
-            if dcate == DataKey.INPUT and isinstance(arg, (tuple, list)):
-                input, _ = arg  # ignore the transformation matrix whilst inverse
-            # Using tensors straight-away
-            elif isinstance(arg, (Boxes,)):
-                input = arg.data  # all boxes are in (B, N, 4, 2) format now.
-            else:
-                input = arg
-            for (name, module), _param in zip_longest(list(self.get_forward_sequence(params))[::-1], params[::-1]):
-                if isinstance(module, (_AugmentationBase, ImageSequential)):
-                    _mb = [p for p in params if name in p]
-                    if len(_mb) > 0:
-                        param = _mb[0]
-                    elif isinstance(_param, ParamItem):
-                        param = _param
-                    else:
-                        param = None
-                else:
-                    param = None
-
-                if (
-                    isinstance(module, IntensityAugmentationBase2D)
-                    and dcate in DataKey
-                    and not isinstance(module, RandomErasing)
-                ):
-                    pass  # Do nothing
-                elif isinstance(module, ImageSequential) and module.is_intensity_only() and dcate in DataKey:
-                    pass  # Do nothing
-                elif isinstance(module, VideoSequential) and dcate not in [DataKey.INPUT, DataKey.MASK]:
-                    batch_size: int = input.size(0)
-                    input = input.view(-1, *input.shape[2:])
-                    input = ApplyInverse.inverse_by_key(input, module, param, dcate, extra_args=extra_args)
-                    input = input.view(batch_size, -1, *input.shape[1:])
-                elif isinstance(module, PatchSequential):
-                    raise NotImplementedError("Geometric involved PatchSequential is not supported.")
-                elif isinstance(module, (AugmentationSequential)) and dcate in DataKey:
-                    # AugmentationSequential shall not take the extra_args arguments.
-                    input = ApplyInverse.inverse_by_key(input, module, param, dcate)
-                elif (
-                    isinstance(module, (GeometricAugmentationBase2D, ImageSequential, RandomErasing))
-                    and dcate in DataKey
-                ):
-                    input = ApplyInverse.inverse_by_key(input, module, param, dcate, extra_args=extra_args)
-                elif isinstance(module, (SequentialBase,)):
-                    raise ValueError(f"Unsupported Sequential {module}.")
-                else:
-                    raise NotImplementedError(f"data_key {dcate} is not implemented for {module}.")
-            if isinstance(arg, (Boxes,)):
-                arg._data = input
-                outputs[idx] = arg.to_tensor()
-            else:
-                outputs[idx] = input
-
-        _outputs = [i for i in outputs if isinstance(i, Tensor)]
-
-        if len(_outputs) == 1 and isinstance(_outputs, list):
-            return _outputs[0]
-
-        return _outputs
-
-    def __packup_output__(  # type: ignore[override]
-        self, output: List[Tensor], label: Optional[Tensor] = None
-    ) -> Union[Tensor, List[Tensor], Tuple[Union[Tensor, List[Tensor]], Optional[Tensor]]]:
-
-        _out: Union[Tensor, List[Tensor]]
+        outputs: List[DataType] = in_args
+        for param in params[::-1]:
+            module = self.get_submodule(param.name)
+            outputs = self.transform_op.inverse(  # type: ignore
+                *outputs, module=module, param=param, extra_args=self.extra_args
+            )
+            if not isinstance(outputs, (list, tuple)):
+                # Make sure we are unpacking a list whilst post-proc
+                outputs = [outputs]
 
-        if len(output) == 1 and isinstance(output, list):
-            _out = output[0]
-        else:
-            _out = output
+        outputs = self._arguments_postproc(args, outputs, data_keys=self.transform_op.data_keys)  # type: ignore
 
-        if self.return_label:
-            return _out, label
+        if len(outputs) == 1 and isinstance(outputs, list):
+            return outputs[0]
 
-        return _out
+        return outputs
 
-    def _validate_args_datakeys(self, *args: Tensor, data_keys: List[DataKey]):
+    def _validate_args_datakeys(self, *args: DataType, data_keys: List[DataKey]) -> None:
         if len(args) != len(data_keys):
             raise AssertionError(
                 f"The number of inputs must align with the number of data_keys. Got {len(args)} and {len(data_keys)}."
             )
         # TODO: validate args batching, and its consistency
 
-    def _arguments_preproc(self, *args: Tensor, data_keys: List[DataKey]):
-        inp: List[Any] = []
+    def _arguments_preproc(self, *args: DataType, data_keys: List[DataKey]) -> List[DataType]:
+        inp: List[DataType] = []
         for arg, dcate in zip(args, data_keys):
-            if DataKey.get(dcate) in [DataKey.INPUT, DataKey.MASK, DataKey.KEYPOINTS]:
+            if DataKey.get(dcate) in _IMG_MSK_OPTIONS:
                 inp.append(arg)
-            elif DataKey.get(dcate) in [DataKey.BBOX, DataKey.BBOX_XYXY, DataKey.BBOX_XYWH]:
-                if DataKey.get(dcate) in [DataKey.BBOX]:
-                    mode = "vertices_plus"
-                elif DataKey.get(dcate) in [DataKey.BBOX_XYXY]:
-                    mode = "xyxy"
-                elif DataKey.get(dcate) in [DataKey.BBOX_XYWH]:
-                    mode = "xywh"
-                else:
-                    raise ValueError(f"Unsupported mode `{DataKey.get(dcate).name}`.")
-                inp.append(Boxes.from_tensor(arg, mode=mode))
+            elif DataKey.get(dcate) in _KEYPOINTS_OPTIONS:
+                inp.append(self._preproc_keypoints(arg, dcate))
+            elif DataKey.get(dcate) in _BOXES_OPTIONS:
+                inp.append(self._preproc_boxes(arg, dcate))
             else:
                 raise NotImplementedError(f"input type of {dcate} is not implemented.")
         return inp
 
+    def _arguments_postproc(
+        self, in_args: List[DataType], out_args: List[DataType], data_keys: List[DataKey]
+    ) -> List[DataType]:
+        out: List[DataType] = []
+        for in_arg, out_arg, dcate in zip(in_args, out_args, data_keys):
+            if DataKey.get(dcate) in _IMG_MSK_OPTIONS:
+                # It is tensor type already.
+                out.append(out_arg)
+                # TODO: may add the float to integer (for masks), etc.
+
+            elif DataKey.get(dcate) in _KEYPOINTS_OPTIONS:
+                _out_k = self._postproc_keypoint(in_arg, cast(Keypoints, out_arg), dcate)
+                if is_autocast_enabled() and isinstance(in_arg, (Tensor, Keypoints)):
+                    if isinstance(_out_k, list):
+                        _out_k = [i.type(in_arg.dtype) for i in _out_k]
+                    else:
+                        _out_k = _out_k.type(in_arg.dtype)
+                out.append(_out_k)
+
+            elif DataKey.get(dcate) in _BOXES_OPTIONS:
+                _out_b = self._postproc_boxes(in_arg, cast(Boxes, out_arg), dcate)
+                if is_autocast_enabled() and isinstance(in_arg, (Tensor, Boxes)):
+                    if isinstance(_out_b, list):
+                        _out_b = [i.type(in_arg.dtype) for i in _out_b]
+                    else:
+                        _out_b = _out_b.type(in_arg.dtype)
+                out.append(_out_b)
+
+            else:
+                raise NotImplementedError(f"input type of {dcate} is not implemented.")
+
+        return out
+
     def forward(  # type: ignore[override]
         self,
-        *args: Tensor,
-        label: Optional[Tensor] = None,
+        *args: DataType,
         params: Optional[List[ParamItem]] = None,
-        data_keys: Optional[List[Union[str, int, DataKey]]] = None,
-    ) -> Union[Tensor, List[Tensor], Tuple[Union[Tensor, List[Tensor]], Optional[Tensor]]]:
+        data_keys: Optional[Union[List[str], List[int], List[DataKey]]] = None,
+    ) -> Union[DataType, List[DataType]]:
         """Compute multiple tensors simultaneously according to ``self.data_keys``."""
-        if data_keys is None:
-            _data_keys = self.data_keys
-        else:
-            _data_keys = [DataKey.get(inp) for inp in data_keys]
+        self.clear_state()
+
+        self.transform_op.data_keys = self.transform_op.preproc_datakeys(data_keys)
 
-        self._validate_args_datakeys(*args, data_keys=_data_keys)
+        self._validate_args_datakeys(*args, data_keys=self.transform_op.data_keys)
 
-        args = self._arguments_preproc(*args, data_keys=_data_keys)
+        in_args = self._arguments_preproc(*args, data_keys=self.transform_op.data_keys)
 
         if params is None:
             # image data must exist if params is not provided.
-            if DataKey.INPUT in _data_keys:
-                inp = args[_data_keys.index(DataKey.INPUT)]
-                if isinstance(inp, (tuple, list)):
+            if DataKey.INPUT in self.transform_op.data_keys:
+                inp = in_args[self.transform_op.data_keys.index(DataKey.INPUT)]
+                if not isinstance(inp, (Tensor,)):
                     raise ValueError(f"`INPUT` should be a tensor but `{type(inp)}` received.")
                 # A video input shall be BCDHW while an image input shall be BCHW
                 if self.contains_video_sequential or self.contains_3d_augmentation:
                     _, out_shape = self.autofill_dim(inp, dim_range=(3, 5))
                 else:
                     _, out_shape = self.autofill_dim(inp, dim_range=(2, 4))
                 params = self.forward_parameters(out_shape)
             else:
                 raise ValueError("`params` must be provided whilst INPUT is not in data_keys.")
 
-        outputs: List[Optional[Tensor]] = [None] * len(_data_keys)
-
-        self.return_label = self.return_label or label is not None or self.contains_label_operations(params)
-
-        for idx, (arg, dcate) in enumerate(zip(args, _data_keys)):
-            # Forward the param to all input data keys
-            if dcate in self.extra_args:
-                extra_args = self.extra_args[dcate]
-            else:
-                extra_args = {}
-
-            if dcate == DataKey.INPUT:
-                _inp = args[idx]
-
-                _out = super().forward(_inp, label, params=params, extra_args=extra_args)
-                self._transform_matrix = self.get_transformation_matrix(_inp, params=params)
+        outputs: Union[Tensor, List[DataType]] = in_args
+        for param in params:
+            module = self.get_submodule(param.name)
+            outputs = self.transform_op.transform(  # type: ignore
+                *outputs, module=module, param=param, extra_args=self.extra_args
+            )
+            if not isinstance(outputs, (list, tuple)):
+                # Make sure we are unpacking a list whilst post-proc
+                outputs = [outputs]
+            self._update_transform_matrix_by_module(module)
+
+        outputs = self._arguments_postproc(args, outputs, data_keys=self.transform_op.data_keys)  # type: ignore
+        # Restore it back
+        self.transform_op.data_keys = self.data_keys
+
+        self._params = params
+
+        if len(outputs) == 1 and isinstance(outputs, list):
+            return outputs[0]
+
+        return outputs
+
+    def _preproc_boxes(self, arg: DataType, dcate: DataKey) -> Boxes:
+        if DataKey.get(dcate) in [DataKey.BBOX]:
+            mode = "vertices_plus"
+        elif DataKey.get(dcate) in [DataKey.BBOX_XYXY]:
+            mode = "xyxy_plus"
+        elif DataKey.get(dcate) in [DataKey.BBOX_XYWH]:
+            mode = "xywh"
+        else:
+            raise ValueError(f"Unsupported mode `{DataKey.get(dcate).name}`.")
+        if isinstance(arg, (Boxes,)):
+            return arg
+        elif self.contains_video_sequential:
+            arg = cast(Tensor, arg)
+            return VideoBoxes.from_tensor(arg)
+        elif self.contains_3d_augmentation:
+            raise NotImplementedError("3D box handlers are not yet supported.")
+        else:
+            arg = cast(Tensor, arg)
+            return Boxes.from_tensor(arg, mode=mode)
 
-                if self.return_label and isinstance(_out, tuple):
-                    _input, label = _out
-                elif isinstance(_out, Tensor):
-                    _input = _out
-
-                outputs[idx] = _input
-                # NOTE: Skip the rest here.
-                continue
-
-            # Using tensors straight-away
-            if isinstance(arg, (Boxes,)):
-                input = arg.data  # all boxes are in (B, N, 4, 2) format now.
-            else:
-                input = arg
+    def _postproc_boxes(self, in_arg: DataType, out_arg: Boxes, dcate: DataKey) -> Union[Tensor, List[Tensor], Boxes]:
+        if DataKey.get(dcate) in [DataKey.BBOX]:
+            mode = "vertices_plus"
+        elif DataKey.get(dcate) in [DataKey.BBOX_XYXY]:
+            mode = "xyxy_plus"
+        elif DataKey.get(dcate) in [DataKey.BBOX_XYWH]:
+            mode = "xywh"
+        else:
+            raise ValueError(f"Unsupported mode `{DataKey.get(dcate).name}`.")
 
-            for param in params:
-                module = self.get_submodule(param.name)
-                if (
-                    isinstance(module, IntensityAugmentationBase2D)
-                    and dcate in DataKey
-                    and not isinstance(module, RandomErasing)
-                ):
-                    pass  # Do nothing
-                elif isinstance(module, ImageSequential) and module.is_intensity_only() and dcate in DataKey:
-                    pass  # Do nothing
-                elif isinstance(module, VideoSequential) and dcate not in [DataKey.INPUT, DataKey.MASK]:
-                    batch_size: int = input.size(0)
-                    input = input.view(-1, *input.shape[2:])
-                    input, label = ApplyInverse.apply_by_key(input, label, module, param, dcate, extra_args=extra_args)
-                    input = input.view(batch_size, -1, *input.shape[1:])
-                elif isinstance(module, PatchSequential):
-                    raise NotImplementedError("Geometric involved PatchSequential is not supported.")
-                elif (
-                    isinstance(module, (GeometricAugmentationBase2D, ImageSequential, RandomErasing))
-                    and dcate in DataKey
-                ):
-                    input, label = ApplyInverse.apply_by_key(input, label, module, param, dcate, extra_args=extra_args)
-                elif isinstance(module, MixAugmentationBaseV2):
-                    if dcate in [DataKey.BBOX_XYXY, DataKey.BBOX_XYWH]:
-                        dcate = DataKey.BBOX
-                    input = module(input, params=param.data, data_keys=[dcate])
-                elif isinstance(module, (SequentialBase,)):
-                    raise ValueError(f"Unsupported Sequential {module}.")
-                else:
-                    raise NotImplementedError(f"data_key {dcate} is not implemented for {module}.")
+        # TODO: handle 3d scenarios
+        if isinstance(in_arg, (Boxes,)):
+            return out_arg
+        else:
+            return out_arg.to_tensor(mode=mode)
 
-            if isinstance(arg, (Boxes,)):
-                arg._data = input
-                outputs[idx] = arg.to_tensor()
-            else:
-                outputs[idx] = input
-        _outputs = [i for i in outputs if isinstance(i, Tensor)]
-        return self.__packup_output__(_outputs, label)
+    def _preproc_keypoints(self, arg: DataType, dcate: DataKey) -> Keypoints:
+        if self.contains_video_sequential:
+            arg = cast(Union[Tensor, List[Tensor]], arg)
+            return VideoKeypoints.from_tensor(arg)
+        elif self.contains_3d_augmentation:
+            raise NotImplementedError("3D keypoint handlers are not yet supported.")
+        elif isinstance(arg, (Keypoints,)):
+            return arg
+        else:
+            arg = cast(Tensor, arg)
+            # TODO: Add List[Tensor] in the future.
+            return Keypoints.from_tensor(arg)
+
+    def _postproc_keypoint(
+        self, in_arg: DataType, out_arg: Keypoints, dcate: DataKey
+    ) -> Union[Tensor, List[Tensor], Keypoints]:
+        if isinstance(in_arg, (Keypoints,)):
+            return out_arg
+        else:
+            return out_arg.to_tensor()
```

### Comparing `kornia-0.6.9/kornia/augmentation/container/dispatcher.py` & `kornia-0.7.0/kornia/augmentation/container/dispatcher.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 from typing import List, Tuple, Union
 
-import torch.nn as nn
-from torch import Tensor
+from torch import Tensor, nn
 
-from kornia.augmentation.container.augment import AugmentationSequential
+from .augment import AugmentationSequential
 
 
 class ManyToManyAugmentationDispather(nn.Module):
-    """Dispatches different augmentations to different inputs element-wisely.
+    r"""Dispatches different augmentations to different inputs element-wisely.
 
     Args:
-        augmentations: *args: a list of kornia AugmentationSequential modules.
+        augmentations: a list or a sequence of kornia AugmentationSequential modules.
 
     Examples:
         >>> import torch
         >>> input_1, input_2 = torch.randn(2, 3, 5, 6), torch.randn(2, 3, 5, 6)
         >>> mask_1, mask_2 = torch.ones(2, 3, 5, 6), torch.ones(2, 3, 5, 6)
         >>> aug_list = ManyToManyAugmentationDispather(
         ...     AugmentationSequential(
@@ -43,40 +42,40 @@
         return True
 
     def forward(self, *input: Union[List[Tensor], List[Tuple[Tensor]]]) -> Union[List[Tensor], List[Tuple[Tensor]]]:
         return [aug(*inp) for inp, aug in zip(input, self.augmentations)]
 
 
 class ManyToOneAugmentationDispather(nn.Module):
-    """Dispatches different augmentations to a single input and returns a list.
+    r"""Dispatches different augmentations to a single input and returns a list.
 
     Same `datakeys` must be applied across different augmentations. By default, with input
     (image, mask), the augmentations must not mess it as (mask, image) to avoid unexpected
     errors. This check can be cancelled with `strict=False` if needed.
 
     Args:
-        augmentations: *args: a list of kornia AugmentationSequential modules.
+        augmentations: a list or a sequence of kornia AugmentationSequential modules.
 
     Examples:
         >>> import torch
         >>> input = torch.randn(2, 3, 5, 6)
         >>> mask = torch.ones(2, 3, 5, 6)
-        >>> aug_list = ManyToManyAugmentationDispather(
+        >>> aug_list = ManyToOneAugmentationDispather(
         ...     AugmentationSequential(
         ...         kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...         kornia.augmentation.RandomAffine(360, p=1.0),
         ...         data_keys=["input", "mask",],
         ...     ),
         ...     AugmentationSequential(
         ...         kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...         kornia.augmentation.RandomAffine(360, p=1.0),
         ...         data_keys=["input", "mask",],
         ...     )
         ... )
-        >>> output = aug_list((input, mask))
+        >>> output = aug_list(input, mask)
     """
 
     def __init__(self, *augmentations: AugmentationSequential, strict: bool = True) -> None:
         super().__init__()
         self.strict = strict
         self._check_consistency(*augmentations)
         self.augmentations = augmentations
```

### Comparing `kornia-0.6.9/kornia/augmentation/container/image.py` & `kornia-0.7.0/kornia/augmentation/container/image.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,24 @@
-from itertools import zip_longest
-from typing import Any, Dict, Iterator, List, Optional, Tuple, Type, Union, cast
+from typing import Any, Dict, Iterator, List, Optional, Tuple, Union, cast
 
 import torch
 
-import kornia
-from kornia.augmentation import (
-    GeometricAugmentationBase2D,
-    IntensityAugmentationBase2D,
-    MixAugmentationBaseV2,
-    RandomCrop,
-)
+import kornia.augmentation as K
 from kornia.augmentation.base import _AugmentationBase
-from kornia.augmentation.container.base import ParamItem, SequentialBase
-from kornia.augmentation.container.utils import ApplyInverseInterface, InputApplyInverse
 from kornia.augmentation.utils import override_parameters
 from kornia.core import Module, Tensor, as_tensor
+from kornia.utils import eye_like
+
+from .base import ImageSequentialBase
+from .params import ParamItem
 
 __all__ = ["ImageSequential"]
 
 
-class ImageSequential(SequentialBase):
+class ImageSequential(ImageSequentialBase):
     r"""Sequential for creating kornia image processing pipeline.
 
     Args:
         *args : a list of kornia augmentation and image operation modules.
         same_on_batch: apply the same transformation across the batch.
             If None, it will not overwrite the function-wise settings.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
@@ -41,35 +36,33 @@
     .. note::
         Transformation matrix returned only considers the transformation applied in ``kornia.augmentation`` module.
         Those transformations in ``kornia.geometry`` will not be taken into account.
 
     Examples:
         >>> _ = torch.manual_seed(77)
         >>> import kornia
-        >>> input, label = torch.randn(2, 3, 5, 6), torch.tensor([0, 1])
+        >>> input = torch.randn(2, 3, 5, 6)
         >>> aug_list = ImageSequential(
         ...     kornia.color.BgrToRgb(),
         ...     kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...     kornia.filters.MedianBlur((3, 3)),
         ...     kornia.augmentation.RandomAffine(360, p=1.0),
         ...     kornia.enhance.Invert(),
         ...     kornia.augmentation.RandomMixUpV2(p=1.0),
         ...     same_on_batch=True,
         ...     random_apply=10,
         ... )
-        >>> out, lab = aug_list(input, label=label)
-        >>> lab
-        tensor([0, 1])
+        >>> out = aug_list(input)
         >>> out.shape
         torch.Size([2, 3, 5, 6])
 
         Reproduce with provided params.
-        >>> out2, lab2 = aug_list(input, label=label, params=aug_list._params)
-        >>> torch.equal(out, out2), torch.equal(lab, lab2)
-        (True, True)
+        >>> out2 = aug_list(input, params=aug_list._params)
+        >>> torch.equal(out, out2)
+        True
 
     Perform ``OneOf`` transformation with ``random_apply=1`` and ``random_apply_weights`` in ``ImageSequential``.
 
         >>> import kornia
         >>> input = torch.randn(2, 3, 5, 6)
         >>> aug_list = ImageSequential(
         ...     kornia.color.BgrToRgb(),
@@ -98,16 +91,14 @@
         self.random_apply = self._read_random_apply(random_apply, len(args))
         if random_apply_weights is not None and len(random_apply_weights) != len(self):
             raise ValueError(
                 "The length of `random_apply_weights` must be as same as the number of operations."
                 f"Got {len(random_apply_weights)} and {len(self)}."
             )
         self.random_apply_weights = as_tensor(random_apply_weights or torch.ones((len(self),)))
-        self.return_label: Optional[bool] = None
-        self.apply_inverse_func: Type[ApplyInverseInterface] = InputApplyInverse
         self.if_unsupported_ops = if_unsupported_ops
 
     def _read_random_apply(
         self, random_apply: Union[int, bool, Tuple[int, int]], max_length: int
     ) -> Union[Tuple[int, int], bool]:
         """Process the scenarios for random apply."""
         if isinstance(random_apply, (bool,)) and random_apply is False:
@@ -135,14 +126,17 @@
         ):
             raise AssertionError(f"Expect a tuple of (int, int). Got {random_apply}.")
         return random_apply
 
     def get_random_forward_sequence(self, with_mix: bool = True) -> Tuple[Iterator[Tuple[str, Module]], bool]:
         """Get a forward sequence when random apply is in need.
 
+        Args:
+            with_mix: if to require a mix augmentation for the sequence.
+
         Note:
             Mix augmentations (e.g. RandomMixUp) will be only applied once even in a random forward.
         """
         if isinstance(self.random_apply, tuple):
             num_samples = int(torch.randint(*self.random_apply, (1,)).item())
         else:
             raise TypeError(f'random apply should be a tuple. Gotcha {type(self.random_apply)}')
@@ -171,82 +165,52 @@
 
     def get_mix_augmentation_indices(self, named_modules: Iterator[Tuple[str, Module]]) -> List[int]:
         """Get all the mix augmentations since they are label-involved.
 
         Special operations needed for label-involved augmentations.
         """
         # NOTE: MixV2 will not be a special op in the future.
-        return [idx for idx, (_, child) in enumerate(named_modules) if isinstance(child, MixAugmentationBaseV2)]
+        return [idx for idx, (_, child) in enumerate(named_modules) if isinstance(child, K.MixAugmentationBaseV2)]
 
     def get_forward_sequence(self, params: Optional[List[ParamItem]] = None) -> Iterator[Tuple[str, Module]]:
         if params is None:
             # Mix augmentation can only be applied once per forward
             mix_indices = self.get_mix_augmentation_indices(self.named_children())
 
             if self.random_apply:
                 return self.get_random_forward_sequence()[0]
 
             if len(mix_indices) > 1:
                 raise ValueError(
                     "Multiple mix augmentation is prohibited without enabling random_apply."
-                    f"Detected {len(mix_indices)}."
+                    f"Detected {len(mix_indices)} mix augmentations."
                 )
 
             return self.named_children()
 
         return self.get_children_by_params(params)
 
-    def apply_to_input(
-        self,
-        input: Tensor,
-        label: Optional[Tensor],
-        module: Optional[Module],
-        param: ParamItem,
-        extra_args: Dict[str, Any],
-    ) -> Tuple[Tensor, Optional[Tensor]]:
-        if module is None:
-            module = self.get_submodule(param.name)
-        return self.apply_inverse_func.apply_trans(input, label, module, param, extra_args)
-
     def forward_parameters(self, batch_shape: torch.Size) -> List[ParamItem]:
         named_modules: Iterator[Tuple[str, Module]] = self.get_forward_sequence()
 
         params: List[ParamItem] = []
         mod_param: Union[Dict[str, Tensor], List[ParamItem]]
         for name, module in named_modules:
-            if isinstance(module, RandomCrop):
-                mod_param = module.forward_parameters_precrop(batch_shape)
-                param = ParamItem(name, mod_param)
-            elif isinstance(module, (_AugmentationBase, MixAugmentationBaseV2, ImageSequential)):
+            if isinstance(module, (_AugmentationBase, K.MixAugmentationBaseV2, ImageSequentialBase)):
                 mod_param = module.forward_parameters(batch_shape)
                 param = ParamItem(name, mod_param)
             else:
                 param = ParamItem(name, None)
             batch_shape = _get_new_batch_shape(param, batch_shape)
             params.append(param)
         return params
 
-    def contains_label_operations(self, params: List[ParamItem]) -> bool:
-        """Check if current sequential contains label-involved operations like MixUp."""
-        for param in params:
-            if param.name.startswith("RandomMixUp_") or param.name.startswith("RandomCutMix_"):
-                return True
-        return False
-
-    def __packup_output__(
-        self, output: Tensor, label: Optional[Tensor] = None
-    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
-        if self.return_label:
-            # Implicitly indicating the label cannot be optional since there is a mix aug
-            return output, label
-        return output
-
-    def identity_matrix(self, input) -> Tensor:
+    def identity_matrix(self, input: Tensor) -> Tensor:
         """Return identity matrix."""
-        return kornia.eye_like(3, input)
+        return eye_like(3, input)
 
     def get_transformation_matrix(
         self,
         input: Tensor,
         params: Optional[List[ParamItem]] = None,
         recompute: bool = False,
         extra_args: Dict[str, Any] = {},
@@ -262,50 +226,47 @@
         if params is None:
             raise NotImplementedError("requires params to be provided.")
         named_modules: Iterator[Tuple[str, Module]] = self.get_forward_sequence(params)
 
         # Define as 1 for broadcasting
         res_mat: Optional[Tensor] = None
         for (_, module), param in zip(named_modules, params if params is not None else []):
-            if (
-                isinstance(module, (_AugmentationBase,))
-                and not isinstance(module, MixAugmentationBaseV2)
-                and isinstance(param.data, dict)
-            ):
-                to_apply = param.data['batch_prob']
+            if isinstance(module, (K.GeometricAugmentationBase2D,)) and isinstance(param.data, dict):
                 ori_shape = input.shape
                 try:
                     input = module.transform_tensor(input)
                 except ValueError:
                     # Ignore error for 5-dim video
                     pass
                 # Standardize shape
                 if recompute:
-                    mat: Tensor = self.identity_matrix(input)
                     flags = override_parameters(module.flags, extra_args, in_place=False)
-                    mat[to_apply] = module.compute_transformation(input[to_apply], param.data, flags)
-                else:
+                    mat = module.generate_transformation_matrix(input, param.data, flags)
+                elif module._transform_matrix is not None:
                     mat = as_tensor(module._transform_matrix, device=input.device, dtype=input.dtype)
+                else:
+                    raise RuntimeError(f"{module}._transform_matrix is None while `recompute=False`.")
                 res_mat = mat if res_mat is None else mat @ res_mat
                 input = module.transform_output_tensor(input, ori_shape)
                 if module.keepdim and ori_shape != input.shape:
                     res_mat = res_mat.squeeze()
-            elif isinstance(module, (ImageSequential,)):
+            elif isinstance(module, (ImageSequentialBase,)):
                 # If not augmentationSequential
-                if isinstance(module, (kornia.augmentation.AugmentationSequential,)) and not recompute:
+                if isinstance(module, (K.AugmentationSequential,)) and not recompute:
                     mat = as_tensor(module._transform_matrix, device=input.device, dtype=input.dtype)
                 else:
                     maybe_param_data = cast(Optional[List[ParamItem]], param.data)
                     _mat = module.get_transformation_matrix(
                         input, maybe_param_data, recompute=recompute, extra_args=extra_args
                     )
                     mat = module.identity_matrix(input) if _mat is None else _mat
                 res_mat = mat if res_mat is None else mat @ res_mat
         return res_mat
 
+    # TODO: Make this as a class property to avoid running every time.
     def is_intensity_only(self, strict: bool = True) -> bool:
         """Check if all transformations are intensity-based.
 
         Args:
             strict: if strict is False, it will allow non-augmentation Modules to be passed.
                 e.g. `kornia.enhance.AdjustBrightness` will be recognized as non-intensity module
                 if strict is set to True.
@@ -313,95 +274,36 @@
         Note: patch processing would break the continuity of labels (e.g. bbounding boxes, masks).
         """
         for arg in self.children():
             if isinstance(arg, (ImageSequential,)) and not arg.is_intensity_only(strict):
                 return False
             elif isinstance(arg, (ImageSequential,)):
                 pass
-            elif isinstance(arg, IntensityAugmentationBase2D):
+            elif isinstance(arg, K.IntensityAugmentationBase2D):
                 pass
             elif strict:
                 # disallow non-registered ops if in strict mode
                 # TODO: add an ops register module
                 return False
         return True
 
-    def inverse(
-        self, input: Tensor, params: Optional[List[ParamItem]] = None, extra_args: Dict[str, Any] = {}
-    ) -> Tensor:
-        """Inverse transformation.
-
-        Used to inverse a tensor according to the performed transformation by a forward pass, or with respect to
-        provided parameters.
-        """
-        if params is None:
-            if self._params is None:
-                raise ValueError(
-                    "No parameters available for inversing, please run a forward pass first "
-                    "or passing valid params into this function."
-                )
-            params = self._params
-
-        for (name, module), param in zip_longest(list(self.get_forward_sequence(params))[::-1], params[::-1]):
-            if isinstance(module, (_AugmentationBase, ImageSequential)):
-                _mb: List[ParamItem] = [p for p in params if name in p]
-                maybe_param = _mb if len(_mb) > 0 else [param]
-
-            if isinstance(module, IntensityAugmentationBase2D):
-                pass  # Do nothing
-            elif isinstance(module, ImageSequential) and module.is_intensity_only():
-                pass  # Do nothing
-            elif isinstance(module, ImageSequential) and isinstance(maybe_param, ParamItem):
-                input = module.inverse(input, maybe_param, extra_args=extra_args)
-            elif isinstance(module, (GeometricAugmentationBase2D,)):
-                input = self.apply_inverse_func.inverse(input, module, param, extra_args=extra_args)
-            else:
-                pass
-                # raise NotImplementedError(f"`inverse` is not implemented for {module}.")
-
-        return input
-
-    def forward(
-        self,
-        input: Tensor,
-        label: Optional[Tensor] = None,
-        params: Optional[List[ParamItem]] = None,
-        extra_args: Dict[str, Any] = {},
-    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
-        self.clear_state()
-        if params is None:
-            inp = input
-            _, out_shape = self.autofill_dim(inp, dim_range=(2, 4))
-            params = self.forward_parameters(out_shape)
-        if self.return_label is None:
-            self.return_label = label is not None or self.contains_label_operations(params)
-        for param in params:
-            module = self.get_submodule(param.name)
-            input, label = self.apply_to_input(input, label, module, param=param, extra_args=extra_args)
-            if isinstance(module, (_AugmentationBase, MixAugmentationBaseV2, SequentialBase)):
-                param = ParamItem(param.name, module._params)
-            else:
-                param = ParamItem(param.name, None)
-            self.update_params(param)
-        return self.__packup_output__(input, label)
-
 
 def _get_new_batch_shape(param: ParamItem, batch_shape: torch.Size) -> torch.Size:
     """Get the new batch shape if the augmentation changes the image size.
 
     Note:
        Augmentations that change the image size must provide the parameter `output_size`.
     """
     if param.data is None:
         return batch_shape
     if isinstance(param.data, list):
         for p in param.data:
             batch_shape = _get_new_batch_shape(p, batch_shape)
     elif 'output_size' in param.data:
-        if not param.data['batch_prob'][0]:
+        if not (param.data['batch_prob'] > 0.5)[0]:
             # Augmentations that change the image size must be applied equally to all elements in batch.
             # If the augmentation is not applied, return the same batch shape.
             return batch_shape
         new_batch_shape = list(batch_shape)
         new_batch_shape[-2:] = param.data['output_size'][0]
         batch_shape = torch.Size(new_batch_shape)
     return batch_shape
```

### Comparing `kornia-0.6.9/kornia/augmentation/container/patch.py` & `kornia-0.7.0/kornia/augmentation/container/patch.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 from itertools import cycle, islice
-from typing import Any, Dict, Iterator, List, NamedTuple, Optional, Tuple, Union
+from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
 
 import torch
 
-from kornia.augmentation import MixAugmentationBaseV2
+import kornia.augmentation as K
 from kornia.augmentation.base import _AugmentationBase
-from kornia.augmentation.container.base import SequentialBase
-from kornia.augmentation.container.image import ImageSequential, ParamItem
 from kornia.contrib.extract_patches import extract_tensor_patches
 from kornia.core import Module, Tensor, concatenate
 from kornia.core import pad as fpad
+from kornia.geometry.boxes import Boxes
+from kornia.geometry.keypoints import Keypoints
 
-__all__ = ["PatchSequential"]
-
+from .base import SequentialBase
+from .image import ImageSequential
+from .ops import InputSequentialOps
+from .params import ParamItem, PatchParamItem
 
-class PatchParamItem(NamedTuple):
-    indices: List[int]
-    param: ParamItem
+__all__ = ["PatchSequential"]
 
 
 class PatchSequential(ImageSequential):
     r"""Container for performing patch-level image data augmentation.
 
-    .. image:: https://kornia-tutorials.readthedocs.io/en/latest/_images/data_patch_sequential_7_0.png
+    .. image:: _static/img/PatchSequential.png
 
     PatchSequential breaks input images into patches by a given grid size, which will be resembled back
     afterwards.
 
     Different image processing and augmentation methods will be performed on each patch region as
     in :cite:`lin2021patch`.
 
@@ -146,20 +146,15 @@
             random_apply_weights=random_apply_weights,
         )
         if padding not in ("same", "valid"):
             raise ValueError(f"`padding` must be either `same` or `valid`. Got {padding}.")
         self.grid_size = grid_size
         self.padding = padding
         self.patchwise_apply = patchwise_apply
-
-    def contains_label_operations(self, params: List[PatchParamItem]) -> bool:  # type: ignore[override]
-        for param in params:
-            if param.param.name.startswith("RandomMixUp") or param.param.name.startswith("RandomCutMix"):
-                return True
-        return False
+        self._params: Optional[List[PatchParamItem]]  # type: ignore[assignment]
 
     def compute_padding(
         self, input: Tensor, padding: str, grid_size: Optional[Tuple[int, int]] = None
     ) -> Tuple[int, int, int, int]:
         if grid_size is None:
             grid_size = self.grid_size
         if padding == "valid":
@@ -270,144 +265,142 @@
         if not self.same_on_batch and self.random_apply:
             # diff_on_batch and random_apply => patch-wise augmentation
             with_mix = False
             for i in range(batch_shape[0]):
                 seq, mix_added = self.get_random_forward_sequence(with_mix=with_mix)
                 with_mix = mix_added
                 for s in seq:
-                    if isinstance(s[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
+                    if isinstance(s[1], (_AugmentationBase, SequentialBase, K.MixAugmentationBaseV2)):
                         yield ParamItem(s[0], s[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                     else:
                         yield ParamItem(s[0], None), i
         elif not self.same_on_batch and not self.random_apply:
             for i, nchild in enumerate(self.named_children()):
-                if isinstance(nchild[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
+                if isinstance(nchild[1], (_AugmentationBase, SequentialBase, K.MixAugmentationBaseV2)):
                     yield ParamItem(nchild[0], nchild[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                 else:
                     yield ParamItem(nchild[0], None), i
         elif not self.random_apply:
             # same_on_batch + not random_apply => location-wise augmentation
             for i, nchild in enumerate(islice(cycle(self.named_children()), batch_shape[0])):
-                if isinstance(nchild[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
+                if isinstance(nchild[1], (_AugmentationBase, SequentialBase, K.MixAugmentationBaseV2)):
                     yield ParamItem(nchild[0], nchild[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                 else:
                     yield ParamItem(nchild[0], None), i
         else:
             # same_on_batch + random_apply => location-wise augmentation
             with_mix = False
             for i in range(batch_shape[0]):
                 seq, mix_added = self.get_random_forward_sequence(with_mix=with_mix)
                 with_mix = mix_added
                 for s in seq:
-                    if isinstance(s[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
+                    if isinstance(s[1], (_AugmentationBase, SequentialBase, K.MixAugmentationBaseV2)):
                         yield ParamItem(s[0], s[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                     else:
                         yield ParamItem(s[0], None), i
 
-    def apply_by_param(
-        self, input: Tensor, label: Optional[Tensor], params: PatchParamItem
-    ) -> Tuple[Tensor, Optional[Tensor], PatchParamItem]:
-        _input: Tensor
+    def forward_by_params(self, input: Tensor, params: List[PatchParamItem]) -> Tensor:
         in_shape = input.shape
-        _input = input[params.indices]
+        input = input.reshape(-1, *in_shape[-3:])
 
-        _label: Optional[Tensor]
-        if label is not None:
-            _label = label[params.indices]
-        else:
-            _label = label
+        for patch_param in params:
+            # input, out_param = self.apply_by_param(input, params=patch_param)
+            module = self.get_submodule(patch_param.param.name)
+            _input = input[patch_param.indices]
+            output = InputSequentialOps.transform(_input, module, patch_param.param, extra_args={})
+            input[patch_param.indices] = output
 
-        module = self.get_submodule(params.param.name)
-        output, out_label = self.apply_to_input(_input, _label, module, params.param, extra_args={})
+        return input.reshape(in_shape)
 
-        if isinstance(module, (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
-            out_param = ParamItem(params.param.name, module._params)
-        else:
-            out_param = ParamItem(params.param.name, None)
+    def transform_inputs(  # type: ignore[override]
+        self, input: Tensor, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Tensor:
+        pad = self.compute_padding(input, self.padding)
+        input = self.extract_patches(input, self.grid_size, pad)
+        input = self.forward_by_params(input, params)
+        input = self.restore_from_patches(input, self.grid_size, pad=pad)
 
-        if isinstance(output, (tuple,)) and isinstance(input, (tuple,)):
-            input[0][params.indices] = output[0]
-            input[1][params.indices] = output[1]
-        elif isinstance(output, (tuple,)) and not isinstance(input, (tuple,)):
-            input[params.indices] = output[0]
-            input = (input, output[1])
-        elif not isinstance(output, (tuple,)) and isinstance(input, (tuple,)):
-            input[0][params.indices] = output
-        elif not isinstance(output, (tuple,)) and not isinstance(input, (tuple,)):
-            input[params.indices] = output
-
-        # TODO: this label handling is naive that may not be able to handle complex cases.
-        _label = None
-        if label is not None and out_label is not None:
-            if len(out_label.shape) == 1:
-                # Weird the mypy error though it is as same as in the next block
-                _label = torch.ones(in_shape[0] * in_shape[1], device=out_label.device, dtype=out_label.dtype) * -1
-                _label = label
-            else:
-                _label = (
-                    torch.ones(in_shape[0], *out_label.shape[1:], device=out_label.device, dtype=out_label.dtype) * -1
-                )
-                _label[:, 0] = label
-            _label[params.indices] = out_label
-        elif label is None and out_label is not None:
-            if len(out_label.shape) == 1:
-                _label = torch.ones(in_shape[0] * in_shape[1], device=out_label.device, dtype=out_label.dtype) * -1
-            else:
-                _label = (
-                    torch.ones(in_shape[0], *out_label.shape[1:], device=out_label.device, dtype=out_label.dtype) * -1
-                )
-            _label[params.indices] = out_label
+        return input
 
-        return input, _label, PatchParamItem(params.indices, param=out_param)
+    def inverse_inputs(  # type: ignore[override]
+        self, input: Tensor, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Tensor:
+        if self.is_intensity_only():
+            return input
 
-    def forward_by_params(
-        self, input: Tensor, label: Optional[Tensor], params: List[PatchParamItem]
-    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
-        _input: Tensor
-        in_shape = input.shape
-        _input = input.reshape(-1, *in_shape[-3:])
+        raise NotImplementedError("PatchSequential inverse cannot be used with geometric transformations.")
 
-        if label is not None:
-            label = concatenate([label] * in_shape[1], 0)
+    def transform_masks(  # type: ignore[override]
+        self, input: Tensor, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Tensor:
+        if self.is_intensity_only():
+            return input
 
-        self.clear_state()
-        for patch_param in params:
-            _input, label, out_param = self.apply_by_param(_input, label, params=patch_param)
-            self.update_params(out_param)
-        _input = _input.reshape(in_shape)
-        return _input, label
+        raise NotImplementedError("PatchSequential for boxes cannot be used with geometric transformations.")
 
-    def inverse(
-        self, input: Tensor, params: Optional[List[ParamItem]] = None, extra_args: Dict[str, Any] = {}
+    def inverse_masks(  # type: ignore[override]
+        self, input: Tensor, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Tensor:
+        if self.is_intensity_only():
+            return input
+
+        raise NotImplementedError("PatchSequential inverse cannot be used with geometric transformations.")
+
+    def transform_boxes(  # type: ignore[override]
+        self, input: Boxes, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Boxes:
+        if self.is_intensity_only():
+            return input
+
+        raise NotImplementedError("PatchSequential for boxes cannot be used with geometric transformations.")
+
+    def inverse_boxes(  # type: ignore[override]
+        self, input: Boxes, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Boxes:
+        if self.is_intensity_only():
+            return input
+
+        raise NotImplementedError("PatchSequential inverse cannot be used with geometric transformations.")
+
+    def transform_keypoints(  # type: ignore[override]
+        self, input: Keypoints, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Keypoints:
+        if self.is_intensity_only():
+            return input
+
+        raise NotImplementedError("PatchSequential for keypoints cannot be used with geometric transformations.")
+
+    def inverse_keypoints(  # type: ignore[override]
+        self, input: Keypoints, params: List[PatchParamItem], extra_args: Dict[str, Any] = {}
+    ) -> Keypoints:
+        if self.is_intensity_only():
+            return input
+
+        raise NotImplementedError("PatchSequential inverse cannot be used with geometric transformations.")
+
+    def inverse(  # type: ignore[override]
+        self, input: Tensor, params: Optional[List[PatchParamItem]] = None, extra_args: Dict[str, Any] = {}
     ) -> Tensor:
         """Inverse transformation.
 
         Used to inverse a tensor according to the performed transformation by a forward pass, or with respect to
         provided parameters.
         """
         if self.is_intensity_only():
             return input
 
         raise NotImplementedError("PatchSequential inverse cannot be used with geometric transformations.")
 
-    def forward(  # type: ignore[override]
-        self, input: Tensor, label: Optional[Tensor] = None, params: Optional[List[PatchParamItem]] = None
-    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
+    def forward(self, input: Tensor, params: Optional[List[PatchParamItem]] = None) -> Tensor:  # type: ignore[override]
         """Input transformation will be returned if input is a tuple."""
         # BCHW -> B(patch)CHW
         if isinstance(input, (tuple,)):
             raise ValueError("tuple input is not currently supported.")
-        _input: Tensor
-
-        pad = self.compute_padding(input, self.padding)
-        input = self.extract_patches(input, self.grid_size, pad)
 
         if params is None:
             params = self.forward_parameters(input.shape)
 
-        _input, label = self.forward_by_params(input, label, params)
-
-        _input = self.restore_from_patches(_input, self.grid_size, pad=pad)
+        output = self.transform_inputs(input, params=params)
 
-        self.return_label = label is not None or self.contains_label_operations(params)
+        self._params = params
 
-        return self.__packup_output__(_input, label)
+        return output
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/__init__.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 from kornia.augmentation.random_generator._2d.affine import *
 from kornia.augmentation.random_generator._2d.color_jiggle import *
 from kornia.augmentation.random_generator._2d.color_jitter import *
 from kornia.augmentation.random_generator._2d.crop import *
 from kornia.augmentation.random_generator._2d.cutmix import *
+from kornia.augmentation.random_generator._2d.gaussian_blur import *
 from kornia.augmentation.random_generator._2d.jigsaw import *
 from kornia.augmentation.random_generator._2d.mixup import *
 from kornia.augmentation.random_generator._2d.mosaic import *
 from kornia.augmentation.random_generator._2d.motion_blur import *
 from kornia.augmentation.random_generator._2d.perspective import *
 from kornia.augmentation.random_generator._2d.plain_uniform import *
 from kornia.augmentation.random_generator._2d.planckian_jitter import *
 from kornia.augmentation.random_generator._2d.posterize import *
 from kornia.augmentation.random_generator._2d.probability import *
+from kornia.augmentation.random_generator._2d.random_rain import *
 from kornia.augmentation.random_generator._2d.rectangle_earase import *
 from kornia.augmentation.random_generator._2d.resize import *
+from kornia.augmentation.random_generator._2d.shear import *
+from kornia.augmentation.random_generator._2d.translate import *
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/affine.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/affine.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,14 +4,16 @@
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check, _range_bound
 from kornia.core import Tensor, as_tensor, concatenate, stack, tensor, zeros
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["AffineGenerator"]
+
 
 class AffineGenerator(RandomGeneratorBase):
     r"""Get parameters for ``affine`` for a random affine transform.
 
     Args:
         degrees: Range of degrees to select from like (min, max).
         translate: tuple of maximum absolute fraction for horizontal
@@ -30,16 +32,16 @@
 
     Returns:
         A dict of parameters to be passed for transformation.
             - translations (Tensor): element-wise translations with a shape of (B, 2).
             - center (Tensor): element-wise center with a shape of (B, 2).
             - scale (Tensor): element-wise scales with a shape of (B, 2).
             - angle (Tensor): element-wise rotation angles with a shape of (B,).
-            - sx (Tensor): element-wise x-axis shears with a shape of (B,).
-            - sy (Tensor): element-wise y-axis shears with a shape of (B,).
+            - shear_x (Tensor): element-wise x-axis shears with a shape of (B,).
+            - shear_y (Tensor): element-wise y-axis shears with a shape of (B,).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
@@ -127,15 +129,15 @@
         self.translate_x_sampler = translate_x_sampler
         self.translate_y_sampler = translate_y_sampler
         self.scale_2_sampler = scale_2_sampler
         self.scale_4_sampler = scale_4_sampler
         self.shear_x_sampler = shear_x_sampler
         self.shear_y_sampler = shear_y_sampler
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         height = batch_shape[-2]
         width = batch_shape[-1]
 
         _device, _dtype = _extract_device_dtype([self.degrees, self.translate, self.scale, self.shear])
         _common_param_check(batch_size, same_on_batch)
         if not (isinstance(width, (int,)) and isinstance(height, (int,)) and width > 0 and height > 0):
@@ -169,10 +171,18 @@
 
         if self.shear_x_sampler is not None and self.shear_y_sampler is not None:
             sx = _adapted_rsampling((batch_size,), self.shear_x_sampler, same_on_batch)
             sy = _adapted_rsampling((batch_size,), self.shear_y_sampler, same_on_batch)
             sx = sx.to(device=_device, dtype=_dtype)
             sy = sy.to(device=_device, dtype=_dtype)
         else:
-            sx = sy = tensor([0] * batch_size, device=_device, dtype=_dtype)
+            sx = tensor([0] * batch_size, device=_device, dtype=_dtype)
+            sy = tensor([0] * batch_size, device=_device, dtype=_dtype)
 
-        return dict(translations=translations, center=center, scale=_scale, angle=angle, sx=sx, sy=sy)
+        return {
+            "translations": translations,
+            "center": center,
+            "scale": _scale,
+            "angle": angle,
+            "shear_x": sx,
+            "shear_y": sy,
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jiggle.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/color_jiggle.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,14 +5,16 @@
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check, _range_bound
 from kornia.core import Tensor
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["ColorJiggleGenerator"]
+
 
 class ColorJiggleGenerator(RandomGeneratorBase):
     r"""Generate random color jiter parameters for a batch of images following OpenCV.
 
     Args:
         brightness: The brightness factor to apply.
         contrast: The contrast factor to apply.
@@ -67,22 +69,22 @@
 
         self.brightness_sampler = Uniform(brightness[0], brightness[1], validate_args=False)
         self.contrast_sampler = Uniform(contrast[0], contrast[1], validate_args=False)
         self.hue_sampler = Uniform(hue[0], hue[1], validate_args=False)
         self.saturation_sampler = Uniform(saturation[0], saturation[1], validate_args=False)
         self.randperm = partial(torch.randperm, device=device, dtype=dtype)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.brightness, self.contrast, self.hue, self.saturation])
         brightness_factor = _adapted_rsampling((batch_size,), self.brightness_sampler, same_on_batch)
         contrast_factor = _adapted_rsampling((batch_size,), self.contrast_sampler, same_on_batch)
         hue_factor = _adapted_rsampling((batch_size,), self.hue_sampler, same_on_batch)
         saturation_factor = _adapted_rsampling((batch_size,), self.saturation_sampler, same_on_batch)
-        return dict(
-            brightness_factor=brightness_factor.to(device=_device, dtype=_dtype),
-            contrast_factor=contrast_factor.to(device=_device, dtype=_dtype),
-            hue_factor=hue_factor.to(device=_device, dtype=_dtype),
-            saturation_factor=saturation_factor.to(device=_device, dtype=_dtype),
-            order=self.randperm(4).to(device=_device, dtype=_dtype).long(),
-        )
+        return {
+            "brightness_factor": brightness_factor.to(device=_device, dtype=_dtype),
+            "contrast_factor": contrast_factor.to(device=_device, dtype=_dtype),
+            "hue_factor": hue_factor.to(device=_device, dtype=_dtype),
+            "saturation_factor": saturation_factor.to(device=_device, dtype=_dtype),
+            "order": self.randperm(4).to(device=_device, dtype=_dtype).long(),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jitter.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/color_jitter.py`

 * *Files 5% similar despite different names*

```diff
@@ -5,14 +5,16 @@
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check, _range_bound
 from kornia.core import Tensor
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["ColorJitterGenerator"]
+
 
 class ColorJitterGenerator(RandomGeneratorBase):
     r"""Generate random color jiter parameters for a batch of images following Pil.
 
     This implementation is for maintaining compatibility with torchvision. It does not
     follow the color theory and is not be actively maintained. Prefer using
     :func:`kornia.augmentation.ColorJiggleGenerator`
@@ -71,22 +73,22 @@
 
         self.brightness_sampler = Uniform(brightness[0], brightness[1], validate_args=False)
         self.contrast_sampler = Uniform(contrast[0], contrast[1], validate_args=False)
         self.hue_sampler = Uniform(hue[0], hue[1], validate_args=False)
         self.saturation_sampler = Uniform(saturation[0], saturation[1], validate_args=False)
         self.randperm = partial(torch.randperm, device=device, dtype=dtype)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.brightness, self.contrast, self.hue, self.saturation])
         brightness_factor = _adapted_rsampling((batch_size,), self.brightness_sampler, same_on_batch)
         contrast_factor = _adapted_rsampling((batch_size,), self.contrast_sampler, same_on_batch)
         hue_factor = _adapted_rsampling((batch_size,), self.hue_sampler, same_on_batch)
         saturation_factor = _adapted_rsampling((batch_size,), self.saturation_sampler, same_on_batch)
-        return dict(
-            brightness_factor=brightness_factor.to(device=_device, dtype=_dtype),
-            contrast_factor=contrast_factor.to(device=_device, dtype=_dtype),
-            hue_factor=hue_factor.to(device=_device, dtype=_dtype),
-            saturation_factor=saturation_factor.to(device=_device, dtype=_dtype),
-            order=self.randperm(4).to(device=_device, dtype=_dtype).long(),
-        )
+        return {
+            "brightness_factor": brightness_factor.to(device=_device, dtype=_dtype),
+            "contrast_factor": contrast_factor.to(device=_device, dtype=_dtype),
+            "hue_factor": hue_factor.to(device=_device, dtype=_dtype),
+            "saturation_factor": saturation_factor.to(device=_device, dtype=_dtype),
+            "order": self.randperm(4).to(device=_device, dtype=_dtype).long(),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/crop.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/crop.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,14 +5,16 @@
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check
 from kornia.core import Device, Tensor, tensor, where, zeros
 from kornia.geometry.bbox import bbox_generator
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["CropGenerator", "ResizedCropGenerator", "center_crop_generator"]
+
 
 class CropGenerator(RandomGeneratorBase):
     r"""Get parameters for ```crop``` transformation for crop transform.
 
     Args:
         size (tuple): Desired size of the crop operation, like (h, w).
             If tensor, it must be (B, 2).
@@ -39,23 +41,24 @@
         if self.resize_to is not None:
             repr += f", resize_to={self.resize_to}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         self.rand_sampler = Uniform(tensor(0.0, device=device, dtype=dtype), tensor(1.0, device=device, dtype=dtype))
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.size if isinstance(self.size, Tensor) else None])
 
         if batch_size == 0:
-            return dict(
-                src=zeros([0, 4, 2], device=_device, dtype=_dtype), dst=zeros([0, 4, 2], device=_device, dtype=_dtype)
-            )
+            return {
+                "src": zeros([0, 4, 2], device=_device, dtype=_dtype),
+                "dst": zeros([0, 4, 2], device=_device, dtype=_dtype),
+            }
 
         input_size = (batch_shape[-2], batch_shape[-1])
         if not isinstance(self.size, Tensor):
             size = tensor(self.size, device=_device, dtype=_dtype).repeat(batch_size, 1)
         else:
             size = self.size.to(device=_device, dtype=_dtype)
         if size.shape != torch.Size([batch_size, 2]):
@@ -121,15 +124,15 @@
                 device=_device,
                 dtype=_dtype,
             ).repeat(batch_size, 1, 1)
             _output_size = tensor(self.resize_to, device=_device, dtype=torch.long).expand(batch_size, -1)
 
         _input_size = tensor(input_size, device=_device, dtype=torch.long).expand(batch_size, -1)
 
-        return dict(src=crop_src, dst=crop_dst, input_size=_input_size, output_size=_output_size)
+        return {"src": crop_src, "dst": crop_dst, "input_size": _input_size, "output_size": _output_size}
 
 
 class ResizedCropGenerator(CropGenerator):
     r"""Get cropping heights and widths for ```crop``` transformation for resized crop transform.
 
     Args:
         output_size (Tuple[int, int]): expected output size of each edge.
@@ -190,25 +193,25 @@
         scale = torch.as_tensor(self.scale, device=device, dtype=dtype)
         ratio = torch.as_tensor(self.ratio, device=device, dtype=dtype)
         _joint_range_check(scale, "scale")
         _joint_range_check(ratio, "ratio")
         self.rand_sampler = Uniform(tensor(0.0, device=device, dtype=dtype), tensor(1.0, device=device, dtype=dtype))
         self.log_ratio_sampler = Uniform(torch.log(ratio[0]), torch.log(ratio[1]), validate_args=False)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         size = (batch_shape[-2], batch_shape[-1])
         _device, _dtype = _extract_device_dtype([self.scale, self.ratio])
 
         if batch_size == 0:
-            return dict(
-                src=zeros([0, 4, 2], device=_device, dtype=_dtype),
-                dst=zeros([0, 4, 2], device=_device, dtype=_dtype),
-                size=zeros([0, 2], device=_device, dtype=_dtype),
-            )
+            return {
+                "src": zeros([0, 4, 2], device=_device, dtype=_dtype),
+                "dst": zeros([0, 4, 2], device=_device, dtype=_dtype),
+                "size": zeros([0, 2], device=_device, dtype=_dtype),
+            }
 
         rand = _adapted_rsampling((batch_size, 10), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
         area = (rand * (self.scale[1] - self.scale[0]) + self.scale[0]) * size[0] * size[1]
         log_ratio = _adapted_rsampling((batch_size, 10), self.log_ratio_sampler, same_on_batch).to(
             device=_device, dtype=_dtype
         )
         aspect_ratio = torch.exp(log_ratio)
@@ -303,8 +306,8 @@
     points_dst: Tensor = tensor(
         [[[0, 0], [dst_w - 1, 0], [dst_w - 1, dst_h - 1], [0, dst_h - 1]]], device=device, dtype=torch.long
     ).expand(batch_size, -1, -1)
 
     _input_size = tensor((height, width), device=device, dtype=torch.long).expand(batch_size, -1)
     _output_size = tensor(size, device=device, dtype=torch.long).expand(batch_size, -1)
 
-    return dict(src=points_src, dst=points_dst, input_size=_input_size, output_size=_output_size)
+    return {"src": points_src, "dst": points_dst, "input_size": _input_size, "output_size": _output_size}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/cutmix.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/cutmix.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,20 @@
 from typing import Dict, Optional, Tuple, Union
 
 import torch
 from torch.distributions import Bernoulli, Beta, Uniform
 
-from kornia.augmentation.random_generator._2d.probability import random_prob_generator
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
-from kornia.augmentation.utils import (
-    _adapted_beta,
-    _adapted_rsampling,
-    _adapted_sampling,
-    _adapted_uniform,
-    _common_param_check,
-    _joint_range_check,
-)
+from kornia.augmentation.utils import _adapted_rsampling, _adapted_sampling, _common_param_check, _joint_range_check
+from kornia.core import as_tensor, tensor, zeros
 from kornia.geometry.bbox import bbox_generator
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["CutmixGenerator"]
+
 
 class CutmixGenerator(RandomGeneratorBase):
     r"""Generate cutmix indexes and lambdas for a batch of inputs.
 
     Args:
         p (float): probability of applying cutmix.
         num_mix (int): number of images to mix with. Default is 1.
@@ -57,53 +52,59 @@
 
     def __repr__(self) -> str:
         repr = f"cut_size={self.cut_size}, beta={self.beta}, num_mix={self.num_mix}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         if self.beta is None:
-            self._beta = torch.tensor(1.0, device=device, dtype=dtype)
+            self._beta = tensor(1.0, device=device, dtype=dtype)
         else:
-            self._beta = torch.as_tensor(self.beta, device=device, dtype=dtype)
+            self._beta = as_tensor(self.beta, device=device, dtype=dtype)
         if self.cut_size is None:
-            self._cut_size = torch.tensor([0.0, 1.0], device=device, dtype=dtype)
+            self._cut_size = tensor([0.0, 1.0], device=device, dtype=dtype)
         else:
-            self._cut_size = torch.as_tensor(self.cut_size, device=device, dtype=dtype)
+            self._cut_size = as_tensor(self.cut_size, device=device, dtype=dtype)
 
         _joint_range_check(self._cut_size, 'cut_size', bounds=(0, 1))
 
         self.beta_sampler = Beta(self._beta, self._beta)
-        self.prob_sampler = Bernoulli(torch.tensor(float(self.p), device=device, dtype=dtype))
+        self.prob_sampler = Bernoulli(tensor(float(self.p), device=device, dtype=dtype))
         self.rand_sampler = Uniform(
-            torch.tensor(0.0, device=device, dtype=dtype),
-            torch.tensor(1.0, device=device, dtype=dtype),
-            validate_args=False,
+            tensor(0.0, device=device, dtype=dtype), tensor(1.0, device=device, dtype=dtype), validate_args=False
+        )
+        self.pair_sampler = Uniform(
+            tensor(0.0, device=device, dtype=dtype), tensor(1.0, device=device, dtype=dtype), validate_args=False
         )
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
         batch_size = batch_shape[0]
         height = batch_shape[-2]
         width = batch_shape[-1]
 
         if not (type(height) is int and height > 0 and type(width) is int and width > 0):
             raise AssertionError(f"'height' and 'width' must be integers. Got {height}, {width}.")
         _device, _dtype = _extract_device_dtype([self.beta, self.cut_size])
         _common_param_check(batch_size, same_on_batch)
 
         if batch_size == 0:
-            return dict(
-                mix_pairs=torch.zeros([0, 3], device=_device, dtype=torch.long),
-                crop_src=torch.zeros([0, 4, 2], device=_device, dtype=_dtype),
-            )
+            return {
+                "mix_pairs": zeros([0, 3], device=_device, dtype=torch.long),
+                "crop_src": zeros([0, 4, 2], device=_device, dtype=_dtype),
+            }
 
         with torch.no_grad():
             batch_probs: torch.Tensor = _adapted_sampling(
                 (batch_size * self.num_mix,), self.prob_sampler, same_on_batch
             )
-        mix_pairs: torch.Tensor = torch.rand(self.num_mix, batch_size, device=_device, dtype=_dtype).argsort(dim=1)
+            mix_pairs: torch.Tensor = (
+                _adapted_sampling((self.num_mix, batch_size), self.pair_sampler, same_on_batch)
+                .to(device=_device, dtype=_dtype)
+                .argsort(dim=1)
+            )
+
         cutmix_betas: torch.Tensor = _adapted_rsampling((batch_size * self.num_mix,), self.beta_sampler, same_on_batch)
 
         # Note: torch.clamp does not accept tensor, cutmix_betas.clamp(cut_size[0], cut_size[1]) throws:
         # Argument 1 to "clamp" of "_TensorBase" has incompatible type "Tensor"; expected "float"
         cutmix_betas = torch.min(torch.max(cutmix_betas, self._cut_size[0]), self._cut_size[1])
         cutmix_rate = torch.sqrt(1.0 - cutmix_betas) * batch_probs
 
@@ -113,165 +114,26 @@
 
         if same_on_batch:
             _gen_shape = (cut_height.size(0),)
             cut_height = cut_height[0]
             cut_width = cut_width[0]
 
         # Reserve at least 1 pixel for cropping.
-        x_start: torch.Tensor = _adapted_rsampling(_gen_shape, self.rand_sampler, same_on_batch) * (
+        x_start = _adapted_rsampling(_gen_shape, self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype) * (
             width - cut_width - 1
         )
-        y_start: torch.Tensor = _adapted_rsampling(_gen_shape, self.rand_sampler, same_on_batch) * (
+        y_start = _adapted_rsampling(_gen_shape, self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype) * (
             height - cut_height - 1
         )
-        x_start = x_start.floor().to(device=_device, dtype=_dtype)
-        y_start = y_start.floor().to(device=_device, dtype=_dtype)
+        x_start = x_start.floor()
+        y_start = y_start.floor()
 
         crop_src = bbox_generator(x_start.squeeze(), y_start.squeeze(), cut_width, cut_height)
 
         # (B * num_mix, 4, 2) => (num_mix, batch_size, 4, 2)
         crop_src = crop_src.view(self.num_mix, batch_size, 4, 2)
 
-        return dict(
-            mix_pairs=mix_pairs.to(device=_device, dtype=torch.long),
-            crop_src=crop_src.floor().to(device=_device, dtype=_dtype),
-            image_shape=torch.as_tensor(batch_shape[-2:], device=_device, dtype=_dtype),
-        )
-
-
-def random_cutmix_generator(
-    batch_size: int,
-    width: int,
-    height: int,
-    p: float = 0.5,
-    num_mix: int = 1,
-    beta: Optional[torch.Tensor] = None,
-    cut_size: Optional[torch.Tensor] = None,
-    same_on_batch: bool = False,
-    device: torch.device = torch.device('cpu'),
-    dtype: torch.dtype = torch.float32,
-) -> Dict[str, torch.Tensor]:
-    r"""Generate cutmix indexes and lambdas for a batch of inputs.
-
-    Args:
-        batch_size (int): the number of images. If batchsize == 1, the output will be as same as the input.
-        width (int): image width.
-        height (int): image height.
-        p (float): probability of applying cutmix.
-        num_mix (int): number of images to mix with. Default is 1.
-        beta (torch.Tensor, optional): hyperparameter for generating cut size from beta distribution.
-            If None, it will be set to 1.
-        cut_size (torch.Tensor, optional): controlling the minimum and maximum cut ratio from [0, 1].
-            If None, it will be set to [0, 1], which means no restriction.
-        same_on_batch (bool): apply the same transformation across the batch. Default: False.
-        device (torch.device): the device on which the random numbers will be generated. Default: cpu.
-        dtype (torch.dtype): the data type of the generated random numbers. Default: float32.
-
-    Returns:
-        params Dict[str, torch.Tensor]: parameters to be passed for transformation.
-            - mix_pairs (torch.Tensor): element-wise probabilities with a shape of (num_mix, B).
-            - crop_src (torch.Tensor): element-wise probabilities with a shape of (num_mix, B, 4, 2).
-
-    Note:
-        The generated random numbers are not reproducible across different devices and dtypes.
-
-    Examples:
-        >>> rng = torch.manual_seed(0)
-        >>> random_cutmix_generator(3, 224, 224, p=0.5, num_mix=2)
-        {'mix_pairs': tensor([[2, 0, 1],
-                [1, 2, 0]]), 'crop_src': tensor([[[[ 35.,  25.],
-                  [208.,  25.],
-                  [208., 198.],
-                  [ 35., 198.]],
-        <BLANKLINE>
-                 [[156., 137.],
-                  [155., 137.],
-                  [155., 136.],
-                  [156., 136.]],
-        <BLANKLINE>
-                 [[  3.,  12.],
-                  [210.,  12.],
-                  [210., 219.],
-                  [  3., 219.]]],
-        <BLANKLINE>
-        <BLANKLINE>
-                [[[ 83., 125.],
-                  [177., 125.],
-                  [177., 219.],
-                  [ 83., 219.]],
-        <BLANKLINE>
-                 [[ 54.,   8.],
-                  [205.,   8.],
-                  [205., 159.],
-                  [ 54., 159.]],
-        <BLANKLINE>
-                 [[ 97.,  70.],
-                  [ 96.,  70.],
-                  [ 96.,  69.],
-                  [ 97.,  69.]]]])}
-    """
-    _device, _dtype = _extract_device_dtype([beta, cut_size])
-    beta = torch.as_tensor(1.0 if beta is None else beta, device=device, dtype=dtype)
-    cut_size = torch.as_tensor([0.0, 1.0] if cut_size is None else cut_size, device=device, dtype=dtype)
-    if not (num_mix >= 1 and isinstance(num_mix, (int,))):
-        raise AssertionError(f"`num_mix` must be an integer greater than 1. Got {num_mix}.")
-    if not (type(height) is int and height > 0 and type(width) is int and width > 0):
-        raise AssertionError(f"'height' and 'width' must be integers. Got {height}, {width}.")
-    _joint_range_check(cut_size, 'cut_size', bounds=(0, 1))
-    _common_param_check(batch_size, same_on_batch)
-
-    if batch_size == 0:
-        return dict(
-            mix_pairs=torch.zeros([0, 3], device=_device, dtype=torch.long),
-            crop_src=torch.zeros([0, 4, 2], device=_device, dtype=torch.long),
-        )
-
-    batch_probs: torch.Tensor = random_prob_generator(
-        batch_size * num_mix, p, same_on_batch, device=device, dtype=dtype
-    )
-    mix_pairs: torch.Tensor = torch.rand(num_mix, batch_size, device=device, dtype=dtype).argsort(dim=1)
-    cutmix_betas: torch.Tensor = _adapted_beta((batch_size * num_mix,), beta, beta, same_on_batch=same_on_batch)
-    # Note: torch.clamp does not accept tensor, cutmix_betas.clamp(cut_size[0], cut_size[1]) throws:
-    # Argument 1 to "clamp" of "_TensorBase" has incompatible type "Tensor"; expected "float"
-    cutmix_betas = torch.min(torch.max(cutmix_betas, cut_size[0]), cut_size[1])
-    cutmix_rate = torch.sqrt(1.0 - cutmix_betas) * batch_probs
-
-    cut_height = (cutmix_rate * height).floor().to(device=device, dtype=_dtype)
-    cut_width = (cutmix_rate * width).floor().to(device=device, dtype=_dtype)
-    _gen_shape = (1,)
-
-    if same_on_batch:
-        _gen_shape = (cut_height.size(0),)
-        cut_height = cut_height[0]
-        cut_width = cut_width[0]
-
-    # Reserve at least 1 pixel for cropping.
-    x_start = (
-        _adapted_uniform(
-            _gen_shape,
-            torch.zeros_like(cut_width, device=device, dtype=dtype),
-            (width - cut_width - 1).to(device=device, dtype=dtype),
-            same_on_batch,
-        )
-        .floor()
-        .to(device=device, dtype=_dtype)
-    )
-    y_start = (
-        _adapted_uniform(
-            _gen_shape,
-            torch.zeros_like(cut_height, device=device, dtype=dtype),
-            (height - cut_height - 1).to(device=device, dtype=dtype),
-            same_on_batch,
-        )
-        .floor()
-        .to(device=device, dtype=_dtype)
-    )
-
-    crop_src = bbox_generator(x_start.squeeze(), y_start.squeeze(), cut_width, cut_height)
-
-    # (B * num_mix, 4, 2) => (num_mix, batch_size, 4, 2)
-    crop_src = crop_src.view(num_mix, batch_size, 4, 2)
-
-    return dict(
-        mix_pairs=mix_pairs.to(device=_device, dtype=torch.long),
-        crop_src=crop_src.floor().to(device=_device, dtype=_dtype),
-    )
+        return {
+            "mix_pairs": mix_pairs.to(device=_device, dtype=torch.long),
+            "crop_src": crop_src.floor().to(device=_device, dtype=_dtype),
+            "image_shape": as_tensor(batch_shape[-2:], device=_device, dtype=_dtype),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/jigsaw.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/jigsaw.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,14 +2,16 @@
 
 import torch
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.random_generator.utils import randperm
 from kornia.augmentation.utils import _common_param_check
 
+__all__ = ["JigsawGenerator"]
+
 
 class JigsawGenerator(RandomGeneratorBase):
     r"""Generate Jigsaw permutation indices for a batch of inputs.
 
     Args:
         grid: the Jigsaw puzzle grid. e.g. (2, 2) means
             each output will mix image patches in a 2x2 grid.
@@ -33,23 +35,23 @@
         repr = f"grid={self.grid}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         self._device = device
         self._dtype = dtype
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
 
         perm_times = self.grid[0] * self.grid[1]
         # Generate mosiac order in one shot
         if batch_size == 0:
             rand_ids = torch.zeros([0, perm_times], device=self._device)
         elif same_on_batch:
             rand_ids = randperm(perm_times, ensure_perm=self.ensure_perm, device=self._device)
             rand_ids = torch.stack([rand_ids] * batch_size)
         else:
             rand_ids = torch.stack(
                 [randperm(perm_times, ensure_perm=self.ensure_perm, device=self._device) for _ in range(batch_size)]
             )
-        return dict(permutation=rand_ids)
+        return {"permutation": rand_ids}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/mixup.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_3d/motion_blur.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,122 +1,97 @@
-from typing import Dict, Optional, Tuple, Union
+from typing import Dict, Tuple, Union
 
 import torch
-from torch.distributions import Bernoulli, Uniform
+from torch.distributions import Uniform
 
-from kornia.augmentation.random_generator._2d.probability import random_prob_generator
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
-from kornia.augmentation.utils import (
-    _adapted_rsampling,
-    _adapted_sampling,
-    _adapted_uniform,
-    _common_param_check,
-    _joint_range_check,
-)
+from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _range_bound, _tuple_range_reader
+from kornia.core import Tensor, stack
 from kornia.utils.helpers import _extract_device_dtype
 
 
-class MixupGenerator(RandomGeneratorBase):
-    r"""Generate mixup indexes and lambdas for a batch of inputs.
+class MotionBlurGenerator3D(RandomGeneratorBase):
+    r"""Get parameters for motion blur.
 
     Args:
-        lambda_val (torch.Tensor, optional): min-max strength for mixup images, ranged from [0., 1.].
-            If None, it will be set to tensor([0., 1.]), which means no restrictions.
+        kernel_size: motion kernel size (odd and positive).
+            If int, the kernel will have a fixed size.
+            If Tuple[int, int], it will randomly generate the value from the range batch-wisely.
+        angle: angle of the motion blur in degrees (anti-clockwise rotation).
+            If float, it will generate the value from (-angle, angle).
+        direction: forward/backward direction of the motion blur.
+            Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
+            while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
+            uniformly (but still angled) motion blur.
+            If float, it will generate the value from (-direction, direction).
+            If Tuple[int, int], it will randomly generate the value from the range.
 
     Returns:
         A dict of parameters to be passed for transformation.
-            - mix_pairs (torch.Tensor): element-wise probabilities with a shape of (B,).
-            - mixup_lambdas (torch.Tensor): element-wise probabilities with a shape of (B,).
+            - ksize_factor (Tensor): element-wise kernel size factors with a shape of (B,).
+            - angle_factor (Tensor): element-wise angle factors with a shape of (B,).
+            - direction_factor (Tensor): element-wise direction factors with a shape of (B,).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
-    def __init__(self, lambda_val: Optional[Union[torch.Tensor, Tuple[float, float]]] = None, p: float = 1.0) -> None:
+    def __init__(
+        self,
+        kernel_size: Union[int, Tuple[int, int]],
+        angle: Union[
+            Tensor,
+            float,
+            Tuple[float, float, float],
+            Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]],
+        ],
+        direction: Union[Tensor, float, Tuple[float, float]],
+    ) -> None:
         super().__init__()
-        self.lambda_val = lambda_val
-        self.p = p
+        self.kernel_size = kernel_size
+        self.angle = angle
+        self.direction = direction
 
     def __repr__(self) -> str:
-        repr = f"lambda_val={self.lambda_val}"
+        repr = f"kernel_size={self.kernel_size}, angle={self.angle}, direction={self.direction}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
-        if self.lambda_val is None:
-            lambda_val = torch.tensor([0.0, 1.0], device=device, dtype=dtype)
+        angle: Tensor = _tuple_range_reader(self.angle, 3, device=device, dtype=dtype)
+        direction = _range_bound(self.direction, 'direction', center=0.0, bounds=(-1, 1)).to(device=device, dtype=dtype)
+        if isinstance(self.kernel_size, int):
+            if not (self.kernel_size >= 3 and self.kernel_size % 2 == 1):
+                raise AssertionError(f"`kernel_size` must be odd and greater than 3. Got {self.kernel_size}.")
+            self.ksize_sampler = Uniform(self.kernel_size // 2, self.kernel_size // 2, validate_args=False)
+        elif isinstance(self.kernel_size, tuple):
+            # kernel_size is fixed across the batch
+            if len(self.kernel_size) != 2:
+                raise AssertionError(f"`kernel_size` must be (2,) if it is a tuple. Got {self.kernel_size}.")
+            self.ksize_sampler = Uniform(self.kernel_size[0] // 2, self.kernel_size[1] // 2, validate_args=False)
         else:
-            lambda_val = torch.as_tensor(self.lambda_val, device=device, dtype=dtype)
+            raise TypeError(f"Unsupported type: {type(self.kernel_size)}")
 
-        _joint_range_check(lambda_val, 'lambda_val', bounds=(0, 1))
-        self.lambda_sampler = Uniform(lambda_val[0], lambda_val[1], validate_args=False)
-        self.prob_sampler = Bernoulli(torch.tensor(float(self.p), device=device, dtype=dtype))
+        self.yaw_sampler = Uniform(angle[0][0], angle[0][1], validate_args=False)
+        self.pitch_sampler = Uniform(angle[1][0], angle[1][1], validate_args=False)
+        self.roll_sampler = Uniform(angle[2][0], angle[2][1], validate_args=False)
+        self.direction_sampler = Uniform(direction[0], direction[1], validate_args=False)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
-
         _common_param_check(batch_size, same_on_batch)
-        _device, _dtype = _extract_device_dtype([self.lambda_val])
-
-        with torch.no_grad():
-            batch_probs: torch.Tensor = _adapted_sampling((batch_size,), self.prob_sampler, same_on_batch)
-        mixup_pairs: torch.Tensor = torch.randperm(batch_size, device=_device, dtype=_dtype).long()
-        mixup_lambdas: torch.Tensor = _adapted_rsampling((batch_size,), self.lambda_sampler, same_on_batch)
-        mixup_lambdas = mixup_lambdas * batch_probs
-
-        return dict(
-            mixup_pairs=mixup_pairs.to(device=_device, dtype=torch.long),
-            mixup_lambdas=mixup_lambdas.to(device=_device, dtype=_dtype),
-        )
-
-
-def random_mixup_generator(
-    batch_size: int,
-    p: float = 0.5,
-    lambda_val: Optional[torch.Tensor] = None,
-    same_on_batch: bool = False,
-    device: torch.device = torch.device('cpu'),
-    dtype: torch.dtype = torch.float32,
-) -> Dict[str, torch.Tensor]:
-    r"""Generate mixup indexes and lambdas for a batch of inputs.
-
-    Args:
-        batch_size (int): the number of images. If batchsize == 1, the output will be as same as the input.
-        p (flot): probability of applying mixup.
-        lambda_val (torch.Tensor, optional): min-max strength for mixup images, ranged from [0., 1.].
-            If None, it will be set to tensor([0., 1.]), which means no restrictions.
-        same_on_batch (bool): apply the same transformation across the batch. Default: False.
-        device (torch.device): the device on which the random numbers will be generated. Default: cpu.
-        dtype (torch.dtype): the data type of the generated random numbers. Default: float32.
-
-    Returns:
-        params Dict[str, torch.Tensor]: parameters to be passed for transformation.
-            - mix_pairs (torch.Tensor): element-wise probabilities with a shape of (B,).
-            - mixup_lambdas (torch.Tensor): element-wise probabilities with a shape of (B,).
-
-    Note:
-        The generated random numbers are not reproducible across different devices and dtypes.
-
-    Examples:
-        >>> rng = torch.manual_seed(0)
-        >>> random_mixup_generator(5, 0.7)
-        {'mixup_pairs': tensor([4, 0, 3, 1, 2]), 'mixup_lambdas': tensor([0.6323, 0.0000, 0.4017, 0.0223, 0.1689])}
-    """
-    _common_param_check(batch_size, same_on_batch)
-    _device, _dtype = _extract_device_dtype([lambda_val])
-    lambda_val = torch.as_tensor([0.0, 1.0] if lambda_val is None else lambda_val, device=device, dtype=dtype)
-    _joint_range_check(lambda_val, 'lambda_val', bounds=(0, 1))
-
-    batch_probs: torch.Tensor = random_prob_generator(
-        batch_size, p, same_on_batch=same_on_batch, device=device, dtype=dtype
-    )
-    mixup_pairs: torch.Tensor = torch.randperm(batch_size, device=device, dtype=dtype).long()
-    mixup_lambdas: torch.Tensor = _adapted_uniform(
-        (batch_size,), lambda_val[0], lambda_val[1], same_on_batch=same_on_batch
-    )
-    mixup_lambdas = mixup_lambdas * batch_probs
-
-    return dict(
-        mixup_pairs=mixup_pairs.to(device=_device, dtype=torch.long),
-        mixup_lambdas=mixup_lambdas.to(device=_device, dtype=_dtype),
-    )
+        # self.ksize_factor.expand((batch_size, -1))
+        _device, _dtype = _extract_device_dtype([self.angle, self.direction])
+        yaw_factor = _adapted_rsampling((batch_size,), self.yaw_sampler, same_on_batch)
+        pitch_factor = _adapted_rsampling((batch_size,), self.pitch_sampler, same_on_batch)
+        roll_factor = _adapted_rsampling((batch_size,), self.roll_sampler, same_on_batch)
+        angle_factor = stack([yaw_factor, pitch_factor, roll_factor], 1)
+
+        direction_factor = _adapted_rsampling((batch_size,), self.direction_sampler, same_on_batch)
+        ksize_factor = _adapted_rsampling((batch_size,), self.ksize_sampler, same_on_batch).int() * 2 + 1
+
+        return {
+            'ksize_factor': ksize_factor.to(device=_device, dtype=torch.int32),
+            'angle_factor': angle_factor.to(device=_device, dtype=_dtype),
+            'direction_factor': direction_factor.to(device=_device, dtype=_dtype),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/mosaic.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/mosaic.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,14 +5,16 @@
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check
 from kornia.core import Tensor
 from kornia.geometry.bbox import bbox_generator
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["MosaicGenerator"]
+
 
 class MosaicGenerator(RandomGeneratorBase):
     r"""Generate mixup indexes and lambdas for a batch of inputs.
 
     Args:
         output_size: the output tensor width and height after mosaicing.
         mosaic_grid: the number of images and image arrangement. e.g. (2, 2) means
@@ -53,15 +55,15 @@
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         self.start_ratio_range_sampler = Uniform(
             torch.tensor(self.start_ratio_range[0], device=device, dtype=dtype),
             torch.tensor(self.start_ratio_range[1], device=device, dtype=dtype),
             validate_args=False,
         )
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
         batch_size = batch_shape[0]
         input_sizes = (batch_shape[-2], batch_shape[-1])
         # output_size = input_sizes if self.output_size is None else self.output_size
 
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.mosaic_grid])
 
@@ -94,13 +96,13 @@
         # B x 3
 
         batch_shapes: Tensor
         if batch_size == 0:
             batch_shapes = torch.zeros([0, 3], device=_device, dtype=torch.long)
         else:
             batch_shapes = torch.stack([torch.as_tensor(batch_shape[1:], device=_device) for _ in range(batch_size)])
-        return dict(
-            permutation=mosiac_ids.to(device=_device, dtype=torch.long),
-            src=crop_src,
-            dst=crop_dst,
-            batch_shapes=batch_shapes,
-        )
+        return {
+            "permutation": mosiac_ids.to(device=_device, dtype=torch.long),
+            "src": crop_src,
+            "dst": crop_dst,
+            "batch_shapes": batch_shapes,
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/motion_blur.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/motion_blur.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,14 +4,16 @@
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _range_bound
 from kornia.core import Tensor
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["MotionBlurGenerator"]
+
 
 class MotionBlurGenerator(RandomGeneratorBase):
     r"""Get parameters for motion blur.
 
     Args:
         kernel_size: motion kernel size (odd and positive).
             If int, the kernel will have a fixed size.
@@ -66,21 +68,21 @@
             self.ksize_sampler = Uniform(self.kernel_size[0] // 2, self.kernel_size[1] // 2, validate_args=False)
         else:
             raise TypeError(f"Unsupported type: {type(self.kernel_size)}")
 
         self.angle_sampler = Uniform(angle[0], angle[1], validate_args=False)
         self.direction_sampler = Uniform(direction[0], direction[1], validate_args=False)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         # self.ksize_factor.expand((batch_size, -1))
         _device, _dtype = _extract_device_dtype([self.angle, self.direction])
         angle_factor = _adapted_rsampling((batch_size,), self.angle_sampler, same_on_batch)
         direction_factor = _adapted_rsampling((batch_size,), self.direction_sampler, same_on_batch)
         ksize_factor = _adapted_rsampling((batch_size,), self.ksize_sampler, same_on_batch).int() * 2 + 1
 
-        return dict(
-            ksize_factor=ksize_factor.to(device=_device, dtype=torch.int32),
-            angle_factor=angle_factor.to(device=_device, dtype=_dtype),
-            direction_factor=direction_factor.to(device=_device, dtype=_dtype),
-        )
+        return {
+            "ksize_factor": ksize_factor.to(device=_device, dtype=torch.int32),
+            "angle_factor": angle_factor.to(device=_device, dtype=_dtype),
+            "direction_factor": direction_factor.to(device=_device, dtype=_dtype),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/perspective.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/perspective.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,19 @@
-from typing import Dict, Union
+from typing import Dict, Tuple, Union
 
 import torch
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check
 from kornia.core import Tensor, tensor
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["PerspectiveGenerator"]
+
 
 class PerspectiveGenerator(RandomGeneratorBase):
     r"""Get parameters for ``perspective`` for a random perspective transform.
 
     Args:
         distortion_scale: the degree of distortion, ranged from 0 to 1.
         sampling_method: ``'basic'`` | ``'area_preserving'``. Default: ``'basic'``
@@ -45,15 +47,15 @@
         self._distortion_scale = torch.as_tensor(self.distortion_scale, device=device, dtype=dtype)
         if not (self._distortion_scale.dim() == 0 and 0 <= self._distortion_scale <= 1):
             raise AssertionError(f"'distortion_scale' must be a scalar within [0, 1]. Got {self._distortion_scale}.")
         self.rand_val_sampler = Uniform(
             tensor(0, device=device, dtype=dtype), tensor(1, device=device, dtype=dtype), validate_args=False
         )
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         height = batch_shape[-2]
         width = batch_shape[-1]
 
         _device, _dtype = _extract_device_dtype([self.distortion_scale])
         _common_param_check(batch_size, same_on_batch)
         if not (type(height) is int and height > 0 and type(width) is int and width > 0):
@@ -77,8 +79,8 @@
             pts_norm = tensor([[[1, 1], [-1, 1], [-1, -1], [1, -1]]], device=_device, dtype=_dtype)
             offset = factor * rand_val * pts_norm
         elif self.sampling_method == "area_preserving":
             offset = 2 * factor * (rand_val - 0.5)
 
         end_points = start_points + offset
 
-        return dict(start_points=start_points, end_points=end_points)
+        return {"start_points": start_points, "end_points": end_points}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/plain_uniform.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/plain_uniform.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,14 +4,16 @@
 from torch.distributions import Distribution, Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _range_bound
 from kornia.core import Tensor, as_tensor
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["ParameterBound", "PlainUniformGenerator"]
+
 # factor, name, center, range
 ParameterBound = Tuple[Any, str, Optional[float], Optional[Tuple[float, float]]]
 
 
 class PlainUniformGenerator(RandomGeneratorBase):
     r"""Generate random parameters that distributed uniformly.
 
@@ -40,22 +42,25 @@
         {'factor_1': tensor([0.7196, 0.7307]), 'factor_2': tensor([ 0.3278, -0.3657])}
     """
 
     def __init__(self, *samplers: ParameterBound) -> None:
         super().__init__()
         self.samplers = samplers
         names = []
-        for factor, name, _, _ in samplers:
+        for factor, name, center, bound in samplers:
             if name in names:
                 raise RuntimeError(f"factor name `{name}` has already been registered. Please check the duplication.")
             names.append(name)
             if isinstance(factor, torch.nn.Parameter):
                 self.register_parameter(name, factor)
             elif isinstance(factor, Tensor):
                 self.register_buffer(name, factor)
+            else:
+                factor = _range_bound(factor, name, center=center, bounds=bound)
+                self.register_buffer(name, factor)
 
     def __repr__(self) -> str:
         repr = ", ".join([f"{name}={factor}" for factor, name, _, _ in self.samplers])
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         self.sampler_dict: Dict[str, Distribution] = {}
@@ -64,15 +69,15 @@
                 factor = as_tensor(factor, device=device, dtype=dtype)
             elif center is None or bound is None:
                 raise ValueError(f"`center` and `bound` should be both None or provided. Got {center} and {bound}.")
             else:
                 factor = _range_bound(factor, name, center=center, bounds=bound, device=device, dtype=dtype)
             self.sampler_dict.update({name: Uniform(factor[0], factor[1], validate_args=False)})
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([t for t, _, _, _ in self.samplers])
 
         return {
             name: _adapted_rsampling((batch_size,), dist, same_on_batch).to(device=_device, dtype=_dtype)
             for name, dist in self.sampler_dict.items()
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/planckian_jitter.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/planckian_jitter.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,28 +1,30 @@
-from typing import Dict, List
+from typing import Dict, List, Tuple
 
 import torch
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check, _range_bound
 
+__all__ = ["PlanckianJitterGenerator"]
+
 
 class PlanckianJitterGenerator(RandomGeneratorBase):
     r"""Generate random planckian jitter parameters for a batch of images."""
 
     def __init__(self, domain: List[float]) -> None:
         super().__init__()
         self.domain = domain
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         idx_range = _range_bound(self.domain, 'idx_range', device=device, dtype=dtype)
 
         _joint_range_check(idx_range, 'idx_range', (0, self.domain[1]))
         self.pl_idx_dist = Uniform(idx_range[0], idx_range[1], validate_args=False)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         pl_idx = _adapted_rsampling((batch_size,), self.pl_idx_dist, same_on_batch)
 
-        return dict(idx=pl_idx.long())
+        return {"idx": pl_idx.long()}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/posterize.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/posterize.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,50 +4,52 @@
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check
 from kornia.core import Tensor, as_tensor
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["PosterizeGenerator"]
+
 
 class PosterizeGenerator(RandomGeneratorBase):
     r"""Generate random posterize parameters for a batch of images.
 
     Args:
-        bits: Integer that ranged from (0, 8], in which 0 gives black image and 8 gives the original.
-            If int x, bits will be generated from (x, 8).
+        bits: floats that ranged from (0, 8], in which 0 gives black image and 8 gives the original.
+            If float x, bits will be generated from (x, 8).
             If tuple (x, y), bits will be generated from (x, y).
 
     Returns:
         A dict of parameters to be passed for transformation.
             - bits_factor (Tensor): element-wise bit factors with a shape of (B,).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
-    def __init__(self, bits: Union[int, Tuple[int, int], Tensor]) -> None:
+    def __init__(self, bits: Union[float, Tuple[float, float], Tensor]) -> None:
         super().__init__()
-        self.bits = bits
+        self.bits_factor = bits
 
     def __repr__(self) -> str:
-        repr = f"bits={self.bits}"
+        repr = f"bits={self.bits_factor}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
-        bits = as_tensor(self.bits, device=device, dtype=dtype)
+        bits = as_tensor(self.bits_factor, device=device, dtype=dtype)
         if len(bits.size()) == 0:
             bits = bits.repeat(2)
             bits[1] = 8
         elif not (len(bits.size()) == 1 and bits.size(0) == 2):
             raise ValueError(f"'bits' shall be either a scalar or a length 2 tensor. Got {bits}.")
         _joint_range_check(bits, 'bits', (0, 8))
         self.bit_sampler = Uniform(bits[0], bits[1], validate_args=False)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
-        _device, _ = _extract_device_dtype([self.bits if isinstance(self.bits, Tensor) else None])
+        _device, _ = _extract_device_dtype([self.bits_factor if isinstance(self.bits_factor, Tensor) else None])
         bits_factor = _adapted_rsampling((batch_size,), self.bit_sampler, same_on_batch)
-        return dict(bits_factor=bits_factor.to(device=_device, dtype=torch.int32))
+        return {"bits_factor": bits_factor.round().to(device=_device, dtype=torch.int32)}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/probability.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/probability.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,18 @@
-from typing import Dict
+from typing import Dict, Tuple
 
 import torch
 from torch.distributions import Bernoulli
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_sampling, _common_param_check
 from kornia.core import Tensor, tensor
 
+__all__ = ["ProbabilityGenerator"]
+
 
 class ProbabilityGenerator(RandomGeneratorBase):
     r"""Generate random probabilities for a batch of inputs.
 
     Args:
         p: probability to generate an 1-d binary mask. Default value is 0.5.
 
@@ -32,18 +34,18 @@
         repr = f"p={self.p}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         p = torch.tensor(float(self.p), device=device, dtype=dtype)
         self.sampler = Bernoulli(p)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         probs_mask: Tensor = _adapted_sampling((batch_size,), self.sampler, same_on_batch).bool()
-        return dict(probs=probs_mask)
+        return {"probs": probs_mask}
 
 
 def random_prob_generator(
     batch_size: int,
     p: float = 0.5,
     same_on_batch: bool = False,
     device: torch.device = torch.device('cpu'),
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/rectangle_earase.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/rectangle_earase.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,14 +4,16 @@
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check
 from kornia.core import Tensor, as_tensor, tensor, where
 from kornia.utils.helpers import _extract_device_dtype
 
+__all__ = ["RectangleEraseGenerator"]
+
 
 class RectangleEraseGenerator(RandomGeneratorBase):
     r"""Get parameters for ```erasing``` transformation for erasing transform.
 
     Args:
         scale (Tensor): range of size of the origin size cropped. Shape (2).
         ratio (Tensor): range of aspect ratio of the origin aspect ratio cropped. Shape (2).
@@ -65,15 +67,15 @@
             )
         else:
             self.ratio_sampler = Uniform(ratio[0], ratio[1], validate_args=False)
         self.uniform_sampler = Uniform(
             tensor(0, device=device, dtype=dtype), tensor(1, device=device, dtype=dtype), validate_args=False
         )
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         height = batch_shape[-2]
         width = batch_shape[-1]
         if not (type(height) is int and height > 0 and type(width) is int and width > 0):
             raise AssertionError(f"'height' and 'width' must be integers. Got {height}, {width}.")
 
         _common_param_check(batch_size, same_on_batch)
@@ -120,14 +122,14 @@
         ys_ratio = _adapted_rsampling((batch_size,), self.uniform_sampler, same_on_batch).to(
             device=_device, dtype=_dtype
         )
 
         xs = xs_ratio * (width - widths + 1)
         ys = ys_ratio * (height - heights + 1)
 
-        return dict(
-            widths=widths.floor(),
-            heights=heights.floor(),
-            xs=xs.floor(),
-            ys=ys.floor(),
-            values=tensor([self.value] * batch_size, device=_device, dtype=_dtype),
-        )
+        return {
+            "widths": widths.floor(),
+            "heights": heights.floor(),
+            "xs": xs.floor(),
+            "ys": ys.floor(),
+            "values": tensor([self.value] * batch_size, device=_device, dtype=_dtype),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_2d/resize.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_2d/resize.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,14 +4,16 @@
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _common_param_check
 from kornia.core import Device, Tensor, tensor
 from kornia.geometry.bbox import bbox_generator
 from kornia.geometry.transform.affwarp import _side_to_image_size
 
+__all__ = ["ResizeGenerator"]
+
 
 class ResizeGenerator(RandomGeneratorBase):
     r"""Get parameters for ```resize``` transformation for resize transform.
 
     Args:
         resize_to: Desired output size of the crop, like (h, w).
         side: Which side to resize if `resize_to` is only of type int.
@@ -39,52 +41,54 @@
         return repr
 
     def make_samplers(self, device: Device, dtype: torch.dtype) -> None:
         self.device = device
         self.dtype = dtype
         pass
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device = self.device
         _dtype = self.dtype
 
         if batch_size == 0:
-            return dict(
-                src=torch.zeros([0, 4, 2], device=_device, dtype=_dtype),
-                dst=torch.zeros([0, 4, 2], device=_device, dtype=_dtype),
-            )
+            return {
+                "src": torch.zeros([0, 4, 2], device=_device, dtype=_dtype),
+                "dst": torch.zeros([0, 4, 2], device=_device, dtype=_dtype),
+            }
 
         input_size = h, w = (batch_shape[-2], batch_shape[-1])
 
         src = bbox_generator(
             tensor(0, device=_device, dtype=_dtype),
             tensor(0, device=_device, dtype=_dtype),
             tensor(input_size[1], device=_device, dtype=_dtype),
             tensor(input_size[0], device=_device, dtype=_dtype),
         ).repeat(batch_size, 1, 1)
 
         if isinstance(self.output_size, int):
             aspect_ratio = w / h
-            self.output_size = _side_to_image_size(self.output_size, aspect_ratio, self.side)
+            output_size = _side_to_image_size(self.output_size, aspect_ratio, self.side)
+        else:
+            output_size = self.output_size
 
         if not (
-            len(self.output_size) == 2
-            and isinstance(self.output_size[0], (int,))
-            and isinstance(self.output_size[1], (int,))
-            and self.output_size[0] > 0
-            and self.output_size[1] > 0
+            len(output_size) == 2
+            and isinstance(output_size[0], (int,))
+            and isinstance(output_size[1], (int,))
+            and output_size[0] > 0
+            and output_size[1] > 0
         ):
-            raise AssertionError(f"`resize_to` must be a tuple of 2 positive integers. Got {self.output_size}.")
+            raise AssertionError(f"`resize_to` must be a tuple of 2 positive integers. Got {output_size}.")
 
         dst = bbox_generator(
             tensor(0, device=_device, dtype=_dtype),
             tensor(0, device=_device, dtype=_dtype),
-            tensor(self.output_size[1], device=_device, dtype=_dtype),
-            tensor(self.output_size[0], device=_device, dtype=_dtype),
+            tensor(output_size[1], device=_device, dtype=_dtype),
+            tensor(output_size[0], device=_device, dtype=_dtype),
         ).repeat(batch_size, 1, 1)
 
         _input_size = tensor(input_size, device=_device, dtype=torch.long).expand(batch_size, -1)
-        _output_size = tensor(self.output_size, device=_device, dtype=torch.long).expand(batch_size, -1)
+        _output_size = tensor(output_size, device=_device, dtype=torch.long).expand(batch_size, -1)
 
-        return dict(src=src, dst=dst, input_size=_input_size, output_size=_output_size)
+        return {"src": src, "dst": dst, "input_size": _input_size, "output_size": _output_size}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_3d/affine.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_3d/affine.py`

 * *Files 2% similar despite different names*

```diff
@@ -139,15 +139,15 @@
 
         self.uniform_sampler = Uniform(
             torch.tensor(0, device=device, dtype=dtype),
             torch.tensor(1, device=device, dtype=dtype),
             validate_args=False,
         )
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
         batch_size = batch_shape[0]
         depth = batch_shape[-3]
         height = batch_shape[-2]
         width = batch_shape[-1]
 
         if not (
             type(depth) is int and depth > 0 and type(height) is int and height > 0 and type(width) is int and width > 0
@@ -201,19 +201,19 @@
             syx = _adapted_rsampling((batch_size,), self.syx_sampler, same_on_batch)
             syz = _adapted_rsampling((batch_size,), self.syz_sampler, same_on_batch)
             szx = _adapted_rsampling((batch_size,), self.szx_sampler, same_on_batch)
             szy = _adapted_rsampling((batch_size,), self.szy_sampler, same_on_batch)
         else:
             sxy = sxz = syx = syz = szx = szy = torch.tensor([0] * batch_size, device=_device, dtype=_dtype)
 
-        return dict(
-            translations=torch.as_tensor(translations, device=_device, dtype=_dtype),
-            center=torch.as_tensor(center, device=_device, dtype=_dtype),
-            scale=torch.as_tensor(scale, device=_device, dtype=_dtype),
-            angles=torch.as_tensor(angles, device=_device, dtype=_dtype),
-            sxy=torch.as_tensor(sxy, device=_device, dtype=_dtype),
-            sxz=torch.as_tensor(sxz, device=_device, dtype=_dtype),
-            syx=torch.as_tensor(syx, device=_device, dtype=_dtype),
-            syz=torch.as_tensor(syz, device=_device, dtype=_dtype),
-            szx=torch.as_tensor(szx, device=_device, dtype=_dtype),
-            szy=torch.as_tensor(szy, device=_device, dtype=_dtype),
-        )
+        return {
+            'translations': torch.as_tensor(translations, device=_device, dtype=_dtype),
+            'center': torch.as_tensor(center, device=_device, dtype=_dtype),
+            'scale': torch.as_tensor(scale, device=_device, dtype=_dtype),
+            'angles': torch.as_tensor(angles, device=_device, dtype=_dtype),
+            'sxy': torch.as_tensor(sxy, device=_device, dtype=_dtype),
+            'sxz': torch.as_tensor(sxz, device=_device, dtype=_dtype),
+            'syx': torch.as_tensor(syx, device=_device, dtype=_dtype),
+            'syz': torch.as_tensor(syz, device=_device, dtype=_dtype),
+            'szx': torch.as_tensor(szx, device=_device, dtype=_dtype),
+            'szy': torch.as_tensor(szy, device=_device, dtype=_dtype),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_3d/crop.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_3d/crop.py`

 * *Files 4% similar despite different names*

```diff
@@ -41,15 +41,15 @@
         if self.resize_to is not None:
             repr += f", resize_to={self.resize_to}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         self.rand_sampler = Uniform(tensor(0.0, device=device, dtype=dtype), tensor(1.0, device=device, dtype=dtype))
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size, _, depth, height, width = batch_shape
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.size if isinstance(self.size, Tensor) else None])
 
         if not isinstance(self.size, Tensor):
             size = tensor(self.size, device=_device, dtype=_dtype).repeat(batch_size, 1)
         else:
@@ -71,21 +71,22 @@
 
         x_diff = width - size[:, 2] + 1
         y_diff = height - size[:, 1] + 1
         z_diff = depth - size[:, 0] + 1
 
         if (x_diff < 0).any() or (y_diff < 0).any() or (z_diff < 0).any():
             raise ValueError(
-                f"input_size {(depth, height, width)} cannot be smaller than crop size {str(size)} in any dimension."
+                f"input_size {(depth, height, width)} cannot be smaller than crop size {size!s} in any dimension."
             )
 
         if batch_size == 0:
-            return dict(
-                src=zeros([0, 8, 3], device=_device, dtype=_dtype), dst=zeros([0, 8, 3], device=_device, dtype=_dtype)
-            )
+            return {
+                "src": zeros([0, 8, 3], device=_device, dtype=_dtype),
+                "dst": zeros([0, 8, 3], device=_device, dtype=_dtype),
+            }
 
         x_start = _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
         y_start = _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
         z_start = _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
 
         x_start = (x_start * x_diff).floor()
         y_start = (y_start * y_diff).floor()
@@ -128,15 +129,15 @@
                         [0, self.resize_to[-2] - 1, self.resize_to[-3] - 1],
                     ]
                 ],
                 device=_device,
                 dtype=_dtype,
             ).repeat(batch_size, 1, 1)
 
-        return dict(src=crop_src.to(device=_device), dst=crop_dst.to(device=_device))
+        return {"src": crop_src.to(device=_device), "dst": crop_dst.to(device=_device)}
 
 
 def center_crop_generator3d(
     batch_size: int,
     depth: int,
     height: int,
     width: int,
@@ -167,15 +168,15 @@
         type(depth) is int and depth > 0 and type(height) is int and height > 0 and type(width) is int and width > 0
     ):
         raise AssertionError(f"'depth', 'height' and 'width' must be integers. Got {depth}, {height}, {width}.")
     if not (depth >= size[0] and height >= size[1] and width >= size[2]):
         raise AssertionError(f"Crop size must be smaller than input size. Got ({depth}, {height}, {width}) and {size}.")
 
     if batch_size == 0:
-        return dict(src=zeros([0, 8, 3]), dst=zeros([0, 8, 3]))
+        return {"src": zeros([0, 8, 3]), "dst": zeros([0, 8, 3])}
     # unpack input sizes
     dst_d, dst_h, dst_w = size
     src_d, src_h, src_w = (depth, height, width)
 
     # compute start/end offsets
     dst_d_half = dst_d / 2
     dst_h_half = dst_h / 2
@@ -228,8 +229,8 @@
                 [dst_w - 1, dst_h - 1, dst_d - 1],
                 [0, dst_h - 1, dst_d - 1],
             ]
         ],
         device=device,
         dtype=torch.long,
     ).expand(batch_size, -1, -1)
-    return dict(src=points_src, dst=points_dst)
+    return {"src": points_src, "dst": points_dst}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_3d/perspective.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_3d/perspective.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Dict, Union
+from typing import Dict, Tuple, Union
 
 import torch
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check
 from kornia.core import Tensor, as_tensor, stack, tensor
@@ -38,15 +38,15 @@
         self._distortion_scale = as_tensor(self.distortion_scale, device=device, dtype=dtype)
         if not (self._distortion_scale.dim() == 0 and 0 <= self._distortion_scale <= 1):
             raise AssertionError(f"'distortion_scale' must be a scalar within [0, 1]. Got {self._distortion_scale}")
         self.rand_sampler = Uniform(
             tensor(0, device=device, dtype=dtype), tensor(1, device=device, dtype=dtype), validate_args=False
         )
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         depth = batch_shape[-3]
         height = batch_shape[-2]
         width = batch_shape[-1]
 
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.distortion_scale])
@@ -82,8 +82,8 @@
         pts_norm = tensor(
             [[[1, 1, 1], [-1, 1, 1], [-1, -1, 1], [1, -1, 1], [1, 1, -1], [-1, 1, -1], [-1, -1, -1], [1, -1, -1]]],
             device=_device,
             dtype=_dtype,
         )
         end_points = start_points + factor * rand_val * pts_norm
 
-        return dict(start_points=start_points, end_points=end_points)
+        return {"start_points": start_points, "end_points": end_points}
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/_3d/rotation.py` & `kornia-0.7.0/kornia/augmentation/random_generator/_3d/rotation.py`

 * *Files 3% similar despite different names*

```diff
@@ -52,17 +52,21 @@
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         degrees = _tuple_range_reader(self.degrees, 3, device, dtype)
         self.yaw_sampler = Uniform(degrees[0][0], degrees[0][1], validate_args=False)
         self.pitch_sampler = Uniform(degrees[1][0], degrees[1][1], validate_args=False)
         self.roll_sampler = Uniform(degrees[2][0], degrees[2][1], validate_args=False)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.degrees])
 
-        return dict(
-            yaw=_adapted_rsampling((batch_size,), self.yaw_sampler, same_on_batch).to(device=_device, dtype=_dtype),
-            pitch=_adapted_rsampling((batch_size,), self.pitch_sampler, same_on_batch).to(device=_device, dtype=_dtype),
-            roll=_adapted_rsampling((batch_size,), self.roll_sampler, same_on_batch).to(device=_device, dtype=_dtype),
-        )
+        return {
+            "yaw": _adapted_rsampling((batch_size,), self.yaw_sampler, same_on_batch).to(device=_device, dtype=_dtype),
+            "pitch": _adapted_rsampling((batch_size,), self.pitch_sampler, same_on_batch).to(
+                device=_device, dtype=_dtype
+            ),
+            "roll": _adapted_rsampling((batch_size,), self.roll_sampler, same_on_batch).to(
+                device=_device, dtype=_dtype
+            ),
+        }
```

### Comparing `kornia-0.6.9/kornia/augmentation/random_generator/base.py` & `kornia-0.7.0/kornia/augmentation/random_generator/base.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,21 @@
-from typing import Callable, Dict, Optional
+from typing import Any, Callable, Dict, Optional, Tuple, Type, TypeVar
 
 import torch
 from torch.distributions import Distribution
 
 from kornia.core import Device, Module, Tensor
 
+T = TypeVar('T')
+
 
 class _PostInitInjectionMetaClass(type):
     """To inject the ``__post_init__`` function after the creation of each instance."""
 
-    def __call__(cls, *args, **kwargs):
+    def __call__(cls: Type[T], *args: Any, **kwargs: Any) -> T:
         obj = type.__call__(cls, *args, **kwargs)
         obj.__post_init__()
         return obj
 
 
 class RandomGeneratorBase(Module, metaclass=_PostInitInjectionMetaClass):
     """Base class for generating random augmentation parameters."""
@@ -37,23 +39,23 @@
         """
         if self.device != device or self.dtype != dtype:
             self.make_samplers(device, dtype)
             self.device = device
             self.dtype = dtype
 
     # TODO: refine the logic with module.to()
-    def to(self, *args, **kwargs):
+    def to(self, *args: Any, **kwargs: Any) -> 'RandomGeneratorBase':
         device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
         self.set_rng_device_and_dtype(device=device, dtype=dtype)
         return self
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         raise NotImplementedError
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+    def forward(self, batch_shape: Tuple[int, ...], same_on_batch: bool = False) -> Dict[str, Tensor]:
         raise NotImplementedError
 
 
 class DistributionWithMapper(Distribution):
     """Wraps a distribution with a value mapper function.
 
     This is used to restrict the output values of a given distribution by a value mapper function.
@@ -78,30 +80,30 @@
         tensor([0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980])
     """
 
     def __init__(self, dist: Distribution, map_fn: Optional[Callable[[Tensor], Tensor]] = None) -> None:
         self.dist = dist
         self.map_fn = map_fn
 
-    def rsample(self, sample_shape: torch.Size) -> Tensor:  # type: ignore[override]
-        out = self.dist.rsample(sample_shape)
+    def rsample(self, sample_shape: Tuple[int, ...]) -> Tensor:  # type: ignore[override]
+        out = self.dist.rsample(torch.Size(sample_shape))
         if self.map_fn is not None:
             out = self.map_fn(out)
         return out
 
-    def sample(self, sample_shape: torch.Size) -> Tensor:  # type: ignore[override]
-        out = self.dist.sample(sample_shape)
+    def sample(self, sample_shape: Tuple[int, ...]) -> Tensor:  # type: ignore[override]
+        out = self.dist.sample(torch.Size(sample_shape))
         if self.map_fn is not None:
             out = self.map_fn(out)
         return out
 
-    def sample_n(self, n) -> Tensor:
+    def sample_n(self, n: int) -> Tensor:
         out = self.dist.sample_n(n)
         if self.map_fn is not None:
             out = self.map_fn(out)
         return out
 
-    def __getattr__(self, attr):
+    def __getattr__(self, attr: str) -> Any:
         try:
             return getattr(self, attr)
         except AttributeError:
             return getattr(self.dist, attr)
```

### Comparing `kornia-0.6.9/kornia/augmentation/utils/__init__.py` & `kornia-0.7.0/kornia/augmentation/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/augmentation/utils/helpers.py` & `kornia-0.7.0/kornia/utils/helpers.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,318 +1,311 @@
+import platform
+import sys
+import warnings
 from functools import wraps
-from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+from inspect import isclass, isfunction
+from typing import TYPE_CHECKING, Any, Callable, List, Optional, Tuple, Union, overload
 
 import torch
-from torch.distributions import Beta, Uniform
+from torch.linalg import inv_ex
 
-from kornia.core import Tensor, as_tensor
-from kornia.utils import _extract_device_dtype
+from kornia.core import Tensor
+from kornia.utils._compat import torch_version_ge
 
 
-def _validate_input(f: Callable[..., Any]) -> Callable[..., Any]:
-    r"""Validate the 2D input of the wrapped function.
+def get_cuda_device_if_available(index: int = 0) -> torch.device:
+    """Tries to get cuda device, if fail, returns cpu.
 
     Args:
-        f: a function that takes the first argument as tensor.
+        index: cuda device index
 
     Returns:
-        the wrapped function after input is validated.
+        torch.device
     """
+    if torch.cuda.is_available():
+        return torch.device(f'cuda:{index}')
 
-    @wraps(f)
-    def wrapper(input: Tensor, *args, **kwargs):
-        if not torch.is_tensor(input):
-            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
+    return torch.device('cpu')
 
-        _validate_shape(input.shape, required_shapes=('BCHW',))
-        _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])
 
-        return f(input, *args, **kwargs)
-
-    return wrapper
-
-
-def _validate_input3d(f: Callable[..., Any]) -> Callable[..., Any]:
-    r"""Validate the 3D input of the wrapped function.
-
-    Args:
-        f: a function that takes the first argument as tensor.
+def get_mps_device_if_available() -> torch.device:
+    """Tries to get mps device, if fail, returns cpu.
 
     Returns:
-        the wrapped function after input is validated.
+        torch.device
     """
+    dev = 'cpu'
+    if hasattr(torch.backends, 'mps'):
+        if torch.backends.mps.is_available():
+            dev = 'mps'
+    return torch.device(dev)
 
-    @wraps(f)
-    def wrapper(input: Tensor, *args, **kwargs):
-        if not torch.is_tensor(input):
-            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
-
-        input_shape = len(input.shape)
-        if input_shape != 5:
-            raise AssertionError(f'Expect input of 5 dimensions, got {input_shape} instead')
-        _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])
-
-        return f(input, *args, **kwargs)
-
-    return wrapper
 
+def get_cuda_or_mps_device_if_available() -> torch.device:
+    """Checks OS and platform and runs get_cuda_device_if_available or get_mps_device_if_available.
 
-def _infer_batch_shape(input: Union[Tensor, Tuple[Tensor, Tensor]]) -> torch.Size:
-    r"""Infer input shape.
-
-    Input may be either (tensor,) or (tensor, transform_matrix)
-    """
-    if isinstance(input, tuple):
-        tensor = _transform_input(input[0])
-    else:
-        tensor = _transform_input(input)
-    return tensor.shape
-
-
-def _infer_batch_shape3d(input: Union[Tensor, Tuple[Tensor, Tensor]]) -> torch.Size:
-    r"""Infer input shape.
-
-    Input may be either (tensor,) or (tensor, transform_matrix)
+    Returns:
+        torch.device
     """
-    if isinstance(input, tuple):
-        tensor = _transform_input3d(input[0])
+    if sys.platform == "darwin" and platform.machine() == "arm64":
+        return get_mps_device_if_available()
     else:
-        tensor = _transform_input3d(input)
-    return tensor.shape
+        return get_cuda_device_if_available()
 
 
-def _transform_input(input: Tensor) -> Tensor:
-    r"""Reshape an input tensor to be (*, C, H, W). Accept either (H, W), (C, H, W) or (*, C, H, W).
-    Args:
-        input: Tensor
-
-    Returns:
-        Tensor
-    """
-    if not torch.is_tensor(input):
-        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
-
-    if len(input.shape) not in [2, 3, 4]:
-        raise ValueError(f"Input size must have a shape of either (H, W), (C, H, W) or (*, C, H, W). Got {input.shape}")
+@overload
+def map_location_to_cpu(storage: Tensor, location: str) -> Tensor:
+    ...
+
+
+@overload
+def map_location_to_cpu(storage: str) -> str:
+    ...
+
+
+def map_location_to_cpu(storage: Union[str, Tensor], *args: Any, **kwargs: Any) -> Union[str, Tensor]:
+    """Map location of device to CPU, util for loading things from HUB."""
+    return storage
+
+
+def deprecated(
+    replace_with: Optional[str] = None, version: Optional[str] = None, extra_reason: Optional[str] = None
+) -> Any:
+    def _deprecated(func: Callable[..., Any]) -> Any:
+        @wraps(func)
+        def wrapper(*args: Any, **kwargs: Any) -> Any:
+            name = ""
+            beginning = f'Since kornia {version} the ' if version is not None else ''
+
+            if isclass(func):
+                name = func.__class__.__name__
+            if isfunction(func):
+                name = func.__name__
+            warnings.simplefilter('always', DeprecationWarning)
+            if replace_with is not None:
+                warnings.warn(
+                    f"{beginning}`{name}` is deprecated in favor of `{replace_with}`.{extra_reason}",
+                    category=DeprecationWarning,
+                    stacklevel=2,
+                )
+            else:
+                warnings.warn(
+                    f"{beginning}`{name}` is deprecated and will be removed in the future versions.{extra_reason}",
+                    category=DeprecationWarning,
+                    stacklevel=2,
+                )
+            warnings.simplefilter('default', DeprecationWarning)
+            return func(*args, **kwargs)
 
-    if len(input.shape) == 2:
-        input = input.unsqueeze(0)
+        return wrapper
 
-    if len(input.shape) == 3:
-        input = input.unsqueeze(0)
+    return _deprecated
 
-    return input
 
+def _extract_device_dtype(tensor_list: List[Optional[Any]]) -> Tuple[torch.device, torch.dtype]:
+    """Check if all the input are in the same device (only if when they are Tensor).
 
-def _transform_input3d(input: Tensor) -> Tensor:
-    r"""Reshape an input tensor to be (*, C, D, H, W). Accept either (D, H, W), (C, D, H, W) or (*, C, D, H, W).
-    Args:
-        input: Tensor
+    If so, it would return a tuple of (device, dtype). Default: (cpu, ``get_default_dtype()``).
 
     Returns:
-        Tensor
+        [torch.device, torch.dtype]
     """
-    if not torch.is_tensor(input):
-        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
+    device, dtype = None, None
+    for tensor in tensor_list:
+        if tensor is not None:
+            if not isinstance(tensor, (Tensor,)):
+                continue
+            _device = tensor.device
+            _dtype = tensor.dtype
+            if device is None and dtype is None:
+                device = _device
+                dtype = _dtype
+            elif device != _device or dtype != _dtype:
+                raise ValueError(
+                    "Passed values are not in the same device and dtype."
+                    f"Got ({device}, {dtype}) and ({_device}, {_dtype})."
+                )
+    if device is None:
+        # TODO: update this when having torch.get_default_device()
+        device = torch.device('cpu')
+    if dtype is None:
+        dtype = torch.get_default_dtype()
+    return (device, dtype)
+
+
+def _torch_inverse_cast(input: Tensor) -> Tensor:
+    """Helper function to make torch.inverse work with other than fp32/64.
+
+    The function torch.inverse is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What
+    this function does, is cast input data type to fp32, apply torch.inverse, and cast back to the input dtype.
+    """
+    if not isinstance(input, Tensor):
+        raise AssertionError(f"Input must be Tensor. Got: {type(input)}.")
+    dtype: torch.dtype = input.dtype
+    if dtype not in (torch.float32, torch.float64):
+        dtype = torch.float32
+    return torch.inverse(input.to(dtype)).to(input.dtype)
+
+
+def _torch_histc_cast(input: Tensor, bins: int, min: int, max: int) -> Tensor:
+    """Helper function to make torch.histc work with other than fp32/64.
+
+    The function torch.histc is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What
+    this function does, is cast input data type to fp32, apply torch.inverse, and cast back to the input dtype.
+    """
+    if not isinstance(input, Tensor):
+        raise AssertionError(f"Input must be Tensor. Got: {type(input)}.")
+    dtype: torch.dtype = input.dtype
+    if dtype not in (torch.float32, torch.float64):
+        dtype = torch.float32
+    return torch.histc(input.to(dtype), bins, min, max).to(input.dtype)
+
+
+def _torch_svd_cast(input: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
+    """Helper function to make torch.svd work with other than fp32/64.
+
+    The function torch.svd is only implemented for fp32/64 which makes
+    impossible to be used by fp16 or others. What this function does, is cast
+    input data type to fp32, apply torch.svd, and cast back to the input dtype.
+
+    NOTE: in torch 1.8.1 this function is recommended to use as torch.linalg.svd
+    """
+    # if not isinstance(input, torch.Tensor):
+    #    raise AssertionError(f"Input must be torch.Tensor. Got: {type(input)}.")
+    dtype = input.dtype
+    if dtype not in (torch.float32, torch.float64):
+        dtype = torch.float32
+
+    out1, out2, out3H = torch.linalg.svd(input.to(dtype))
+    if torch_version_ge(1, 11):
+        out3 = out3H.mH
+    else:
+        out3 = out3H.transpose(-1, -2)
+    return (out1.to(input.dtype), out2.to(input.dtype), out3.to(input.dtype))
 
-    if len(input.shape) not in [3, 4, 5]:
-        raise ValueError(
-            f"Input size must have a shape of either (D, H, W), (C, D, H, W) or (*, C, D, H, W). Got {input.shape}"
-        )
 
-    if len(input.shape) == 3:
-        input = input.unsqueeze(0)
+def _torch_linalg_svdvals(input: Tensor) -> Tensor:
+    """Helper function to make torch.linalg.svdvals work with other than fp32/64.
 
-    if len(input.shape) == 4:
-        input = input.unsqueeze(0)
+    The function torch.svd is only implemented for fp32/64 which makes
+    impossible to be used by fp16 or others. What this function does, is cast
+    input data type to fp32, apply torch.svd, and cast back to the input dtype.
+
+    NOTE: in torch 1.8.1 this function is recommended to use as torch.linalg.svd
+    """
+    if not isinstance(input, Tensor):
+        raise AssertionError(f"Input must be Tensor. Got: {type(input)}.")
+    dtype: torch.dtype = input.dtype
+    if dtype not in (torch.float32, torch.float64):
+        dtype = torch.float32
+
+    if TYPE_CHECKING:
+        # TODO: remove this branch when kornia relies on torch >= 1.10
+        out: Tensor
+    elif torch_version_ge(1, 10):
+        out = torch.linalg.svdvals(input.to(dtype))
+    else:
+        # TODO: remove this branch when kornia relies on torch >= 1.10
+        _, out, _ = torch.linalg.svd(input.to(dtype))
+    return out.to(input.dtype)
+
+
+# TODO: return only `Tensor` and review all the calls to adjust
+def _torch_solve_cast(A: Tensor, B: Tensor) -> Tensor:
+    """Helper function to make torch.solve work with other than fp32/64.
+
+    The function torch.solve is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What
+    this function does, is cast input data type to fp32, apply torch.svd, and cast back to the input dtype.
+    """
+    dtype: torch.dtype = A.dtype
+    if dtype not in (torch.float32, torch.float64):
+        dtype = torch.float32
+
+    out = torch.linalg.solve(A.to(dtype), B.to(dtype))
+
+    return out.to(A.dtype)
+
+
+def safe_solve_with_mask(B: Tensor, A: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
+    r"""Helper function, which avoids crashing because of singular matrix input and outputs the mask of valid
+    solution."""
+    if not torch_version_ge(1, 10):
+        sol = _torch_solve_cast(A, B)
+        warnings.warn('PyTorch version < 1.10, solve validness mask maybe not correct', RuntimeWarning)
+        return sol, sol, torch.ones(len(A), dtype=torch.bool, device=A.device)
+    # Based on https://github.com/pytorch/pytorch/issues/31546#issuecomment-694135622
+    if not isinstance(B, Tensor):
+        raise AssertionError(f"B must be Tensor. Got: {type(B)}.")
+    dtype: torch.dtype = B.dtype
+    if dtype not in (torch.float32, torch.float64):
+        dtype = torch.float32
+
+    if TYPE_CHECKING:
+        # TODO: remove this branch when kornia relies on torch >= 1.13
+        A_LU: Tensor
+        pivots: Tensor
+        info: Tensor
+    elif torch_version_ge(1, 13):
+        A_LU, pivots, info = torch.linalg.lu_factor_ex(A.to(dtype))
+    else:
+        # TODO: remove this branch when kornia relies on torch >= 1.13
+        A_LU, pivots, info = torch.lu(A.to(dtype), True, get_infos=True)
 
-    return input
+    valid_mask: Tensor = info == 0
+    n_dim_B = len(B.shape)
+    n_dim_A = len(A.shape)
+    if n_dim_A - n_dim_B == 1:
+        B = B.unsqueeze(-1)
+
+    if TYPE_CHECKING:
+        # TODO: remove this branch when kornia relies on torch >= 1.13
+        X: Tensor
+    elif torch_version_ge(1, 13):
+        X = torch.linalg.lu_solve(A_LU, pivots, B.to(dtype))
+    else:
+        # TODO: remove this branch when kornia relies on torch >= 1.13
+        X = torch.lu_solve(B.to(dtype), A_LU, pivots)
 
+    return X.to(B.dtype), A_LU.to(A.dtype), valid_mask
 
-def _validate_input_dtype(input: Tensor, accepted_dtypes: List[torch.dtype]) -> None:
-    r"""Check if the dtype of the input tensor is in the range of accepted_dtypes
-    Args:
-        input: Tensor
-        accepted_dtypes: List. e.g. [torch.float32, torch.float64]
-    """
-    if input.dtype not in accepted_dtypes:
-        raise TypeError(f"Expected input of {accepted_dtypes}. Got {input.dtype}")
 
+def safe_inverse_with_mask(A: Tensor) -> Tuple[Tensor, Tensor]:
+    r"""Helper function, which avoids crashing because of non-invertable matrix input and outputs the mask of valid
+    solution."""
 
-def _transform_output_shape(output: Tensor, shape: Tuple[int, ...]) -> Tensor:
-    r"""Collapse the broadcasted batch dimensions an input tensor to be the specified shape.
-    Args:
-        input: Tensor
-        shape: List/tuple of int
+    if not isinstance(A, Tensor):
+        raise AssertionError(f"A must be Tensor. Got: {type(A)}.")
 
-    Returns:
-        Tensor
-    """
-    out_tensor = output.clone()
+    dtype_original = A.dtype
+    if dtype_original not in (torch.float32, torch.float64):
+        dtype = torch.float32
+    else:
+        dtype = dtype_original
 
-    for dim in range(len(out_tensor.shape) - len(shape)):
-        if out_tensor.shape[0] != 1:
-            raise AssertionError(f'Dimension {dim} of input is ' f'expected to be 1, got {out_tensor.shape[0]}')
-        out_tensor = out_tensor.squeeze(0)
+    inverse, info = inv_ex(A.to(dtype))
+    mask = info == 0
+    return inverse.to(dtype_original), mask
 
-    return out_tensor
 
+def is_autocast_enabled(both: bool = True) -> bool:
+    """Check if torch autocast is enabled.
 
-def _validate_shape(shape: Union[Tuple[int, ...], torch.Size], required_shapes: Tuple[str, ...] = ("BCHW",)) -> None:
-    r"""Check if the dtype of the input tensor is in the range of accepted_dtypes
     Args:
-        shape: tensor shape
-        required_shapes: List. e.g. ["BCHW", "BCDHW"]
-    """
-    passed = False
-    for required_shape in required_shapes:
-        if len(shape) == len(required_shape):
-            passed = True
-            break
-    if not passed:
-        raise TypeError(f"Expected input shape in {required_shape}. Got {shape}.")
-
+        both: if True will consider autocast region for both types of devices
 
-def _validate_input_shape(input: Tensor, channel_index: int, number: int) -> bool:
-    r"""Validate if an input has the right shape.
-
-    e.g. to check if an input is channel first.
-    If channel first, the second channel of an RGB input shall be fixed to 3. To verify using:
-        _validate_input_shape(input, 1, 3)
-    Args:
-        input: Tensor
-        channel_index: int
-        number: int
     Returns:
-        bool
-    """
-    return input.shape[channel_index] == number
-
-
-def _adapted_rsampling(
-    shape: Union[Tuple[int, ...], torch.Size], dist: torch.distributions.Distribution, same_on_batch=False
-) -> Tensor:
-    r"""The uniform reparameterized sampling function that accepts 'same_on_batch'.
-
-    If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]). By default,
-    same_on_batch is set to False.
-    """
-    if isinstance(shape, tuple):
-        shape = torch.Size(shape)
-
-    if same_on_batch:
-        return dist.rsample(torch.Size((1, *shape[1:]))).repeat(shape[0], *[1] * (len(shape) - 1))
-    return dist.rsample(shape)
-
-
-def _adapted_sampling(
-    shape: Union[Tuple[int, ...], torch.Size], dist: torch.distributions.Distribution, same_on_batch=False
-) -> Tensor:
-    r"""The uniform sampling function that accepts 'same_on_batch'.
-
-    If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]). By default,
-    same_on_batch is set to False.
-    """
-    if isinstance(shape, tuple):
-        shape = torch.Size(shape)
+        Return a Bool,
+        will always return False for a torch without support, otherwise will be: if both is True
+        `torch.is_autocast_enabled() or torch.is_autocast_cpu_enabled()`. If both is False will return just
+        `torch.is_autocast_enabled()`.
+    """
+    if TYPE_CHECKING:
+        # TODO: remove this branch when kornia relies on torch >= 1.10.2
+        return False
 
-    if same_on_batch:
-        return dist.sample(torch.Size((1, *shape[1:]))).repeat(shape[0], *[1] * (len(shape) - 1))
-    return dist.sample(shape)
-
-
-def _adapted_uniform(
-    shape: Union[Tuple[int, ...], torch.Size],
-    low: Union[float, int, Tensor],
-    high: Union[float, int, Tensor],
-    same_on_batch: bool = False,
-) -> Tensor:
-    r"""The uniform sampling function that accepts 'same_on_batch'.
+    if not torch_version_ge(1, 10, 2):
+        return False
 
-    If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]).
-    By default, same_on_batch is set to False.
+    if both:
+        return torch.is_autocast_enabled() or torch.is_autocast_cpu_enabled()
 
-    By default, sampling happens on the default device and dtype. If low/high is a tensor, sampling will happen
-    in the same device/dtype as low/high tensor.
-    """
-    device, dtype = _extract_device_dtype(
-        [low if isinstance(low, Tensor) else None, high if isinstance(high, Tensor) else None]
-    )
-    low = as_tensor(low, device=device, dtype=dtype)
-    high = as_tensor(high, device=device, dtype=dtype)
-    # validate_args=False to fix pytorch 1.7.1 error:
-    #     ValueError: Uniform is not defined when low>= high.
-    dist = Uniform(low, high, validate_args=False)
-    return _adapted_rsampling(shape, dist, same_on_batch)
-
-
-def _adapted_beta(
-    shape: Union[Tuple[int, ...], torch.Size],
-    a: Union[float, int, Tensor],
-    b: Union[float, int, Tensor],
-    same_on_batch: bool = False,
-) -> Tensor:
-    r"""The beta sampling function that accepts 'same_on_batch'.
-
-    If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]).
-    By default, same_on_batch is set to False.
-
-    By default, sampling happens on the default device and dtype. If a/b is a tensor, sampling will happen
-    in the same device/dtype as a/b tensor.
-    """
-    device, dtype = _extract_device_dtype([a if isinstance(a, Tensor) else None, b if isinstance(b, Tensor) else None])
-    a = as_tensor(a, device=device, dtype=dtype)
-    b = as_tensor(b, device=device, dtype=dtype)
-    dist = Beta(a, b, validate_args=False)
-    return _adapted_rsampling(shape, dist, same_on_batch)
-
-
-def _shape_validation(param: Tensor, shape: Union[Tuple[int, ...], List[int]], name: str) -> None:
-    if param.shape != torch.Size(shape):
-        raise AssertionError(f"Invalid shape for {name}. Expected {shape}. Got {param.shape}")
-
-
-def deepcopy_dict(params: Dict[str, Any]) -> Dict[str, Any]:
-    """Perform deep copy on any dict.
-
-    Support tensor copying here.
-    """
-    out = {}
-    for k, v in params.items():
-        # NOTE: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol
-        if isinstance(v, Tensor):
-            out.update({k: v.clone()})
-        else:
-            out.update({k: v})
-    return out
-
-
-def override_parameters(
-    params: Dict[str, Any],
-    params_override: Optional[Dict[str, Any]] = None,
-    if_none_exist: str = 'ignore',
-    in_place: bool = False,
-) -> Dict[str, Any]:
-    """Override params dict w.r.t params_override.
-
-    Args:
-        params: source parameters.
-        params_override: key-values to override the source parameters.
-        if_none_exist: behaviour if the key in `params_override` does not exist in `params`.
-            'raise' | 'ignore'.
-        in_place: if to override in-place or not.
-    """
-
-    if params_override is None:
-        return params
-    out = params if in_place else deepcopy_dict(params)
-    for k, v in params_override.items():
-        if k in params_override:
-            out[k] = v
-        else:
-            if if_none_exist == 'ignore':
-                pass
-            elif if_none_exist == 'raise':
-                raise RuntimeError(f"Param `{k}` not existed in `{params_override}`.")
-            else:
-                raise ValueError(f"`{if_none_exist}` is not a valid option.")
-    return out
+    return torch.is_autocast_enabled()
```

### Comparing `kornia-0.6.9/kornia/augmentation/utils/param_validation.py` & `kornia-0.7.0/kornia/augmentation/utils/param_validation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,39 +1,41 @@
 from typing import Any, List, Optional, Tuple, Union
 
 import torch
 
 from kornia.core import Tensor, as_tensor, tensor
 
 
-def _common_param_check(batch_size: int, same_on_batch: Optional[bool] = None):
+def _common_param_check(batch_size: int, same_on_batch: Optional[bool] = None) -> None:
     """Valid batch_size and same_on_batch params."""
     if not (type(batch_size) is int and batch_size >= 0):
         raise AssertionError(f"`batch_size` shall be a positive integer. Got {batch_size}.")
     if same_on_batch is not None and type(same_on_batch) is not bool:
         raise AssertionError(f"`same_on_batch` shall be boolean. Got {same_on_batch}.")
 
 
 def _range_bound(
     factor: Union[Tensor, float, Tuple[float, float], List[float]],
     name: str,
-    center: float = 0.0,
-    bounds: Tuple[float, float] = (0, float('inf')),
+    center: Optional[float] = 0.0,
+    bounds: Optional[Tuple[float, float]] = (0, float('inf')),
     check: Optional[str] = 'joint',
     device: torch.device = torch.device('cpu'),
     dtype: torch.dtype = torch.get_default_dtype(),
 ) -> Tensor:
     r"""Check inputs and compute the corresponding factor bounds."""
     if not isinstance(factor, (Tensor)):
         factor = tensor(factor, device=device, dtype=dtype)
     factor_bound: Tensor
 
     if factor.dim() == 0:
         if factor < 0:
-            raise ValueError(f"If {name} is a single number number, it must be non negative. Got {factor}")
+            raise ValueError(f"If {name} is a single number, it must be non negative. Got {factor}.")
+        if center is None or bounds is None:
+            raise ValueError(f"`center` and `bounds` cannot be None for single number. Got {center}, {bounds}.")
         # Should be something other than clamp
         # Currently, single value factor will not out of scope as long as the user provided it.
         # Note: I personally think throw an error will be better than a coarse clamp.
         factor_bound = factor.repeat(2) * tensor([-1.0, 1.0], device=factor.device, dtype=factor.dtype) + center
         factor_bound = factor_bound.clamp(bounds[0], bounds[1]).to(device=device, dtype=dtype)
     else:
         factor_bound = as_tensor(factor, device=device, dtype=dtype)
@@ -133,42 +135,41 @@
             input_range_tmp = input_range.to(device=device, dtype=dtype)
 
         else:
             raise ValueError(
                 f"Degrees must be a {list(target_shape)} tensor for the degree range for independent operation."
                 f"Got {input_range}"
             )
-    else:
-        if isinstance(input_range, (float, int)):
-            if input_range < 0:
-                raise ValueError(f"If input_range is only one number it must be a positive number. Got{input_range}")
-            input_range_tmp = tensor([-input_range, input_range], device=device, dtype=dtype).repeat(target_shape[0], 1)
+    elif isinstance(input_range, (float, int)):
+        if input_range < 0:
+            raise ValueError(f"If input_range is only one number it must be a positive number. Got{input_range}")
+        input_range_tmp = tensor([-input_range, input_range], device=device, dtype=dtype).repeat(target_shape[0], 1)
+
+    elif (
+        isinstance(input_range, (tuple, list))
+        and len(input_range) == 2
+        and isinstance(input_range[0], (float, int))
+        and isinstance(input_range[1], (float, int))
+    ):
+        input_range_tmp = tensor(input_range, device=device, dtype=dtype).repeat(target_shape[0], 1)
+
+    elif (
+        isinstance(input_range, (tuple, list))
+        and len(input_range) == target_shape[0]
+        and all(isinstance(x, (float, int)) for x in input_range)
+    ):
+        input_range_tmp = tensor([(-s, s) for s in input_range], device=device, dtype=dtype)
+
+    elif (
+        isinstance(input_range, (tuple, list))
+        and len(input_range) == target_shape[0]
+        and all(isinstance(x, (tuple, list)) for x in input_range)
+    ):
+        input_range_tmp = tensor(input_range, device=device, dtype=dtype)
 
-        elif (
-            isinstance(input_range, (tuple, list))
-            and len(input_range) == 2
-            and isinstance(input_range[0], (float, int))
-            and isinstance(input_range[1], (float, int))
-        ):
-            input_range_tmp = tensor(input_range, device=device, dtype=dtype).repeat(target_shape[0], 1)
-
-        elif (
-            isinstance(input_range, (tuple, list))
-            and len(input_range) == target_shape[0]
-            and all(isinstance(x, (float, int)) for x in input_range)
-        ):
-            input_range_tmp = tensor([(-s, s) for s in input_range], device=device, dtype=dtype)
-
-        elif (
-            isinstance(input_range, (tuple, list))
-            and len(input_range) == target_shape[0]
-            and all(isinstance(x, (tuple, list)) for x in input_range)
-        ):
-            input_range_tmp = tensor(input_range, device=device, dtype=dtype)
-
-        else:
-            raise TypeError(
-                "If not pass a tensor, it must be float, (float, float) for isotropic operation or a tuple of "
-                f"{target_size} floats or {target_size} (float, float) for independent operation. Got {input_range}."
-            )
+    else:
+        raise TypeError(
+            "If not pass a tensor, it must be float, (float, float) for isotropic operation or a tuple of "
+            f"{target_size} floats or {target_size} (float, float) for independent operation. Got {input_range}."
+        )
 
     return input_range_tmp
```

### Comparing `kornia-0.6.9/kornia/color/__init__.py` & `kornia-0.7.0/kornia/color/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+from .colormap import AUTUMN, ApplyColorMap, ColorMap, RGBColor, apply_colormap
 from .gray import BgrToGrayscale, GrayscaleToRgb, RgbToGrayscale, bgr_to_grayscale, grayscale_to_rgb, rgb_to_grayscale
 from .hls import HlsToRgb, RgbToHls, hls_to_rgb, rgb_to_hls
 from .hsv import HsvToRgb, RgbToHsv, hsv_to_rgb, rgb_to_hsv
 from .lab import LabToRgb, RgbToLab, lab_to_rgb, rgb_to_lab
 from .luv import LuvToRgb, RgbToLuv, luv_to_rgb, rgb_to_luv
 from .raw import CFA, RawToRgb, RgbToRaw, raw_to_rgb, rgb_to_raw
 from .rgb import (
@@ -103,10 +104,15 @@
     "rgba_to_rgb",
     "rgba_to_bgr",
     "RgbaToRgb",
     "RgbaToBgr",
     "RgbToLinearRgb",
     "Sepia",
     "sepia",
+    "AUTUMN",
+    "ApplyColorMap",
+    "ColorMap",
+    "RGBColor",
+    "apply_colormap",
 ]
 
 sepia = sepia_from_rgb
```

### Comparing `kornia-0.6.9/kornia/color/gray.py` & `kornia-0.7.0/kornia/color/gray.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-from typing import Optional
+from __future__ import annotations
 
 import torch
 
 from kornia.color.rgb import bgr_to_rgb
 from kornia.core import Module, Tensor, concatenate
-from kornia.testing import KORNIA_CHECK_IS_TENSOR
+from kornia.core.check import KORNIA_CHECK_IS_TENSOR
 
 
 def grayscale_to_rgb(image: Tensor) -> Tensor:
     r"""Convert a grayscale image to RGB version of image.
 
     .. image:: _static/img/grayscale_to_rgb.png
 
     The image data is assumed to be in the range of (0, 1).
 
     Args:
-        image: grayscale image to be converted to RGB with shape :math:`(*,1,H,W)`.
+        image: grayscale image tensor to be converted to RGB with shape :math:`(*,1,H,W)`.
 
     Returns:
         RGB version of the image with shape :math:`(*,3,H,W)`.
 
     Example:
         >>> input = torch.randn(2, 1, 4, 5)
         >>> gray = grayscale_to_rgb(input) # 2x3x4x5
     """
     KORNIA_CHECK_IS_TENSOR(image)
 
-    if image.dim() < 3 or image.size(-3) != 1:
+    if len(image.shape) < 3 or image.shape[-3] != 1:
         raise ValueError(f"Input size must have a shape of (*, 1, H, W). " f"Got {image.shape}.")
 
     return concatenate([image, image, image], -3)
 
 
-def rgb_to_grayscale(image: Tensor, rgb_weights: Optional[Tensor] = None) -> Tensor:
+def rgb_to_grayscale(image: Tensor, rgb_weights: Tensor | None = None) -> Tensor:
     r"""Convert a RGB image to grayscale version of image.
 
     .. image:: _static/img/rgb_to_grayscale.png
 
     The image data is assumed to be in the range of (0, 1).
 
     Args:
@@ -77,15 +77,15 @@
     g: Tensor = image[..., 1:2, :, :]
     b: Tensor = image[..., 2:3, :, :]
 
     w_r, w_g, w_b = rgb_weights.unbind()
     return w_r * r + w_g * g + w_b * b
 
 
-def bgr_to_grayscale(image: torch.Tensor) -> torch.Tensor:
+def bgr_to_grayscale(image: Tensor) -> Tensor:
     r"""Convert a BGR image to grayscale.
 
     The image data is assumed to be in the range of (0, 1). First flips to RGB, then converts.
 
     Args:
         image: BGR image to be converted to grayscale with shape :math:`(*,3,H,W)`.
 
@@ -141,16 +141,18 @@
 
     Example:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> gray = RgbToGrayscale()
         >>> output = gray(input)  # 2x1x4x5
     """
 
-    def __init__(self, rgb_weights: Optional[Tensor] = None) -> None:
+    def __init__(self, rgb_weights: Tensor | None = None) -> None:
         super().__init__()
+        if rgb_weights is None:
+            rgb_weights = Tensor([0.299, 0.587, 0.114])
         self.rgb_weights = rgb_weights
 
     def forward(self, image: Tensor) -> Tensor:
         return rgb_to_grayscale(image, rgb_weights=self.rgb_weights)
 
 
 class BgrToGrayscale(Module):
```

### Comparing `kornia-0.6.9/kornia/color/hls.py` & `kornia-0.7.0/kornia/color/hls.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 
 import torch
 
 from kornia.core import Module, Tensor, stack, tensor, where
 
 
 def rgb_to_hls(image: Tensor, eps: float = 1e-8) -> Tensor:
-    r"""Convert a RGB image to HLS.
+    r"""Convert an RGB image to HLS.
 
     .. image:: _static/img/rgb_to_hls.png
 
     The image data is assumed to be in the range of (0, 1).
 
     NOTE: this method cannot be compiled with JIT in pytohrch < 1.7.0
 
@@ -47,41 +47,38 @@
         h = l_  # assign to any tensor...
         image_hls = l_  # assign to any tensor...
     else:
         # define the resulting image to avoid the torch.stack([h, l, s])
         # so, h, l and s require inplace operations
         # NOTE: stack() increases in a 10% the cost in colab
         image_hls = torch.empty_like(image)
-        h = torch.select(image_hls, -3, 0)
-        l_ = torch.select(image_hls, -3, 1)
-        s = torch.select(image_hls, -3, 2)
+        h, l_, s = image_hls[..., 0, :, :], image_hls[..., 1, :, :], image_hls[..., 2, :, :]
         torch.add(maxc, minc, out=l_)  # l = max + min
         torch.sub(maxc, minc, out=s)  # s = max - min
 
     # precompute image / (max - min)
     im = image / (s + eps).unsqueeze(-3)
 
     # epsilon cannot be inside the torch.where to avoid precision issues
     s /= where(l_ < 1.0, l_, 2.0 - l_) + eps  # saturation
     l_ /= 2  # luminance
 
     # note that r,g and b were previously div by (max - min)
-    r = torch.select(im, -3, 0)
-    g = torch.select(im, -3, 1)
-    b = torch.select(im, -3, 2)
+    r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
     # h[imax == 0] = (((g - b) / (max - min)) % 6)[imax == 0]
     # h[imax == 1] = (((b - r) / (max - min)) + 2)[imax == 1]
     # h[imax == 2] = (((r - g) / (max - min)) + 4)[imax == 2]
-    cond = imax.unsqueeze(-3) == _RGB2HSL_IDX
+    cond = imax[..., None, :, :] == _RGB2HSL_IDX
     if image.requires_grad:
-        h = torch.mul((g - b) % 6, torch.select(cond, -3, 0))
+        h = ((g - b) % 6) * cond[..., 0, :, :]
     else:
-        torch.mul((g - b).remainder(6), torch.select(cond, -3, 0), out=h)
-    h += torch.add(b - r, 2) * torch.select(cond, -3, 1)
-    h += torch.add(r - g, 4) * torch.select(cond, -3, 2)
+        # replacing `torch.mul` with `out=h` with python * operator gives wrong results
+        torch.mul((g - b) % 6, cond[..., 0, :, :], out=h)
+    h += (b - r + 2) * cond[..., 1, :, :]
+    h += (r - g + 4) * cond[..., 2, :, :]
     # h = 2.0 * math.pi * (60.0 * h) / 360.0
     h *= math.pi / 3.0  # hue [0, 2*pi]
 
     if image.requires_grad:
         return stack([h, l_, s], -3)
     return image_hls
 
@@ -106,28 +103,28 @@
 
     if len(image.shape) < 3 or image.shape[-3] != 3:
         raise ValueError(f"Input size must have a shape of (*, 3, H, W). Got {image.shape}")
 
     _HLS2RGB = tensor([[[0.0]], [[8.0]], [[4.0]]], device=image.device, dtype=image.dtype)  # 3x1x1
 
     im: Tensor = image.unsqueeze(-4)
-    h: Tensor = torch.select(im, -3, 0)
-    l: Tensor = torch.select(im, -3, 1)
-    s: Tensor = torch.select(im, -3, 2)
-    h = h * (6 / math.pi)  # h * 360 / (2 * math.pi) / 30
-    a = s * torch.min(l, 1.0 - l)
+    h_ch: Tensor = im[..., 0, :, :]
+    l_ch: Tensor = im[..., 1, :, :]
+    s_ch: Tensor = im[..., 2, :, :]
+    h_ch = h_ch * (6 / math.pi)  # h * 360 / (2 * math.pi) / 30
+    a = s_ch * torch.min(l_ch, 1.0 - l_ch)
 
     # kr = (0 + h) % 12
     # kg = (8 + h) % 12
     # kb = (4 + h) % 12
-    k: Tensor = (h + _HLS2RGB) % 12
+    k: Tensor = (h_ch + _HLS2RGB) % 12
 
     # l - a * max(min(min(k - 3.0, 9.0 - k), 1), -1)
     mink = torch.min(k - 3.0, 9.0 - k)
-    return torch.addcmul(l, a, mink.clamp_(min=-1.0, max=1.0), value=-1)
+    return torch.addcmul(l_ch, a, mink.clamp_(min=-1.0, max=1.0), value=-1)
 
 
 class RgbToHls(Module):
     r"""Convert an image from RGB to HLS.
 
     The image data is assumed to be in the range of (0, 1).
```

### Comparing `kornia-0.6.9/kornia/color/hsv.py` & `kornia-0.7.0/kornia/color/hsv.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import math
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 
 def rgb_to_hsv(image: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
     r"""Convert an image from RGB to HSV.
 
     .. image:: _static/img/rgb_to_hsv.png
```

### Comparing `kornia-0.6.9/kornia/color/lab.py` & `kornia-0.7.0/kornia/color/lab.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,35 +1,34 @@
-import torch
-import torch.nn as nn
-
-from .rgb import linear_rgb_to_rgb, rgb_to_linear_rgb
-from .xyz import rgb_to_xyz, xyz_to_rgb
+"""The RGB to Lab color transformations were translated from scikit image's rgb2lab and lab2rgb.
 
+https://github.com/scikit-image/scikit-image/blob/a48bf6774718c64dade4548153ae16065b595ca9/skimage/color/colorconv.py
 """
-The RGB to Lab color transformations were translated from scikit image's rgb2lab and lab2rgb
 
-https://github.com/scikit-image/scikit-image/blob/a48bf6774718c64dade4548153ae16065b595ca9/skimage/color/colorconv.py
 
-"""
+import torch
+from torch import nn
+
+from .rgb import linear_rgb_to_rgb, rgb_to_linear_rgb
+from .xyz import rgb_to_xyz, xyz_to_rgb
 
 
 def rgb_to_lab(image: torch.Tensor) -> torch.Tensor:
     r"""Convert a RGB image to Lab.
 
     .. image:: _static/img/rgb_to_lab.png
 
-    The image data is assumed to be in the range of :math:`[0, 1]`. Lab
+    The input RGB image is assumed to be in the range of :math:`[0, 1]`. Lab
     color is computed using the D65 illuminant and Observer 2.
 
     Args:
         image: RGB Image to be converted to Lab with shape :math:`(*, 3, H, W)`.
 
     Returns:
         Lab version of the image with shape :math:`(*, 3, H, W)`.
-        The L channel values are in the range 0..100. a and b are in the range -127..127.
+        The L channel values are in the range 0..100. a and b are in the range -128..127.
 
     Example:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> output = rgb_to_lab(input)  # 2x3x4x5
     """
     if not isinstance(image, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(image)}")
@@ -63,20 +62,24 @@
 
     return out
 
 
 def lab_to_rgb(image: torch.Tensor, clip: bool = True) -> torch.Tensor:
     r"""Convert a Lab image to RGB.
 
+    The L channel is assumed to be in the range of :math:`[0, 100]`.
+    a and b channels are in the range of :math:`[-128, 127]`.
+
     Args:
         image: Lab image to be converted to RGB with shape :math:`(*, 3, H, W)`.
         clip: Whether to apply clipping to insure output RGB values in range :math:`[0, 1]`.
 
     Returns:
         Lab version of the image with shape :math:`(*, 3, H, W)`.
+        The output RGB image are in the range of :math:`[0, 1]`.
 
     Example:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> output = lab_to_rgb(input)  # 2x3x4x5
     """
     if not isinstance(image, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(image)}")
```

### Comparing `kornia-0.6.9/kornia/color/luv.py` & `kornia-0.7.0/kornia/color/luv.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,21 @@
+"""The RGB to Luv color transformations were translated from scikit image's rgb2luv and luv2rgb.
+
+https://github.com/scikit-image/scikit-image/blob/a48bf6774718c64dade4548153ae16065b595ca9/skimage/color/colorconv.py
+"""
+
+
 from typing import Tuple
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from .rgb import linear_rgb_to_rgb, rgb_to_linear_rgb
 from .xyz import rgb_to_xyz, xyz_to_rgb
 
-"""
-The RGB to Luv color transformations were translated from scikit image's rgb2luv and luv2rgb
-
-https://github.com/scikit-image/scikit-image/blob/a48bf6774718c64dade4548153ae16065b595ca9/skimage/color/colorconv.py
-
-"""
-
 
 def rgb_to_luv(image: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:
     r"""Convert a RGB image to Luv.
 
     .. image:: _static/img/rgb_to_luv.png
 
     The image data is assumed to be in the range of :math:`[0, 1]`. Luv
```

### Comparing `kornia-0.6.9/kornia/color/raw.py` & `kornia-0.7.0/kornia/color/raw.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from enum import Enum
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 
 class CFA(Enum):
     r"""Define the configuration of the color filter array.
 
     So far only bayer images is supported and the enum sets the pixel order for bayer. Note that this can change due
     to things like rotations and cropping of images. Take care if including the translations in pipeline.
```

### Comparing `kornia-0.6.9/kornia/color/rgb.py` & `kornia-0.7.0/kornia/color/rgb.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Union, cast
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 
 def rgb_to_bgr(image: torch.Tensor) -> torch.Tensor:
     r"""Convert a RGB image to BGR.
 
     .. image:: _static/img/rgb_to_bgr.png
 
@@ -186,15 +186,14 @@
     Returns:
         linear RGB version of the image with shape of :math:`(*,3,H,W)`.
 
     Example:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> output = rgb_to_linear_rgb(input) # 2x3x4x5
     """
-
     if not isinstance(image, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(image)}")
 
     if len(image.shape) < 3 or image.shape[-3] != 3:
         raise ValueError(f"Input size must have a shape of (*, 3, H, W).Got {image.shape}")
 
     lin_rgb: torch.Tensor = torch.where(image > 0.04045, torch.pow(((image + 0.055) / 1.055), 2.4), image / 12.92)
@@ -211,15 +210,14 @@
     Returns:
         sRGB version of the image with shape of shape :math:`(*,3,H,W)`.
 
     Example:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> output = linear_rgb_to_rgb(input) # 2x3x4x5
     """
-
     if not isinstance(image, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(image)}")
 
     if len(image.shape) < 3 or image.shape[-3] != 3:
         raise ValueError(f"Input size must have a shape of (*, 3, H, W).Got {image.shape}")
 
     threshold = 0.0031308
```

### Comparing `kornia-0.6.9/kornia/color/sepia.py` & `kornia-0.7.0/kornia/color/sepia.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/color/xyz.py` & `kornia-0.7.0/kornia/color/xyz.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import torch
-import torch.nn as nn
+from torch import nn
 
 
 def rgb_to_xyz(image: torch.Tensor) -> torch.Tensor:
     r"""Convert a RGB image to XYZ.
 
     .. image:: _static/img/rgb_to_xyz.png
```

### Comparing `kornia-0.6.9/kornia/color/ycbcr.py` & `kornia-0.7.0/kornia/color/ycbcr.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 import torch
-import torch.nn as nn
-from torch import Tensor
+from torch import Tensor, nn
 
 
 def _rgb_to_y(r: Tensor, g: Tensor, b: Tensor) -> Tensor:
     y: Tensor = 0.299 * r + 0.587 * g + 0.114 * b
     return y
```

### Comparing `kornia-0.6.9/kornia/color/yuv.py` & `kornia-0.7.0/kornia/color/yuv.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Tuple
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 
 def rgb_to_yuv(image: torch.Tensor) -> torch.Tensor:
     r"""Convert an RGB image to YUV.
 
     .. image:: _static/img/rgb_to_yuv.png
 
@@ -64,15 +64,15 @@
         raise ValueError(f"Input size must have a shape of (*, 3, H, W). Got {image.shape}")
 
     if len(image.shape) < 2 or image.shape[-2] % 2 == 1 or image.shape[-1] % 2 == 1:
         raise ValueError(f"Input H&W must be evenly disible by 2. Got {image.shape}")
 
     yuvimage = rgb_to_yuv(image)
 
-    return (yuvimage[..., :1, :, :], torch.nn.functional.avg_pool2d(yuvimage[..., 1:3, :, :], (2, 2)))
+    return (yuvimage[..., :1, :, :], yuvimage[..., 1:3, :, :].unfold(-2, 2, 2).unfold(-2, 2, 2).mean((-1, -2)))
 
 
 def rgb_to_yuv422(image: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
     r"""Convert an RGB image to YUV 422 (subsampled).
 
     The image data is assumed to be in the range of (0, 1). Input need to be padded to be evenly divisible by 2
     vertical. This function will output chroma siting (0.5)
@@ -95,15 +95,15 @@
         raise ValueError(f"Input size must have a shape of (*, 3, H, W). Got {image.shape}")
 
     if len(image.shape) < 2 or image.shape[-2] % 2 == 1 or image.shape[-1] % 2 == 1:
         raise ValueError(f"Input H&W must be evenly disible by 2. Got {image.shape}")
 
     yuvimage = rgb_to_yuv(image)
 
-    return (yuvimage[..., :1, :, :], torch.nn.functional.avg_pool2d(yuvimage[..., 1:3, :, :], (1, 2)))
+    return (yuvimage[..., :1, :, :], yuvimage[..., 1:3, :, :].unfold(-1, 2, 2).mean(-1))
 
 
 def yuv_to_rgb(image: torch.Tensor) -> torch.Tensor:
     r"""Convert an YUV image to RGB.
 
     The image data is assumed to be in the range of (0, 1) for luma and (-0.5, 0.5) for chroma.
 
@@ -151,15 +151,14 @@
         RGB version of the image with shape :math:`(*, 3, H, W)`.
 
     Example:
         >>> inputy = torch.rand(2, 1, 4, 6)
         >>> inputuv = torch.rand(2, 2, 2, 3)
         >>> output = yuv420_to_rgb(inputy, inputuv)  # 2x3x4x6
     """
-
     if not isinstance(imagey, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(imagey)}")
 
     if not isinstance(imageuv, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(imageuv)}")
 
     if len(imagey.shape) < 3 or imagey.shape[-3] != 1:
@@ -202,15 +201,14 @@
         RGB version of the image with shape :math:`(*, 3, H, W)`.
 
     Example:
         >>> inputy = torch.rand(2, 1, 4, 6)
         >>> inputuv = torch.rand(2, 2, 2, 3)
         >>> output = yuv420_to_rgb(inputy, inputuv)  # 2x3x4x5
     """
-
     if not isinstance(imagey, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(imagey)}")
 
     if not isinstance(imageuv, torch.Tensor):
         raise TypeError(f"Input type is not a torch.Tensor. Got {type(imageuv)}")
 
     if len(imagey.shape) < 3 or imagey.shape[-3] != 1:
```

### Comparing `kornia-0.6.9/kornia/constants.py` & `kornia-0.7.0/kornia/constants.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,15 +23,15 @@
             return any(val.name.upper() == other.upper() for val in self)
 
         elif isinstance(other, int):
             return any(val.value == other for val in self)
 
         return any(val == other for val in self)
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return ' | '.join(f"{self.__name__}.{val.name}" for val in self)
 
 
 def _get(cls: Type[T], value: TKEnum[T]) -> T:
     if isinstance(value, str):
         return cls[value.upper()]
 
@@ -114,19 +114,20 @@
 
         elif data == DType.FLOAT32:
             return torch.float32
 
         elif data == DType.FLOAT64:
             return torch.float64
 
-        raise ValueError()
+        raise ValueError
 
 
 # TODO: (low-priority) add INPUT3D, MASK3D, BBOX3D, LAFs etc.
 class DataKey(Enum, metaclass=_KORNIA_EnumMeta):
+    IMAGE = 0
     INPUT = 0
     MASK = 1
     BBOX = 2
     BBOX_XYXY = 3
     BBOX_XYWH = 4
     KEYPOINTS = 5
     CLASS = 6
```

### Comparing `kornia-0.6.9/kornia/contrib/__init__.py` & `kornia-0.7.0/kornia/contrib/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,29 +10,33 @@
     compute_padding,
     extract_tensor_patches,
 )
 from .face_detection import *
 from .histogram_matching import histogram_matching, interp
 from .image_stitching import ImageStitcher
 from .lambda_module import Lambda
+from .models.tiny_vit import TinyViT
+from .object_detection import ObjectDetector
 from .vit import VisionTransformer
 from .vit_mobile import MobileViT
 
 __all__ = [
     "connected_components",
     "extract_tensor_patches",
     "ExtractTensorPatches",
     "combine_tensor_patches",
     "CombineTensorPatches",
     "compute_padding",
     "histogram_matching",
     "interp",
     "VisionTransformer",
     "MobileViT",
+    "TinyViT",
     "ClassificationHead",
     "Lambda",
     "ImageStitcher",
     "EdgeDetector",
     "distance_transform",
     "DistanceTransform",
     "diamond_square",
+    "ObjectDetector",
 ]
```

### Comparing `kornia-0.6.9/kornia/contrib/classification.py` & `kornia-0.7.0/kornia/contrib/classification.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/contrib/connected_components.py` & `kornia-0.7.0/kornia/contrib/connected_components.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/contrib/diamond_square.py` & `kornia-0.7.0/kornia/contrib/diamond_square.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # based on: https://github.com/anguelos/tormentor/blob/e8050ac235b0c7ad3c7d931cfa47c308a305c486/diamond_square/diamond_square.py  # noqa: E501
 import math
 from typing import Callable, List, Optional, Tuple, Union
 
 import torch
 
 from kornia.core import Tensor
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.enhance import normalize_min_max
 from kornia.filters import filter2d
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 
 # the default kernels for the diamond square
 default_diamond_kernel: List[List[float]] = [[0.25, 0.0, 0.25], [0.0, 0.0, 0.0], [0.25, 0.0, 0.25]]
 default_square_kernel: List[List[float]] = [[0.0, 0.25, 0.0], [0.25, 0.0, 0.25], [0.0, 0.25, 0.0]]
 
 
 def _diamond_square_seed(
@@ -138,15 +138,15 @@
 
 
 def diamond_square(
     output_size: Tuple[int, int, int, int],
     roughness: Union[float, Tensor] = 0.5,
     random_scale: Union[float, Tensor] = 1.0,
     random_fn: Callable[..., Tensor] = torch.rand,
-    normalize_range: Optional[Tuple[int, int]] = None,
+    normalize_range: Optional[Tuple[float, float]] = None,
     device: Optional[torch.device] = None,
     dtype: Optional[torch.dtype] = None,
 ) -> Tensor:
     """Generates Plasma Fractal Images using the diamond square algorithm.
 
     See: https://en.wikipedia.org/wiki/Diamond-square_algorithm
 
@@ -203,9 +203,9 @@
     # slice to match with the output size
     img = img[..., :width, :height]
     img = img.view(output_size)
 
     # normalize the output in the range using min-max
     if normalize_range is not None:
         min_val, max_val = normalize_range
-        img = normalize_min_max(img, min_val, max_val)
+        img = normalize_min_max(img.contiguous(), min_val, max_val)
     return img
```

### Comparing `kornia-0.6.9/kornia/contrib/distance_transform.py` & `kornia-0.7.0/kornia/contrib/distance_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import math
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from kornia.filters import filter2d
 from kornia.utils import create_meshgrid
 
 
 def distance_transform(image: torch.Tensor, kernel_size: int = 3, h: float = 0.35) -> torch.Tensor:
     r"""Approximates the Manhattan distance transform of images using cascaded convolution operations.
@@ -75,15 +75,15 @@
     r"""Module that approximates the Manhattan (city block) distance transform of images using convolutions.
 
     Args:
         kernel_size: size of the convolution kernel.
         h: value that influence the approximation of the min function.
     """
 
-    def __init__(self, kernel_size: int = 3, h: float = 0.35):
+    def __init__(self, kernel_size: int = 3, h: float = 0.35) -> None:
         super().__init__()
         self.kernel_size = kernel_size
         self.h = h
 
     def forward(self, image: torch.Tensor) -> torch.Tensor:
         # If images have multiple channels, view the channels in the batch dimension to match kernel shape.
         if image.shape[1] > 1:
```

### Comparing `kornia-0.6.9/kornia/contrib/edge_detection.py` & `kornia-0.7.0/kornia/contrib/edge_detection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import List
 
 from kornia.core import Module, Tensor
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.filters.dexined import DexiNed
-from kornia.testing import KORNIA_CHECK_SHAPE
 
 
 class EdgeDetector(Module):
     r"""Detect edges in a given image using a CNN.
 
     By default, it uses the method described in :cite:`xsoria2020dexined`.
```

### Comparing `kornia-0.6.9/kornia/contrib/extract_patches.py` & `kornia-0.7.0/kornia/contrib/extract_patches.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/contrib/face_detection.py` & `kornia-0.7.0/kornia/contrib/face_detection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # based on: https://github.com/ShiqiYu/libfacedetection.train/blob/74f3aa77c63234dd954d21286e9a60703b8d0868/tasks/task1/yufacedetectnet.py  # noqa
 import math
 from enum import Enum
 from typing import Dict, List, Optional, Tuple
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 
 from kornia.geometry.bbox import nms as nms_kornia
 from kornia.utils.helpers import map_location_to_cpu
 
 __all__ = ["FaceDetector", "FaceDetectorResult", "FaceKeypoint"]
 
 
@@ -224,15 +224,15 @@
         return self.postprocess(out, img.shape[-2], img.shape[-1])
 
 
 # utils for the network
 
 
 class ConvDPUnit(nn.Sequential):
-    def __init__(self, in_channels, out_channels, withBNRelu=True):
+    def __init__(self, in_channels: int, out_channels: int, withBNRelu: bool = True) -> None:
         super().__init__()
         self.add_module("conv1", nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=True, groups=1))
         self.add_module("conv2", nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=True, groups=out_channels))
         if withBNRelu:
             self.add_module("bn", nn.BatchNorm2d(out_channels))
             self.add_module("relu", nn.ReLU(inplace=True))
 
@@ -250,15 +250,15 @@
     def __init__(self, in_channels: int, out_channels: int, withBNRelu: bool = True) -> None:
         super().__init__()
         self.add_module("conv1", ConvDPUnit(in_channels, in_channels, True))
         self.add_module("conv2", ConvDPUnit(in_channels, out_channels, withBNRelu))
 
 
 class YuFaceDetectNet(nn.Module):
-    def __init__(self, phase, pretrained: bool):
+    def __init__(self, phase: str, pretrained: bool) -> None:
         super().__init__()
         self.phase = phase
         self.num_classes = 2
 
         self.model0 = Conv_head(3, 16, 16)
         self.model1 = Conv4layerBlock(16, 64)
         self.model2 = Conv4layerBlock(64, 64)
```

### Comparing `kornia-0.6.9/kornia/contrib/histogram_matching.py` & `kornia-0.7.0/kornia/contrib/histogram_matching.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,15 +15,14 @@
 
     Returns:
         The transformed output image as the same shape as the source image.
 
     Note:
         This function does not matches histograms element-wisely if input a batched tensor.
     """
-
     oldshape = source.shape
     source = source.ravel()
     template = template.ravel()
 
     # get the set of unique pixel values and their corresponding indices and counts.
     _, bin_idx, s_counts = torch.unique(source, return_inverse=True, return_counts=True)
     t_values, t_counts = torch.unique(template, return_counts=True)
```

### Comparing `kornia-0.6.9/kornia/contrib/image_stitching.py` & `kornia-0.7.0/kornia/contrib/image_stitching.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Dict, List, Optional, Tuple
+from typing import Dict, Optional, Tuple
 
 import torch
 
 from kornia.color import rgb_to_grayscale
 from kornia.core import Module, Tensor, concatenate, where, zeros_like
 from kornia.feature import LocalFeatureMatcher, LoFTR
 from kornia.geometry.homography import find_homography_dlt_iterated
@@ -35,61 +35,60 @@
 
     def __init__(self, matcher: Module, estimator: str = 'ransac', blending_method: str = "naive") -> None:
         super().__init__()
         self.matcher = matcher
         self.estimator = estimator
         self.blending_method = blending_method
         if estimator not in ['ransac', 'vanilla']:
-            raise NotImplementedError(f"Unsupported estimator {estimator}. Use ransac or vanilla instead.")
+            raise NotImplementedError(f"Unsupported estimator {estimator}. Use `ransac` or `vanilla` instead.")
         if estimator == "ransac":
             self.ransac = RANSAC('homography')
 
     def _estimate_homography(self, keypoints1: Tensor, keypoints2: Tensor) -> Tensor:
         """Estimate homography by the matched keypoints.
 
         Args:
             keypoints1: matched keypoint set from an image, shaped as :math:`(N, 2)`.
             keypoints2: matched keypoint set from the other image, shaped as :math:`(N, 2)`.
         """
-        homo: Tensor
         if self.estimator == "vanilla":
             homo = find_homography_dlt_iterated(
                 keypoints2[None], keypoints1[None], torch.ones_like(keypoints1[None, :, 0])
             )
         elif self.estimator == "ransac":
             homo, _ = self.ransac(keypoints2, keypoints1)
             homo = homo[None]
         else:
-            raise NotImplementedError(f"Unsupported estimator {self.estimator}. Use ransac or vanilla instead.")
+            raise NotImplementedError(f"Unsupported estimator {self.estimator}. Use `ransac` or `vanilla` instead.")
         return homo
 
-    def estimate_transform(self, **kwargs) -> Tensor:
+    def estimate_transform(self, *args: Tensor, **kwargs: Tensor) -> Tensor:
         """Compute the corresponding homography."""
-        homos: List[Tensor] = []
         kp1, kp2, idx = kwargs['keypoints0'], kwargs['keypoints1'], kwargs['batch_indexes']
-        for i in range(len(idx.unique())):
-            homos.append(self._estimate_homography(kp1[idx == i], kp2[idx == i]))
+        homos = [self._estimate_homography(kp1[idx == i], kp2[idx == i]) for i in range(len(idx.unique()))]
+
         if len(homos) == 0:
             raise RuntimeError("Compute homography failed. No matched keypoints found.")
+
         return concatenate(homos)
 
     def blend_image(self, src_img: Tensor, dst_img: Tensor, mask: Tensor) -> Tensor:
         """Blend two images together."""
         out: Tensor
         if self.blending_method == "naive":
             out = where(mask == 1, src_img, dst_img)
         else:
-            raise NotImplementedError(f"Unsupported blending method {self.blending_method}. Use naive.")
+            raise NotImplementedError(f"Unsupported blending method {self.blending_method}. Use `naive`.")
         return out
 
     def preprocess(self, image_1: Tensor, image_2: Tensor) -> Dict[str, Tensor]:
         """Preprocess input to the required format."""
         # TODO: probably perform histogram matching here.
-        if isinstance(self.matcher, LoFTR) or isinstance(self.matcher, LocalFeatureMatcher):
-            input_dict: Dict[str, Tensor] = {  # LofTR works on grayscale images only
+        if isinstance(self.matcher, (LoFTR, LocalFeatureMatcher)):
+            input_dict = {  # LofTR works on grayscale images only
                 "image0": rgb_to_grayscale(image_1),
                 "image1": rgb_to_grayscale(image_2),
             }
         else:
             raise NotImplementedError(f"The preprocessor for {self.matcher} has not been implemented.")
         return input_dict
 
@@ -97,29 +96,29 @@
         # NOTE: assumes no batch mode. This method keeps all valid regions after stitching.
         mask_ = mask.sum((0, 1))
         index = int(mask_.bool().any(0).long().argmin().item())
         if index == 0:  # If no redundant space
             return image
         return image[..., :index]
 
-    def on_matcher(self, data) -> Dict[str, Tensor]:
+    def on_matcher(self, data: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return self.matcher(data)
 
     def stitch_pair(
         self,
         images_left: Tensor,
         images_right: Tensor,
         mask_left: Optional[Tensor] = None,
         mask_right: Optional[Tensor] = None,
     ) -> Tuple[Tensor, Tensor]:
         # Compute the transformed images
         input_dict = self.preprocess(images_left, images_right)
         out_shape = (images_left.shape[-2], images_left.shape[-1] + images_right.shape[-1])
         correspondences = self.on_matcher(input_dict)
-        homo: Tensor = self.estimate_transform(**correspondences)
+        homo = self.estimate_transform(**correspondences)
         src_img = warp_perspective(images_right, homo, out_shape)
         dst_img = concatenate([images_left, zeros_like(images_right)], -1)
 
         # Compute the transformed masks
         if mask_left is None:
             mask_left = torch.ones_like(images_left)
         if mask_right is None:
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `kornia-0.6.9/kornia/contrib/lambda_module.py` & `kornia-0.7.0/kornia/contrib/lambda_module.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Callable
+from typing import Any, Callable
 
 from kornia.core import Module, Tensor
 
 
 class Lambda(Module):
     """Applies user-defined lambda as a transform.
 
@@ -19,13 +19,13 @@
         >>> f(x).shape
         torch.Size([1, 1, 5, 5])
     """
 
     def __init__(self, func: Callable[..., Tensor]) -> None:
         super().__init__()
         if not callable(func):
-            raise TypeError(f"Argument lambd should be callable, got {repr(type(func).__name__)}")
+            raise TypeError(f"Argument lambd should be callable, got {type(func).__name__!r}")
 
         self.func = func
 
-    def forward(self, img: Tensor, *args, **kwargs) -> Tensor:
+    def forward(self, img: Tensor, *args: Any, **kwargs: Any) -> Tensor:
         return self.func(img, *args, **kwargs)
```

### Comparing `kornia-0.6.9/kornia/contrib/vit.py` & `kornia-0.7.0/kornia/contrib/vit.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,28 +1,31 @@
 """Module that implement Vision Transformer (ViT).
 
 Paper: https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1
 
-Based on: https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632
-Added some tricks from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
+Based on: `https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632`
+
+Added some tricks from: `https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py`
 """
-from typing import List, Optional, Tuple
+from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import torch
 from torch import nn
 
+from kornia.core import Module, Tensor
+
 __all__ = ["VisionTransformer"]
 
 
-class ResidualAdd(nn.Module):
-    def __init__(self, fn) -> None:
+class ResidualAdd(Module):
+    def __init__(self, fn: Callable[..., Tensor]) -> None:
         super().__init__()
         self.fn = fn
 
-    def forward(self, x, **kwargs) -> None:
+    def forward(self, x: Tensor, **kwargs: Dict[str, Any]) -> Tensor:
         res = x
         x = self.fn(x, **kwargs)
         x += res
         return x
 
 
 class FeedForward(nn.Sequential):
@@ -32,15 +35,15 @@
             nn.GELU(),
             nn.Dropout(dropout_rate),
             nn.Linear(hidden_features, out_features),
             nn.Dropout(dropout_rate),  # added one extra as in timm
         )
 
 
-class MultiHeadAttention(nn.Module):
+class MultiHeadAttention(Module):
     def __init__(self, emb_size: int, num_heads: int, att_drop: float, proj_drop: float) -> None:
         super().__init__()
         self.emb_size = emb_size
         self.num_heads = num_heads
         head_size = emb_size // num_heads  # from timm
         self.scale = head_size**-0.5  # from timm
 
@@ -87,15 +90,15 @@
                     FeedForward(embed_dim, embed_dim, embed_dim, dropout_rate=dropout_rate),
                     nn.Dropout(dropout_rate),
                 )
             ),
         )
 
 
-class TransformerEncoder(nn.Module):
+class TransformerEncoder(Module):
     def __init__(
         self,
         embed_dim: int = 768,
         depth: int = 12,
         num_heads: int = 12,
         dropout_rate: float = 0.0,
         dropout_attn: float = 0.0,
@@ -111,24 +114,24 @@
         out = x
         for m in self.blocks.children():
             out = m(out)
             self.results.append(out)
         return out
 
 
-class PatchEmbedding(nn.Module):
+class PatchEmbedding(Module):
     """Compute the 2d image patch embedding ready to pass to transformer encoder."""
 
     def __init__(
         self,
         in_channels: int = 3,
         out_channels: int = 768,
         patch_size: int = 16,
         image_size: int = 224,
-        backbone: Optional[nn.Module] = None,
+        backbone: Optional[Module] = None,
     ) -> None:
         super().__init__()
         self.in_channels = in_channels
         self.out_channels = out_channels
         self.patch_size = patch_size
 
         # logic needed in case a backbone is passed
@@ -154,15 +157,15 @@
         # prepend the cls token to the input
         x = torch.cat([cls_tokens, x], dim=1)  # Bx(N+1)xE
         # add position embedding
         x += self.positions
         return x
 
 
-class VisionTransformer(nn.Module):
+class VisionTransformer(Module):
     """Vision transformer (ViT) module.
 
     The module is expected to be used as operator for different vision tasks.
 
     The method is inspired from existing implementations of the paper :cite:`dosovitskiy2020vit`.
 
     .. warning::
@@ -192,28 +195,28 @@
         patch_size: int = 16,
         in_channels: int = 3,
         embed_dim: int = 768,
         depth: int = 12,
         num_heads: int = 12,
         dropout_rate: float = 0.0,
         dropout_attn: float = 0.0,
-        backbone: Optional[nn.Module] = None,
+        backbone: Optional[Module] = None,
     ) -> None:
         super().__init__()
         self.image_size = image_size
         self.patch_size = patch_size
         self.in_channels = in_channels
         self.embed_size = embed_dim
 
         self.patch_embedding = PatchEmbedding(in_channels, embed_dim, patch_size, image_size, backbone)
         hidden_dim = self.patch_embedding.out_channels
         self.encoder = TransformerEncoder(hidden_dim, depth, num_heads, dropout_rate, dropout_attn)
 
     @property
-    def encoder_results(self):
+    def encoder_results(self) -> List[Tensor]:
         return self.encoder.results
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         if not isinstance(x, torch.Tensor):
             raise TypeError(f"Input x type is not a torch.Tensor. Got: {type(x)}")
 
         if self.image_size not in (*x.shape[-2:],) and x.shape[-3] != self.in_channels:
```

### Comparing `kornia-0.6.9/kornia/contrib/vit_mobile.py` & `kornia-0.7.0/kornia/contrib/vit_mobile.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,71 +1,72 @@
-from typing import Tuple
+from typing import Any, Dict, Tuple
 
 import torch
 from torch import nn
 
+from kornia.core import Module, Tensor
 
-def conv_1x1_bn(inp: int, oup: int) -> nn.Module:
+
+def conv_1x1_bn(inp: int, oup: int) -> Module:
     return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.SiLU())
 
 
-def conv_nxn_bn(inp: int, oup: int, kernal_size: int = 3, stride: int = 1) -> nn.Module:
+def conv_nxn_bn(inp: int, oup: int, kernal_size: int = 3, stride: int = 1) -> Module:
     return nn.Sequential(nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False), nn.BatchNorm2d(oup), nn.SiLU())
 
 
-class PreNorm(nn.Module):
-    def __init__(self, dim: int, fn: nn.Module) -> None:
+class PreNorm(Module):
+    def __init__(self, dim: int, fn: Module) -> None:
         super().__init__()
         self.norm = nn.LayerNorm(dim)
         self.fn = fn
 
-    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
+    def forward(self, x: Tensor, **kwargs: Dict[str, Any]) -> Tensor:
         return self.fn(self.norm(x), **kwargs)
 
 
-class FeedForward(nn.Module):
+class FeedForward(Module):
     def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.0) -> None:
         super().__init__()
         self.net = nn.Sequential(
             nn.Linear(dim, hidden_dim), nn.SiLU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout)
         )
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         return self.net(x)
 
 
-class Attention(nn.Module):
+class Attention(Module):
     def __init__(self, dim: int, heads: int = 8, dim_head: int = 64, dropout: float = 0.0) -> None:
         super().__init__()
         inner_dim = dim_head * heads
         project_out = not (heads == 1 and dim_head == dim)
 
         self.heads = heads
         self.scale = dim_head**-0.5
 
         self.attend = nn.Softmax(dim=-1)
         self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
 
         self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout)) if project_out else nn.Identity()
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         qkv = self.to_qkv(x).chunk(3, dim=-1)
 
-        B, P, N, HD = qkv[0].shape
-        q, k, v = map(lambda t: t.contiguous().view(B, P, self.heads, N, HD // self.heads), qkv)
+        b, p, n, hd = qkv[0].shape
+        q, k, v = (t.reshape(b, p, n, self.heads, hd // self.heads).transpose(2, 3) for t in qkv)
 
         dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
         attn = self.attend(dots)
         out = torch.matmul(attn, v)
-        B, P, H, N, D = out.shape
-        out = out.view(B, P, N, H * D)
+        out = out.transpose(2, 3).reshape(b, p, n, hd)
         return self.to_out(out)
 
 
-class Transformer(nn.Module):
+class Transformer(Module):
     """Transformer block described in ViT.
 
     Paper: https://arxiv.org/abs/2010.11929
     Based on: https://github.com/lucidrains/vit-pytorch
 
     Args:
         dim: input dimension.
@@ -85,22 +86,22 @@
                     [
                         PreNorm(dim, Attention(dim, heads, dim_head, dropout)),
                         PreNorm(dim, FeedForward(dim, mlp_dim, dropout)),
                     ]
                 )
             )
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         for attn, ff in self.layers:
             x = attn(x) + x
             x = ff(x) + x
         return x
 
 
-class MV2Block(nn.Module):
+class MV2Block(Module):
     """MV2 block described in MobileNetV2.
 
     Paper: https://arxiv.org/pdf/1801.04381
     Based on: https://github.com/tonylins/pytorch-mobilenet-v2
 
     Args:
         inp: input channel.
@@ -137,22 +138,22 @@
                 nn.BatchNorm2d(hidden_dim),
                 nn.SiLU(),
                 # pointwise
                 nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                 nn.BatchNorm2d(oup),
             )
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         if self.use_res_connect:
             return x + self.conv(x)
         else:
             return self.conv(x)
 
 
-class MobileViTBlock(nn.Module):
+class MobileViTBlock(Module):
     """MobileViT block mentioned in MobileViT.
 
     Args:
         dim: input dimension of Transformer.
         depth: depth of Transformer.
         channel: input channel.
         kernel_size: kernel size.
@@ -178,35 +179,45 @@
         self.conv2 = conv_1x1_bn(channel, dim)
 
         self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)
 
         self.conv3 = conv_1x1_bn(dim, channel)
         self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         y = x.clone()
 
         # Local representations
         x = self.conv1(x)
         x = self.conv2(x)
 
-        # Global representations
         b, d, h, w = x.shape
-        x = x.view(b, self.ph * self.pw, (h // self.ph) * (w // self.pw), d)
+        nh, nw = h // self.ph, w // self.pw
+
+        # Global representations
+        # [b, d, h, w] -> [b * d * nh, nw, ph, pw]
+        x = x.reshape(b * d * nh, self.ph, nw, self.pw).transpose(1, 2)
+        # [b * d * nh, nw, ph, pw] -> [b, (ph pw), (nh nw), d]
+        x = x.reshape(b, d, nh * nw, self.ph * self.pw).transpose(1, 3)
+
         x = self.transformer(x)
-        x = x.view(b, d, h, w)
+
+        # [b, (ph pw), (nh nw), d] -> [b * d * nh, nw, ph, pw]
+        x = x.transpose(1, 3).reshape(b * d * nh, nw, self.ph, self.pw)
+        # [b * d * nh, nw, ph, pw] -> [b, d, h, w]
+        x = x.transpose(1, 2).reshape(b, d, h, w)
 
         # Fusion
         x = self.conv3(x)
         x = torch.cat((x, y), 1)
         x = self.conv4(x)
         return x
 
 
-class MobileViT(nn.Module):
+class MobileViT(Module):
     """Module MobileViT. Default arguments is for MobileViT XXS.
 
     Paper: https://arxiv.org/abs/2110.02178
     Based on: https://github.com/chinhsuanwu/mobilevit-pytorch
 
     Args:
         mode: 'xxs', 'xs' or 's', defaults to 'xxs'.
@@ -261,15 +272,15 @@
         )
         self.mvit.append(
             MobileViTBlock(dims[2], depth[2], channels[9], kernel_size, patch_size, int(dims[2] * 4), dropout=dropout)
         )
 
         self.conv2 = conv_1x1_bn(channels[-2], channels[-1])
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         x = self.conv1(x)
         x = self.mv2[0](x)
 
         x = self.mv2[1](x)
         x = self.mv2[2](x)
         x = self.mv2[3](x)  # Repeat
```

### Comparing `kornia-0.6.9/kornia/core/tensor_wrapper.py` & `kornia-0.7.0/kornia/core/tensor_wrapper.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,53 +1,56 @@
+# type: ignore
 # pytorch tensor wrapper class
 # insipired by:
 # https://github.com/pytorch/pytorch/blob/591dfffa38848de54b7f5f4e49260847024c9281/test/test_overrides.py#L748
 import collections
-from typing import Any
 
 import torch
 from torch import Tensor
 
-# wrap inputs if necessary
 
 # TODO: promote to KORNIA_WRAP
-
-
 def wrap(v, cls):
+    # wrap inputs if necessary
     if type(v) in {tuple, list}:
         return type(v)(wrap(vi, cls) for vi in v)
 
     return cls(v) if isinstance(v, Tensor) else v
 
 
 # TODO: promote to KORNIA_UNWRAP
-
-
 def unwrap(v):
     if type(v) in {tuple, list}:
         return type(v)(unwrap(vi) for vi in v)
 
-    return v._data if not isinstance(v, Tensor) else v
+    return v._data if isinstance(v, TensorWrapper) else v
 
 
 class TensorWrapper:
     def __init__(self, data: Tensor) -> None:
         self.__dict__["_data"] = data
         self.__dict__["used_attrs"] = set()
         self.__dict__["used_calls"] = set()
 
-    @property
-    def data(self) -> Tensor:
-        return self._data
+    def unwrap(self):
+        return unwrap(self)
+
+    def __getstate__(self):
+        return self.__dict__
+
+    def __setstate__(self, state):
+        self.__dict__.update(state)
 
     def __repr__(self) -> str:
-        return f"{self.data}"
+        return f"{self._data}"
 
     def __getattr__(self, name):
-        if name in self.__dict__:
+        if name == "data":
+            return self._data
+        elif name in self.__dict__:
             return self.__dict__[name]
         self.used_attrs.add(name)
 
         val = getattr(self._data, name)
 
         # NOTE: not clear is really needed
         # If it's a method
@@ -59,22 +62,22 @@
         #    # Otherwise append self to args
         #    return lambda *a, **kw: wrap(
         #        #self.__torch_function__(c, (type(self),), args=(self,) + a, kwargs=kw), type(self)
         #    )
 
         return wrap(val, type(self))
 
-    def __setattr__(self, name, value):
+    def __setattr__(self, name, value) -> None:
         if name in self.__dict__:
             self.__dict__[name] = value
 
         self.used_attrs.add(name)
         setattr(self._data, name, value)
 
-    def __setitem__(self, key, value):
+    def __setitem__(self, key, value) -> None:
         self._data[key] = value
 
     def __getitem__(self, key):
         return wrap(self._data[key], type(self))
 
     @classmethod
     def __torch_function__(cls, func, types, args=(), kwargs=None):
@@ -95,20 +98,29 @@
 
         return wrap(func(*args, **kwargs), cls)
 
     # TODO: `def __add__(self, other) -> Self:` when mypy release >0.991
     def __add__(self, other):
         return self.__unary_op__(torch.add, other)
 
+    def __radd__(self, other):
+        return self.__unary_op__(torch.add, other)
+
     def __mul__(self, other):
         return self.__unary_op__(torch.mul, other)
 
+    def __rmul__(self, other):
+        return self.__unary_op__(torch.mul, other)
+
     def __sub__(self, other):
         return self.__unary_op__(torch.sub, other)
 
+    def __rsub__(self, other):
+        return self.__unary_op__(torch.sub, other)
+
     def __truediv__(self, other):
         return self.__unary_op__(torch.true_divide, other)
 
     def __floordiv__(self, other):
         return self.__unary_op__(torch.floor_divide, other)
 
     def __ge__(self, other):
@@ -125,22 +137,22 @@
 
     def __eq__(self, other):
         return self.__unary_op__(torch.eq, other)
 
     def __ne__(self, other):
         return self.__unary_op__(torch.ne, other)
 
-    def __bool__(self):
+    def __bool__(self) -> bool:
         return self.__unary_op__(Tensor.__bool__)
 
-    def __int__(self):
+    def __int__(self) -> int:
         return self.__unary_op__(Tensor.__int__)
 
     def __neg__(self):
         return self.__unary_op__(Tensor.negative)
 
-    def __unary_op__(self, func: Any, other=None):
+    def __unary_op__(self, func, other=None):
         args = (self, other) if other is not None else (self,)
         return self.__torch_function__(func, (type(self),), args)
 
-    def __len__(self):
+    def __len__(self) -> int:
         return len(self._data)
```

### Comparing `kornia-0.6.9/kornia/enhance/__init__.py` & `kornia-0.7.0/kornia/enhance/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -28,14 +28,15 @@
     posterize,
     sharpness,
     solarize,
 )
 from .core import AddWeighted, add_weighted
 from .equalization import equalize_clahe
 from .histogram import histogram, histogram2d, image_histogram2d
+from .integral import IntegralImage, IntegralTensor, integral_image, integral_tensor
 from .normalize import Denormalize, Normalize, denormalize, normalize, normalize_min_max
 from .shift_rgb import shift_rgb
 from .zca import ZCAWhitening, linear_transform, zca_mean, zca_whiten
 
 __all__ = [
     "adjust_brightness",
     "adjust_brightness_accumulative",
@@ -78,8 +79,12 @@
     "denormalize",
     "Normalize",
     "Denormalize",
     "zca_mean",
     "zca_whiten",
     "linear_transform",
     "ZCAWhitening",
+    "integral_tensor",
+    "integral_image",
+    "IntegralImage",
+    "IntegralTensor",
 ]
```

### Comparing `kornia-0.6.9/kornia/enhance/adjust.py` & `kornia-0.7.0/kornia/enhance/adjust.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,25 +2,24 @@
 from typing import Optional, Union
 
 import torch
 from torch import Tensor
 from torch.nn import Module, Parameter
 
 from kornia.color import hsv_to_rgb, rgb_to_grayscale, rgb_to_hsv
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_COLOR_OR_GRAY, KORNIA_CHECK_IS_TENSOR
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_COLOR_OR_GRAY, KORNIA_CHECK_IS_TENSOR
 from kornia.utils.helpers import _torch_histc_cast
 from kornia.utils.image import perform_keep_shape_image, perform_keep_shape_video
 
 
 def adjust_saturation_raw(image: Tensor, factor: Union[float, Tensor]) -> Tensor:
     r"""Adjust color saturation of an image.
 
     Expecting image to be in hsv format already.
     """
-
     KORNIA_CHECK_IS_TENSOR(image, "Expected shape (*, H, W)")
     KORNIA_CHECK(isinstance(factor, (float, Tensor)), "Factor should be float or Tensor.")
 
     if isinstance(factor, float):
         # TODO: figure out how to create later a tensor without importing torch
         factor = torch.as_tensor(factor, device=image.device, dtype=image.dtype)
     elif isinstance(factor, Tensor):
@@ -66,15 +65,14 @@
         torch.Size([1, 3, 3, 3])
 
         >>> x = torch.ones(2, 3, 3, 3)
         >>> y = torch.tensor([1., 2.])
         >>> adjust_saturation_with_gray_subtraction(x, y).shape
         torch.Size([2, 3, 3, 3])
     """
-
     KORNIA_CHECK_IS_TENSOR(image, "Expected shape (*, H, W)")
     KORNIA_CHECK(isinstance(factor, (float, Tensor)), "Factor should be float or Tensor.")
     KORNIA_CHECK_IS_COLOR_OR_GRAY(image, "Image should be an RGB or gray image")
 
     if image.shape[-3] == 1:
         return image
 
@@ -125,15 +123,14 @@
         torch.Size([1, 3, 3, 3])
 
         >>> x = torch.ones(2, 3, 3, 3)
         >>> y = torch.tensor([1., 2.])
         >>> adjust_saturation(x, y).shape
         torch.Size([2, 3, 3, 3])
     """
-
     # convert the rgb image to hsv
     x_hsv: Tensor = rgb_to_hsv(image)
 
     # perform the conversion
     x_adjusted: Tensor = adjust_saturation_raw(x_hsv, factor)
 
     # convert back to rgb
@@ -143,15 +140,14 @@
 
 
 def adjust_hue_raw(image: Tensor, factor: Union[float, Tensor]) -> Tensor:
     r"""Adjust hue of an image.
 
     Expecting image to be in hsv format already.
     """
-
     KORNIA_CHECK_IS_TENSOR(image, "Expected shape (*, H, W)")
     KORNIA_CHECK(
         isinstance(factor, (float, Tensor)),
         f"The factor should be a float number or Tensor in the range between" f" [-PI, PI]. Got {type(factor)}",
     )
 
     if isinstance(factor, float):
@@ -203,15 +199,14 @@
         torch.Size([1, 3, 2, 2])
 
         >>> x = torch.ones(2, 3, 3, 3)
         >>> y = torch.ones(2) * 3.141516
         >>> adjust_hue(x, y).shape
         torch.Size([2, 3, 3, 3])
     """
-
     # convert the rgb image to hsv
     x_hsv: Tensor = rgb_to_hsv(image)
 
     # perform the conversion
     x_adjusted: Tensor = adjust_hue_raw(x_hsv, factor)
 
     # convert back to rgb
@@ -225,15 +220,15 @@
 
     .. image:: _static/img/adjust_contrast.png
 
     The input image is expected to be in the range of [0, 1].
 
     Args:
         input: Image to be adjusted in the shape of :math:`(*, H, W)`.
-        gamma: Non negative real number, same as \gamma in the equation.
+        gamma: Non negative real number, same as y\gammay in the equation.
             gamma larger than 1 make the shadows darker, while gamma smaller than 1 make
             dark regions lighter.
         gain: The constant multiplier.
 
     Return:
         Adjusted image in the shape of :math:`(*, H, W)`.
 
@@ -249,15 +244,14 @@
 
         >>> x = torch.ones(2, 5, 3, 3)
         >>> y1 = torch.ones(2) * 1.0
         >>> y2 = torch.ones(2) * 2.0
         >>> adjust_gamma(x, y1, y2).shape
         torch.Size([2, 5, 3, 3])
     """
-
     if not isinstance(input, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
 
     if not isinstance(gamma, (float, Tensor)):
         raise TypeError(f"The gamma should be a positive float or Tensor. Got {type(gamma)}")
 
     if not isinstance(gain, (float, Tensor)):
@@ -311,15 +305,15 @@
 
     Args:
         image: Image to be adjusted in the shape of :math:`(*, H, W)`.
         factor: Contrast adjust factor per element
             in the batch. 0 generates a completely black image, 1 does not modify
             the input image while any other non-negative number modify the
             brightness by this factor.
-            clip_output: whether to clip the output image with range of [0, 1].
+        clip_output: whether to clip the output image with range of [0, 1].
 
     Return:
         Adjusted image in the shape of :math:`(*, H, W)`.
 
     .. note::
        See a working example `here <https://kornia-tutorials.readthedocs.io/en/latest/
        image_enhancement.html>`__.
@@ -345,15 +339,15 @@
     elif isinstance(factor, Tensor):
         factor = factor.to(image.device, image.dtype)
 
     # make factor broadcastable
     while len(factor.shape) != len(image.shape):
         factor = factor[..., None]
 
-    KORNIA_CHECK(any(factor >= 0), f"Contrast factor must be positive. Got {factor}")
+    KORNIA_CHECK(any(factor >= 0), "Contrast factor must be positive.")
 
     # Apply contrast factor to each channel
     img_adjust: Tensor = image * factor
 
     # Truncate between pixel values
     if clip_output:
         img_adjust = img_adjust.clamp(min=0.0, max=1.0)
@@ -399,30 +393,30 @@
     elif isinstance(factor, Tensor):
         factor = factor.to(image.device, image.dtype)
 
     # make factor broadcastable
     while len(factor.shape) != len(image.shape):
         factor = factor[..., None]
 
-    KORNIA_CHECK(any(factor >= 0), f"Contrast factor must be positive. Got {factor}")
+    KORNIA_CHECK(any(factor >= 0), "Contrast factor must be positive.")
 
     if image.shape[-3] == 3:
         img_mean = rgb_to_grayscale(image).mean((-2, -1), True)
     else:
         img_mean = image.mean()
 
     # Apply contrast factor subtracting the mean
     img_adjust: Tensor = image * factor + img_mean * (1 - factor)
 
     img_adjust = img_adjust.clamp(min=0.0, max=1.0)
 
     return img_adjust
 
 
-def adjust_brightness(image: Tensor, factor: Union[float, Tensor], clip_output=True) -> Tensor:
+def adjust_brightness(image: Tensor, factor: Union[float, Tensor], clip_output: bool = True) -> Tensor:
     r"""Adjust the brightness of an image tensor.
 
     .. image:: _static/img/adjust_brightness.png
 
     This implementation follows Szeliski's book convention, where brightness is defined as
     an `additive` operation directly to raw pixel and shift its values according the applied
     factor and range of the image values. Beware that other framework might use different
@@ -481,15 +475,15 @@
     # truncate between pixel values
     if clip_output:
         img_adjust = img_adjust.clamp(min=0.0, max=1.0)
 
     return img_adjust
 
 
-def adjust_brightness_accumulative(image: Tensor, factor: Union[float, Tensor], clip_output=True) -> Tensor:
+def adjust_brightness_accumulative(image: Tensor, factor: Union[float, Tensor], clip_output: bool = True) -> Tensor:
     r"""Adjust the brightness accumulatively of an image tensor.
 
     This implementation follows PIL convention.
 
     The input image and factor is expected to be in the range of [0, 1].
 
     Args:
@@ -538,16 +532,16 @@
 
 def adjust_sigmoid(image: Tensor, cutoff: float = 0.5, gain: float = 10, inv: bool = False) -> Tensor:
     """Adjust sigmoid correction on the input image tensor.
 
     The input image is expected to be in the range of [0, 1].
 
     Reference:
-    [1]: Gustav J. Braun, "Image Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions",
-        http://markfairchild.org/PDFs/PAP07.pdf
+        [1]: Gustav J. Braun, "Image Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions",
+             http://markfairchild.org/PDFs/PAP07.pdf
 
     Args:
         image: Image to be adjusted in the shape of :math:`(*, H, W)`.
         cutoff: The cutoff of sigmoid function.
         gain: The multiplier of sigmoid function.
         inv: If is set to True the function will return the inverse sigmoid correction.
 
@@ -718,15 +712,15 @@
 
     Returns:
         Image with reduced color channels with shape :math:`(*, C, H, W)`.
 
     Example:
         >>> x = torch.rand(1, 6, 3, 3)
         >>> out = posterize(x, bits=8)
-        >>> torch.testing.assert_allclose(x, out)
+        >>> torch.testing.assert_close(x, out)
 
         >>> x = torch.rand(2, 6, 3, 3)
         >>> bits = torch.tensor([4, 2])
         >>> posterize(x, bits).shape
         torch.Size([2, 6, 3, 3])
     """
     if not isinstance(input, Tensor):
@@ -744,21 +738,21 @@
 
     # TODO: Make a differentiable version
     # Current version:
     # Ref: https://github.com/open-mmlab/mmcv/pull/132/files#diff-309c9320c7f71bedffe89a70ccff7f3bR19
     # Ref: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py#L222
     # Potential approach: implementing kornia.LUT with floating points
     # https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/functional.py#L472
-    def _left_shift(input: Tensor, shift: Tensor):
+    def _left_shift(input: Tensor, shift: Tensor) -> Tensor:
         return ((input * 255).to(torch.uint8) * (2**shift)).to(input.dtype) / 255.0
 
-    def _right_shift(input: Tensor, shift: Tensor):
+    def _right_shift(input: Tensor, shift: Tensor) -> Tensor:
         return (input * 255).to(torch.uint8) / (2**shift).to(input.dtype) / 255.0
 
-    def _posterize_one(input: Tensor, bits: Tensor):
+    def _posterize_one(input: Tensor, bits: Tensor) -> Tensor:
         # Single bits value condition
         if bits == 0:
             return torch.zeros_like(input)
         if bits == 8:
             return input.clone()
         bits = 8 - bits
         return _left_shift(_right_shift(input, bits), bits)
@@ -869,15 +863,15 @@
     diff = (input2 - input1) * factor
     res = input1 + diff
     if factor > 0.0 and factor < 1.0:
         return res
     return torch.clamp(res, 0, 1)
 
 
-def _build_lut(histo, step):
+def _build_lut(histo: Tensor, step: Tensor) -> Tensor:
     # Compute the cumulative sum, shifting by step // 2
     # and then normalization by step.
     step_trunc = torch.div(step, 2, rounding_mode='trunc')
     lut = torch.div(torch.cumsum(histo, 0) + step_trunc, step, rounding_mode='trunc')
     # Shift lut, prepending with 0.
     lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])
     # Clip the counts to be in range.  This is done
@@ -1151,15 +1145,15 @@
 
 class AdjustGamma(Module):
     r"""Perform gamma correction on an image.
 
     The input image is expected to be in the range of [0, 1].
 
     Args:
-        gamma: Non negative real number, same as \gamma in the equation.
+        gamma: Non negative real number, same as y\gammay in the equation.
           gamma larger than 1 make the shadows darker, while gamma smaller than 1 make
           dark regions lighter.
         gain: The constant multiplier.
 
     Shape:
         - Input: Image to be adjusted in the shape of :math:`(*, N)`.
         - Output: Adjusted image in the shape of :math:`(*, N)`.
@@ -1300,16 +1294,16 @@
 
 class AdjustSigmoid(Module):
     r"""Adjust the contrast of an image tensor or performs sigmoid correction on the input image tensor.
 
     The input image is expected to be in the range of [0, 1].
 
     Reference:
-    [1]: Gustav J. Braun, "Image Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions",
-        http://markfairchild.org/PDFs/PAP07.pdf
+        [1]: Gustav J. Braun, "Image Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions",
+             http://markfairchild.org/PDFs/PAP07.pdf
 
     Args:
         image: Image to be adjusted in the shape of :math:`(*, H, W)`.
         cutoff: The cutoff of sigmoid function.
         gain: The multiplier of sigmoid function.
         inv: If is set to True the function will return the negative sigmoid correction.
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `kornia-0.6.9/kornia/enhance/core.py` & `kornia-0.7.0/kornia/enhance/core.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Union
 
 from kornia.core import Module, Tensor, tensor
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR
 
 
 def add_weighted(
     src1: Tensor, alpha: Union[float, Tensor], src2: Tensor, beta: Union[float, Tensor], gamma: Union[float, Tensor]
 ) -> Tensor:
     r"""Calculate the weighted sum of two Tensors.
 
@@ -83,15 +83,15 @@
         >>> output.shape
         torch.Size([1, 1, 5, 5])
 
     Notes:
         Tensor alpha/beta/gamma have to be with shape broadcastable to src1 and src2 shapes.
     """
 
-    def __init__(self, alpha, beta, gamma) -> None:
+    def __init__(self, alpha: Union[float, Tensor], beta: Union[float, Tensor], gamma: Union[float, Tensor]) -> None:
         super().__init__()
         self.alpha = alpha
         self.beta = beta
         self.gamma = gamma
 
     def forward(self, src1: Tensor, src2: Tensor) -> Tensor:
         return add_weighted(src1, self.alpha, src2, self.beta, self.gamma)
```

### Comparing `kornia-0.6.9/kornia/enhance/equalization.py` & `kornia-0.7.0/kornia/enhance/equalization.py`

 * *Files 5% similar despite different names*

```diff
@@ -293,18 +293,18 @@
     # border region (h)
     t, b, _, _ = preinterp_tiles_equalized[:, 1:-1, 0].unbind(2)
     tiles_equalized[:, 1:-1, 0] = torch.addcmul(b, tih.squeeze(1), torch.sub(t, b))
     t, b, _, _ = preinterp_tiles_equalized[:, 1:-1, gh - 1].unbind(2)
     tiles_equalized[:, 1:-1, gh - 1] = torch.addcmul(b, tih.squeeze(1), torch.sub(t, b))
 
     # border region (w)
-    l, r, _, _ = preinterp_tiles_equalized[:, 0, 1:-1].unbind(2)
-    tiles_equalized[:, 0, 1:-1] = torch.addcmul(r, tiw, torch.sub(l, r))
-    l, r, _, _ = preinterp_tiles_equalized[:, gw - 1, 1:-1].unbind(2)
-    tiles_equalized[:, gw - 1, 1:-1] = torch.addcmul(r, tiw, torch.sub(l, r))
+    left, right, _, _ = preinterp_tiles_equalized[:, 0, 1:-1].unbind(2)
+    tiles_equalized[:, 0, 1:-1] = torch.addcmul(right, tiw, torch.sub(left, right))
+    left, right, _, _ = preinterp_tiles_equalized[:, gw - 1, 1:-1].unbind(2)
+    tiles_equalized[:, gw - 1, 1:-1] = torch.addcmul(right, tiw, torch.sub(left, right))
 
     # same type as the input
     return tiles_equalized.div(255.0)
 
 
 @perform_keep_shape_image
 def equalize_clahe(
```

### Comparing `kornia-0.6.9/kornia/enhance/histogram.py` & `kornia-0.7.0/kornia/enhance/histogram.py`

 * *Files 0% similar despite different names*

```diff
@@ -57,15 +57,14 @@
         kernel_values1: shape [BxNxNUM_BINS].
         kernel_values2: shape [BxNxNUM_BINS].
         epsilon: scalar, for numerical stability.
 
     Returns:
         shape [BxNUM_BINSxNUM_BINS].
     """
-
     if not isinstance(kernel_values1, torch.Tensor):
         raise TypeError(f"Input kernel_values1 type is not a torch.Tensor. Got {type(kernel_values1)}")
 
     if not isinstance(kernel_values2, torch.Tensor):
         raise TypeError(f"Input kernel_values2 type is not a torch.Tensor. Got {type(kernel_values2)}")
 
     if not kernel_values1.dim() == 3:
@@ -104,15 +103,14 @@
     Examples:
         >>> x = torch.rand(1, 10)
         >>> bins = torch.torch.linspace(0, 255, 128)
         >>> hist = histogram(x, bins, bandwidth=torch.tensor(0.9))
         >>> hist.shape
         torch.Size([1, 128])
     """
-
     pdf, _ = marginal_pdf(x.unsqueeze(2), bins, bandwidth, epsilon)
 
     return pdf
 
 
 def histogram2d(
     x1: torch.Tensor, x2: torch.Tensor, bins: torch.Tensor, bandwidth: torch.Tensor, epsilon: float = 1e-10
```

### Comparing `kornia-0.6.9/kornia/enhance/normalize.py` & `kornia-0.7.0/kornia/enhance/normalize.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Module containing functionals for intensity normalisation."""
 
 from typing import List, Tuple, Union
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 __all__ = ["normalize", "normalize_min_max", "denormalize", "Normalize", "Denormalize"]
 
 
 class Normalize(nn.Module):
     r"""Normalize a tensor image with mean and standard deviation.
 
@@ -41,33 +41,33 @@
     def __init__(
         self,
         mean: Union[torch.Tensor, Tuple[float], List[float], float],
         std: Union[torch.Tensor, Tuple[float], List[float], float],
     ) -> None:
         super().__init__()
 
-        if isinstance(mean, float):
+        if isinstance(mean, (int, float)):
             mean = torch.tensor([mean])
 
-        if isinstance(std, float):
+        if isinstance(std, (int, float)):
             std = torch.tensor([std])
 
         if isinstance(mean, (tuple, list)):
             mean = torch.tensor(mean)
 
         if isinstance(std, (tuple, list)):
             std = torch.tensor(std)
 
         self.mean = mean
         self.std = std
 
     def forward(self, input: torch.Tensor) -> torch.Tensor:
         return normalize(input, self.mean, self.std)
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr = f"(mean={self.mean}, std={self.std})"
         return self.__class__.__name__ + repr
 
 
 def normalize(data: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -> torch.Tensor:
     r"""Normalize an image/video tensor with mean and standard deviation.
 
@@ -161,15 +161,15 @@
 
         self.mean = mean
         self.std = std
 
     def forward(self, input: torch.Tensor) -> torch.Tensor:
         return denormalize(input, self.mean, self.std)
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr = f"(mean={self.mean}, std={self.std})"
         return self.__class__.__name__ + repr
 
 
 def denormalize(data: torch.Tensor, mean: Union[torch.Tensor, float], std: Union[torch.Tensor, float]) -> torch.Tensor:
     r"""Denormalize an image/video tensor with mean and standard deviation.
```

### Comparing `kornia-0.6.9/kornia/enhance/shift_rgb.py` & `kornia-0.7.0/kornia/enhance/shift_rgb.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,17 @@
 import torch
 
-from kornia.testing import KORNIA_CHECK_IS_COLOR, KORNIA_CHECK_IS_TENSOR
+from kornia.core.check import KORNIA_CHECK_IS_COLOR, KORNIA_CHECK_IS_TENSOR
 
 
 def shift_rgb(image: torch.Tensor, r_shift: torch.Tensor, g_shift: torch.Tensor, b_shift: torch.Tensor) -> torch.Tensor:
     """Shift rgb channels.
 
     Shift each image's channel by either r_shift for red, g_shift for green and b_shift for blue channels.
     """
-
     KORNIA_CHECK_IS_TENSOR(image)
     KORNIA_CHECK_IS_COLOR(image, f"with shape {image.shape}")
 
     shifts = [r_shift, g_shift, b_shift]
 
     shifted = (image + torch.stack(shifts, dim=1).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)
```

### Comparing `kornia-0.6.9/kornia/enhance/zca.py` & `kornia-0.7.0/kornia/enhance/zca.py`

 * *Files 1% similar despite different names*

```diff
@@ -66,40 +66,38 @@
         self,
         dim: int = 0,
         eps: float = 1e-6,
         unbiased: bool = True,
         detach_transforms: bool = True,
         compute_inv: bool = False,
     ) -> None:
-
         super().__init__()
 
         self.dim = dim
         self.eps = eps
         self.unbiased = unbiased
         self.detach_transforms = detach_transforms
         self.compute_inv = compute_inv
 
         self.fitted = False
 
         self.mean_vector: Tensor
         self.transform_matrix: Tensor
         self.transform_inv: Optional[Tensor]
 
-    def fit(self, x: Tensor):
+    def fit(self, x: Tensor) -> 'ZCAWhitening':
         r"""Fit ZCA whitening matrices to the data.
 
         Args:
 
             x: Input data.
 
         returns:
             Returns a fitted ZCAWhiten object instance.
         """
-
         T, mean, T_inv = zca_mean(x, self.dim, self.unbiased, self.eps, self.compute_inv)
 
         self.mean_vector = mean
         self.transform_matrix = T
         if T_inv is None:
             self.transform_inv = torch.empty([0])
         else:
@@ -120,15 +118,14 @@
         Args:
             x: Input data.
             include_fit: Indicates whether to fit the data as part of the forward pass.
 
         Returns:
             The transformed data.
         """
-
         if include_fit:
             self.fit(x)
 
         if not self.fitted:
             raise RuntimeError("Needs to be fitted first before running. Please call fit or set include_fit to True.")
 
         x_whiten = linear_transform(x, self.transform_matrix, self.mean_vector, self.dim)
@@ -140,15 +137,14 @@
 
         Args:
             x: Whitened data.
 
         Returns:
             Original data.
         """
-
         if not self.fitted:
             raise RuntimeError("Needs to be fitted first before running. Please call fit or set include_fit to True.")
 
         if not self.compute_inv:
             raise RuntimeError("Did not compute inverse ZCA. Please set compute_inv to True")
 
         if self.transform_inv is None:
@@ -213,17 +209,15 @@
     if not isinstance(return_inverse, bool):
         raise TypeError(f"Argument return_inverse must be of type bool {type(return_inverse)}")
 
     inp_size = inp.size()
 
     if dim >= len(inp_size) or dim < -len(inp_size):
         raise IndexError(
-            "Dimension out of range (expected to be in range of [{},{}], but got {}".format(
-                -len(inp_size), len(inp_size) - 1, dim
-            )
+            f"Dimension out of range (expected to be in range of [{-len(inp_size)},{len(inp_size) - 1}], but got {dim}"
         )
 
     if dim < 0:
         dim = len(inp_size) + dim
 
     feat_dims = concatenate([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])
 
@@ -282,15 +276,14 @@
     Examples:
         >>> x = torch.tensor([[0,1],[1,0],[-1,0]], dtype = torch.float32)
         >>> zca_whiten(x)
         tensor([[ 0.0000,  1.1547],
                 [ 1.0000, -0.5773],
                 [-1.0000, -0.5773]])
     """
-
     if not isinstance(inp, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(inp)}")
 
     if not isinstance(eps, float):
         raise TypeError(f"eps type is not a float. Got{type(eps)}")
 
     if not isinstance(unbiased, bool):
@@ -342,22 +335,19 @@
         >>> inp = torch.ones((10,2))
         >>> transform_mat = torch.ones((2,2))
         >>> mean = torch.zeros((1,2))
         >>> out = linear_transform(inp, transform_mat, mean)
         >>> print(out.shape, out.unique()) # Should a be (10,2) tensor of 2s
         torch.Size([10, 2]) tensor([2.])
     """
-
     inp_size = inp.size()
 
     if dim >= len(inp_size) or dim < -len(inp_size):
         raise IndexError(
-            "Dimension out of range (expected to be in range of [{},{}], but got {}".format(
-                -len(inp_size), len(inp_size) - 1, dim
-            )
+            f"Dimension out of range (expected to be in range of [{-len(inp_size)},{len(inp_size) - 1}], but got {dim}"
         )
 
     if dim < 0:
         dim = len(inp_size) + dim
 
     feat_dims = concatenate([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])
```

### Comparing `kornia-0.6.9/kornia/feature/__init__.py` & `kornia-0.7.0/kornia/feature/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,23 @@
 from .affine_shape import LAFAffineShapeEstimator, LAFAffNetShapeEstimator, PatchAffineShapeEstimator
 from .defmo import DeFMO
+from .disk import DISK, DISKFeatures
 from .hardnet import HardNet, HardNet8
 from .hynet import TLU, FilterResponseNorm2d, HyNet
 from .integrated import (
     GFTTAffNetHardNet,
+    HesAffNetHardNet,
     KeyNetAffNetHardNet,
     KeyNetHardNet,
     LAFDescriptor,
+    LightGlueMatcher,
     LocalFeature,
     LocalFeatureMatcher,
     SIFTFeature,
+    SIFTFeatureScaleSpace,
     get_laf_descriptors,
 )
 from .keynet import KeyNet, KeyNetDetector
 from .laf import (
     KORNIA_CHECK_LAF,
     denormalize_laf,
     ellipse_to_laf,
@@ -26,17 +30,19 @@
     laf_from_three_points,
     laf_is_inside_image,
     laf_to_boundary_points,
     laf_to_three_points,
     make_upright,
     normalize_laf,
     perspective_transform_lafs,
+    rotate_laf,
     scale_laf,
     set_laf_orientation,
 )
+from .lightglue import LightGlue
 from .loftr import LoFTR
 from .matching import (
     DescriptorMatcher,
     GeometryAwareDescriptorMatcher,
     match_adalam,
     match_fginn,
     match_mnn,
@@ -44,23 +50,25 @@
     match_smnn,
     match_snn,
 )
 from .mkd import MKDDescriptor
 from .orientation import LAFOrienter, OriNet, PatchDominantGradientOrientation
 from .responses import (
     BlobDoG,
+    BlobDoGSingle,
     BlobHessian,
     CornerGFTT,
     CornerHarris,
     dog_response,
+    dog_response_single,
     gftt_response,
     harris_response,
     hessian_response,
 )
-from .scale_space_detector import PassLAF, ScaleSpaceDetector
+from .scale_space_detector import MultiResolutionDetector, PassLAF, ScaleSpaceDetector
 from .siftdesc import DenseSIFTDescriptor, SIFTDescriptor
 from .sold2 import SOLD2, SOLD2_detector
 from .sosnet import SOSNet
 from .tfeat import TFeat
 
 __all__ = [
     "match_nn",
@@ -70,40 +78,46 @@
     "match_fginn",
     "match_adalam",
     "DescriptorMatcher",
     "GeometryAwareDescriptorMatcher",
     "get_laf_descriptors",
     "LAFDescriptor",
     "LocalFeature",
+    "MultiResolutionDetector",
     "SIFTFeature",
+    "SIFTFeatureScaleSpace",
     "GFTTAffNetHardNet",
+    "HesAffNetHardNet",
     "LocalFeatureMatcher",
     "SOSNet",
     "KeyNet",
     "harris_response",
     "gftt_response",
     "hessian_response",
     "dog_response",
+    "dog_response_single",
     "CornerHarris",
     "CornerGFTT",
     "BlobHessian",
     "BlobDoG",
+    "BlobDoGSingle",
     "extract_patches_from_pyramid",
     "extract_patches_simple",
     "normalize_laf",
     "denormalize_laf",
     "laf_to_boundary_points",
     "ellipse_to_laf",
     "make_upright",
     "get_laf_scale",
     "get_laf_center",
     "get_laf_orientation",
     "set_laf_orientation",
     "get_laf_descriptors",
     "scale_laf",
+    "rotate_laf",
     "SIFTDescriptor",
     "DenseSIFTDescriptor",
     "MKDDescriptor",
     "HardNet",
     "HardNet8",
     "HyNet",
     "TLU",
@@ -137,8 +151,12 @@
     "KeyNetAffNetHardNet",
     "LAFDescriptor",
     "DescriptorMatcher",
     "LoFTR",
     "perspective_transform_lafs",
     "SOLD2_detector",
     "SOLD2",
+    "DISK",
+    "DISKFeatures",
+    "LightGlue",
+    "LightGlueMatcher",
 ]
```

### Comparing `kornia-0.6.9/kornia/feature/adalam/adalam.py` & `kornia-0.7.0/kornia/feature/adalam/adalam.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,17 +2,17 @@
 # https://github.com/cavalli1234/AdaLAM
 # Copyright (c) 2020, Luca Cavalli
 
 from typing import Optional, Tuple, Union
 
 import torch
 
-from kornia.core import Tensor
+from kornia.core import Tensor, as_tensor
+from kornia.core.check import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 from kornia.feature.laf import get_laf_center, get_laf_orientation, get_laf_scale
-from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 
 from .core import AdalamConfig, _no_match, adalam_core
 from .utils import dist_matrix
 
 
 def get_adalam_default_config() -> AdalamConfig:
     return AdalamConfig(
@@ -32,16 +32,16 @@
 
 def match_adalam(
     desc1: Tensor,
     desc2: Tensor,
     lafs1: Tensor,
     lafs2: Tensor,
     config: Optional[AdalamConfig] = None,
-    hw1: Optional[Tensor] = None,
-    hw2: Optional[Tensor] = None,
+    hw1: Optional[Tuple[int, int]] = None,
+    hw2: Optional[Tuple[int, int]] = None,
     dm: Optional[Tensor] = None,
 ) -> Tuple[Tensor, Tensor]:
     """Function, which performs descriptor matching, followed by AdaLAM filtering (see :cite:`AdaLAM2020` for more
     details)
 
     If the distance matrix dm is not provided, :py:func:`torch.cdist` is used.
 
@@ -59,19 +59,28 @@
         - Long tensor indexes of matching descriptors in desc1 and desc2. Shape: :math:`(B3, 2)`,
           where 0 <= B3 <= B1.
     """
     KORNIA_CHECK_SHAPE(desc1, ["B", "DIM"])
     KORNIA_CHECK_SHAPE(desc2, ["B", "DIM"])
     KORNIA_CHECK_LAF(lafs1)
     KORNIA_CHECK_LAF(lafs2)
+    config_ = get_adalam_default_config()
     if config is None:
-        config_ = get_adalam_default_config()
         config_['device'] = desc1.device
     else:
-        config_ = config
+        config_ = get_adalam_default_config()
+        for key, val in config.items():
+            if key not in config_.keys():
+                print(
+                    f"WARNING: custom configuration contains a key which is not recognized ({key}). "
+                    f"Known configurations are {list(config_.keys())}."
+                )
+                continue
+            # TypedDict does not support variable names. https://stackoverflow.com/a/59583427/1983544
+            config_[key] = val  # type: ignore
     adalam_object = AdalamFilter(config_)
     idxs, quality = adalam_object.match_and_filter(
         get_laf_center(lafs1).reshape(-1, 2),
         get_laf_center(lafs2).reshape(-1, 2),
         desc1,
         desc2,
         hw1,
@@ -82,15 +91,15 @@
         get_laf_scale(lafs2).reshape(-1),
         return_dist=True,
     )
     return quality, idxs
 
 
 class AdalamFilter:
-    def __init__(self, custom_config: Optional[AdalamConfig] = None):
+    def __init__(self, custom_config: Optional[AdalamConfig] = None) -> None:
         """This class acts as a wrapper to the method AdaLAM for outlier filtering.
 
         init args:
             custom_config: dictionary overriding the default configuration. Missing parameters are kept as default.
                            See documentation of DEFAULT_CONFIG for specific explanations on the accepted parameters.
         """
         if custom_config is not None:
@@ -164,26 +173,26 @@
                 s2=s2,
                 config=self.config,
                 return_dist=return_dist,
             )
 
     def match_and_filter(
         self,
-        k1,
-        k2,
-        d1,
-        d2,
-        im1shape=None,
-        im2shape=None,
-        o1=None,
-        o2=None,
-        s1=None,
-        s2=None,
+        k1: Tensor,
+        k2: Tensor,
+        d1: Tensor,
+        d2: Tensor,
+        im1shape: Optional[Tuple[int, int]] = None,
+        im2shape: Optional[Tuple[int, int]] = None,
+        o1: Optional[Tensor] = None,
+        o2: Optional[Tensor] = None,
+        s1: Optional[Tensor] = None,
+        s2: Optional[Tensor] = None,
         return_dist: bool = False,
-    ):
+    ) -> Union[Tuple[Tensor, Tensor], Tensor]:
         """Standard matching and filtering with AdaLAM. This function:
 
             - performs some elementary sanity check on the inputs;
             - wraps input arrays into torch tensors and loads to GPU if necessary;
             - extracts nearest neighbors;
             - finds mutual nearest neighbors if required;
             - finally calls AdaLAM filtering.
@@ -193,61 +202,73 @@
                 Expected an array with shape (num_keypoints_in_source_image, 2).
             k2: keypoint locations in the destination image, in pixel coordinates.
                 Expected an array with shape (num_keypoints_in_destination_image, 2).
             d1: descriptors in the source image.
                 Expected an array with shape (num_keypoints_in_source_image, descriptor_size).
             d2: descriptors in the destination image.
                 Expected an array with shape (num_keypoints_in_destination_image, descriptor_size).
-            im1shape: Shape of the source image. If None, it is inferred from keypoints max and min, at the cost of wasted runtime. So please provide it.
-                      Expected a tuple with (width, height) or (height, width) of source image
-            im2shape: Shape of the destination image. If None, it is inferred from keypoints max and min, at the cost of wasted runtime. So please provide it.
-                      Expected a tuple with (width, height) or (height, width) of destination image
-            o1/o2: keypoint orientations in degrees. They can be None if 'orientation_difference_threshold' in config is set to None.
-                   See documentation on 'orientation_difference_threshold' in the DEFAULT_CONFIG.
+            im1shape: Shape of the source image. If None, it is inferred from keypoints max and min, at the cost of
+                      wasted runtime. So please provide it. Expected a tuple with (width, height) or (height, width)
+                      of source image
+            im2shape: Shape of the destination image. If None, it is inferred from keypoints max and min, at the cost
+                      of wasted runtime. So please provide it. Expected a tuple with (width, height) or (height, width)
+                      of destination image
+            o1/o2: keypoint orientations in degrees. They can be None if 'orientation_difference_threshold' in config
+                   is set to None. See documentation on 'orientation_difference_threshold' in the DEFAULT_CONFIG.
                    Expected an array with shape (num_keypoints_in_source/destination_image,)
             s1/s2: keypoint scales. They can be None if 'scale_rate_threshold' in config is set to None.
                    See documentation on 'scale_rate_threshold' in the DEFAULT_CONFIG.
                    Expected an array with shape (num_keypoints_in_source/destination_image,)
             return_dist: if True, inverse confidence value is also outputted.
 
         Returns:
             Filtered putative matches.
             A long tensor with shape (num_filtered_matches, 2) with indices of corresponding keypoints in k1 and k2.
-        """  # noqa: E501
+        """
         if s1 is None or s2 is None:
             if self.config['scale_rate_threshold'] is not None:
                 raise AttributeError(
                     "Current configuration considers keypoint scales for filtering, but scales have not been provided.\n"  # noqa: E501
                     "Please either provide scales or set 'scale_rate_threshold' to None to disable scale filtering"
                 )
         if o1 is None or o2 is None:
             if self.config['orientation_difference_threshold'] is not None:
                 raise AttributeError(
                     "Current configuration considers keypoint orientations for filtering, but orientations have not been provided.\n"  # noqa: E501
                     "Please either provide orientations or set 'orientation_difference_threshold' to None to disable orientations filtering"  # noqa: E501
                 )
-        k1, k2, d1, d2, o1, o2, s1, s2 = self.__to_torch(k1, k2, d1, d2, o1, o2, s1, s2)
-        if (len(d2) <= 1) or (len(d1) <= 1):
-            idxs, dists = _no_match(d1)
+        _k1 = as_tensor(k1, device=self.config['device'], dtype=torch.float32)
+        _k2 = as_tensor(k2, device=self.config['device'], dtype=torch.float32)
+        _d1 = as_tensor(d1, device=self.config['device'], dtype=torch.float32)
+        _d2 = as_tensor(d2, device=self.config['device'], dtype=torch.float32)
+        if o1 is not None:
+            _o1 = as_tensor(o1, device=self.config['device'], dtype=torch.float32)
+            _o2 = as_tensor(o2, device=self.config['device'], dtype=torch.float32)
+        else:
+            _o1, _o2 = o1, o2
+        if s1 is not None:
+            _s1 = as_tensor(s1, device=self.config['device'], dtype=torch.float32)
+            _s2 = as_tensor(s2, device=self.config['device'], dtype=torch.float32)
+        else:
+            _s1, _s2 = s1, s2
+
+        if (len(_d2) <= 1) or (len(_d1) <= 1):
+            idxs, dists = _no_match(_d1)
             if return_dist:
                 return idxs, dists
             return idxs
-        distmat = dist_matrix(d1, d2, is_normalized=False)
+
+        distmat = dist_matrix(_d1, _d2, is_normalized=False)
         dd12, nn12 = torch.topk(distmat, k=2, dim=1, largest=False)  # (n1, 2)
 
         putative_matches = nn12[:, 0]
         scores = dd12[:, 0] / dd12[:, 1].clamp_min_(1e-3)
+
         if self.config['force_seed_mnn']:
             dd21, nn21 = torch.min(distmat, dim=0)  # (n2,)
-            mnn = nn21[putative_matches] == torch.arange(k1.shape[0], device=self.config['device'])
+            mnn = nn21[putative_matches] == torch.arange(_k1.shape[0], device=self.config['device'])
         else:
             mnn = None
 
         return self.filter_matches(
-            k1, k2, putative_matches, scores, mnn, im1shape, im2shape, o1, o2, s1, s2, return_dist
-        )
-
-    def __to_torch(self, *args):
-        return (
-            a if a is None or torch.is_tensor(a) else torch.tensor(a, device=self.config['device'], dtype=torch.float32)
-            for a in args
+            _k1, _k2, putative_matches, scores, mnn, im1shape, im2shape, _o1, _o2, _s1, _s2, return_dist
         )
```

### Comparing `kornia-0.6.9/kornia/feature/adalam/core.py` & `kornia-0.7.0/kornia/feature/adalam/core.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 import math
 from typing import Optional, Tuple, Union
 
 import torch
 from typing_extensions import NotRequired, TypedDict
 
-from kornia.core import Device, Tensor, concatenate, tensor, where
+from kornia.core import Tensor, concatenate, tensor, where
 
 from .ransac import ransac
 from .utils import dist_matrix, orientation_diff
 
 
 class AdalamConfig(TypedDict):
-    """
-    area_ratio: Ratio between seed circle area and image area. Higher values produce more seeds with smaller
+    """A config structure for the Adalam model.
+
+    Args:
+        area_ratio: Ratio between seed circle area and image area. Higher values produce more seeds with smaller
         neighborhoods
     search_expansion: Expansion factor of the seed circle radius for the purpose of collecting neighborhoods.
         Increases neighborhood radius without changing seed distribution
     ransac_iters: Fixed number of inner GPU-RANSAC iterations
     min_inliers: Minimum number of inliers required to accept inliers coming from a neighborhood
     min_confidence: Threshold used by the confidence-based GPU-RANSAC
     orientation_difference_threshold: Maximum difference in orientations for a point to be accepted in a
@@ -39,31 +41,33 @@
     min_inliers: NotRequired[int]
     min_confidence: NotRequired[int]
     orientation_difference_threshold: NotRequired[int]
     scale_rate_threshold: NotRequired[float]
     detected_scale_rate_threshold: NotRequired[int]
     refit: NotRequired[bool]
     force_seed_mnn: NotRequired[bool]
-    device: NotRequired[Device]
+    device: NotRequired[torch.device]
     mnn: NotRequired[Tensor]
 
 
-def _no_match(dm: Tensor):
+def _no_match(dm: Tensor) -> Tuple[Tensor, Tensor]:
     """Helper function, which output empty tensors.
 
     Returns:
             - Descriptor distance of matching descriptors, shape of :math:`(0, 1)`.
             - Long tensor indexes of matching descriptors in desc1 and desc2, shape of :math:`(0, 2)`.
     """
     dists = torch.empty(0, 1, device=dm.device, dtype=dm.dtype)
     idxs = torch.empty(0, 2, device=dm.device, dtype=torch.long)
     return dists, idxs
 
 
-def select_seeds(dist1: Tensor, R1: Union[float, Tensor], scores1: Tensor, fnn12: Tensor, mnn: Optional[Tensor]):
+def select_seeds(
+    dist1: Tensor, R1: Union[float, Tensor], scores1: Tensor, fnn12: Tensor, mnn: Optional[Tensor]
+) -> Tuple[Tensor, Tensor]:
     """Select seed correspondences among the set of available matches.
 
     dist1: Precomputed distance matrix between keypoints in image I_1
     R1: Base radius of neighborhoods in image I_1
     scores1: Confidence scores on the putative_matches. Usually holds Lowe's ratio scores.
     fnn12: Matches between keypoints of I_1 and I_2.
            The i-th entry of fnn12 is j if and only if keypoint k_i in image I_1 is matched to keypoint k_j in image I_2
@@ -107,15 +111,15 @@
     R1: Union[float, Tensor],
     R2: Union[float, Tensor],
     fnn12: Tensor,
     ORIENTATION_THR: float,
     SCALE_RATE_THR: float,
     SEARCH_EXP: float,
     MIN_INLIERS: float,
-):
+) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
     """Assign keypoints to seed points. This checks both the distance and the agreement of the local transformation
     if available.
 
     o1: Orientations of keypoints in image I_1
     o2: Orientations of keypoints in image I_2
     s1: Scales of keypoints in image I_1
     s2: Scales of keypoints in image I_2
@@ -179,15 +183,15 @@
     fnn12: Tensor,
     fnn_to_seed_local_consistency_map_corr: Tensor,
     k1: Tensor,
     k2: Tensor,
     im1seeds: Tensor,
     im2seeds: Tensor,
     scores: Tensor,
-):
+) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
     """Prepare local neighborhoods around each seed for the parallel RANSACs. This involves two steps: 1) Collect
     all selected keypoints and refer them with respect to their seed point 2) Sort keypoints by score for the
     progressive sampling to pick the best samples first.
 
     fnn12: Matches between keypoints of I_1 and I_2.
            The i-th entry of fnn12 is j if and only if keypoint k_i in image I_1 is matched to keypoint k_j in image I_2
     fnn_to_seed_local_consistency_map_corr: Boolean matrix of size (num_seeds, num_keypoints).
@@ -229,20 +233,15 @@
 
     # Finally we need to sort keypoints by scores in a way that assignments to the same seed are close together
     # To achieve this we assume scores lie in (0, 1) and add the integer index of the corresponding seed
     expanded_local_scores = scores[tokp1] + ransidx.type(scores.dtype)
 
     sorting_perm = torch.argsort(expanded_local_scores)
 
-    im1loc = im1loc[sorting_perm]
-    im2loc = im2loc[sorting_perm]
-    tokp1 = tokp1[sorting_perm]
-    tokp2 = tokp2[sorting_perm]
-
-    return im1loc, im2loc, ransidx, tokp1, tokp2
+    return im1loc[sorting_perm], im2loc[sorting_perm], ransidx, tokp1[sorting_perm], tokp2[sorting_perm]
 
 
 def adalam_core(
     k1: Tensor,
     k2: Tensor,
     fnn12: Tensor,
     scores1: Tensor,
@@ -262,37 +261,39 @@
     Inputs:
         k1: keypoint locations in the source image, in pixel coordinates.
             Expected a float32 tensor with shape (num_keypoints_in_source_image, 2).
         k2: keypoint locations in the destination image, in pixel coordinates.
             Expected a float32 tensor with shape (num_keypoints_in_destination_image, 2).
         fn12: Initial set of putative matches to be filtered.
               The current implementation assumes that these are unfiltered nearest neighbor matches,
-              so it requires this to be a list of indices a_i such that the source keypoint i is associated to the destination keypoint a_i.
-              For now to use AdaLAM on different inputs a workaround on the input format is required.
-              Expected a long tensor with shape (num_keypoints_in_source_image,).
+              so it requires this to be a list of indices a_i such that the source keypoint i is associated to the
+              destination keypoint a_i. For now to use AdaLAM on different inputs a workaround on the input format is
+              required. Expected a long tensor with shape (num_keypoints_in_source_image,).
         scores1: Confidence scores on the putative_matches. Usually holds Lowe's ratio scores.
-        mnn: A mask indicating which putative matches are also mutual nearest neighbors. See documentation on 'force_seed_mnn' in the DEFAULT_CONFIG.
-             If None, it disables the mutual nearest neighbor filtering on seed point selection.
-             Expected a bool tensor with shape (num_keypoints_in_source_image,)
-        im1shape: Shape of the source image. If None, it is inferred from keypoints max and min, at the cost of wasted runtime. So please provide it.
-                  Expected a tuple with (width, height) or (height, width) of source image
-        im2shape: Shape of the destination image. If None, it is inferred from keypoints max and min, at the cost of wasted runtime. So please provide it.
-                  Expected a tuple with (width, height) or (height, width) of destination image
-        o1/o2: keypoint orientations in degrees. They can be None if 'orientation_difference_threshold' in config is set to None.
-               See documentation on 'orientation_difference_threshold' in the DEFAULT_CONFIG.
+        mnn: A mask indicating which putative matches are also mutual nearest neighbors. See documentation on
+             'force_seed_mnn' in the DEFAULT_CONFIG. If None, it disables the mutual nearest neighbor filtering on seed
+             point selection. Expected a bool tensor with shape (num_keypoints_in_source_image,)
+        im1shape: Shape of the source image. If None, it is inferred from keypoints max and min, at the cost of wasted
+                  runtime. So please provide it. Expected a tuple with (width, height) or (height, width) of source
+                  image
+        im2shape: Shape of the destination image. If None, it is inferred from keypoints max and min, at the cost of
+                  wasted runtime. So please provide it. Expected a tuple with (width, height) or (height, width) of
+                  destination image
+        o1/o2: keypoint orientations in degrees. They can be None if 'orientation_difference_threshold' in config is
+               set to None. See documentation on 'orientation_difference_threshold' in the DEFAULT_CONFIG.
                Expected a float32 tensor with shape (num_keypoints_in_source/destination_image,)
         s1/s2: keypoint scales. They can be None if 'scale_rate_threshold' in config is set to None.
                See documentation on 'scale_rate_threshold' in the DEFAULT_CONFIG.
                Expected a float32 tensor with shape (num_keypoints_in_source/destination_image,)
         return_dist: if True, inverse confidence value is also outputted. Default is False
 
     Returns:
         idxs: A long tensor with shape (num_filtered_matches, 2) with indices of corresponding keypoints in k1 and k2.
         dists: inverse confidence ratio.
-    """  # noqa: E501
+    """
     AREA_RATIO = config['area_ratio']
     SEARCH_EXP = config['search_expansion']
     RANSAC_ITERS = config['ransac_iters']
     MIN_INLIERS = config['min_inliers']
     MIN_CONF = config['min_confidence']
     ORIENTATION_THR = config['orientation_difference_threshold']
     SCALE_RATE_THR = config['scale_rate_threshold']
@@ -362,15 +363,15 @@
         fnn12, local_neighs_mask, k1, k2, im1seeds, im2seeds, scores1
     )
     im1loc = im1loc / (R1 * SEARCH_EXP)
     im2loc = im2loc / (R2 * SEARCH_EXP)
 
     # Run the parallel confidence-based RANSACs to perform local affine verification
     inlier_idx, _, inl_confidence, inlier_counts = ransac(
-        xsamples=im1loc, ysamples=im2loc, rdims=rdims, iters=RANSAC_ITERS, refit=REFIT, config=config
+        xsamples=im1loc, ysamples=im2loc, rdims=rdims, iters=RANSAC_ITERS, refit=REFIT, config=dict(config)
     )
 
     conf = inl_confidence[ransidx[inlier_idx]]
     cnt = inlier_counts[ransidx[inlier_idx]].float()
     dist_ratio = 1.0 / conf
     passed_inliers_mask = (conf >= MIN_CONF) & (cnt * (1 - dist_ratio) >= MIN_INLIERS)
     accepted_inliers = inlier_idx[passed_inliers_mask]
```

### Comparing `kornia-0.6.9/kornia/feature/adalam/ransac.py` & `kornia-0.7.0/kornia/feature/adalam/ransac.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,33 @@
-from typing import Tuple
+from typing import Any, Dict, Tuple, Union
 
 import torch
 
 from kornia.core import Tensor
 
 from .utils import arange_sequence, batch_2x2_ellipse, batch_2x2_inv, draw_first_k_couples, piecewise_arange
 
 
-def stable_sort_residuals(residuals, ransidx):
+def stable_sort_residuals(residuals: Tensor, ransidx: Tensor) -> Tuple[Tensor, Tensor]:
     logres = torch.log(residuals + 1e-10)
     minlogres = torch.min(logres)
     maxlogres = torch.max(logres)
 
     sorting_score = ransidx.unsqueeze(0).float() + 0.99 * (logres - minlogres) / (maxlogres - minlogres)
 
     sorting_idxes = torch.argsort(sorting_score, dim=-1)  # (niters, numsamples)
 
     iters_range = torch.arange(residuals.shape[0], device=residuals.device)
 
     return residuals[iters_range.unsqueeze(-1), sorting_idxes], sorting_idxes
 
 
-def group_sum_and_cumsum(scores_mat, end_group_idx, group_idx=None):
+def group_sum_and_cumsum(
+    scores_mat: Tensor, end_group_idx: Tensor, group_idx: Union[Tensor, slice, None] = None
+) -> Tuple[Tensor, Union[Tensor, None]]:
     cumulative_scores = torch.cumsum(scores_mat, dim=1)
     ending_cumusums = cumulative_scores[:, end_group_idx]
     shifted_ending_cumusums = torch.cat(
         [
             torch.zeros(size=(ending_cumusums.shape[0], 1), dtype=ending_cumusums.dtype, device=scores_mat.device),
             ending_cumusums[:, :-1],
         ],
@@ -35,15 +37,17 @@
 
     if group_idx is not None:
         grouped_cumsums = cumulative_scores - shifted_ending_cumusums[:, group_idx]
         return grouped_sums, grouped_cumsums
     return grouped_sums, None
 
 
-def confidence_based_inlier_selection(residuals, ransidx, rdims, idxoffsets, dv, min_confidence):
+def confidence_based_inlier_selection(
+    residuals: Tensor, ransidx: Tensor, rdims: Tensor, idxoffsets: Tensor, dv: torch.device, min_confidence: Tensor
+) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
     numransacs = rdims.shape[0]
     numiters = residuals.shape[0]
 
     sorted_res, sorting_idxes = stable_sort_residuals(residuals, ransidx)
     sorted_res_sqr = sorted_res**2
 
     too_perfect_fits = sorted_res_sqr <= 1e-8
@@ -54,14 +58,17 @@
     )
 
     duplicates_per_sample = res_dup_counts[inv_indices]
     inlier_weights = (1.0 / duplicates_per_sample).repeat(numiters, 1)
     inlier_weights[too_perfect_fits] = 0.0
 
     balanced_rdims, weights_cumsums = group_sum_and_cumsum(inlier_weights, end_rans_indexing, ransidx)
+    if not isinstance(weights_cumsums, Tensor):
+        raise TypeError('Expected the `weights_cumsums` to be a Tensor!')
+
     progressive_inl_rates = weights_cumsums.float() / (balanced_rdims.repeat_interleave(rdims, dim=1)).float()
 
     good_inl_mask = (sorted_res_sqr * min_confidence <= progressive_inl_rates) | too_perfect_fits
 
     inlier_weights[~good_inl_mask] = 0.0
     inlier_counts_matrix, _ = group_sum_and_cumsum(inlier_weights, end_rans_indexing)
 
@@ -93,15 +100,17 @@
 
     padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]
     padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]
 
     return padded_inlier_x, padded_inlier_y
 
 
-def ransac(xsamples, ysamples, rdims: Tensor, config, iters=128, refit=True):
+def ransac(
+    xsamples: Tensor, ysamples: Tensor, rdims: Tensor, config: Dict[str, Any], iters: int = 128, refit: bool = True
+) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
     DET_THR = config['detected_scale_rate_threshold']
     MIN_CONFIDENCE = config['min_confidence']
     dv: torch.device = config['device']
 
     numransacs = rdims.shape[0]
     ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)
     idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)
```

### Comparing `kornia-0.6.9/kornia/feature/adalam/utils.py` & `kornia-0.7.0/kornia/feature/adalam/utils.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,46 +1,47 @@
 import math
+from typing import Tuple
 
 import torch
 
 from kornia.core import Tensor
 
 
-def arange_sequence(ranges):
-    """
-    returns a sequence of the ranges specified by the argument.
+def arange_sequence(ranges: Tensor) -> Tensor:
+    """Returns a sequence of the ranges specified by the argument.
+
     Example:
     [2, 5, 1, 2] -> [0, 1, 0, 1, 2, 3, 4, 0, 0, 1]
     """
     maxcnt = torch.max(ranges).item()
     numuni = ranges.shape[0]
     complete_ranges = torch.arange(maxcnt, device=ranges.device).unsqueeze(0).expand(numuni, -1)
 
     return complete_ranges[complete_ranges < ranges.unsqueeze(-1)]
 
 
-def dist_matrix(d1, d2, is_normalized=False):
+def dist_matrix(d1: Tensor, d2: Tensor, is_normalized: bool = False) -> Tensor:
     if is_normalized:
         return 2 - 2.0 * d1 @ d2.t()
     x_norm = (d1**2).sum(1).view(-1, 1)
     y_norm = (d2**2).sum(1).view(1, -1)
     # print(x_norm, y_norm)
     distmat = x_norm + y_norm - 2.0 * d1 @ d2.t()
     # distmat[torch.isnan(distmat)] = np.inf
     return distmat
 
 
-def orientation_diff(o1, o2):
+def orientation_diff(o1: Tensor, o2: Tensor) -> Tensor:
     diff = o2 - o1
     diff[diff < -180] += 360
     diff[diff >= 180] -= 360
     return diff
 
 
-def piecewise_arange(piecewise_idxer: Tensor):
+def piecewise_arange(piecewise_idxer: Tensor) -> Tensor:
     """
     count repeated indices
     Example:
     [0, 0, 0, 3, 3, 3, 3, 1, 1, 2] -> [0, 1, 2, 0, 1, 2, 3, 0, 1, 0]
     """
     dv = piecewise_idxer.device
     # print(piecewise_idxer)
@@ -51,15 +52,15 @@
     numuni = uni.shape[0]
     tmp = torch.zeros(size=(numuni, maxcnt), device=dv).bool()
     ranges = torch.arange(maxcnt, device=dv).unsqueeze(0).expand(numuni, -1)
     tmp[ranges < counts.unsqueeze(-1)] = True
     return ranges[tmp]
 
 
-def batch_2x2_inv(m, check_dets=False):
+def batch_2x2_inv(m: Tensor, check_dets: bool = False) -> Tensor:
     a = m[..., 0, 0]
     b = m[..., 0, 1]
     c = m[..., 1, 0]
     d = m[..., 1, 1]
     minv = torch.empty_like(m)
     det = a * d - b * c
     if check_dets:
@@ -67,31 +68,31 @@
     minv[..., 0, 0] = d
     minv[..., 1, 1] = a
     minv[..., 0, 1] = -b
     minv[..., 1, 0] = -c
     return minv / det.unsqueeze(-1).unsqueeze(-1)
 
 
-def batch_2x2_Q(m):
+def batch_2x2_Q(m: Tensor) -> Tensor:
     return batch_2x2_inv(batch_2x2_invQ(m), check_dets=True)
 
 
-def batch_2x2_invQ(m):
+def batch_2x2_invQ(m: Tensor) -> Tensor:
     return m @ m.transpose(-1, -2)
 
 
-def batch_2x2_det(m):
+def batch_2x2_det(m: Tensor) -> Tensor:
     a = m[..., 0, 0]
     b = m[..., 0, 1]
     c = m[..., 1, 0]
     d = m[..., 1, 1]
     return a * d - b * c
 
 
-def batch_2x2_ellipse(m):
+def batch_2x2_ellipse(m: Tensor) -> Tuple[Tensor, Tensor]:
     am = m[..., 0, 0]
     bm = m[..., 0, 1]
     cm = m[..., 1, 0]
     dm = m[..., 1, 1]
 
     a = am**2 + bm**2
     b = am * cm + bm * dm
@@ -107,30 +108,31 @@
     eigenvecs = torch.stack([b.unsqueeze(-1) / dens, torch.ones_like(dens)], dim=-2)
     eigenvecs = eigenvecs / torch.norm(eigenvecs, dim=-2, keepdim=True)
 
     # err = eigenvecs @ torch.diag_embed(eigenvals) @ eigenvecs.transpose(-2, -1) - q
     return eigenvals, eigenvecs
 
 
-def draw_first_k_couples(k: int, rdims: Tensor, dv: torch.device):
+def draw_first_k_couples(k: int, rdims: Tensor, dv: torch.device) -> Tensor:
     # exhaustive search over the first n samples:
     # n(n+1)/2 = n2/2 + n/2 couples
     # max n for which we can exhaustively sample with k couples:
     # n2/2 + n/2 = k
     # n = sqrt(1/4 + 2k)-1/2 = (sqrt(8k+1)-1)/2
     max_exhaustive_search = int(math.sqrt(2 * k + 0.25) - 0.5)
-    residual_search = k - max_exhaustive_search * (max_exhaustive_search + 1) / 2
+    residual_search = int(k - max_exhaustive_search * (max_exhaustive_search + 1) / 2)
+
     repeats = torch.cat(
         [
             torch.arange(max_exhaustive_search, dtype=torch.long, device=dv) + 1,
             torch.tensor([residual_search], dtype=torch.long, device=dv),
         ]
     )
     idx_sequence = torch.stack([repeats.repeat_interleave(repeats), arange_sequence(repeats)], dim=-1)
     return torch.remainder(idx_sequence.unsqueeze(-1), rdims)
 
 
-def random_samples_indices(iters, rdims, dv):
+def random_samples_indices(iters: int, rdims: Tensor, dv: torch.device) -> Tensor:
     rands = torch.rand(size=(iters, 2, rdims.shape[0]), device=dv)
     scaled_rands = rands * (rdims - 1e-8).float()
     rand_samples_rel = scaled_rands.long()
     return rand_samples_rel
```

### Comparing `kornia-0.6.9/kornia/feature/affine_shape.py` & `kornia-0.7.0/kornia/feature/affine_shape.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 import math
 import warnings
 from typing import Dict, Optional
 
 import torch
-import torch.nn as nn
+from torch import nn
 
+from kornia.core.check import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 from kornia.filters.kernels import get_gaussian_kernel2d
 from kornia.filters.sobel import SpatialGradient
-from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 from kornia.utils.helpers import map_location_to_cpu
 
 from .laf import (
     ellipse_to_laf,
     extract_patches_from_pyramid,
     get_laf_orientation,
     get_laf_scale,
@@ -30,30 +30,34 @@
     The method determines the affine shape of the local feature as in :cite:`baumberg2000`.
 
     Args:
         patch_size: the input image patch size.
         eps: for safe division.
     """
 
-    def __init__(self, patch_size: int = 19, eps: float = 1e-10):
+    def __init__(self, patch_size: int = 19, eps: float = 1e-10) -> None:
         super().__init__()
         self.patch_size: int = patch_size
         self.gradient: nn.Module = SpatialGradient('sobel', 1)
         self.eps: float = eps
         sigma: float = float(self.patch_size) / math.sqrt(2.0)
         self.weighting: torch.Tensor = get_gaussian_kernel2d((self.patch_size, self.patch_size), (sigma, sigma), True)
 
-    def __repr__(self):
-        return self.__class__.__name__ + '(' 'patch_size=' + str(self.patch_size) + ', ' + 'eps=' + str(self.eps) + ')'
+    def __repr__(self) -> str:
+        return f"{self.__class__.__name__}(patch_size={self.patch_size}, eps={self.eps})"
 
     def forward(self, patch: torch.Tensor) -> torch.Tensor:
-        """Args:
-            patch: (torch.Tensor) shape [Bx1xHxW]
+        """
+        Args:
+            patch: :math:`(B, 1, H, W)`
+
         Returns:
-            torch.Tensor: ellipse_shape shape [Bx1x3]"""
+            torch.Tensor: ellipse_shape :math:`(B, 1, 3)`
+
+        """
         KORNIA_CHECK_SHAPE(patch, ["B", "1", "H", "W"])
         self.weighting = self.weighting.to(patch.dtype).to(patch.device)
         grads: torch.Tensor = self.gradient(patch) * self.weighting
         # unpack the edges
         gx: torch.Tensor = grads[:, :, 0]
         gy: torch.Tensor = grads[:, :, 1]
         # abc == 1st axis, mixture, 2nd axis. Ellipse_shape is a 2nd moment matrix.
@@ -103,36 +107,31 @@
                 "`LAFAffineShapeEstimator` default behaviour is changed "
                 "and now it does preserve original LAF orientation. "
                 "Make sure your code accounts for this.",
                 DeprecationWarning,
                 stacklevel=2,
             )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            self.__class__.__name__ + '('
-            'patch_size='
-            + str(self.patch_size)
-            + ', '
-            + 'affine_shape_detector='
-            + str(self.affine_shape_detector)
-            + ', '
-            + 'preserve_orientation='
-            + str(self.preserve_orientation)
-            + ')'
+            f'{self.__class__.__name__}'
+            f'(patch_size={self.patch_size}, '
+            f'affine_shape_detector={self.affine_shape_detector}, '
+            f'preserve_orientation={self.preserve_orientation})'
         )
 
     def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:
         """
         Args:
-            laf: (torch.Tensor) shape [BxNx2x3]
-            img: (torch.Tensor) shape [Bx1xHxW]
+            LAF: :math:`(B, N, 2, 3)`
+            img: :math:`(B, 1, H, W)`
 
         Returns:
-            torch.Tensor: laf_out shape [BxNx2x3]"""
+            LAF_out: :math:`(B, N, 2, 3)`
+        """
         KORNIA_CHECK_LAF(laf)
         KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
         B, N = laf.shape[:2]
         PS: int = self.patch_size
         patches: torch.Tensor = extract_patches_from_pyramid(img, make_upright(laf), PS, True).view(-1, 1, PS, PS)
         ellipse_shape: torch.Tensor = self.affine_shape_detector(patches)
         ellipses = torch.cat([laf.view(-1, 2, 3)[..., 2].unsqueeze(1), ellipse_shape], dim=2).view(B, N, 5)
@@ -157,15 +156,15 @@
     Then original LAF shape is replaced with estimated one. The original LAF orientation is not preserved,
     so it is recommended to first run LAFAffineShapeEstimator and then LAFOrienter.
 
     Args:
         pretrained: Download and set pretrained weights to the model.
     """
 
-    def __init__(self, pretrained: bool = False, preserve_orientation: bool = True):
+    def __init__(self, pretrained: bool = False, preserve_orientation: bool = True) -> None:
         super().__init__()
         self.features = nn.Sequential(
             nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False),
             nn.BatchNorm2d(16, affine=False),
             nn.ReLU(),
             nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),
             nn.BatchNorm2d(16, affine=False),
@@ -211,19 +210,19 @@
         # the patches extractor with F.grid_sample are very noisy, making the detector
         # training totally unstable.
         return (x - mp.detach()) / (sp.detach() + eps)
 
     def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:
         """
         Args:
-            laf: shape [BxNx2x3]
-            img: shape [Bx1xHxW]
+            LAF: :math:`(B, N, 2, 3)`
+            img: :math:`(B, 1, H, W)`
 
         Returns:
-            laf_out shape [BxNx2x3]
+            LAF_out: :math:`(B, N, 2, 3)`
         """
         KORNIA_CHECK_LAF(laf)
         KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
         B, N = laf.shape[:2]
         PS: int = self.patch_size
         patches: torch.Tensor = extract_patches_from_pyramid(img, make_upright(laf), PS, True).view(-1, 1, PS, PS)
         xy = self.features(self._normalize_input(patches)).view(-1, 3)
```

### Comparing `kornia-0.6.9/kornia/feature/defmo.py` & `kornia-0.7.0/kornia/feature/defmo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Callable, Dict, List, Optional, Type
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from kornia.core import Module, Tensor, concatenate, stack
 from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls["defmo_encoder"] = "http://ptak.felk.cvut.cz/personal/rozumden/defmo_saved_models/encoder_best.pt"
 urls["defmo_rendering"] = "http://ptak.felk.cvut.cz/personal/rozumden/defmo_saved_models/rendering_best.pt"
@@ -202,28 +202,28 @@
         return x
 
     def forward(self, x: Tensor) -> Tensor:
         return self._forward_impl(x)
 
 
 class EncoderDeFMO(Module):
-    def __init__(self):
+    def __init__(self) -> None:
         super().__init__()
         model = ResNet(Bottleneck, [3, 4, 6, 3])  # ResNet50
         modelc1 = nn.Sequential(*list(model.children())[:3])
         modelc2 = nn.Sequential(*list(model.children())[4:8])
         modelc1[0] = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
         self.net = nn.Sequential(modelc1, modelc2)
 
     def forward(self, input_data: Tensor) -> Tensor:
         return self.net(input_data)
 
 
 class RenderingDeFMO(Module):
-    def __init__(self):
+    def __init__(self) -> None:
         super().__init__()
         self.tsr_steps: int = 24
         model = nn.Sequential(
             nn.Conv2d(2049, 1024, kernel_size=3, stride=1, padding=1, bias=False),
             nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
             nn.ReLU(inplace=True),
             Bottleneck(1024, 256),
@@ -242,17 +242,15 @@
         self.times = torch.linspace(0, 1, self.tsr_steps)
 
     def forward(self, latent: Tensor) -> Tensor:
         times = self.times.to(latent.device).unsqueeze(0).repeat(latent.shape[0], 1)
         renders = []
         for ki in range(times.shape[1]):
             t_tensor = (
-                # TODO: replace by after deprecate pytorch 1.6
-                # times[list(range(times.shape[0])), ki]
-                times[[x for x in range(times.shape[0])], ki]  # skipcq: PYL-R1721
+                times[list(range(times.shape[0])), ki]
                 .unsqueeze(-1)
                 .unsqueeze(-1)
                 .unsqueeze(-1)
                 .repeat(1, 1, latent.shape[2], latent.shape[3])
             )
             latenti = concatenate((t_tensor, latent), 1)
             result = self.net(latenti)
```

### Comparing `kornia-0.6.9/kornia/feature/hardnet.py` & `kornia-0.7.0/kornia/feature/hardnet.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from typing import Dict
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 
-from kornia.testing import KORNIA_CHECK_SHAPE, is_mps_tensor_safe
+from kornia.core.check import KORNIA_CHECK_SHAPE
+from kornia.testing import is_mps_tensor_safe
 from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls["hardnet++"] = "https://github.com/DagnyT/hardnet/raw/master/pretrained/pretrained_all_datasets/HardNet++.pth"
 urls[
     "liberty_aug"
 ] = "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth"
@@ -110,15 +111,15 @@
     Examples:
         >>> input = torch.rand(16, 1, 32, 32)
         >>> hardnet = HardNet8()
         >>> descs = hardnet(input) # 16x128
     """
     patch_size = 32
 
-    def __init__(self, pretrained: bool = False):
+    def __init__(self, pretrained: bool = False) -> None:
         super().__init__()
         self.features = nn.Sequential(
             nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False),
             nn.BatchNorm2d(32, affine=False),
             nn.ReLU(),
             nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),
             nn.BatchNorm2d(32, affine=False),
@@ -149,15 +150,15 @@
         # use torch.hub to load pretrained model
         if pretrained:
             pretrained_dict = torch.hub.load_state_dict_from_url(urls['hardnet8v2'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
 
     @staticmethod
-    def weights_init(m):
+    def weights_init(m: object) -> None:
         if isinstance(m, nn.Conv2d):
             nn.init.orthogonal_(m.weight.data, gain=0.6)
             if m.bias is not None:
                 nn.init.constant_(m.bias.data, 0.01)
 
     @staticmethod
     def _normalize_input(x: torch.Tensor, eps: float = 1e-7) -> torch.Tensor:
```

### Comparing `kornia-0.6.9/kornia/feature/hynet.py` & `kornia-0.7.0/kornia/feature/hynet.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Dict
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from kornia.core import Module, Parameter, Tensor, tensor, zeros
 from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls[
     "liberty"
@@ -45,16 +45,15 @@
     def __init__(
         self,
         num_features: int,
         eps: float = 1e-6,
         is_bias: bool = True,
         is_scale: bool = True,
         is_eps_leanable: bool = False,
-    ):
-
+    ) -> None:
         super().__init__()
 
         self.num_features = num_features
         self.init_eps = eps
         self.is_eps_leanable = is_eps_leanable
         self.is_bias = is_bias
         self.is_scale = is_scale
@@ -63,21 +62,21 @@
         self.bias = Parameter(zeros(1, num_features, 1, 1), requires_grad=True)
         if is_eps_leanable:
             self.eps = Parameter(tensor(1), requires_grad=True)
         else:
             self.register_buffer('eps', tensor([eps]))
         self.reset_parameters()
 
-    def reset_parameters(self):
+    def reset_parameters(self) -> None:
         nn.init.ones_(self.weight)
         nn.init.zeros_(self.bias)
         if self.is_eps_leanable:
             nn.init.constant_(self.eps, self.init_eps)
 
-    def extra_repr(self):
+    def extra_repr(self) -> str:
         return 'num_features={num_features}, eps={init_eps}'.format(**self.__dict__)
 
     def forward(self, x: Tensor) -> Tensor:
         # Compute the mean norm of activations per channel.
         nu2 = x.pow(2).mean(dim=[2, 3], keepdim=True)
 
         # Perform FRN.
@@ -105,26 +104,26 @@
         torch.Tensor
 
     Shape:
         - Input: :math:`(B, \text{num_features}, H, W)`
         - Output: :math:`(B, \text{num_features}, H, W)`
     """
 
-    def __init__(self, num_features: int):
+    def __init__(self, num_features: int) -> None:
         """max(y, tau) = max(y - tau, 0) + tau = ReLU(y - tau) + tau"""
         super().__init__()
         self.num_features = num_features
         self.tau = Parameter(-torch.ones(1, num_features, 1, 1), requires_grad=True)
         self.reset_parameters()
 
-    def reset_parameters(self):
+    def reset_parameters(self) -> None:
         # nn.init.zeros_(self.tau)
         nn.init.constant_(self.tau, -1)
 
-    def extra_repr(self):
+    def extra_repr(self) -> str:
         return 'num_features={num_features}'.format(**self.__dict__)
 
     def forward(self, x: Tensor) -> Tensor:
         return torch.max(x, self.tau)
 
 
 class HyNet(Module):
@@ -160,15 +159,15 @@
         self,
         pretrained: bool = False,
         is_bias: bool = True,
         is_bias_FRN: bool = True,
         dim_desc: int = 128,
         drop_rate: float = 0.3,
         eps_l2_norm: float = 1e-10,
-    ):
+    ) -> None:
         super().__init__()
         self.eps_l2_norm = eps_l2_norm
         self.dim_desc = dim_desc
         self.drop_rate = drop_rate
         self.layer1 = nn.Sequential(
             FilterResponseNorm2d(1, is_bias=is_bias_FRN),
             TLU(1),
@@ -214,15 +213,14 @@
 
         self.desc_norm = nn.LocalResponseNorm(2 * self.dim_desc, 2.0 * self.dim_desc, 0.5, 0.0)
         # use torch.hub to load pretrained model
         if pretrained:
             pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
-        return
 
     def forward(self, x: Tensor) -> Tensor:
         x = self.layer1(x)
         x = self.layer2(x)
         x = self.layer3(x)
         x = self.layer4(x)
         x = self.layer5(x)
```

### Comparing `kornia-0.6.9/kornia/feature/integrated.py` & `kornia-0.7.0/kornia/feature/integrated.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,25 +1,32 @@
 import warnings
-from typing import Dict, List, Optional, Tuple
+from typing import ClassVar, Dict, List, Optional, Tuple
 
 import torch
 
 from kornia.color import rgb_to_grayscale
 from kornia.core import Device, Module, Tensor, concatenate
+from kornia.core.check import KORNIA_CHECK_LAF
 from kornia.geometry.subpix import ConvQuadInterp3d
 from kornia.geometry.transform import ScalePyramid
-from kornia.testing import KORNIA_CHECK_LAF
 
 from .affine_shape import LAFAffNetShapeEstimator
 from .hardnet import HardNet
 from .keynet import KeyNetDetector
 from .laf import extract_patches_from_pyramid, get_laf_center, scale_laf
+from .lightglue import LightGlue
+from .matching import GeometryAwareDescriptorMatcher, _no_match
 from .orientation import LAFOrienter, OriNet, PassLAF
-from .responses import BlobDoG, CornerGFTT
-from .scale_space_detector import ScaleSpaceDetector
+from .responses import BlobDoG, BlobDoGSingle, BlobHessian, CornerGFTT
+from .scale_space_detector import (
+    Detector_config,
+    MultiResolutionDetector,
+    ScaleSpaceDetector,
+    get_default_detector_config,
+)
 from .siftdesc import SIFTDescriptor
 
 
 def get_laf_descriptors(
     img: Tensor, lafs: Tensor, patch_descriptor: Module, patch_size: int = 32, grayscale_descriptor: bool = True
 ) -> Tensor:
     r"""Function to get local descriptors, corresponding to LAFs (keypoints).
@@ -73,25 +80,18 @@
             patch_descriptor_module = HardNet(True)
         self.descriptor = patch_descriptor_module
         self.patch_size = patch_size
         self.grayscale_descriptor = grayscale_descriptor
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'descriptor='
-            + self.descriptor.__repr__()
-            + ', '
-            + 'patch_size='
-            + str(self.patch_size)
-            + ', '
-            + 'grayscale_descriptor='
-            + str(self.grayscale_descriptor)
-            + ')'
+            f"{self.__class__.__name__}"
+            f"(descriptor={self.descriptor.__repr__()}, "
+            f"patch_size={self.patch_size}, "
+            f"grayscale_descriptor='{self.grayscale_descriptor})"
         )
 
     def forward(self, img: Tensor, lafs: Tensor) -> Tensor:
         r"""Three stage local feature detection.
 
         First the location and scale of interest points are determined by
         detect function. Then affine shape and orientation.
@@ -140,24 +140,55 @@
         descs = self.descriptor(img, lafs)
         return (lafs, responses, descs)
 
 
 class SIFTFeature(LocalFeature):
     """Convenience module, which implements DoG detector + (Root)SIFT descriptor.
 
+    Using `kornia.feature.MultiResolutionDetector` without blur pyramid Still not as good as OpenCV/VLFeat because of
+    https://github.com/kornia/kornia/pull/884,
+    but we are working on it
+    """
+
+    def __init__(
+        self,
+        num_features: int = 8000,
+        upright: bool = False,
+        rootsift: bool = True,
+        device: Device = torch.device('cpu'),
+        config: Detector_config = get_default_detector_config(),
+    ) -> None:
+        patch_size: int = 41
+        detector = MultiResolutionDetector(
+            BlobDoGSingle(1.0, 1.6),
+            num_features,
+            config,
+            ori_module=PassLAF() if upright else LAFOrienter(19),
+            aff_module=PassLAF(),
+        ).to(device)
+        descriptor = LAFDescriptor(
+            SIFTDescriptor(patch_size=patch_size, rootsift=rootsift), patch_size=patch_size, grayscale_descriptor=True
+        ).to(device)
+        super().__init__(detector, descriptor)
+
+
+class SIFTFeatureScaleSpace(LocalFeature):
+    """Convenience module, which implements DoG detector + (Root)SIFT descriptor. Using
+    `kornia.feature.ScaleSpaceDetector` with blur pyramid.
+
     Still not as good as OpenCV/VLFeat because of https://github.com/kornia/kornia/pull/884, but we are working on it
     """
 
     def __init__(
         self,
         num_features: int = 8000,
         upright: bool = False,
         rootsift: bool = True,
         device: Device = torch.device('cpu'),
-    ):
+    ) -> None:
         patch_size: int = 41
         detector = ScaleSpaceDetector(
             num_features,
             resp_module=BlobDoG(),
             nms_module=ConvQuadInterp3d(10),
             scale_pyr_module=ScalePyramid(3, 1.6, 32, double_image=True),
             ori_module=PassLAF() if upright else LAFOrienter(19),
@@ -170,54 +201,82 @@
         ).to(device)
         super().__init__(detector, descriptor)
 
 
 class GFTTAffNetHardNet(LocalFeature):
     """Convenience module, which implements GFTT detector + AffNet-HardNet descriptor."""
 
-    def __init__(self, num_features: int = 8000, upright: bool = False, device: Device = torch.device('cpu')):
-        detector = ScaleSpaceDetector(
+    def __init__(
+        self,
+        num_features: int = 8000,
+        upright: bool = False,
+        device: Device = torch.device('cpu'),
+        config: Detector_config = get_default_detector_config(),
+    ) -> None:
+        detector = MultiResolutionDetector(
+            CornerGFTT(),
             num_features,
-            resp_module=CornerGFTT(),
-            nms_module=ConvQuadInterp3d(10, 1e-5),
-            scale_pyr_module=ScalePyramid(3, 1.6, 32, double_image=False),
+            config,
+            ori_module=PassLAF() if upright else LAFOrienter(19),
+            aff_module=LAFAffNetShapeEstimator(True).eval(),
+        ).to(device)
+        descriptor = LAFDescriptor(None, patch_size=32, grayscale_descriptor=True).to(device)
+        super().__init__(detector, descriptor)
+
+
+class HesAffNetHardNet(LocalFeature):
+    """Convenience module, which implements GFTT detector + AffNet-HardNet descriptor."""
+
+    def __init__(
+        self,
+        num_features: int = 2048,
+        upright: bool = False,
+        device: Device = torch.device('cpu'),
+        config: Detector_config = get_default_detector_config(),
+    ) -> None:
+        detector = MultiResolutionDetector(
+            BlobHessian(),
+            num_features,
+            config,
             ori_module=PassLAF() if upright else LAFOrienter(19),
             aff_module=LAFAffNetShapeEstimator(True).eval(),
-            mr_size=6.0,
         ).to(device)
         descriptor = LAFDescriptor(None, patch_size=32, grayscale_descriptor=True).to(device)
         super().__init__(detector, descriptor)
 
 
 class KeyNetHardNet(LocalFeature):
     """Convenience module, which implements KeyNet detector + HardNet descriptor."""
 
     def __init__(
         self,
         num_features: int = 8000,
         upright: bool = False,
         device: Device = torch.device('cpu'),
         scale_laf: float = 1.0,
-    ):
+    ) -> None:
         ori_module = PassLAF() if upright else LAFOrienter(angle_detector=OriNet(True))
         detector = KeyNetDetector(True, num_features=num_features, ori_module=ori_module).to(device)
         descriptor = LAFDescriptor(None, patch_size=32, grayscale_descriptor=True).to(device)
         super().__init__(detector, descriptor, scale_laf)
 
 
 class KeyNetAffNetHardNet(LocalFeature):
-    """Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor."""
+    """Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.
+
+    .. image:: _static/img/keynet_affnet.jpg
+    """
 
     def __init__(
         self,
         num_features: int = 8000,
         upright: bool = False,
         device: Device = torch.device('cpu'),
         scale_laf: float = 1.0,
-    ):
+    ) -> None:
         ori_module = PassLAF() if upright else LAFOrienter(angle_detector=OriNet(True))
         detector = KeyNetDetector(
             True, num_features=num_features, ori_module=ori_module, aff_module=LAFAffNetShapeEstimator(True).eval()
         ).to(device)
         descriptor = LAFDescriptor(None, patch_size=32, grayscale_descriptor=True).to(device)
         super().__init__(detector, descriptor, scale_laf)
 
@@ -332,7 +391,81 @@
             'keypoints0': concatenate(out_keypoints0, dim=0).view(-1, 2),
             'keypoints1': concatenate(out_keypoints1, dim=0).view(-1, 2),
             'lafs0': concatenate(out_lafs0, dim=0).view(1, -1, 2, 3),
             'lafs1': concatenate(out_lafs1, dim=0).view(1, -1, 2, 3),
             'confidence': concatenate(out_confidence, dim=0).view(-1),
             'batch_indexes': concatenate(out_batch_indexes, dim=0).view(-1),
         }
+
+
+class LightGlueMatcher(GeometryAwareDescriptorMatcher):
+    """LightGlue-based matcher in kornia API.
+
+    This is based on the original code from paper "LightGlue: Local Feature Matching at Light Speed".
+    See :cite:`LightGlue2023` for more details.
+
+    Args:
+        feature_name: type of feature for matching, can be `disk` or `superpoint`.
+        params: LightGlue params.
+    """
+
+    known_modes: ClassVar[List[str]] = ['superpoint', 'disk']
+
+    def __init__(self, feature_name: str = 'disk', params: Dict = {}) -> None:  # type: ignore
+        feature_name_: str = feature_name.lower()
+        super().__init__(feature_name_)
+        self.feature_name = feature_name_
+        self.params = params
+        self.matcher = LightGlue(self.feature_name, **params)
+
+    def forward(
+        self,
+        desc1: Tensor,
+        desc2: Tensor,
+        lafs1: Tensor,
+        lafs2: Tensor,
+        hw1: Optional[Tuple[int, int]] = None,
+        hw2: Optional[Tuple[int, int]] = None,
+    ) -> Tuple[Tensor, Tensor]:
+        """
+        Args:
+            desc1: Batch of descriptors of a shape :math:`(B1, D)`.
+            desc2: Batch of descriptors of a shape :math:`(B2, D)`.
+            lafs1: LAFs of a shape :math:`(1, B1, 2, 3)`.
+            lafs2: LAFs of a shape :math:`(1, B1, 2, 3)`.
+
+        Return:
+            - Descriptor distance of matching descriptors, shape of :math:`(B3, 1)`.
+            - Long tensor indexes of matching descriptors in desc1 and desc2,
+                shape of :math:`(B3, 2)` where :math:`0 <= B3 <= B1`.
+        """
+        if (desc1.shape[0] < 2) or (desc2.shape[0] < 2):
+            return _no_match(desc1)
+        keypoints1 = get_laf_center(lafs1)
+        keypoints2 = get_laf_center(lafs2)
+
+        dev = lafs1.device
+        if hw1 is None:
+            hw1_ = keypoints1.max(dim=1)[0].squeeze().flip(0)
+        else:
+            hw1_ = torch.tensor(hw1, device=dev)
+        if hw2 is None:
+            hw2_ = keypoints2.max(dim=1)[0].squeeze().flip(0)
+        else:
+            hw2_ = torch.tensor(hw2, device=dev)
+        input_dict = {
+            "image0": {
+                "keypoints": keypoints1,
+                "descriptors": desc1[None],
+                "image_size": hw1_.flip(0).reshape(-1, 2).to(dev),
+            },
+            "image1": {
+                "keypoints": keypoints2,
+                "descriptors": desc2[None],
+                "image_size": hw2_.flip(0).reshape(-1, 2).to(dev),
+            },
+        }
+        pred = self.matcher(input_dict)
+        matches0, mscores0 = pred['matches0'], pred['matching_scores0']
+        valid = matches0 > -1
+        matches = torch.stack([torch.where(valid)[1], matches0[valid]], -1)
+        return mscores0[valid].reshape(-1, 1), matches
```

### Comparing `kornia-0.6.9/kornia/feature/laf.py` & `kornia-0.7.0/kornia/feature/laf.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,120 +1,123 @@
 import math
-from typing import Optional, Union
+from typing import List, Optional, Tuple, Union
 
 import torch
 import torch.nn.functional as F
 
 from kornia.core import Tensor, concatenate, stack, tensor, zeros
+from kornia.core.check import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 from kornia.geometry.conversions import angle_to_rotation_matrix, convert_points_from_homogeneous, rad2deg
 from kornia.geometry.linalg import transform_points
 from kornia.geometry.transform import pyrdown
-from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 
 
 def get_laf_scale(LAF: Tensor) -> Tensor:
     """Return a scale of the LAFs.
 
     Args:
-        LAF: tensor [BxNx2x3] or [BxNx2x2].
+        LAF: :math:`(B, N, 2, 3)`
 
     Returns:
-        tensor  BxNx1x1.
-
-    Shape:
-        - Input: :math: `(B, N, 2, 3)`
-        - Output: :math: `(B, N, 1, 1)`
+        scale :math:`(B, N, 1, 1)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> output = get_laf_scale(input)  # BxNx1x1
     """
     KORNIA_CHECK_LAF(LAF)
     eps = 1e-10
     out = LAF[..., 0:1, 0:1] * LAF[..., 1:2, 1:2] - LAF[..., 1:2, 0:1] * LAF[..., 0:1, 1:2] + eps
     return out.abs().sqrt()
 
 
 def get_laf_center(LAF: Tensor) -> Tensor:
-    """Return a center (keypoint) of the LAFs.
+    """Return a center (keypoint) of the LAFs. The convention is that center of 5-pixel image (coordinates from 0
+    to 4) is 2, and not 2.5.
 
     Args:
-        LAF: tensor [BxNx2x3].
+        LAF: :math:`(B, N, 2, 3)`
 
     Returns:
-        tensor  BxNx2.
-
-    Shape:
-        - Input: :math: `(B, N, 2, 3)`
-        - Output: :math: `(B, N, 2)`
+        xy :math:`(B, N, 2)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> output = get_laf_center(input)  # BxNx2
     """
     KORNIA_CHECK_LAF(LAF)
     out = LAF[..., 2]
     return out
 
 
 def get_laf_orientation(LAF: Tensor) -> Tensor:
     """Return orientation of the LAFs, in degrees.
 
     Args:
-        LAF: (Tensor): tensor [BxNx2x3].
+        LAF: :math:`(B, N, 2, 3)`
 
     Returns:
-        Tensor: tensor  BxNx1 .
-
-    Shape:
-        - Input: :math: `(B, N, 2, 3)`
-        - Output: :math: `(B, N, 1)`
+        angle in degrees :math:`(B, N, 1)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> output = get_laf_orientation(input)  # BxNx1
     """
     KORNIA_CHECK_LAF(LAF)
     angle_rad = torch.atan2(LAF[..., 0, 1], LAF[..., 0, 0])
     return rad2deg(angle_rad).unsqueeze(-1)
 
 
+def rotate_laf(LAF: Tensor, angles_degrees: Tensor) -> Tensor:
+    """Apply additional rotation to the the LAFs. Compared to `set_laf_orientation`, the resulting rotation is
+    original LAF orientation plus angles_degrees.
+
+    Args:
+        LAF: :math:`(B, N, 2, 3)`
+        angles: :math:`(B, N, 1)` in degrees.
+
+    Returns:
+        LAF oriented with angles :math:`(B, N, 2, 3)`
+    """
+    KORNIA_CHECK_LAF(LAF)
+    B, N = LAF.shape[:2]
+    rotmat = angle_to_rotation_matrix(angles_degrees).view(B * N, 2, 2)
+    out_laf = LAF.clone()
+    out_laf[:, :, :2, :2] = torch.bmm(LAF[:, :, :2, :2].reshape(B * N, 2, 2), rotmat).reshape(B, N, 2, 2)
+    return out_laf
+
+
 def set_laf_orientation(LAF: Tensor, angles_degrees: Tensor) -> Tensor:
     """Change the orientation of the LAFs.
 
     Args:
-        LAF: tensor [BxNx2x3].
-        angles: tensor BxNx1, in degrees.
+        LAF: :math:`(B, N, 2, 3)`
+        angles: :math:`(B, N, 1)` in degrees.
 
     Returns:
-        tensor [BxNx2x3].
-
-    Shape:
-        - Input: :math: `(B, N, 2, 3)`, `(B, N, 1)`
-        - Output: :math: `(B, N, 2, 3)`
+        LAF oriented with angles :math:`(B, N, 2, 3)`
     """
     KORNIA_CHECK_LAF(LAF)
     B, N = LAF.shape[:2]
-    rotmat = angle_to_rotation_matrix(angles_degrees).view(B * N, 2, 2)
-    laf_out = concatenate(
-        [torch.bmm(make_upright(LAF).view(B * N, 2, 3)[:, :2, :2], rotmat), LAF.view(B * N, 2, 3)[:, :2, 2:]], dim=2
-    ).view(B, N, 2, 3)
-    return laf_out
+    ori = get_laf_orientation(LAF).reshape_as(angles_degrees)
+    return rotate_laf(LAF, angles_degrees - ori)
 
 
 def laf_from_center_scale_ori(xy: Tensor, scale: Optional[Tensor] = None, ori: Optional[Tensor] = None) -> Tensor:
-    """Return orientation of the LAFs, in radians. Useful to create kornia LAFs from OpenCV keypoints.
+    """Creates a LAF from keypoint center, scale and orientation.
+
+    Useful to create kornia LAFs from OpenCV keypoints.
 
     Args:
-        xy: tensor [BxNx2].
-        scale: tensor [BxNx1x1]. If not provided, scale = 1 is assumed
-        ori: tensor [BxNx1]. If not provided orientation = 0 is assumed
+        xy: :math:`(B, N, 2)`.
+        scale: :math:`(B, N, 1, 1)`. If not provided, scale = 1.0 is assumed
+        angle in degrees: :math:`(B, N, 1)`. If not provided orientation = 0 is assumed
 
     Returns:
-        tensor BxNx2x3.
+        LAF :math:`(B, N, 2, 3)`
     """
     KORNIA_CHECK_SHAPE(xy, ["B", "N", "2"])
     device = xy.device
     dtype = xy.dtype
     B, N = xy.shape[:2]
     if scale is None:
         scale = torch.ones(B, N, 1, 1, device=device, dtype=dtype)
@@ -129,24 +132,19 @@
 
 def scale_laf(laf: Tensor, scale_coef: Union[float, Tensor]) -> Tensor:
     """Multiplies region part of LAF ([:, :, :2, :2]) by a scale_coefficient.
 
     So the center, shape and orientation of the local feature stays the same, but the region area changes.
 
     Args:
-        laf: tensor [BxNx2x3] or [BxNx2x2].
+        LAF :math:`(B, N, 2, 3)`
         scale_coef: broadcastable tensor or float.
 
     Returns:
-        tensor BxNx2x3.
-
-    Shape:
-        - Input: :math:`(B, N, 2, 3)`
-        - Input: :math:`(B, N,)` or ()
-        - Output: :math:`(B, N, 1, 1)`
+        LAF :math:`(B, N, 2, 3)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> scale = 0.5
         >>> output = scale_laf(input, scale)  # BxNx2x3
     """
     if (type(scale_coef) is not float) and (type(scale_coef) is not Tensor):
@@ -156,23 +154,19 @@
     return concatenate([scale_coef * centerless_laf, laf[:, :, :, 2:]], dim=3)
 
 
 def make_upright(laf: Tensor, eps: float = 1e-9) -> Tensor:
     """Rectify the affine matrix, so that it becomes upright.
 
     Args:
-        laf: tensor of LAFs.
-        eps : for safe division.
+        laf: :math:`(B, N, 2, 3)`
+        eps: for safe division.
 
     Returns:
-        tensor of same shape.
-
-    Shape:
-        - Input: :math:`(B, N, 2, 3)`
-        - Output:  :math:`(B, N, 2, 3)`
+        laf: :math:`(B, N, 2, 3)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> output = make_upright(input)  #  BxNx2x3
     """
     KORNIA_CHECK_LAF(laf)
     det = get_laf_scale(laf)
@@ -197,33 +191,25 @@
 
     Ellipse (a, b, c) and upright covariance matrix [a11 a12; 0 a22] are connected
     by inverse matrix square root: A = invsqrt([a b; b c]).
 
     See also https://github.com/vlfeat/vlfeat/blob/master/toolbox/sift/vl_frame2oell.m
 
     Args:
-        ells: tensor of ellipses in Oxford format [x y a b c].
+        ells: tensor :math:`(B, N, 5)` of ellipses in Oxford format [x y a b c].
 
     Returns:
-        tensor of ellipses in LAF format.
-
-    Shape:
-        - Input: :math:`(B, N, 5)`
-        - Output:  :math:`(B, N, 2, 3)`
+        LAF :math:`(B, N, 2, 3)`
 
     Example:
         >>> input = torch.ones(1, 10, 5)  # BxNx5
         >>> output = ellipse_to_laf(input)  #  BxNx2x3
     """
-    n_dims = len(ells.size())
-    if n_dims != 3:
-        raise TypeError("ellipse shape should be must be [BxNx5]. " "Got {}".format(ells.size()))
-    B, N, dim = ells.size()
-    if dim != 5:
-        raise TypeError("ellipse shape should be must be [BxNx5]. " "Got {}".format(ells.size()))
+    KORNIA_CHECK_SHAPE(ells, ["B", "N", "5"])
+    B, N, _ = ells.shape
     # Previous implementation was incorrectly using Cholesky decomp as matrix sqrt
     # ell_shape = concatenate([concatenate([ells[..., 2:3], ells[..., 3:4]], dim=2).unsqueeze(2),
     #                       concatenate([ells[..., 3:4], ells[..., 4:5]], dim=2).unsqueeze(2)], dim=2).view(-1, 2, 2)
     # out = torch.matrix_power(torch.cholesky(ell_shape, False), -1).view(B, N, 2, 2)
 
     # We will calculate 2x2 matrix square root via special case formula
     # https://en.wikipedia.org/wiki/Square_root_of_a_matrix
@@ -243,23 +229,19 @@
 
 def laf_to_boundary_points(LAF: Tensor, n_pts: int = 50) -> Tensor:
     """Convert LAFs to boundary points of the regions + center.
 
     Used for local features visualization, see visualize_laf function.
 
     Args:
-        LAF:
+        LAF: :math:`(B, N, 2, 3)`
         n_pts: number of points to output.
 
     Returns:
-        tensor of boundary points.
-
-    Shape:
-        - Input: :math:`(B, N, 2, 3)`
-        - Output:  :math:`(B, N, n_pts, 2)`
+        tensor of boundary points LAF: :math:`(B, N, n_pts, 2)`
     """
     KORNIA_CHECK_LAF(LAF)
     B, N, _, _ = LAF.size()
     pts = concatenate(
         [
             torch.sin(torch.linspace(0, 2 * math.pi, n_pts - 1)).unsqueeze(-1),
             torch.cos(torch.linspace(0, 2 * math.pi, n_pts - 1)).unsqueeze(-1),
@@ -272,142 +254,132 @@
     pts = pts.to(LAF.device).to(LAF.dtype)
     aux = tensor([0.0, 0.0, 1.0]).view(1, 1, 3).expand(B * N, 1, 3)
     HLAF = concatenate([LAF.view(-1, 2, 3), aux.to(LAF.device).to(LAF.dtype)], dim=1)
     pts_h = torch.bmm(HLAF, pts.permute(0, 2, 1)).permute(0, 2, 1)
     return convert_points_from_homogeneous(pts_h.view(B, N, n_pts, 3))
 
 
-def get_laf_pts_to_draw(LAF: Tensor, img_idx: int = 0):
-    """Return numpy array for drawing LAFs (local features).
+def get_laf_pts_to_draw(LAF: Tensor, img_idx: int = 0) -> Tuple[List[int], List[int]]:
+    """Returns list for drawing LAFs (local features).
 
     Args:
-        LAF:
-        n_pts: number of boundary points to output.
+        LAF: :math:`(B, N, 2, 3)`
+        n_pts: number of points to output.
 
     Returns:
-        tensor of boundary points.
-
-    Shape:
-        - Input: :math:`(B, N, 2, 3)`
-        - Output:  :math:`(B, N, n_pts, 2)`
+        List of boundary points x, y`
 
     Examples:
         x, y = get_laf_pts_to_draw(LAF, img_idx)
         plt.figure()
         plt.imshow(kornia.utils.tensor_to_image(img[img_idx]))
         plt.plot(x, y, 'r')
         plt.show()
     """
     # TODO: Refactor doctest
     KORNIA_CHECK_LAF(LAF)
     pts = laf_to_boundary_points(LAF[img_idx : img_idx + 1])[0]
-    pts_np = pts.detach().permute(1, 0, 2).cpu().numpy()
-    return (pts_np[..., 0], pts_np[..., 1])
+    pts_np = pts.detach().permute(1, 0, 2).cpu()
+    return (pts_np[..., 0].tolist(), pts_np[..., 1].tolist())
 
 
 def denormalize_laf(LAF: Tensor, images: Tensor) -> Tensor:
-    """De-normalize LAFs from scale to image scale.
+    """De-normalize LAFs from scale to image scale. The convention is that center of 5-pixel image (coordinates
+    from 0 to 4) is 2, and not 2.5.
 
         B,N,H,W = images.size()
-        MIN_SIZE = min(H,W)
+        MIN_SIZE = min(H - 1, W -1)
         [a11 a21 x]
         [a21 a22 y]
         becomes
-        [a11*MIN_SIZE a21*MIN_SIZE x*W]
-        [a21*MIN_SIZE a22*MIN_SIZE y*H]
+        [a11*MIN_SIZE a21*MIN_SIZE x*(W-1)]
+        [a21*MIN_SIZE a22*MIN_SIZE y*(W-1)]
 
     Args:
-        LAF:
-        images: images, LAFs are detected in.
+        LAF: :math:`(B, N, 2, 3)`
+        images: :math:`(B, CH, H, W)`
 
     Returns:
-        the denormalized lafs.
-
-    Shape:
-        - Input: :math:`(B, N, 2, 3)`
-        - Output:  :math:`(B, N, 2, 3)`
+        the denormalized LAF: :math:`(B, N, 2, 3)`, scale in pixels
     """
     KORNIA_CHECK_LAF(LAF)
     _, _, h, w = images.size()
-    wf = float(w)
-    hf = float(h)
+    wf = float(w - 1)
+    hf = float(h - 1)
     min_size = min(hf, wf)
-    coef = torch.ones(1, 1, 2, 3).to(LAF.dtype).to(LAF.device) * min_size
+    coef = torch.ones(1, 1, 2, 3, dtype=LAF.dtype, device=LAF.device) * min_size
     coef[0, 0, 0, 2] = wf
     coef[0, 0, 1, 2] = hf
     return coef.expand_as(LAF) * LAF
 
 
 def normalize_laf(LAF: Tensor, images: Tensor) -> Tensor:
     """Normalize LAFs to [0,1] scale from pixel scale. See below:
+
         B,N,H,W = images.size()
-        MIN_SIZE = min(H,W)
+        MIN_SIZE =  min(H - 1, W -1)
         [a11 a21 x]
         [a21 a22 y]
         becomes:
-        [a11/MIN_SIZE a21/MIN_SIZE x/W]
-        [a21/MIN_SIZE a22/MIN_SIZE y/H]
+        [a11/MIN_SIZE a21/MIN_SIZE x/(W-1)]
+        [a21/MIN_SIZE a22/MIN_SIZE y/(H-1)]
 
     Args:
-        LAF: (Tensor).
-        images: (Tensor) images, LAFs are detected in
+        LAF: :math:`(B, N, 2, 3)`
+        images: :math:`(B, CH, H, W)`
 
     Returns:
-        LAF: (Tensor).
-
-    Shape:
-        - Input: :math:`(B, N, 2, 3)`
-        - Output:  :math:`(B, N, 2, 3)`
+        the denormalized LAF: :math:`(B, N, 2, 3)`, scale in image percentage (0, 1)
     """
     KORNIA_CHECK_LAF(LAF)
     _, _, h, w = images.size()
-    wf = float(w)
-    hf = float(h)
+    wf = float(w - 1)
+    hf = float(h - 1)
     min_size = min(hf, wf)
-    coef = torch.ones(1, 1, 2, 3).to(LAF.dtype).to(LAF.device) / min_size
+    coef = torch.ones(1, 1, 2, 3, dtype=LAF.dtype, device=LAF.device) / min_size
     coef[0, 0, 0, 2] = 1.0 / wf
     coef[0, 0, 1, 2] = 1.0 / hf
     return coef.expand_as(LAF) * LAF
 
 
 def generate_patch_grid_from_normalized_LAF(img: Tensor, LAF: Tensor, PS: int = 32) -> Tensor:
     """Helper function for affine grid generation.
 
     Args:
         img: image tensor of shape :math:`(B, CH, H, W)`.
         LAF: laf with shape :math:`(B, N, 2, 3)`.
         PS: patch size to be extracted.
 
     Returns:
-        grid
+        grid :math:`(B*N, PS, PS, 2)`
     """
     KORNIA_CHECK_LAF(LAF)
     B, N, _, _ = LAF.size()
     _, ch, h, w = img.size()
 
     # norm, then renorm is needed for allowing detection on one resolution
     # and extraction at arbitrary other
     LAF_renorm = denormalize_laf(LAF, img)
 
     grid = F.affine_grid(LAF_renorm.view(B * N, 2, 3), [B * N, ch, PS, PS], align_corners=False)
-    grid[..., :, 0] = 2.0 * grid[..., :, 0].clone() / float(w) - 1.0
-    grid[..., :, 1] = 2.0 * grid[..., :, 1].clone() / float(h) - 1.0
+    grid[..., :, 0] = 2.0 * grid[..., :, 0].clone() / float(w - 1) - 1.0
+    grid[..., :, 1] = 2.0 * grid[..., :, 1].clone() / float(h - 1) - 1.0
     return grid
 
 
 def extract_patches_simple(
     img: Tensor, laf: Tensor, PS: int = 32, normalize_lafs_before_extraction: bool = True
 ) -> Tensor:
     """Extract patches defined by LAFs from image tensor.
 
     No smoothing applied, huge aliasing (better use extract_patches_from_pyramid).
 
     Args:
-        img: images, LAFs are detected in.
-        laf:
+        img: images, LAFs are detected in  :math:`(B, CH, H, W)`.
+        laf: :math:`(B, N, 2, 3)`.
         PS: patch size.
         normalize_lafs_before_extraction: if True, lafs are normalized to image size.
 
     Returns:
         patches with shape :math:`(B, N, CH, PS,PS)`.
     """
     KORNIA_CHECK_LAF(laf)
@@ -433,47 +405,52 @@
     img: Tensor, laf: Tensor, PS: int = 32, normalize_lafs_before_extraction: bool = True
 ) -> Tensor:
     """Extract patches defined by LAFs from image tensor.
 
     Patches are extracted from appropriate pyramid level.
 
     Args:
-        laf:
-        images: images, LAFs are detected in.
+        img: images, LAFs are detected in  :math:`(B, CH, H, W)`.
+        laf: :math:`(B, N, 2, 3)`.
         PS: patch size.
         normalize_lafs_before_extraction: if True, lafs are normalized to image size.
 
     Returns:
         patches with shape :math:`(B, N, CH, PS,PS)`.
     """
     KORNIA_CHECK_LAF(laf)
     if normalize_lafs_before_extraction:
         nlaf = normalize_laf(laf, img)
     else:
         nlaf = laf
     B, N, _, _ = laf.size()
     _, ch, h, w = img.size()
     scale = 2.0 * get_laf_scale(denormalize_laf(nlaf, img)) / float(PS)
-    pyr_idx = scale.log2().relu().long()
+    max_level = min(img.size(2), img.size(3)) // PS
+    pyr_idx = scale.log2().clamp(min=0.0, max=max(0, max_level - 1)).long()
     cur_img = img
     cur_pyr_level = 0
-    out = zeros(B, N, ch, PS, PS).to(nlaf.dtype).to(nlaf.device)
-    while min(cur_img.size(2), cur_img.size(3)) >= PS:
+    out = torch.zeros(B, N, ch, PS, PS).to(nlaf.dtype).to(nlaf.device)
+    we_are_in_business = True
+    while we_are_in_business:
         _, ch, h, w = cur_img.size()
         # for loop temporarily, to be refactored
         for i in range(B):
             scale_mask = (pyr_idx[i] == cur_pyr_level).squeeze()
-            if (scale_mask.float().sum()) == 0:
+            if (scale_mask.float().sum().item()) == 0:
                 continue
             scale_mask = (scale_mask > 0).view(-1)
             grid = generate_patch_grid_from_normalized_LAF(cur_img[i : i + 1], nlaf[i : i + 1, scale_mask, :, :], PS)
             patches = F.grid_sample(
-                cur_img[i : i + 1].expand(grid.size(0), ch, h, w), grid, padding_mode="border", align_corners=False
+                cur_img[i : i + 1].expand(grid.shape[0], ch, h, w), grid, padding_mode="border", align_corners=False
             )
             out[i].masked_scatter_(scale_mask.view(-1, 1, 1, 1), patches)
+        we_are_in_business = min(cur_img.size(2), cur_img.size(3)) >= PS
+        if not we_are_in_business:
+            break
         cur_img = pyrdown(cur_img)
         cur_pyr_level += 1
     return out
 
 
 def laf_is_inside_image(laf: Tensor, images: Tensor, border: int = 0) -> Tensor:
     """Check if the LAF is touching or partly outside the image boundary.
@@ -494,30 +471,30 @@
     good_lafs_mask = (
         (pts[..., 0] >= border) * (pts[..., 0] <= w - border) * (pts[..., 1] >= border) * (pts[..., 1] <= h - border)
     )
     good_lafs_mask = good_lafs_mask.min(dim=2)[0]
     return good_lafs_mask
 
 
-def laf_to_three_points(laf: Tensor):
+def laf_to_three_points(laf: Tensor) -> Tensor:
     """Convert local affine frame(LAF) to alternative representation: coordinates of LAF center, LAF-x unit vector,
     LAF-y unit vector.
 
     Args:
         laf:  :math:`(B, N, 2, 3)`.
 
     Returns:
         threepts :math:`(B, N, 2, 3)`.
     """
     KORNIA_CHECK_LAF(laf)
     three_pts = stack([laf[..., 2] + laf[..., 0], laf[..., 2] + laf[..., 1], laf[..., 2]], dim=-1)
     return three_pts
 
 
-def laf_from_three_points(threepts: Tensor):
+def laf_from_three_points(threepts: Tensor) -> Tensor:
     """Convert three points to local affine frame.
 
     Order is (0,0), (0, 1), (1, 0).
 
     Args:
         threepts: :math:`(B, N, 2, 3)`.
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/backbone/resnet_fpn.py` & `kornia-0.7.0/kornia/feature/loftr/backbone/resnet_fpn.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,36 +1,40 @@
-import torch.nn as nn
+from typing import Any, Dict, List, Type, Union
+
 import torch.nn.functional as F
+from torch import nn
+
+from kornia.core import Module, Tensor
 
 
-def conv1x1(in_planes, out_planes, stride=1):
+def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
     """1x1 convolution without padding."""
     return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=False)
 
 
-def conv3x3(in_planes, out_planes, stride=1):
+def conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
     """3x3 convolution with padding."""
     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)
 
 
-class BasicBlock(nn.Module):
-    def __init__(self, in_planes, planes, stride=1):
+class BasicBlock(Module):
+    def __init__(self, in_planes: int, planes: int, stride: int = 1) -> None:
         super().__init__()
         self.conv1 = conv3x3(in_planes, planes, stride)
         self.conv2 = conv3x3(planes, planes)
         self.bn1 = nn.BatchNorm2d(planes)
         self.bn2 = nn.BatchNorm2d(planes)
         self.relu = nn.ReLU(inplace=True)
 
         if stride == 1:
             self.downsample = None
         else:
             self.downsample = nn.Sequential(conv1x1(in_planes, planes, stride=stride), nn.BatchNorm2d(planes))
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Tensor:
         y = x
         y = self.relu(self.bn1(self.conv1(y)))
         y = self.bn2(self.conv2(y))
 
         if self.downsample is not None:
             x = self.downsample(x)
 
@@ -39,15 +43,15 @@
 
 class ResNetFPN_8_2(nn.Module):
     """ResNet+FPN, output resolution are 1/8 and 1/2.
 
     Each block has 2 layers.
     """
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         super().__init__()
         # Config
         block = BasicBlock
         initial_dim = config['initial_dim']
         block_dims = config['block_dims']
 
         # Class Variable
@@ -82,23 +86,23 @@
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
             elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                 nn.init.constant_(m.weight, 1)
                 nn.init.constant_(m.bias, 0)
 
-    def _make_layer(self, block, dim, stride=1):
+    def _make_layer(self, block: Union[Type[BasicBlock], Module], dim: int, stride: int = 1) -> nn.Sequential:
         layer1 = block(self.in_planes, dim, stride=stride)
         layer2 = block(dim, dim, stride=1)
         layers = (layer1, layer2)
 
         self.in_planes = dim
         return nn.Sequential(*layers)
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> List[Tensor]:
         # ResNet Backbone
         x0 = self.relu(self.bn1(self.conv1(x)))
         x1 = self.layer1(x0)  # 1/2
         x2 = self.layer2(x1)  # 1/4
         x3 = self.layer3(x2)  # 1/8
 
         # FPN
@@ -117,15 +121,15 @@
 
 class ResNetFPN_16_4(nn.Module):
     """ResNet+FPN, output resolution are 1/16 and 1/4.
 
     Each block has 2 layers.
     """
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         super().__init__()
         # Config
         block = BasicBlock
         initial_dim = config['initial_dim']
         block_dims = config['block_dims']
 
         # Class Variable
@@ -162,23 +166,23 @@
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
             elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                 nn.init.constant_(m.weight, 1)
                 nn.init.constant_(m.bias, 0)
 
-    def _make_layer(self, block, dim, stride=1):
+    def _make_layer(self, block: Union[Type[BasicBlock], Module], dim: int, stride: int = 1) -> nn.Sequential:
         layer1 = block(self.in_planes, dim, stride=stride)
         layer2 = block(dim, dim, stride=1)
         layers = (layer1, layer2)
 
         self.in_planes = dim
         return nn.Sequential(*layers)
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> List[Tensor]:
         # ResNet Backbone
         x0 = self.relu(self.bn1(self.conv1(x)))
         x1 = self.layer1(x0)  # 1/2
         x2 = self.layer2(x1)  # 1/4
         x3 = self.layer3(x2)  # 1/8
         x4 = self.layer4(x3)  # 1/16
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/loftr.py` & `kornia-0.7.0/kornia/feature/loftr/loftr.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,24 @@
-from typing import Any, Dict, Optional, Union
+from __future__ import annotations
+
+from typing import Any
 
 import torch
 
 from kornia.core import Module, Tensor
 from kornia.geometry import resize
 from kornia.utils.helpers import map_location_to_cpu
 
 from .backbone import build_backbone
 from .loftr_module import FinePreprocess, LocalFeatureTransformer
 from .utils.coarse_matching import CoarseMatching
 from .utils.fine_matching import FineMatching
 from .utils.position_encoding import PositionEncodingSine
 
-urls: Dict[str, str] = {}
+urls: dict[str, str] = {}
 urls["outdoor"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_outdoor.ckpt"
 urls["indoor_new"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_indoor_ds_new.ckpt"
 urls["indoor"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_indoor.ckpt"
 
 # Comments: the config below is the one corresponding to the pretrained models
 # Some do not change there anything, unless you want to retrain it.
 
@@ -70,15 +72,15 @@
         >>> img1 = torch.rand(1, 1, 320, 200)
         >>> img2 = torch.rand(1, 1, 128, 128)
         >>> input = {"image0": img1, "image1": img2}
         >>> loftr = LoFTR('outdoor')
         >>> out = loftr(input)
     """
 
-    def __init__(self, pretrained: Optional[str] = 'outdoor', config: Dict[str, Any] = default_cfg):
+    def __init__(self, pretrained: str | None = 'outdoor', config: dict[str, Any] = default_cfg) -> None:
         super().__init__()
         # Misc
         self.config = config
         if pretrained == 'indoor_new':
             self.config['coarse']['temp_bug_fix'] = True
         # Modules
         self.backbone = build_backbone(config)
@@ -95,15 +97,15 @@
             if pretrained not in urls.keys():
                 raise ValueError(f"pretrained should be None or one of {urls.keys()}")
 
             pretrained_dict = torch.hub.load_state_dict_from_url(urls[pretrained], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict['state_dict'])
         self.eval()
 
-    def forward(self, data: Dict[str, Tensor]) -> Dict[str, Tensor]:
+    def forward(self, data: dict[str, Tensor]) -> dict[str, Tensor]:
         """
         Args:
             data: dictionary containing the input data in the following format:
 
         Keyword Args:
             image0: left image with shape :math:`(N, 1, H1, W1)`.
             image1: right image with shape :math:`(N, 1, H2, W2)`.
@@ -112,17 +114,16 @@
 
         Returns:
             - ``keypoints0``, matching keypoints from image0 :math:`(NC, 2)`.
             - ``keypoints1``, matching keypoints from image1 :math:`(NC, 2)`.
             - ``confidence``, confidence score [0, 1] :math:`(NC)`.
             - ``batch_indexes``, batch indexes for the keypoints and lafs :math:`(NC)`.
         """
-
         # 1. Local Feature CNN
-        _data: Dict[str, Union[Tensor, int, torch.Size]] = {
+        _data: dict[str, Tensor | int | torch.Size] = {
             'bs': data['image0'].size(0),
             'hw0_i': data['image0'].shape[2:],
             'hw1_i': data['image1'].shape[2:],
         }
 
         if _data['hw0_i'] == _data['hw1_i']:  # faster & better BN convergence
             feats_c, feats_f = self.backbone(torch.cat([data['image0'], data['image1']], dim=0))
@@ -166,27 +167,27 @@
         feat_f0_unfold, feat_f1_unfold = self.fine_preprocess(feat_f0, feat_f1, feat_c0, feat_c1, _data)
         if feat_f0_unfold.size(0) != 0:  # at least one coarse level predicted
             feat_f0_unfold, feat_f1_unfold = self.loftr_fine(feat_f0_unfold, feat_f1_unfold)
 
         # 5. match fine-level
         self.fine_matching(feat_f0_unfold, feat_f1_unfold, _data)
 
-        rename_keys: Dict[str, str] = {
+        rename_keys: dict[str, str] = {
             "mkpts0_f": 'keypoints0',
             "mkpts1_f": 'keypoints1',
             "mconf": 'confidence',
             "b_ids": 'batch_indexes',
         }
-        out: Dict[str, Tensor] = {}
+        out: dict[str, Tensor] = {}
         for k, v in rename_keys.items():
             _d = _data[k]
             if isinstance(_d, Tensor):
                 out[v] = _d
             else:
                 raise TypeError(f'Expected Tensor for item `{k}`. Gotcha {type(_d)}')
         return out
 
-    def load_state_dict(self, state_dict, *args, **kwargs):
+    def load_state_dict(self, state_dict: dict[str, Any], *args: Any, **kwargs: Any) -> Any:  # type: ignore[override]
         for k in list(state_dict.keys()):
             if k.startswith('matcher.'):
                 state_dict[k.replace('matcher.', '', 1)] = state_dict.pop(k)
         return super().load_state_dict(state_dict, *args, **kwargs)
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/loftr_module/fine_preprocess.py` & `kornia-0.7.0/kornia/feature/loftr/loftr_module/fine_preprocess.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,18 @@
+from typing import Any, Dict, Tuple
+
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
+
+from kornia.core import Module, Tensor
 
 
-class FinePreprocess(nn.Module):
-    def __init__(self, config):
+class FinePreprocess(Module):
+    def __init__(self, config: Dict[str, Any]) -> None:
         super().__init__()
 
         self.config = config
         self.cat_c_feat = config['fine_concat_coarse_feat']
         self.W = self.config['fine_window_size']
 
         d_model_c = self.config['coarse']['d_model']
@@ -16,20 +20,22 @@
         self.d_model_f = d_model_f
         if self.cat_c_feat:
             self.down_proj = nn.Linear(d_model_c, d_model_f, bias=True)
             self.merge_feat = nn.Linear(2 * d_model_f, d_model_f, bias=True)
 
         self._reset_parameters()
 
-    def _reset_parameters(self):
+    def _reset_parameters(self) -> None:
         for p in self.parameters():
             if p.dim() > 1:
                 nn.init.kaiming_normal_(p, mode="fan_out", nonlinearity="relu")
 
-    def forward(self, feat_f0, feat_f1, feat_c0, feat_c1, data):
+    def forward(
+        self, feat_f0: Tensor, feat_f1: Tensor, feat_c0: Tensor, feat_c1: Tensor, data: Dict[str, Any]
+    ) -> Tuple[Tensor, Tensor]:
         W = self.W
         stride = data['hw0_f'][0] // data['hw0_c'][0]
 
         data.update({'W': W})
         if data['b_ids'].shape[0] == 0:
             feat0 = torch.empty(0, self.W**2, self.d_model_f, device=feat_f0.device)
             feat1 = torch.empty(0, self.W**2, self.d_model_f, device=feat_f0.device)
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/loftr_module/linear_attention.py` & `kornia-0.7.0/kornia/feature/loftr/loftr_module/linear_attention.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,35 +1,37 @@
 """Linear Transformer proposed in "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
 Modified from: https://github.com/idiap/fast-
 transformers/blob/master/fast_transformers/attention/linear_attention.py."""
 
 from typing import Optional
 
 import torch
-from torch.nn import Dropout, Module
+from torch.nn import Dropout
 
+from kornia.core import Module, Tensor
 
-def elu_feature_map(x):
+
+def elu_feature_map(x: Tensor) -> Tensor:
     return torch.nn.functional.elu(x) + 1
 
 
 class LinearAttention(Module):
-    def __init__(self, eps=1e-6):
+    def __init__(self, eps: float = 1e-6) -> None:
         super().__init__()
         self.feature_map = elu_feature_map
         self.eps = eps
 
     def forward(
         self,
-        queries: torch.Tensor,
-        keys: torch.Tensor,
-        values: torch.Tensor,
-        q_mask: Optional[torch.Tensor] = None,
-        kv_mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+        queries: Tensor,
+        keys: Tensor,
+        values: Tensor,
+        q_mask: Optional[Tensor] = None,
+        kv_mask: Optional[Tensor] = None,
+    ) -> Tensor:
         """Multi-Head linear attention proposed in "Transformers are RNNs"
         Args:
             queries: [N, L, H, D]
             keys: [N, S, H, D]
             values: [N, S, H, D]
             q_mask: [N, L]
             kv_mask: [N, S]
@@ -52,35 +54,41 @@
         Z = 1 / (torch.einsum("nlhd,nhd->nlh", Q, K.sum(dim=1)) + self.eps)
         queried_values = torch.einsum("nlhd,nhdv,nlh->nlhv", Q, KV, Z) * v_length
 
         return queried_values.contiguous()
 
 
 class FullAttention(Module):
-    def __init__(self, use_dropout=False, attention_dropout=0.1):
+    def __init__(self, use_dropout: bool = False, attention_dropout: float = 0.1) -> None:
         super().__init__()
         self.use_dropout = use_dropout
         self.dropout = Dropout(attention_dropout)
 
-    def forward(self, queries, keys, values, q_mask=None, kv_mask=None):
+    def forward(
+        self,
+        queries: Tensor,
+        keys: Tensor,
+        values: Tensor,
+        q_mask: Optional[Tensor] = None,
+        kv_mask: Optional[Tensor] = None,
+    ) -> Tensor:
         """Multi-head scaled dot-product attention, a.k.a full attention.
 
         Args:
             queries: [N, L, H, D]
             keys: [N, S, H, D]
             values: [N, S, H, D]
             q_mask: [N, L]
             kv_mask: [N, S]
         Returns:
             queried_values: (N, L, H, D)
         """
-
         # Compute the unnormalized attention and apply the masks
         QK = torch.einsum("nlhd,nshd->nlsh", queries, keys)
-        if kv_mask is not None:
+        if kv_mask is not None and q_mask is not None:
             QK.masked_fill_(~(q_mask[:, :, None, None] * kv_mask[:, None, :, None]), float('-inf'))
 
         # Compute the attention and the weighted average
         softmax_temp = 1.0 / queries.size(3) ** 0.5  # sqrt(D)
         A = torch.softmax(softmax_temp * QK, dim=2)
         if self.use_dropout:
             A = self.dropout(A)
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/loftr_module/transformer.py` & `kornia-0.7.0/kornia/feature/loftr/loftr_module/transformer.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,22 @@
+from __future__ import annotations
+
 import copy
-from typing import Optional
+from typing import Any, Literal
 
 import torch
-import torch.nn as nn
+from torch import nn
+
+from kornia.core import Module, Tensor
 
 from .linear_attention import FullAttention, LinearAttention
 
 
-class LoFTREncoderLayer(nn.Module):
-    def __init__(self, d_model, nhead, attention='linear'):
+class LoFTREncoderLayer(Module):
+    def __init__(self, d_model: int, nhead: int, attention: Literal['linear'] | None = 'linear') -> None:
         super().__init__()
 
         self.dim = d_model // nhead
         self.nhead = nhead
 
         # multi-head attention
         self.q_proj = nn.Linear(d_model, d_model, bias=False)
@@ -27,26 +31,22 @@
         )
 
         # norm and dropout
         self.norm1 = nn.LayerNorm(d_model)
         self.norm2 = nn.LayerNorm(d_model)
 
     def forward(
-        self,
-        x: torch.Tensor,
-        source: torch.Tensor,
-        x_mask: Optional[torch.Tensor] = None,
-        source_mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+        self, x: Tensor, source: Tensor, x_mask: Tensor | None = None, source_mask: Tensor | None = None
+    ) -> Tensor:
         """
         Args:
-            x (torch.Tensor): [N, L, C]
-            source (torch.Tensor): [N, S, C]
-            x_mask (torch.Tensor): [N, L] (optional)
-            source_mask (torch.Tensor): [N, S] (optional)
+            x: [N, L, C]
+            source: [N, S, C]
+            x_mask: [N, L] (optional)
+            source_mask: [N, S] (optional)
         """
         bs = x.size(0)
         query, key, value = x, source, source
 
         # multi-head attention
         query = self.q_proj(query).view(bs, -1, self.nhead, self.dim)  # [N, L, (H, D)]
         key = self.k_proj(key).view(bs, -1, self.nhead, self.dim)  # [N, S, (H, D)]
@@ -58,42 +58,43 @@
         # feed-forward network
         message = self.mlp(torch.cat([x, message], dim=2))
         message = self.norm2(message)
 
         return x + message
 
 
-class LocalFeatureTransformer(nn.Module):
+class LocalFeatureTransformer(Module):
     """A Local Feature Transformer (LoFTR) module."""
 
-    def __init__(self, config):
+    def __init__(self, config: dict[str, Any]) -> None:
         super().__init__()
 
         self.config = config
         self.d_model = config['d_model']
         self.nhead = config['nhead']
         self.layer_names = config['layer_names']
         encoder_layer = LoFTREncoderLayer(config['d_model'], config['nhead'], config['attention'])
         self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(len(self.layer_names))])
         self._reset_parameters()
 
-    def _reset_parameters(self):
+    def _reset_parameters(self) -> None:
         for p in self.parameters():
             if p.dim() > 1:
                 nn.init.xavier_uniform_(p)
 
-    def forward(self, feat0, feat1, mask0=None, mask1=None):
+    def forward(
+        self, feat0: Tensor, feat1: Tensor, mask0: None | Tensor = None, mask1: None | Tensor = None
+    ) -> tuple[Tensor, Tensor]:
         """
         Args:
-            feat0 (torch.Tensor): [N, L, C]
-            feat1 (torch.Tensor): [N, S, C]
-            mask0 (torch.Tensor): [N, L] (optional)
-            mask1 (torch.Tensor): [N, S] (optional)
+            feat0: [N, L, C]
+            feat1: [N, S, C]
+            mask0: [N, L] (optional)
+            mask1: [N, S] (optional)
         """
-
         if self.d_model != feat0.size(2):
             msg = "the feature number of src and transformer must be equal"
             raise ValueError(msg)
 
         for layer, name in zip(self.layers, self.layer_names):
             if name == 'self':
                 feat0 = layer(feat0, feat0, mask0, mask0)
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/utils/coarse_matching.py` & `kornia-0.7.0/kornia/feature/loftr/utils/coarse_matching.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,15 +1,19 @@
+from typing import Any, Dict, Optional, Union
+
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
+
+from kornia.core import Module, Tensor
 
 INF = 1e9
 
 
-def mask_border(m, b: int, v):
+def mask_border(m: Tensor, b: int, v: Union[Tensor, float, int, bool]) -> None:
     """Mask borders with value
     Args:
         m (torch.Tensor): [N, H0, W0, H1, W1]
         b (int)
         v (m.dtype)
     """
     if b <= 0:
@@ -21,15 +25,17 @@
     m[:, :, :, :, :b] = v
     m[:, -b:] = v
     m[:, :, -b:] = v
     m[:, :, :, -b:] = v
     m[:, :, :, :, -b:] = v
 
 
-def mask_border_with_padding(m, bd, v, p_m0, p_m1):
+def mask_border_with_padding(
+    m: Tensor, bd: int, v: Union[Tensor, float, int, bool], p_m0: Tensor, p_m1: Tensor
+) -> None:
     if bd <= 0:
         return
 
     m[:, :bd] = v
     m[:, :, :bd] = v
     m[:, :, :, :bd] = v
     m[:, :, :, :, :bd] = v
@@ -39,28 +45,28 @@
     for b_idx, (h0, w0, h1, w1) in enumerate(zip(h0s, w0s, h1s, w1s)):
         m[b_idx, h0 - bd :] = v
         m[b_idx, :, w0 - bd :] = v
         m[b_idx, :, :, h1 - bd :] = v
         m[b_idx, :, :, :, w1 - bd :] = v
 
 
-def compute_max_candidates(p_m0, p_m1):
+def compute_max_candidates(p_m0: Tensor, p_m1: Tensor) -> Tensor:
     """Compute the max candidates of all pairs within a batch.
 
     Args:
         p_m0, p_m1 (torch.Tensor): padded masks
     """
     h0s, w0s = p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0]
     h1s, w1s = p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0]
     max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])
     return max_cand
 
 
-class CoarseMatching(nn.Module):
-    def __init__(self, config):
+class CoarseMatching(Module):
+    def __init__(self, config: Dict[str, Any]) -> None:
         super().__init__()
         self.config = config
         # general config
         self.thr = config['thr']
         self.border_rm = config['border_rm']
         # -- # for training fine-level LoFTR
         self.train_coarse_percent = config['train_coarse_percent']
@@ -76,17 +82,24 @@
             except ImportError:
                 raise ImportError("download superglue.py first!")
             self.log_optimal_transport = log_optimal_transport
             self.bin_score = nn.Parameter(torch.tensor(config['skh_init_bin_score'], requires_grad=True))
             self.skh_iters = config['skh_iters']
             self.skh_prefilter = config['skh_prefilter']
         else:
-            raise NotImplementedError()
+            raise NotImplementedError
 
-    def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):
+    def forward(
+        self,
+        feat_c0: Tensor,
+        feat_c1: Tensor,
+        data: Dict[str, Tensor],
+        mask_c0: Optional[Tensor] = None,
+        mask_c1: Optional[Tensor] = None,
+    ) -> None:
         """
         Args:
             feat0 (torch.Tensor): [N, L, C]
             feat1 (torch.Tensor): [N, S, C]
             data (dict)
             mask_c0 (torch.Tensor): [N, L] (optional)
             mask_c1 (torch.Tensor): [N, S] (optional)
@@ -100,26 +113,26 @@
                 'mkpts1_c' (torch.Tensor): [M, 2],
                 'mconf' (torch.Tensor): [M]}
             NOTE: M' != M during training.
         """
         _, L, S, _ = feat_c0.size(0), feat_c0.size(1), feat_c1.size(1), feat_c0.size(2)
 
         # normalize
-        feat_c0, feat_c1 = map(lambda feat: feat / feat.shape[-1] ** 0.5, [feat_c0, feat_c1])
+        feat_c0, feat_c1 = (feat / feat.shape[-1] ** 0.5 for feat in [feat_c0, feat_c1])
 
         if self.match_type == 'dual_softmax':
             sim_matrix = torch.einsum("nlc,nsc->nls", feat_c0, feat_c1) / self.temperature
-            if mask_c0 is not None:
+            if mask_c0 is not None and mask_c1 is not None:
                 sim_matrix.masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)
             conf_matrix = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)
 
         elif self.match_type == 'sinkhorn':
             # sinkhorn, dustbin included
             sim_matrix = torch.einsum("nlc,nsc->nls", feat_c0, feat_c1)
-            if mask_c0 is not None:
+            if mask_c0 is not None and mask_c1 is not None:
                 sim_matrix[:, :L, :S].masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)
 
             # build uniform prior & use sinkhorn
             log_assign_matrix = self.log_optimal_transport(sim_matrix, self.bin_score, self.skh_iters)
             assign_matrix = log_assign_matrix.exp()
             conf_matrix = assign_matrix[:, :-1, :-1]
 
@@ -135,15 +148,15 @@
 
         data.update({'conf_matrix': conf_matrix})
 
         # predict coarse matches from conf_matrix
         data.update(**self.get_coarse_match(conf_matrix, data))
 
     @torch.no_grad()
-    def get_coarse_match(self, conf_matrix, data):
+    def get_coarse_match(self, conf_matrix: Tensor, data: Dict[str, Tensor]) -> Dict[str, Tensor]:
         """
         Args:
             conf_matrix (torch.Tensor): [N, L, S]
             data (dict): with keys ['hw0_i', 'hw1_i', 'hw0_c', 'hw1_c']
         Returns:
             coarse_matches (dict): {
                 'b_ids' (torch.Tensor): [M'],
@@ -219,17 +232,17 @@
             gt_pad_indices = torch.randint(
                 len(data['spv_b_ids']),
                 (max(num_matches_train - num_matches_pred, self.train_pad_num_gt_min),),
                 device=_device,
             )
             mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)  # set conf of gt paddings to all zero
 
-            b_ids, i_ids, j_ids, mconf = map(
-                lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0),
-                *zip(
+            b_ids, i_ids, j_ids, mconf = (  # type: ignore
+                torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0)  # type: ignore[has-type]
+                for x, y in zip(
                     [b_ids, data['spv_b_ids']],
                     [i_ids, data['spv_i_ids']],
                     [j_ids, data['spv_j_ids']],
                     [mconf, mconf_gt],
                 )
             )
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/utils/fine_matching.py` & `kornia-0.7.0/kornia/feature/loftr/utils/fine_matching.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,23 +1,26 @@
+from __future__ import annotations
+
 import math
+from typing import Any
 
 import torch
-import torch.nn as nn
 
+from kornia.core import Module, Tensor
 from kornia.geometry.subpix import dsnt
 from kornia.utils.grid import create_meshgrid
 
 
-class FineMatching(nn.Module):
+class FineMatching(Module):
     """FineMatching with s2d paradigm."""
 
-    def __init__(self):
+    def __init__(self) -> None:
         super().__init__()
 
-    def forward(self, feat_f0, feat_f1, data):
+    def forward(self, feat_f0: Tensor, feat_f1: Tensor, data: dict[str, Any]) -> None:
         """
         Args:
             feat0 (torch.Tensor): [M, WW, C]
             feat1 (torch.Tensor): [M, WW, C]
             data (dict)
         Update:
             data (dict):{
@@ -64,15 +67,15 @@
         # for fine-level supervision
         data.update({'expec_f': torch.cat([coords_normalized, std.unsqueeze(1)], -1)})
 
         # compute absolute kpt coords
         self.get_fine_match(coords_normalized, data)
 
     @torch.no_grad()
-    def get_fine_match(self, coords_normed, data):
+    def get_fine_match(self, coords_normed: Tensor, data: dict[str, Any]) -> None:
         W, _, _, scale = self.W, self.WW, self.C, self.scale
 
         # mkpts0_f and mkpts1_f
         mkpts0_f = data['mkpts0_c']
         scale1 = scale * data['scale1'][data['b_ids']] if 'scale0' in data else scale
         mkpts1_f = data['mkpts1_c'] + (coords_normed * (W // 2) * scale1)[: len(data['mconf'])]
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/utils/geometry.py` & `kornia-0.7.0/kornia/feature/loftr/utils/geometry.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,25 +1,31 @@
+from typing import Tuple
+
 import torch
 
+from kornia.core import Tensor
+
 
 @torch.no_grad()
-def warp_kpts(kpts0, depth0, depth1, T_0to1, K0, K1):
+def warp_kpts(
+    kpts0: Tensor, depth0: Tensor, depth1: Tensor, T_0to1: Tensor, K0: Tensor, K1: Tensor
+) -> Tuple[Tensor, Tensor]:
     """Warp kpts0 from I0 to I1 with depth, K and Rt Also check covisibility and depth consistency. Depth is
     consistent if relative error < 0.2 (hard-coded).
 
     Args:
-        kpts0 (torch.Tensor): [N, L, 2] - <x, y>,
-        depth0 (torch.Tensor): [N, H, W],
-        depth1 (torch.Tensor): [N, H, W],
-        T_0to1 (torch.Tensor): [N, 3, 4],
-        K0 (torch.Tensor): [N, 3, 3],
-        K1 (torch.Tensor): [N, 3, 3],
+        kpts0: [N, L, 2] - <x, y>,
+        depth0: [N, H, W],
+        depth1: [N, H, W],
+        T_0to1: [N, 3, 4],
+        K0: [N, 3, 3],
+        K1: [N, 3, 3],
     Returns:
-        calculable_mask (torch.Tensor): [N, L]
-        warped_keypoints0 (torch.Tensor): [N, L, 2] <x0_hat, y1_hat>
+        calculable_mask: [N, L]
+        warped_keypoints0: [N, L, 2] <x0_hat, y1_hat>
     """
     kpts0_long = kpts0.round().long()
 
     # Sample depth, get calculable_mask on depth != 0
     kpts0_depth = torch.stack(
         [depth0[i, kpts0_long[i, :, 1], kpts0_long[i, :, 0]] for i in range(kpts0.shape[0])], dim=0
     )  # (N, L)
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/utils/position_encoding.py` & `kornia-0.7.0/kornia/feature/loftr/utils/position_encoding.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 import math
+from typing import Tuple
 
 import torch
 
 from kornia.core import Module, Tensor, zeros
 
 
 class PositionEncodingSine(Module):
     """This is a sinusoidal position encoding that generalized to 2-dimensional images."""
 
     pe: Tensor
 
-    def __init__(self, d_model, max_shape=(256, 256), temp_bug_fix=True):
+    def __init__(self, d_model: int, max_shape: Tuple[int, int] = (256, 256), temp_bug_fix: bool = True) -> None:
         """
         Args:
             max_shape (tuple): for 1/8 featmap, the max length of 256 corresponds to 2048 pixels
             temp_bug_fix (bool): As noted in this [issue](https://github.com/zju3dv/LoFTR/issues/41),
                 the original implementation of LoFTR includes a bug in the pos-enc impl, which has little impact
                 on the final performance. For now, we keep both impls for backward compatibility.
                 We will remove the buggy impl after re-training all variants of our released models.
@@ -22,15 +23,15 @@
         super().__init__()
         self.d_model = d_model
         self.temp_bug_fix = temp_bug_fix
 
         pe = self._create_position_encoding(max_shape)
         self.register_buffer('pe', pe, persistent=False)  # [1, C, H, W]
 
-    def _create_position_encoding(self, max_shape):
+    def _create_position_encoding(self, max_shape: Tuple[int, int]) -> Tensor:
         """Creates a position encoding from scratch.
 
         For 1/8 feature map (which is standard): If the input image size is H, W (both divisible by 8), the max_shape
         should be (H//8, W//8).
         """
         pe = zeros((self.d_model, *max_shape))
         y_position = torch.ones(max_shape).cumsum(0).float().unsqueeze(0)
@@ -46,23 +47,23 @@
         div_term = div_term[:, None, None]  # [C//4, 1, 1]
         pe[0::4, :, :] = torch.sin(x_position * div_term)
         pe[1::4, :, :] = torch.cos(x_position * div_term)
         pe[2::4, :, :] = torch.sin(y_position * div_term)
         pe[3::4, :, :] = torch.cos(y_position * div_term)
         return pe.unsqueeze(0)
 
-    def update_position_encoding_size(self, max_shape):
+    def update_position_encoding_size(self, max_shape: Tuple[int, int]) -> None:
         """Updates position encoding to new max_shape.
 
         For 1/8 feature map (which is standard): If the input image size is H, W (both divisible by 8), the max_shape
         should be (H//8, W//8).
         """
         self.pe = self._create_position_encoding(max_shape).to(self.pe.device)
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Tensor:
         """
         Args:
             x: [N, C, H, W]
         """
         if x.size(2) > self.pe.size(2) or x.size(3) > self.pe.size(3):
             max_shape = (max(x.size(2), self.pe.size(2)), max(x.size(3), self.pe.size(3)))
             self.update_position_encoding_size(max_shape)
```

### Comparing `kornia-0.6.9/kornia/feature/loftr/utils/supervision.py` & `kornia-0.7.0/kornia/feature/loftr/utils/supervision.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,33 @@
+from __future__ import annotations
+
+from typing import Any
+
 import torch
 
+from kornia.core import Tensor
 from kornia.utils import create_meshgrid
 
 from .geometry import warp_kpts
 
-#  Coarse-Level supervision
-
 
+#  Coarse-Level supervision
 @torch.no_grad()
-def mask_pts_at_padded_regions(grid_pt, mask):
+def mask_pts_at_padded_regions(grid_pt: Tensor, mask: Tensor) -> Tensor:
     """For megadepth dataset, zero-padding exists in images."""
     n, h, w = mask.shape
     mask = mask.reshape(n, h * w).unsqueeze(-1).repeat(1, 1, 2)
     # from einops import repeat
     # mask = repeat(mask, 'n h w -> n (h w) c', c=2)
     grid_pt[~mask.bool()] = 0
     return grid_pt
 
 
 @torch.no_grad()
-def spvs_coarse(data, config):
+def spvs_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:
     """
     Update:
         data (dict): {
             "conf_matrix_gt": [N, hw0, hw1],
             'spv_b_ids': [M]
             'spv_i_ids': [M]
             'spv_j_ids': [M]
@@ -38,15 +42,15 @@
     # 1. misc
     device = data['image0'].device
     N, _, H0, W0 = data['image0'].shape
     _, _, H1, W1 = data['image1'].shape
     scale = config['LOFTR']['RESOLUTION'][0]
     scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale
     scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale
-    h0, w0, h1, w1 = map(lambda x: x // scale, [H0, W0, H1, W1])
+    h0, w0, h1, w1 = (x // scale for x in [H0, W0, H1, W1])
 
     # 2. warp grids
     # create kpts in meshgrid and resize them to image resolution
     grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)  # [N, hw, 2]
     grid_pt0_i = scale0 * grid_pt0_c
     grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)
     grid_pt1_i = scale1 * grid_pt1_c
@@ -67,15 +71,15 @@
     # 3. check if mutual nearest neighbor
     w_pt0_c_round = w_pt0_c[:, :, :].round().long()
     nearest_index1 = w_pt0_c_round[..., 0] + w_pt0_c_round[..., 1] * w1
     w_pt1_c_round = w_pt1_c[:, :, :].round().long()
     nearest_index0 = w_pt1_c_round[..., 0] + w_pt1_c_round[..., 1] * w0
 
     # corner case: out of boundary
-    def out_bound_mask(pt, w, h):
+    def out_bound_mask(pt: Tensor, w: Tensor, h: Tensor) -> Tensor:
         return (pt[..., 0] < 0) + (pt[..., 0] >= w) + (pt[..., 1] < 0) + (pt[..., 1] >= h)
 
     nearest_index1[out_bound_mask(w_pt0_c_round, w1, h1)] = 0
     nearest_index0[out_bound_mask(w_pt1_c_round, w0, h0)] = 0
 
     loop_back = torch.stack([nearest_index0[_b][_i] for _b, _i in enumerate(nearest_index1)], dim=0)
     correct_0to1 = loop_back == torch.arange(h0 * w0, device=device)[None].repeat(N, 1)
@@ -98,29 +102,27 @@
 
     data.update({'spv_b_ids': b_ids, 'spv_i_ids': i_ids, 'spv_j_ids': j_ids})
 
     # 6. save intermediate results (for fast fine-level computation)
     data.update({'spv_w_pt0_i': w_pt0_i, 'spv_pt1_i': grid_pt1_i})
 
 
-def compute_supervision_coarse(data, config):
+def compute_supervision_coarse(data: dict[str, Any], config: dict[str, Any]) -> None:
     if len(set(data['dataset_name'])) != 1:
         raise ValueError("Do not support mixed datasets training!")
     data_source = data['dataset_name'][0]
     if data_source.lower() in ['scannet', 'megadepth']:
         spvs_coarse(data, config)
     else:
         raise ValueError(f'Unknown data source: {data_source}')
 
 
 #  Fine-Level supervision
-
-
 @torch.no_grad()
-def spvs_fine(data, config):
+def spvs_fine(data: dict[str, Any], config: dict[str, Any]) -> None:
     """
     Update:
         data (dict):{
             "expec_f_gt": [M, 2]}
     """
     # 1. misc
     # w_pt0_i, pt1_i = data.pop('spv_w_pt0_i'), data.pop('spv_pt1_i')
@@ -134,13 +136,13 @@
     # 3. compute gt
     scale = scale * data['scale1'][b_ids] if 'scale0' in data else scale
     # `expec_f_gt` might exceed the window, i.e. abs(*) > 1, which would be filtered later
     expec_f_gt = (w_pt0_i[b_ids, i_ids] - pt1_i[b_ids, j_ids]) / scale / radius  # [M, 2]
     data.update({"expec_f_gt": expec_f_gt})
 
 
-def compute_supervision_fine(data, config):
+def compute_supervision_fine(data: dict[str, Any], config: dict[str, Any]) -> None:
     data_source = data['dataset_name'][0]
     if data_source.lower() in ['scannet', 'megadepth']:
         spvs_fine(data, config)
     else:
         raise NotImplementedError
```

### Comparing `kornia-0.6.9/kornia/feature/matching.py` & `kornia-0.7.0/kornia/feature/matching.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,35 +1,36 @@
-from typing import Dict, Optional, Tuple
+from typing import Any, ClassVar, Dict, List, Optional, Tuple
 
 import torch
-import torch.nn as nn
 
+from kornia.core import Module, Tensor, concatenate
+from kornia.core.check import KORNIA_CHECK_DM_DESC, KORNIA_CHECK_SHAPE
 from kornia.feature.laf import get_laf_center
-from kornia.testing import KORNIA_CHECK_DM_DESC, KORNIA_CHECK_SHAPE, Tensor, is_mps_tensor_safe
+from kornia.testing import is_mps_tensor_safe
 
 from .adalam import get_adalam_default_config, match_adalam
 
 
-def _cdist(d1: torch.Tensor, d2: torch.Tensor) -> torch.Tensor:
+def _cdist(d1: Tensor, d2: Tensor) -> Tensor:
     r"""Manual `torch.cdist` for M1."""
     if (not is_mps_tensor_safe(d1)) and (not is_mps_tensor_safe(d2)):
         return torch.cdist(d1, d2)
     d1_sq = (d1**2).sum(dim=1, keepdim=True)
     d2_sq = (d2**2).sum(dim=1, keepdim=True)
     dm = d1_sq.repeat(1, d2.size(0)) + d2_sq.repeat(1, d1.size(0)).t() - 2.0 * d1 @ d2.t()
     dm = dm.clamp(min=0.0).sqrt()
     return dm
 
 
-def _get_default_fginn_params():
+def _get_default_fginn_params() -> Dict[str, Any]:
     config = {"th": 0.85, "mutual": False, "spatial_th": 10.0}
     return config
 
 
-def _get_lazy_distance_matrix(desc1: Tensor, desc2: Tensor, dm_: Optional[Tensor] = None):
+def _get_lazy_distance_matrix(desc1: Tensor, desc2: Tensor, dm_: Optional[Tensor] = None) -> Tensor:
     """Helper function, which checks validity of provided distance matrix, or calculates L2-distance matrix dm is
     not provided.
 
     Args:
         desc1: Batch of descriptors of a shape :math:`(B1, D)`.
         desc2: Batch of descriptors of a shape :math:`(B2, D)`.
         dm: Tensor containing the distances from each descriptor in desc1
@@ -39,15 +40,15 @@
         dm = _cdist(desc1, desc2)
     else:
         KORNIA_CHECK_DM_DESC(desc1, desc2, dm_)
         dm = dm_
     return dm
 
 
-def _no_match(dm: Tensor):
+def _no_match(dm: Tensor) -> Tuple[Tensor, Tensor]:
     """Helper function, which output empty tensors.
 
     Returns:
             - Descriptor distance of matching descriptors, shape of :math:`(0, 1)`.
             - Long tensor indexes of matching descriptors in desc1 and desc2, shape of :math:`(0, 2)`.
     """
     dists = torch.empty(0, 1, device=dm.device, dtype=dm.dtype)
@@ -70,18 +71,18 @@
         - Descriptor distance of matching descriptors, shape of :math:`(B1, 1)`.
         - Long tensor indexes of matching descriptors in desc1 and desc2, shape of :math:`(B1, 2)`.
     """
     KORNIA_CHECK_SHAPE(desc1, ["B", "DIM"])
     KORNIA_CHECK_SHAPE(desc2, ["B", "DIM"])
     if (len(desc1) == 0) or (len(desc2) == 0):
         return _no_match(desc1)
-    distance_matrix: Tensor = _get_lazy_distance_matrix(desc1, desc2, dm)
+    distance_matrix = _get_lazy_distance_matrix(desc1, desc2, dm)
     match_dists, idxs_in_2 = torch.min(distance_matrix, dim=1)
-    idxs_in1: Tensor = torch.arange(0, idxs_in_2.size(0), device=idxs_in_2.device)
-    matches_idxs: Tensor = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)
+    idxs_in1 = torch.arange(0, idxs_in_2.size(0), device=idxs_in_2.device)
+    matches_idxs = concatenate([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], 1)
     return match_dists.view(-1, 1), matches_idxs.view(-1, 2)
 
 
 def match_mnn(desc1: Tensor, desc2: Tensor, dm: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
     """Function, which finds mutual nearest neighbors in desc2 for each vector in desc1.
 
     If the distance matrix dm is not provided, :py:func:`torch.cdist` is used.
@@ -105,19 +106,19 @@
     ms = min(distance_matrix.size(0), distance_matrix.size(1))
     match_dists, idxs_in_2 = torch.min(distance_matrix, dim=1)
     match_dists2, idxs_in_1 = torch.min(distance_matrix, dim=0)
     minsize_idxs = torch.arange(ms, device=distance_matrix.device)
 
     if distance_matrix.size(0) <= distance_matrix.size(1):
         mutual_nns = minsize_idxs == idxs_in_1[idxs_in_2][:ms]
-        matches_idxs = torch.cat([minsize_idxs.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)[mutual_nns]
+        matches_idxs = concatenate([minsize_idxs.view(-1, 1), idxs_in_2.view(-1, 1)], 1)[mutual_nns]
         match_dists = match_dists[mutual_nns]
     else:
         mutual_nns = minsize_idxs == idxs_in_2[idxs_in_1][:ms]
-        matches_idxs = torch.cat([idxs_in_1.view(-1, 1), minsize_idxs.view(-1, 1)], dim=1)[mutual_nns]
+        matches_idxs = concatenate([idxs_in_1.view(-1, 1), minsize_idxs.view(-1, 1)], 1)[mutual_nns]
         match_dists = match_dists2[mutual_nns]
     return match_dists.view(-1, 1), matches_idxs.view(-1, 2)
 
 
 def match_snn(desc1: Tensor, desc2: Tensor, th: float = 0.8, dm: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
     """Function, which finds nearest neighbors in desc2 for each vector in desc1.
 
@@ -147,15 +148,15 @@
     ratio = vals[:, 0] / vals[:, 1]
     mask = ratio <= th
     match_dists = ratio[mask]
     if len(match_dists) == 0:
         return _no_match(distance_matrix)
     idxs_in1 = torch.arange(0, idxs_in_2.size(0), device=distance_matrix.device)[mask]
     idxs_in_2 = idxs_in_2[:, 0][mask]
-    matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)
+    matches_idxs = concatenate([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], 1)
     return match_dists.view(-1, 1), matches_idxs.view(-1, 2)
 
 
 def match_smnn(desc1: Tensor, desc2: Tensor, th: float = 0.95, dm: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
     """Function, which finds mutual nearest neighbors in desc2 for each vector in desc1.
 
     the method satisfies first to second nearest neighbor distance <= th.
@@ -272,40 +273,39 @@
     ratio = vals / vals_2nd
     mask = ratio <= th
     match_dists = ratio[mask]
     if len(match_dists) == 0:
         return _no_match(distance_matrix)
     idxs_in1 = torch.arange(0, idxs_in_2.size(0), device=distance_matrix.device)[mask]
     idxs_in_2 = idxs_in_2[mask]
-    matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)
+    matches_idxs = concatenate([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], 1)
     match_dists, matches_idxs = match_dists.view(-1, 1), matches_idxs.view(-1, 2)
 
     if not mutual:  # returning 1-way matches
         return match_dists, matches_idxs
     _, idxs_in_1_mut = torch.min(distance_matrix, dim=0)
     good_mask = matches_idxs[:, 0] == idxs_in_1_mut[matches_idxs[:, 1]]
     return match_dists[good_mask], matches_idxs[good_mask]
 
 
-class DescriptorMatcher(nn.Module):
+class DescriptorMatcher(Module):
     """Module version of matching functions.
 
     See :func:`~kornia.feature.match_nn`, :func:`~kornia.feature.match_snn`,
         :func:`~kornia.feature.match_mnn` or :func:`~kornia.feature.match_smnn` for more details.
 
     Args:
         match_mode: type of matching, can be `nn`, `snn`, `mnn`, `smnn`.
         th: threshold on distance ratio, or other quality measure.
     """
 
-    known_modes = ['nn', 'mnn', 'snn', 'smnn']
-
     def __init__(self, match_mode: str = 'snn', th: float = 0.8) -> None:
         super().__init__()
         _match_mode: str = match_mode.lower()
+        self.known_modes = ['nn', 'mnn', 'snn', 'smnn']
         if _match_mode not in self.known_modes:
             raise NotImplementedError(f"{match_mode} is not supported. Try one of {self.known_modes}")
         self.match_mode = _match_mode
         self.th = th
 
     def forward(self, desc1: Tensor, desc2: Tensor) -> Tuple[Tensor, Tensor]:
         """
@@ -329,26 +329,26 @@
         elif self.match_mode == 'smnn':
             out = match_smnn(desc1, desc2, self.th)
         else:
             raise NotImplementedError
         return out
 
 
-class GeometryAwareDescriptorMatcher(nn.Module):
+class GeometryAwareDescriptorMatcher(Module):
     """Module version of matching functions.
 
     See :func:`~kornia.feature.match_nn`, :func:`~kornia.feature.match_snn`,
         :func:`~kornia.feature.match_mnn` or :func:`~kornia.feature.match_smnn` for more details.
 
     Args:
         match_mode: type of matching, can be `fginn`.
         th: threshold on distance ratio, or other quality measure.
     """
 
-    known_modes = ['fginn', "adalam"]
+    known_modes: ClassVar[List[str]] = ['fginn', "adalam"]
 
     def __init__(self, match_mode: str = 'fginn', params: Dict[str, Tensor] = {}) -> None:
         super().__init__()
         _match_mode: str = match_mode.lower()
         if _match_mode not in self.known_modes:
             raise NotImplementedError(f"{match_mode} is not supported. Try one of {self.known_modes}")
         self.match_mode = _match_mode
@@ -368,13 +368,13 @@
                 shape of :math:`(B3, 2)` where :math:`0 <= B3 <= B1`.
         """
         if self.match_mode == 'fginn':
             params = _get_default_fginn_params()
             params.update(self.params)
             out = match_fginn(desc1, desc2, lafs1, lafs2, params['th'], params['spatial_th'], params['mutual'])
         elif self.match_mode == 'adalam':
-            params = get_adalam_default_config()
-            params.update(self.params)
-            out = match_adalam(desc1, desc2, lafs1, lafs2, config=params)
+            _params = get_adalam_default_config()
+            _params.update(self.params)  # type: ignore[typeddict-item]
+            out = match_adalam(desc1, desc2, lafs1, lafs2, config=_params)
         else:
             raise NotImplementedError
         return out
```

### Comparing `kornia-0.6.9/kornia/feature/mkd.py` & `kornia-0.7.0/kornia/feature/mkd.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import Any, Dict, List, Tuple, Union
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 
 from kornia.constants import pi
 from kornia.core import Tensor, tensor, zeros
 from kornia.filters import GaussianBlur2d, SpatialGradient
 from kornia.geometry.conversions import cart2pol
 from kornia.utils import create_meshgrid
 from kornia.utils.helpers import map_location_to_cpu
@@ -139,40 +139,32 @@
     def forward(self, x: Tensor) -> Tensor:
         if not isinstance(x, Tensor):
             raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
 
         if not len(x.shape) == 4 or x.shape[1] != 1:
             raise ValueError(f"Invalid input shape, we expect Bx1xHxW. Got: {x.shape}")
 
-        # TODO: unify the two lines below when pytorch 1.6 support is dropped
-        emb0: Tensor = torch.jit.annotate(Tensor, self.emb0)
-        emb0 = emb0.to(x).repeat(x.size(0), 1, 1, 1)
+        if not isinstance(self.emb0, Tensor):
+            raise TypeError(f"Emb0 type is not a Tensor. Got {type(x)}")
+
+        emb0 = self.emb0.to(x).repeat(x.size(0), 1, 1, 1)
         frange = self.frange.to(x) * x
         emb1 = torch.cos(frange)
         emb2 = torch.sin(frange)
         embedding = torch.cat([emb0, emb1, emb2], dim=1)
         embedding = self.weights * embedding
         return embedding
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'patch_size='
-            + str(self.patch_size)
-            + ', '
-            + 'n='
-            + str(self.n)
-            + ', '
-            + 'd='
-            + str(self.d)
-            + ', '
-            + 'coeffs='
-            + str(self.coeffs)
-            + ')'
+            f"{self.__class__.__name__}("
+            f"patch_size={self.patch_size}, "
+            f"n={self.n}, "
+            f"d={self.d}, "
+            f"coeffs={self.coeffs})"
         )
 
 
 class EmbedGradients(nn.Module):
     r"""Module that computes gradient embedding, weighted by sqrt of magnitudes of given patches.
 
     Args:
@@ -221,27 +213,18 @@
         oris = grads[:, 1:, :, :]
         if self.relative:
             oris = oris - self.phi.to(oris)
         y = self.kernel(oris) * self.emb_mags(mags)
         return y
 
     def __repr__(self) -> str:
-        return (
-            self.__class__.__name__
-            + '('
-            + 'patch_size='
-            + str(self.patch_size)
-            + ', '
-            + 'relative='
-            + str(self.relative)
-            + ')'
-        )
+        return f"{self.__class__.__name__}(patch_size={self.patch_size}, relative={self.relative})"
 
 
-def spatial_kernel_embedding(kernel_type, grids: Dict[str, Tensor]) -> Tensor:
+def spatial_kernel_embedding(kernel_type: str, grids: Dict[str, Tensor]) -> Tensor:
     r"""Compute embeddings for cartesian and polar parametrizations."""
     factors = {"phi": 1.0, "rho": pi / sqrt2, "x": pi / 2, "y": pi / 2}
     if kernel_type == 'cart':
         coeffs_ = 'xy'
         params_ = ['x', 'y']
     elif kernel_type == 'polar':
         coeffs_ = 'rhophi'
@@ -359,34 +342,21 @@
         output = output.sum(dim=(2, 3))
         if self.do_l2:
             output = F.normalize(output, dim=1)
         return output
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'kernel_type='
-            + str(self.kernel_type)
-            + ', '
-            + 'fmap_size='
-            + str(self.fmap_size)
-            + ', '
-            + 'in_dims='
-            + str(self.in_dims)
-            + ', '
-            + 'out_dims='
-            + str(self.out_dims)
-            + ', '
-            + 'do_gmask='
-            + str(self.do_gmask)
-            + ', '
-            + 'do_l2='
-            + str(self.do_l2)
-            + ')'
+            f'{self.__class__.__name__}('
+            f'kernel_type={self.kernel_type}, '
+            f'fmap_size={self.fmap_size}, '
+            f'in_dims={self.in_dims}, '
+            f'out_dims={self.out_dims}, '
+            f'do_gmask={self.do_gmask}, '
+            f'do_l2={self.do_l2})'
         )
 
 
 class Whitening(nn.Module):
     r"""Module, performs supervised or unsupervised whitening.
 
     This is based on the paper "Understanding and Improving Kernel Local Descriptors".
@@ -493,25 +463,18 @@
         x = x - self.mean  # Center the data.
         x = x @ self.evecs  # Apply rotation and/or scaling.
         x = torch.sign(x) * torch.pow(torch.abs(x), self.pval)  # Powerlaw.
         return F.normalize(x, dim=1)
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'xform='
-            + str(self.xform)
-            + ', '
-            + 'in_dims='
-            + str(self.in_dims)
-            + ', '
-            + 'output_dims='
-            + str(self.output_dims)
-            + ')'
+            f'{self.__class__.__name__}('
+            f'xform={self.xform}, '
+            f'in_dims={self.in_dims}, '
+            f'output_dims={self.output_dims})'
         )
 
 
 class MKDDescriptor(nn.Module):
     r"""Module that computes Multiple Kernel local descriptors.
 
     This is based on the paper "Understanding and Improving Kernel Local Descriptors".
@@ -616,31 +579,20 @@
         if self.whitening is not None:
             y = self.whitening_layer(y)
 
         return y
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'patch_size='
-            + str(self.patch_size)
-            + ', '
-            + 'kernel_type='
-            + str(self.kernel_type)
-            + ', '
-            + 'whitening='
-            + str(self.whitening)
-            + ', '
-            + 'training_set='
-            + str(self.training_set)
-            + ', '
-            + 'output_dims='
-            + str(self.output_dims)
-            + ')'
+            f'{self.__class__.__name__}('
+            f'patch_size={self.patch_size}, '
+            f'kernel_type={self.kernel_type}, '
+            f'whitening={self.whitening}, '
+            f'training_set={self.training_set}, '
+            f'output_dims={self.output_dims})'
         )
 
 
 def load_whitening_model(kernel_type: str, training_set: str) -> Dict[str, Any]:
     whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=map_location_to_cpu)
     whitening_model = whitening_models[training_set]
     return whitening_model
```

### Comparing `kornia-0.6.9/kornia/feature/orientation.py` & `kornia-0.7.0/kornia/feature/orientation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,100 +1,94 @@
-import math
 from typing import Dict, Optional
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 
 from kornia.constants import pi
-from kornia.filters import SpatialGradient, get_gaussian_kernel2d
+from kornia.core.check import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
+from kornia.filters import SpatialGradient, get_gaussian_discrete_kernel1d, get_gaussian_kernel2d
 from kornia.geometry import rad2deg
-from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 from kornia.utils.helpers import map_location_to_cpu
 
 from .laf import extract_patches_from_pyramid, get_laf_orientation, set_laf_orientation
 
 urls: Dict[str, str] = {}
 urls["orinet"] = "https://github.com/ducha-aiki/affnet/raw/master/pretrained/OriNet.pth"
 
 
 class PassLAF(nn.Module):
     """Dummy module to use instead of local feature orientation or affine shape estimator."""
 
     def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:
         """
         Args:
-            laf: 4d tensor.
-            img: the input image tensor.
+            laf: :math:`(B, N, 2, 3)`
+            img: :math:`(B, 1, H, W)`
 
-        Return:
-            torch.Tensor: unchanged laf from the input."""
+        Returns:
+            LAF, unchanged :math:`(B, N, 2, 3)`
+        """
         return laf
 
 
 class PatchDominantGradientOrientation(nn.Module):
     """Module, which estimates the dominant gradient orientation of the given patches, in radians.
 
     Zero angle points towards right.
 
     Args:
         patch_size: size of the (square) input patch.
         num_angular_bins: number of histogram bins.
         eps: for safe division, and arctan.
     """
 
-    def __init__(self, patch_size: int = 32, num_angular_bins: int = 36, eps: float = 1e-8):
+    def __init__(self, patch_size: int = 32, num_angular_bins: int = 36, eps: float = 1e-8) -> None:
         super().__init__()
         self.patch_size = patch_size
         self.num_ang_bins = num_angular_bins
         self.gradient = SpatialGradient('sobel', 1)
         self.eps = eps
-        self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False, padding_mode="circular")
+        self.angular_smooth = nn.Conv1d(1, 1, kernel_size=5, padding=2, bias=False, padding_mode="circular")
         with torch.no_grad():
-            self.angular_smooth.weight[:] = torch.tensor([[[0.33, 0.34, 0.33]]])
-        sigma: float = float(self.patch_size) / math.sqrt(2.0)
+            self.angular_smooth.weight[:] = get_gaussian_discrete_kernel1d(5, 1.6)
+        sigma: float = float(self.patch_size) / 6.0
         self.weighting = get_gaussian_kernel2d((self.patch_size, self.patch_size), (sigma, sigma), True)
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return (
-            self.__class__.__name__ + '('
-            'patch_size='
-            + str(self.patch_size)
-            + ', '
-            + 'num_ang_bins='
-            + str(self.num_ang_bins)
-            + ', '
-            + 'eps='
-            + str(self.eps)
-            + ')'
+            f'{self.__class__.__name__}('
+            f'patch_size={self.patch_size}, '
+            f'num_ang_bins={self.num_ang_bins}, '
+            f'eps={self.eps})'
         )
 
     def forward(self, patch: torch.Tensor) -> torch.Tensor:
-        """Args:
-            patch: shape [Bx1xHxW]
+        """
+        Args:
+            patch: :math:`(B, 1, H, W)`
+
         Returns:
-            torch.Tensor: angle shape [B]"""
-        if not isinstance(patch, torch.Tensor):
-            raise TypeError(f"Input type is not a torch.Tensor. Got {type(patch)}")
-        if not len(patch.shape) == 4:
-            raise ValueError(f"Invalid input shape, we expect Bx1xHxW. Got: {patch.shape}")
+            angle in radians: :math:`(B)`
+        """
+        KORNIA_CHECK_SHAPE(patch, ["B", "1", "H", "W"])
         _, CH, W, H = patch.size()
         if (W != self.patch_size) or (H != self.patch_size) or (CH != 1):
             raise TypeError(
                 "input shape should be must be [Bx1x{}x{}]. "
                 "Got {}".format(self.patch_size, self.patch_size, patch.size())
             )
         self.weighting = self.weighting.to(patch.dtype).to(patch.device)
         self.angular_smooth = self.angular_smooth.to(patch.dtype).to(patch.device)
         grads: torch.Tensor = self.gradient(patch)
         # unpack the edges
         gx: torch.Tensor = grads[:, :, 0]
         gy: torch.Tensor = grads[:, :, 1]
 
-        mag: torch.Tensor = torch.sqrt(gx * gx + gy * gy + self.eps)
+        mag: torch.Tensor = torch.sqrt(gx * gx + gy * gy + self.eps) * self.weighting
         ori: torch.Tensor = torch.atan2(gy, gx + self.eps) + 2.0 * pi
 
         o_big = float(self.num_ang_bins) * (ori + 1.0 * pi) / (2.0 * pi)
         bo0_big = torch.floor(o_big)
         wo1_big = o_big - bo0_big
         bo0_big = bo0_big % self.num_ang_bins
         bo1_big = (bo0_big + 1) % self.num_ang_bins
@@ -103,17 +97,23 @@
         ang_bins_list = []
         for i in range(0, self.num_ang_bins):
             ang_bins_i = F.adaptive_avg_pool2d(
                 (bo0_big == i).to(patch.dtype) * wo0_big + (bo1_big == i).to(patch.dtype) * wo1_big, (1, 1)
             )
             ang_bins_list.append(ang_bins_i)
         ang_bins = torch.cat(ang_bins_list, 1).view(-1, 1, self.num_ang_bins)
-        ang_bins = self.angular_smooth(ang_bins)
-        values, indices = ang_bins.view(-1, self.num_ang_bins).max(1)
-        angle = -((2.0 * pi * indices.to(patch.dtype) / float(self.num_ang_bins)) - pi)
+        ang_bins = self.angular_smooth(ang_bins).view(-1, self.num_ang_bins)
+        values, indices = ang_bins.max(1)
+        indices_left = (self.num_ang_bins + indices - 1) % self.num_ang_bins
+        indices_right = (indices + 1) % self.num_ang_bins
+        left = torch.gather(ang_bins, 1, indices_left.reshape(-1, 1)).reshape(-1)
+        center = values
+        right = torch.gather(ang_bins, 1, indices_right.reshape(-1, 1)).reshape(-1)
+        c_subpix = 0.5 * (left - right) / (left + right - 2.0 * center)
+        angle = -((2.0 * pi * (indices.to(patch.dtype) + c_subpix) / float(self.num_ang_bins)) - pi)
         return angle
 
 
 class OriNet(nn.Module):
     """Network, which estimates the canonical orientation of the given 32x32 patches, in radians.
 
     Zero angle points towards right. This is based on the original code from paper
@@ -133,15 +133,15 @@
 
     Examples:
         >>> input = torch.rand(16, 1, 32, 32)
         >>> orinet = OriNet()
         >>> angle = orinet(input) # 16
     """
 
-    def __init__(self, pretrained: bool = False, eps: float = 1e-8):
+    def __init__(self, pretrained: bool = False, eps: float = 1e-8) -> None:
         super().__init__()
         self.features = nn.Sequential(
             nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False),
             nn.BatchNorm2d(16, affine=False),
             nn.ReLU(),
             nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),
             nn.BatchNorm2d(16, affine=False),
@@ -176,18 +176,21 @@
         sp, mp = torch.std_mean(x, dim=(-3, -2, -1), keepdim=True)
         # WARNING: we need to .detach() input, otherwise the gradients produced by
         # the patches extractor with F.grid_sample are very noisy, making the detector
         # training totally unstable.
         return (x - mp.detach()) / (sp.detach() + eps)
 
     def forward(self, patch: torch.Tensor) -> torch.Tensor:
-        """Args:
-            patch: (torch.Tensor) shape [Bx1xHxW]
+        """
+        Args:
+            patch: :math:`(B, 1, H, W)`
+
         Returns:
-            patch: (torch.Tensor) shape [B]"""
+            angle in radians: :math:`(B)`
+        """
         xy = self.features(self._normalize_input(patch)).view(-1, 2)
         angle = torch.atan2(xy[:, 0] + 1e-8, xy[:, 1] + self.eps)
         return angle
 
 
 class LAFOrienter(nn.Module):
     """Module, which extracts patches using input images and local affine frames (LAFs).
@@ -198,38 +201,37 @@
     Args:
         patch_size:
         num_angular_bins:
         angle_detector: Patch orientation estimator, e.g. :class:`~kornia.feature.PatchDominantGradientOrientation`
           or OriNet.
     """  # pylint: disable
 
-    def __init__(self, patch_size: int = 32, num_angular_bins: int = 36, angle_detector: Optional[nn.Module] = None):
+    def __init__(
+        self, patch_size: int = 32, num_angular_bins: int = 36, angle_detector: Optional[nn.Module] = None
+    ) -> None:
         super().__init__()
         self.patch_size = patch_size
         self.num_ang_bins = num_angular_bins
         self.angle_detector: nn.Module
         if angle_detector is None:
             self.angle_detector = PatchDominantGradientOrientation(self.patch_size, self.num_ang_bins)
         else:
             self.angle_detector = angle_detector
 
-    def __repr__(self):
-        return (
-            self.__class__.__name__ + '('
-            'patch_size=' + str(self.patch_size) + ', ' + 'angle_detector=' + str(self.angle_detector) + ')'
-        )
+    def __repr__(self) -> str:
+        return f"{self.__class__.__name__}(patch_size={self.patch_size}, angle_detector={self.angle_detector})"
 
     def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:
         """
         Args:
-            laf: shape [BxNx2x3]
-            img: shape [Bx1xHxW]
+            laf: :math:`(B, N, 2, 3)`
+            img: :math:`(B, 1, H, W)`
 
         Returns:
-            laf_out, shape [BxNx2x3]
+            LAF_out: :math:`(B, N, 2, 3)`
         """
         KORNIA_CHECK_LAF(laf)
         KORNIA_CHECK_SHAPE(img, ["B", "C", "H", "W"])
         if laf.size(0) != img.size(0):
             raise ValueError(f"Batch size of laf and img should be the same. Got {img.size(0)}, {laf.size(0)}")
         B, N = laf.shape[:2]
         patches: torch.Tensor = extract_patches_from_pyramid(img, laf, self.patch_size).view(
```

### Comparing `kornia-0.6.9/kornia/feature/responses.py` & `kornia-0.7.0/kornia/feature/responses.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,21 +1,35 @@
 from typing import Optional, Union
 
 import torch
 
 from kornia.core import Module, Tensor, tensor
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.filters import gaussian_blur2d, spatial_gradient
-from kornia.testing import KORNIA_CHECK_SHAPE
+
+
+def _get_kernel_size(sigma: Union[float, int]) -> int:
+    ksize = int(2.0 * 4.0 * sigma + 1.0)
+
+    #  matches OpenCV, but may cause padding problem for small images
+    #  PyTorch does not allow to pad more than original size.
+    #  Therefore there is a hack in forward function
+
+    if ksize % 2 == 0:
+        ksize += 1
+    return ksize
 
 
 def harris_response(
     input: Tensor, k: Union[Tensor, float] = 0.04, grads_mode: str = 'sobel', sigmas: Optional[Tensor] = None
 ) -> Tensor:
     r"""Compute the Harris cornerness function.
 
+    .. image:: _static/img/harris_response.png
+
     Function does not do any normalization or nms. The response map is computed according the following formulation:
 
     .. math::
         R = max(0, det(M) - k \cdot trace(M)^2)
 
     where:
 
@@ -90,14 +104,16 @@
 
     return scores
 
 
 def gftt_response(input: Tensor, grads_mode: str = 'sobel', sigmas: Optional[Tensor] = None) -> Tensor:
     r"""Compute the Shi-Tomasi cornerness function.
 
+    .. image:: _static/img/gftt_response.png
+
     Function does not do any normalization or nms. The response map is computed according the following formulation:
 
     .. math::
         R = min(eig(M))
 
     where:
 
@@ -162,14 +178,16 @@
 
     return scores
 
 
 def hessian_response(input: Tensor, grads_mode: str = 'sobel', sigmas: Optional[Tensor] = None) -> Tensor:
     r"""Compute the absolute of determinant of the Hessian matrix.
 
+    .. image:: _static/img/hessian_response.png
+
     Function does not do any normalization or nms. The response map is computed according the following formulation:
 
     .. math::
         R = det(H)
 
     where:
 
@@ -247,81 +265,125 @@
         the response map per channel with shape :math:`(B, C, D-1, H, W)`.
     """
     KORNIA_CHECK_SHAPE(input, ["B", "C", "L", "H", "W"])
 
     return input[:, :, 1:] - input[:, :, :-1]
 
 
+def dog_response_single(input: Tensor, sigma1: float = 1.0, sigma2: float = 1.6) -> Tensor:
+    r"""Compute the Difference-of-Gaussian response.
+
+    .. image:: _static/img/dog_response_single.png
+
+    Args:
+        input: a given the gaussian 4d tensor :math:`(B, C, H, W)`.
+        sigma1: lower gaussian sigma
+        sigma2: bigger gaussian sigma
+
+    Return:
+        the response map per channel with shape :math:`(B, C, H, W)`.
+    """
+    KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
+    ks1 = _get_kernel_size(sigma1)
+    ks2 = _get_kernel_size(sigma1)
+    g1 = gaussian_blur2d(input, (ks1, ks1), (sigma1, sigma1))
+    g2 = gaussian_blur2d(input, (ks2, ks2), (sigma2, sigma2))
+    return g2 - g1
+
+
 class BlobDoG(Module):
     r"""Module that calculates Difference-of-Gaussians blobs.
 
-    See :func:`~kornia.feature.dog_response` for details.
+    See
+    :func: `~kornia.feature.dog_response` for details.
     """
 
     def __init__(self) -> None:
         super().__init__()
-        return
 
     def __repr__(self) -> str:
         return self.__class__.__name__
 
     def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
         return dog_response(input)
 
 
+class BlobDoGSingle(Module):
+    r"""Module that calculates Difference-of-Gaussians blobs.
+
+    .. image:: _static/img/dog_response_single.png
+
+    See :func:`~kornia.feature.dog_response_single` for details.
+    """
+
+    def __init__(self, sigma1: float = 1.0, sigma2: float = 1.6) -> None:
+        super().__init__()
+        self.sigma1 = sigma1
+        self.sigma2 = sigma2
+
+    def __repr__(self) -> str:
+        return f'{self.__class__.__name__}, sigma1={self.sigma1}, sigma2={self.sigma2})'
+
+    def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
+        return dog_response_single(input, self.sigma1, self.sigma2)
+
+
 class CornerHarris(Module):
     r"""Module that calculates Harris corners.
 
+    .. image:: _static/img/harris_response.png
+
     See :func:`~kornia.feature.harris_response` for details.
     """
     k: Tensor
 
-    def __init__(self, k: Union[float, Tensor], grads_mode='sobel') -> None:
+    def __init__(self, k: Union[float, Tensor], grads_mode: str = 'sobel') -> None:
         super().__init__()
         if isinstance(k, float):
             self.register_buffer('k', tensor(k))
         else:
             self.register_buffer('k', k)
         self.grads_mode: str = grads_mode
-        return
 
     def __repr__(self) -> str:
-        return self.__class__.__name__ + '(k=' + str(self.k) + ', ' + 'grads_mode=' + self.grads_mode + ')'
+        return f'{self.__class__.__name__}(k={self.k}, grads_mode={self.grads_mode})'
 
     def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
         return harris_response(input, self.k, self.grads_mode, sigmas)
 
 
 class CornerGFTT(Module):
     r"""Module that calculates Shi-Tomasi corners.
 
-    See :func:`~kornia.feature.gfft_response` for details.
+    .. image:: _static/img/gftt_response.png
+
+    See :func:`~kornia.feature.gftt_response` for details.
     """
 
-    def __init__(self, grads_mode='sobel') -> None:
+    def __init__(self, grads_mode: str = 'sobel') -> None:
         super().__init__()
         self.grads_mode: str = grads_mode
-        return
 
     def __repr__(self) -> str:
-        return self.__class__.__name__ + 'grads_mode=' + self.grads_mode + ')'
+        return f'{self.__class__.__name__}(grads_mode={self.grads_mode})'
 
     def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
         return gftt_response(input, self.grads_mode, sigmas)
 
 
 class BlobHessian(Module):
     r"""Module that calculates Hessian blobs.
 
+    .. image:: _static/img/hessian_response.png
+
     See :func:`~kornia.feature.hessian_response` for details.
     """
 
-    def __init__(self, grads_mode='sobel') -> None:
+    def __init__(self, grads_mode: str = 'sobel') -> None:
         super().__init__()
         self.grads_mode: str = grads_mode
-        return
 
     def __repr__(self) -> str:
-        return self.__class__.__name__ + 'grads_mode=' + self.grads_mode + ')'
+        return f'{self.__class__.__name__}(grads_mode={self.grads_mode})'
 
     def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
         return hessian_response(input, self.grads_mode, sigmas)
```

### Comparing `kornia-0.6.9/kornia/feature/siftdesc.py` & `kornia-0.7.0/kornia/feature/siftdesc.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 import math
 from typing import Tuple
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from kornia.core import Module, Tensor, concatenate, eye, normalize
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.filters import get_gaussian_kernel2d, spatial_gradient
 from kornia.geometry.conversions import pi
-from kornia.testing import KORNIA_CHECK_SHAPE
 
 
 def _get_reshape_kernel(kd: int, ky: int, kx: int) -> Tensor:
     """Utility function, which returns neigh2channels conv kernel."""
     numel: int = kd * ky * kx
     weight = eye(numel)
     return weight.view(numel, kd, ky, kx)
@@ -77,31 +77,20 @@
         >>> input = torch.rand(23, 1, 32, 32)
         >>> SIFT = SIFTDescriptor(32, 8, 4)
         >>> descs = SIFT(input) # 23x128
     """
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'num_ang_bins='
-            + str(self.num_ang_bins)
-            + ', '
-            + 'num_spatial_bins='
-            + str(self.num_spatial_bins)
-            + ', '
-            + 'patch_size='
-            + str(self.patch_size)
-            + ', '
-            + 'rootsift='
-            + str(self.rootsift)
-            + ', '
-            + 'clipval='
-            + str(self.clipval)
-            + ')'
+            f"{self.__class__.__name__}("
+            f"num_ang_bins={self.num_ang_bins}, "
+            f"num_spatial_bins={self.num_spatial_bins}, "
+            f"patch_size={self.patch_size}, "
+            f"rootsift={self.rootsift}, "
+            f"clipval={self.clipval})"
         )
 
     def __init__(
         self,
         patch_size: int = 41,
         num_ang_bins: int = 8,
         num_spatial_bins: int = 4,
@@ -128,23 +117,22 @@
             1,
             kernel_size=(nw.size(0), nw.size(1)),
             stride=(self.bin_stride, self.bin_stride),
             padding=(self.pad, self.pad),
             bias=False,
         )
         self.pk.weight.data.copy_(nw.reshape(1, 1, nw.size(0), nw.size(1)))
-        return
 
     def get_pooling_kernel(self) -> Tensor:
         return self.pk.weight.detach()
 
     def get_weighting_kernel(self) -> Tensor:
         return self.gk.detach()
 
-    def forward(self, input):
+    def forward(self, input: Tensor) -> Tensor:
         KORNIA_CHECK_SHAPE(input, ["B", "1", f"{self.patch_size}", f"{self.patch_size}"])
         B: int = input.shape[0]
         self.pk = self.pk.to(input.dtype).to(input.device)
 
         grads = spatial_gradient(input, 'diff')
         # unpack the edges
         gx = grads[:, :, 0]
@@ -184,15 +172,16 @@
     num_ang_bins: int = 8,
     num_spatial_bins: int = 4,
     rootsift: bool = True,
     clipval: float = 0.2,
 ) -> Tensor:
     r"""Computes the sift descriptor.
 
-    See :class:`~kornia.feature.SIFTDescriptor` for details.
+    See
+    :class: `~kornia.feature.SIFTDescriptor` for details.
     """
     return SIFTDescriptor(patch_size, num_ang_bins, num_spatial_bins, rootsift, clipval)(input)
 
 
 class DenseSIFTDescriptor(Module):
     """Module, which computes SIFT descriptor densely over the image.
 
@@ -217,34 +206,21 @@
         >>> input =  torch.rand(2, 1, 200, 300)
         >>> SIFT = DenseSIFTDescriptor()
         >>> descs = SIFT(input) # 2x128x194x294
     """
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'num_ang_bins='
-            + str(self.num_ang_bins)
-            + ', '
-            + 'num_spatial_bins='
-            + str(self.num_spatial_bins)
-            + ', '
-            + 'spatial_bin_size='
-            + str(self.spatial_bin_size)
-            + ', '
-            + 'rootsift='
-            + str(self.rootsift)
-            + ', '
-            + 'stride='
-            + str(self.stride)
-            + ', '
-            + 'clipval='
-            + str(self.clipval)
-            + ')'
+            f'{self.__class__.__name__}('
+            f'num_ang_bins={self.num_ang_bins}, '
+            f'num_spatial_bins={self.num_spatial_bins}, '
+            f'spatial_bin_size={self.spatial_bin_size}, '
+            f'rootsift={self.rootsift}, '
+            f'stride={self.stride}, '
+            f'clipval={self.clipval})'
         )
 
     def __init__(
         self,
         num_ang_bins: int = 8,
         num_spatial_bins: int = 4,
         spatial_bin_size: int = 4,
@@ -279,15 +255,14 @@
             stride=(self.stride, self.stride),
             bias=False,
             padding=(self.pad, self.pad),
         )
         self.PoolingConv.weight.data.copy_(
             _get_reshape_kernel(num_ang_bins, num_spatial_bins, num_spatial_bins).float()
         )
-        return
 
     def get_pooling_kernel(self) -> Tensor:
         return self.bin_pooling_kernel.weight.detach()
 
     def forward(self, input: Tensor) -> Tensor:
         KORNIA_CHECK_SHAPE(input, ["B", "1", "H", "W"])
```

### Comparing `kornia-0.6.9/kornia/feature/sold2/backbones.py` & `kornia-0.7.0/kornia/feature/sold2/backbones.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,25 @@
 """Implements several backbone networks."""
-from typing import Any, Dict, List, Optional, Tuple, Type, Union
+from typing import Any, Dict, List, NamedTuple, Optional, Tuple, Type, Union
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 from torch.nn.functional import pixel_shuffle, softmax
 
-from kornia.core import Module
+from kornia.core import Module, Tensor
+
+
+class HourglassConfig(NamedTuple):
+    depth: int
+    num_stacks: int
+    num_blocks: int
+    num_classes: int
+    input_channels: int
+    head: Type[Module]
 
 
 # [Hourglass backbone classes]
 class HourglassBackbone(Module):
     """Hourglass network, taken from https://github.com/zhou13/lcnn.
 
     Args:
@@ -19,34 +28,25 @@
         num_stacks: number of hourglass modules stacked together.
         num_blocks: number of layers in each residual block.
         num_classes: number of heads for the output of a hourglass module.
     """
 
     def __init__(
         self, input_channel: int = 1, depth: int = 4, num_stacks: int = 2, num_blocks: int = 1, num_classes: int = 5
-    ):
+    ) -> None:
         super().__init__()
         self.head = MultitaskHead
-        self.net = hg(
-            **{
-                "head": self.head,
-                "depth": depth,
-                "num_stacks": num_stacks,
-                "num_blocks": num_blocks,
-                "num_classes": num_classes,
-                "input_channels": input_channel,
-            }
-        )
+        self.net = hg(HourglassConfig(depth, num_stacks, num_blocks, num_classes, input_channel, head=self.head))
 
-    def forward(self, input_images: torch.Tensor) -> torch.Tensor:
+    def forward(self, input_images: Tensor) -> Tensor:
         return self.net(input_images)
 
 
 class MultitaskHead(Module):
-    def __init__(self, input_channels: int):
+    def __init__(self, input_channels: int) -> None:
         super().__init__()
 
         m = int(input_channels / 4)
         head_size = [[2], [1], [2]]
         heads = []
         for output_channels in sum(head_size, []):
             heads.append(
@@ -54,35 +54,35 @@
                     nn.Conv2d(input_channels, m, kernel_size=3, padding=1),
                     nn.ReLU(inplace=True),
                     nn.Conv2d(m, output_channels, kernel_size=1),
                 )
             )
         self.heads = nn.ModuleList(heads)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         return torch.cat([head(x) for head in self.heads], dim=1)
 
 
 class Bottleneck2D(Module):
     def __init__(
         self, inplanes: int, planes: int, stride: Union[int, Tuple[int, int]] = 1, downsample: Optional[Module] = None
-    ):
+    ) -> None:
         super().__init__()
 
         self.bn1 = nn.BatchNorm2d(inplanes)
         self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1)
         self.bn2 = nn.BatchNorm2d(planes)
         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1)
         self.bn3 = nn.BatchNorm2d(planes)
         self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1)
         self.relu = nn.ReLU(inplace=True)
         self.downsample = downsample
         self.stride = stride
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         residual = x
 
         out = self.bn1(x)
         out = self.relu(out)
         out = self.conv1(out)
 
         out = self.bn2(out)
@@ -98,15 +98,15 @@
 
         out += residual
 
         return out
 
 
 class Hourglass(Module):
-    def __init__(self, block: Type[Bottleneck2D], num_blocks: int, planes: int, depth: int, expansion: int = 2):
+    def __init__(self, block: Type[Bottleneck2D], num_blocks: int, planes: int, depth: int, expansion: int = 2) -> None:
         super().__init__()
         self.depth = depth
         self.block = block
         self.expansion = expansion
         self.hg = self._make_hour_glass(block, num_blocks, planes, depth)
 
     def _make_residual(self, block: Type[Bottleneck2D], num_blocks: int, planes: int) -> Module:
@@ -122,46 +122,46 @@
             for _ in range(3):
                 res.append(self._make_residual(block, num_blocks, planes))
             if i == 0:
                 res.append(self._make_residual(block, num_blocks, planes))
             hgl.append(nn.ModuleList(res))
         return nn.ModuleList(hgl)
 
-    def _hour_glass_forward(self, n: int, x: torch.Tensor) -> torch.Tensor:
+    def _hour_glass_forward(self, n: int, x: Tensor) -> Tensor:
         up1 = self.hg[n - 1][0](x)
         low1 = F.max_pool2d(x, 2, stride=2)
         low1 = self.hg[n - 1][1](low1)
 
         if n > 1:
             low2 = self._hour_glass_forward(n - 1, low1)
         else:
             low2 = self.hg[n - 1][3](low1)
         low3 = self.hg[n - 1][2](low2)
         up2 = F.interpolate(low3, size=up1.shape[2:])
         out = up1 + up2
         return out
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         return self._hour_glass_forward(self.depth, x)
 
 
 class HourglassNet(Module):
     """Hourglass model from Newell et al ECCV 2016."""
 
     def __init__(
         self,
         block: Type[Bottleneck2D],
-        head: Module,
+        head: Type[Module],
         depth: int,
         num_stacks: int,
         num_blocks: int,
         num_classes: int,
         input_channels: int,
         expansion: int = 2,
-    ):
+    ) -> None:
         super().__init__()
 
         self.inplanes = 64
         self.num_feats = 128
         self.num_stacks = num_stacks
         self.expansion = expansion
         self.conv1 = nn.Conv2d(input_channels, self.inplanes, kernel_size=7, stride=2, padding=3)
@@ -206,15 +206,15 @@
         return nn.Sequential(*layers)
 
     def _make_fc(self, inplanes: int, outplanes: int) -> Module:
         bn = nn.BatchNorm2d(inplanes)
         conv = nn.Conv2d(inplanes, outplanes, kernel_size=1)
         return nn.Sequential(conv, bn, self.relu)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         out = []
         x = self.conv1(x)
         x = self.bn1(x)
         x = self.relu(x)
 
         x = self.layer1(x)
         x = self.maxpool(x)
@@ -231,46 +231,45 @@
                 fc_ = self.fc_[i](y)
                 score_ = self.score_[i](score)
                 x = x + fc_ + score_
 
         return y
 
 
-def hg(**kwargs):
-    model = HourglassNet(
+def hg(cfg: HourglassConfig) -> HourglassNet:
+    return HourglassNet(
         Bottleneck2D,
-        head=kwargs.get("head", lambda c_in, c_out: nn.Conv2d(c_in, c_out, 1)),
-        depth=kwargs["depth"],
-        num_stacks=kwargs["num_stacks"],
-        num_blocks=kwargs["num_blocks"],
-        num_classes=kwargs["num_classes"],
-        input_channels=kwargs["input_channels"],
+        head=cfg.head,
+        depth=cfg.depth,
+        num_stacks=cfg.num_stacks,
+        num_blocks=cfg.num_blocks,
+        num_classes=cfg.num_classes,
+        input_channels=cfg.input_channels,
     )
-    return model
 
 
 # [Backbone decoders]
 class SuperpointDecoder(Module):
     """Junction decoder based on the SuperPoint architecture.
 
     Args:
         input_feat_dim: channel size of the input features.
     Returns:
         the junction heatmap, with shape (B, H, W).
     """
 
-    def __init__(self, input_feat_dim: int = 128, grid_size: int = 8):
+    def __init__(self, input_feat_dim: int = 128, grid_size: int = 8) -> None:
         super().__init__()
         self.relu = nn.ReLU(inplace=True)
         # Perform strided convolution when using lcnn backbone.
         self.convPa = nn.Conv2d(input_feat_dim, 256, kernel_size=3, stride=2, padding=1)
         self.convPb = nn.Conv2d(256, 65, kernel_size=1, stride=1, padding=0)
         self.grid_size = grid_size
 
-    def forward(self, input_features: torch.Tensor) -> torch.Tensor:
+    def forward(self, input_features: Tensor) -> Tensor:
         feat = self.relu(self.convPa(input_features))
         semi = self.convPb(feat)
 
         # Convert from semi-dense to dense heatmap
         junc_prob = softmax(semi, dim=1)
         junc_pred = pixel_shuffle(junc_prob[:, :-1, :, :], self.grid_size)[:, 0]
         return junc_pred
@@ -283,15 +282,15 @@
         input_feat_dim: channel size of the input features.
         num_upsample: how many upsamples are performed.
         output_channel: number of output channels.
     Returns:
         the (B, 1, H, W) line heatmap.
     """
 
-    def __init__(self, input_feat_dim: int = 128, num_upsample: int = 2, output_channel: int = 2):
+    def __init__(self, input_feat_dim: int = 128, num_upsample: int = 2, output_channel: int = 2) -> None:
         super().__init__()
         # Get channel parameters
         self.channel_conf = self.get_channel_conf(num_upsample)
 
         # Define the pixel shuffle
         self.pixshuffle = nn.PixelShuffle(2)
 
@@ -324,15 +323,15 @@
 
     def get_channel_conf(self, num_upsample: int) -> List[int]:
         """Get num of channels based on number of upsampling."""
         if num_upsample == 2:
             return [256, 64, 16]
         return [256, 64, 16, 4]
 
-    def forward(self, input_features: torch.Tensor) -> torch.Tensor:
+    def forward(self, input_features: Tensor) -> Tensor:
         # Iterate til output block
         out = input_features
         for block in self.conv_block_lst[:-1]:
             out = block(out)
             out = self.pixshuffle(out)
 
         # Output layer
@@ -347,21 +346,21 @@
 
     Args:
         input_feat_dim: channel size of the input features.
     Returns:
         the semi-dense descriptors with shape (B, 128, H/4, W/4).
     """
 
-    def __init__(self, input_feat_dim: int = 128):
+    def __init__(self, input_feat_dim: int = 128) -> None:
         super().__init__()
         self.relu = nn.ReLU(inplace=True)
         self.convPa = nn.Conv2d(input_feat_dim, 256, kernel_size=3, stride=1, padding=1)
         self.convPb = nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0)
 
-    def forward(self, input_features: torch.Tensor) -> torch.Tensor:
+    def forward(self, input_features: Tensor) -> Tensor:
         feat = self.relu(self.convPa(input_features))
         semi = self.convPb(feat)
 
         return semi
 
 
 # [Combination of all previous models in one]
@@ -375,15 +374,15 @@
     Returns:
         a Dict with the following values:
             junctions: heatmap of junctions.
             heatmap: line heatmap.
             descriptors: semi-dense descriptors.
     """
 
-    def __init__(self, model_cfg: Dict[str, Any]):
+    def __init__(self, model_cfg: Dict[str, Any]) -> None:
         super().__init__()
         self.cfg = model_cfg
 
         # Backbone
         self.backbone_net = HourglassBackbone(**self.cfg["backbone_cfg"])
         feat_channel = 256
 
@@ -393,15 +392,15 @@
         # Line heatmap decoder
         self.heatmap_decoder = PixelShuffleDecoder(feat_channel, num_upsample=2)
 
         # Descriptor decoder
         if "use_descriptor" in self.cfg:
             self.descriptor_decoder = SuperpointDescriptor(feat_channel)
 
-    def forward(self, input_images):
+    def forward(self, input_images: Tensor) -> Dict[str, Tensor]:
         # The backbone
         features = self.backbone_net(input_images)
 
         # junction decoder
         junctions = self.junction_decoder(features)
 
         # heatmap decoder
```

### Comparing `kornia-0.6.9/kornia/feature/sold2/sold2.py` & `kornia-0.7.0/kornia/feature/sold2/sold2.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from typing import Any, Dict, Optional, Tuple
 
 import torch
 import torch.nn.functional as F
 
 from kornia.core import Module, Tensor, concatenate, pad, stack
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.geometry.conversions import normalize_pixel_coordinates
-from kornia.testing import KORNIA_CHECK_SHAPE
 from kornia.utils import map_location_to_cpu
 
 from .backbones import SOLD2Net
 from .sold2_detector import LineSegmentDetectionModule, line_map_to_segments, prob_to_junctions
 
 urls: Dict[str, str] = {}
 urls["wireframe"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/sold2_wireframe.pth"
@@ -71,15 +71,15 @@
         >>> line_seg1 = outputs["line_segments"][0]
         >>> line_seg2 = outputs["line_segments"][1]
         >>> desc1 = outputs["dense_desc"][0]
         >>> desc2 = outputs["dense_desc"][1]
         >>> matches = sold2.match(line_seg1, line_seg2, desc1[None], desc2[None])
     """
 
-    def __init__(self, pretrained: bool = True, config: Optional[Dict[str, Any]] = None):
+    def __init__(self, pretrained: bool = True, config: Optional[Dict[str, Any]] = None) -> None:
         super().__init__()
         # Initialize some parameters
         self.config = default_cfg if config is None else config
         self.grid_size = self.config["grid_size"]
         self.junc_detect_thresh = self.config.get("detection_thresh", 1 / 65)
         self.max_num_junctions = self.config.get("max_num_junctions", 500)
 
@@ -139,15 +139,15 @@
             desc1, desc2: semi-dense descriptor maps of the images, with shape [1, 128, H/4, W/4].
         Returns:
             A np.array of size [num_lines1] indicating the index in line_seg2 of the matched line,
             for each line in line_seg1. -1 means that the line is not matched.
         """
         return self.line_matcher(line_seg1, line_seg2, desc1, desc2)
 
-    def adapt_state_dict(self, state_dict):
+    def adapt_state_dict(self, state_dict: Dict[str, Any]) -> Dict[str, Any]:
         del state_dict["w_junc"]
         del state_dict["w_heatmap"]
         del state_dict["w_desc"]
         state_dict["heatmap_decoder.conv_block_lst.2.0.weight"] = state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         state_dict["heatmap_decoder.conv_block_lst.2.0.bias"] = state_dict["heatmap_decoder.conv_block_lst.2.bias"]
         del state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         del state_dict["heatmap_decoder.conv_block_lst.2.bias"]
@@ -164,15 +164,15 @@
         self,
         cross_check: bool = True,
         num_samples: int = 10,
         min_dist_pts: int = 8,
         top_k_candidates: int = 10,
         grid_size: int = 8,
         line_score: bool = False,
-    ):
+    ) -> None:
         super().__init__()
         self.cross_check = cross_check
         self.num_samples = num_samples
         self.min_dist_pts = min_dist_pts
         self.top_k_candidates = top_k_candidates
         self.grid_size = grid_size
         self.line_score = line_score  # True to compute saliency on a line
@@ -234,33 +234,33 @@
         Outputs:
             line_points: an N x num_samples x 2 Tensor.
             valid_points: a boolean N x num_samples Tensor.
         """
         KORNIA_CHECK_SHAPE(line_seg, ["N", "2", "2"])
         num_lines = len(line_seg)
         line_lengths = torch.norm(line_seg[:, 0] - line_seg[:, 1], dim=1)
-
+        dev = line_seg.device
         # Sample the points separated by at least min_dist_pts along each line
         # The number of samples depends on the length of the line
         num_samples_lst = torch.clamp(
             torch.div(line_lengths, self.min_dist_pts, rounding_mode='floor'), 2, self.num_samples
         ).int()
-        line_points = torch.empty((num_lines, self.num_samples, 2), dtype=torch.float)
-        valid_points = torch.empty((num_lines, self.num_samples), dtype=torch.bool)
+        line_points = torch.empty((num_lines, self.num_samples, 2), dtype=torch.float, device=dev)
+        valid_points = torch.empty((num_lines, self.num_samples), dtype=torch.bool, device=dev)
         for n_samp in range(2, self.num_samples + 1):
             # Consider all lines where we can fit up to n_samp points
             cur_mask = num_samples_lst == n_samp
             cur_line_seg = line_seg[cur_mask]
             line_points_x = batched_linspace(cur_line_seg[:, 0, 0], cur_line_seg[:, 1, 0], n_samp, dim=-1)
             line_points_y = batched_linspace(cur_line_seg[:, 0, 1], cur_line_seg[:, 1, 1], n_samp, dim=-1)
             cur_line_points = stack([line_points_x, line_points_y], -1)
 
             # Pad
             cur_line_points = pad(cur_line_points, (0, 0, 0, self.num_samples - n_samp))
-            cur_valid_points = torch.ones(len(cur_line_seg), self.num_samples, dtype=torch.bool)
+            cur_valid_points = torch.ones(len(cur_line_seg), self.num_samples, dtype=torch.bool, device=dev)
             cur_valid_points[:, n_samp:] = False
 
             line_points[cur_mask] = cur_line_points
             valid_points[cur_mask] = cur_valid_points
 
         return line_points, valid_points
 
@@ -313,17 +313,17 @@
         """
         KORNIA_CHECK_SHAPE(scores, ["B", "N", "M"])
         b, n, m = scores.shape
 
         # Recalibrate the scores to get a gap score of 0
         gap = 0.1
         nw_scores = scores - gap
-
+        dev = scores.device
         # Run the dynamic programming algorithm
-        nw_grid = torch.zeros(b, n + 1, m + 1, dtype=torch.float)
+        nw_grid = torch.zeros(b, n + 1, m + 1, dtype=torch.float, device=dev)
         for i in range(n):
             for j in range(m):
                 nw_grid[:, i + 1, j + 1] = torch.maximum(
                     torch.maximum(nw_grid[:, i + 1, j], nw_grid[:, i, j + 1]), nw_grid[:, i, j] + nw_scores[:, i, j]
                 )
 
         return nw_grid[:, -1, -1]
@@ -339,15 +339,15 @@
     KORNIA_CHECK_SHAPE(keypoints, ["N", "2"])
     n_points = len(keypoints)
     grid_points = normalize_pixel_coordinates(keypoints[:, [1, 0]], img_size[0], img_size[1])
     grid_points = grid_points.view(-1, n_points, 1, 2)
     return grid_points
 
 
-def batched_linspace(start, end, step, dim):
+def batched_linspace(start: Tensor, end: Tensor, step: int, dim: int) -> Tensor:
     """Batch version of torch.normalize (similar to the numpy one)."""
     intervals = ((end - start) / (step - 1)).unsqueeze(dim)
     broadcast_size = [1] * len(intervals.shape)
     broadcast_size[dim] = step
     samples = torch.arange(step, dtype=torch.float, device=start.device).reshape(broadcast_size)
     samples = start.unsqueeze(dim) + samples * intervals
     return samples
```

### Comparing `kornia-0.6.9/kornia/feature/sold2/sold2_detector.py` & `kornia-0.7.0/kornia/feature/sold2/sold2_detector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import math
 from typing import Any, Dict, Optional, Tuple
 
 import torch
 
 from kornia.core import Module, Tensor, concatenate, stack, tensor, where, zeros
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.geometry.bbox import nms
-from kornia.testing import KORNIA_CHECK_SHAPE
 from kornia.utils import map_location_to_cpu, torch_meshgrid
 
 from .backbones import SOLD2Net
 
 urls: Dict[str, str] = {}
 urls["wireframe"] = "https://www.polybox.ethz.ch/index.php/s/blOrW89gqSLoHOk/download"
 
@@ -57,15 +57,15 @@
 
     Example:
         >>> img = torch.rand(1, 1, 512, 512)
         >>> sold2_detector = SOLD2_detector()
         >>> line_segments = sold2_detector(img)["line_segments"]
     """
 
-    def __init__(self, pretrained: bool = True, config: Optional[Dict[str, Any]] = None):
+    def __init__(self, pretrained: bool = True, config: Optional[Dict[str, Any]] = None) -> None:
         super().__init__()
         # Initialize some parameters
         self.config = default_detector_cfg if config is None else config
         self.grid_size = self.config["grid_size"]
         self.junc_detect_thresh = self.config.get("detection_thresh", 1 / 65)
         self.max_num_junctions = self.config.get("max_num_junctions", 500)
 
@@ -77,15 +77,15 @@
             self.model.load_state_dict(state_dict)
         self.eval()
 
         # Initialize the line detector
         self.line_detector_cfg = self.config["line_detector_cfg"]
         self.line_detector = LineSegmentDetectionModule(**self.config["line_detector_cfg"])
 
-    def adapt_state_dict(self, state_dict):
+    def adapt_state_dict(self, state_dict: Dict[str, Any]) -> Dict[str, Any]:
         del state_dict["w_junc"]
         del state_dict["w_heatmap"]
         del state_dict["w_desc"]
         state_dict["heatmap_decoder.conv_block_lst.2.0.weight"] = state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         state_dict["heatmap_decoder.conv_block_lst.2.0.bias"] = state_dict["heatmap_decoder.conv_block_lst.2.bias"]
         del state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         del state_dict["heatmap_decoder.conv_block_lst.2.bias"]
@@ -150,18 +150,18 @@
         heatmap_low_thresh: float = 0.15,
         heatmap_high_thresh: float = 0.2,
         max_local_patch_radius: float = 3,
         lambda_radius: float = 2.0,
         use_candidate_suppression: bool = False,
         nms_dist_tolerance: float = 3.0,
         use_heatmap_refinement: bool = False,
-        heatmap_refine_cfg=None,
+        heatmap_refine_cfg: Optional[Dict[str, Any]] = None,
         use_junction_refinement: bool = False,
-        junction_refine_cfg=None,
-    ):
+        junction_refine_cfg: Optional[Dict[str, Any]] = None,
+    ) -> None:
         # Line detection parameters
         self.detect_thresh = detect_thresh
 
         # Line sampling parameters
         self.num_samples = num_samples
         self.inlier_thresh = inlier_thresh
         self.local_patch_radius = max_local_patch_radius
@@ -193,15 +193,15 @@
     def detect(self, junctions: Tensor, heatmap: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
         """Main function performing line segment detection."""
         KORNIA_CHECK_SHAPE(heatmap, ["H", "W"])
         H, W = heatmap.shape
         device = junctions.device
 
         # Perform the heatmap refinement
-        if self.use_heatmap_refinement:
+        if self.use_heatmap_refinement and isinstance(self.heatmap_refine_cfg, dict):
             if self.heatmap_refine_cfg["mode"] == "global":
                 heatmap = self.refine_heatmap(
                     heatmap, self.heatmap_refine_cfg["ratio"], self.heatmap_refine_cfg["valid_thresh"]
                 )
             elif self.heatmap_refine_cfg["mode"] == "local":
                 heatmap = self.refine_heatmap_local(
                     heatmap,
@@ -389,14 +389,16 @@
         return candidate_map
 
     def refine_junction_perturb(
         self, junctions: Tensor, line_map: Tensor, heatmap: Tensor, H: int, W: int, device: torch.device
     ) -> Tuple[Tensor, Tensor]:
         """Refine the line endpoints in a similar way as in LSD."""
         # Fetch refinement parameters
+        if not isinstance(self.junction_refine_cfg, dict):
+            raise TypeError(f'Expected to have a dict of config for junction. Gotcha {type(self.junction_refine_cfg)}')
         num_perturbs = self.junction_refine_cfg["num_perturbs"]
         perturb_interval = self.junction_refine_cfg["perturb_interval"]
         side_perturbs = (num_perturbs - 1) // 2
 
         # Fetch the 2D perturb mat
         perturb_vec = torch.arange(
             start=-perturb_interval * side_perturbs,
```

### Comparing `kornia-0.6.9/kornia/feature/sosnet.py` & `kornia-0.7.0/kornia/feature/sosnet.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Dict
 
 import torch
-import torch.nn as nn
+from torch import nn
 
-from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls["lib"] = "https://github.com/yuruntian/SOSNet/raw/master/sosnet-weights/sosnet_32x32_liberty.pth"
 urls["hp_a"] = "https://github.com/yuruntian/SOSNet/raw/master/sosnet-weights/sosnet_32x32_hpatches_a.pth"
 
 
@@ -59,14 +59,13 @@
         )
         self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0))
         # load pretrained model
         if pretrained:
             pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
-        return
 
     def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:
         KORNIA_CHECK_SHAPE(input, ["B", "1", "32", "32"])
         descr = self.desc_norm(self.layers(input) + eps)
         descr = descr.view(descr.size(0), -1)
         return descr
```

### Comparing `kornia-0.6.9/kornia/feature/tfeat.py` & `kornia-0.7.0/kornia/feature/tfeat.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Dict
 
 import torch
-import torch.nn as nn
+from torch import nn
 
-from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls["liberty"] = "https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-liberty.params"  # pylint: disable
 urls[
     "notredame"
 ] = "https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-notredame.params"  # pylint: disable
```

### Comparing `kornia-0.6.9/kornia/filters/__init__.py` & `kornia-0.7.0/kornia/filters/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,22 +1,38 @@
+from __future__ import annotations
+
+from .bilateral import BilateralBlur, JointBilateralBlur, bilateral_blur, joint_bilateral_blur
 from .blur import BoxBlur, box_blur
-from .blur_pool import BlurPool2D, MaxBlurPool2D, blur_pool2d, edge_aware_blur_pool2d, max_blur_pool2d
+from .blur_pool import (
+    BlurPool2D,
+    EdgeAwareBlurPool2D,
+    MaxBlurPool2D,
+    blur_pool2d,
+    edge_aware_blur_pool2d,
+    max_blur_pool2d,
+)
 from .canny import Canny, canny
 from .dexined import DexiNed
 from .filter import filter2d, filter2d_separable, filter3d
-from .gaussian import GaussianBlur2d, gaussian_blur2d
+from .gaussian import GaussianBlur2d, gaussian_blur2d, gaussian_blur2d_t
+from .guided import GuidedBlur, guided_blur
 from .kernels import (
     gaussian,
     get_binary_kernel2d,
+    get_box_kernel1d,
     get_box_kernel2d,
     get_diff_kernel2d,
     get_gaussian_discrete_kernel1d,
     get_gaussian_erf_kernel1d,
     get_gaussian_kernel1d,
+    get_gaussian_kernel1d_t,
     get_gaussian_kernel2d,
+    get_gaussian_kernel2d_t,
+    get_gaussian_kernel3d,
+    get_gaussian_kernel3d_t,
     get_hanning_kernel1d,
     get_hanning_kernel2d,
     get_laplacian_kernel1d,
     get_laplacian_kernel2d,
     get_sobel_kernel2d,
     get_spatial_gradient_kernel2d,
     get_spatial_gradient_kernel3d,
@@ -28,30 +44,33 @@
 from .motion import MotionBlur, MotionBlur3D, motion_blur, motion_blur3d
 from .sobel import Sobel, SpatialGradient, SpatialGradient3d, sobel, spatial_gradient, spatial_gradient3d
 from .unsharp import UnsharpMask, unsharp_mask
 
 __all__ = [
     "gaussian",
     "get_binary_kernel2d",
+    "get_box_kernel1d",
     "get_box_kernel2d",
     "get_gaussian_kernel1d",
     "get_gaussian_discrete_kernel1d",
     "get_gaussian_erf_kernel1d",
     "get_gaussian_kernel2d",
+    "get_gaussian_kernel3d",
     "get_hanning_kernel1d",
     "get_hanning_kernel2d",
     "get_laplacian_kernel1d",
     "get_laplacian_kernel2d",
     "get_motion_kernel2d",
     "get_motion_kernel3d",
     "get_spatial_gradient_kernel2d",
     "get_spatial_gradient_kernel3d",
     "get_sobel_kernel2d",
     "get_diff_kernel2d",
     "gaussian_blur2d",
+    "guided_blur",
     "laplacian",
     "laplacian_1d",
     "unsharp_mask",
     "sobel",
     "spatial_gradient",
     "canny",
     "box_blur",
@@ -67,15 +86,25 @@
     "Laplacian",
     "SpatialGradient",
     "Sobel",
     "Canny",
     "BoxBlur",
     "BlurPool2D",
     "MaxBlurPool2D",
+    "EdgeAwareBlurPool2D",
     "MedianBlur",
     "MotionBlur",
     "MotionBlur3D",
     "SpatialGradient3d",
     "spatial_gradient3d",
     "UnsharpMask",
     "DexiNed",
+    "gaussian_blur2d_t",
+    "get_gaussian_kernel1d_t",
+    "get_gaussian_kernel2d_t",
+    "get_gaussian_kernel3d_t",
+    "bilateral_blur",
+    "joint_bilateral_blur",
+    "BilateralBlur",
+    "JointBilateralBlur",
+    "GuidedBlur",
 ]
```

### Comparing `kornia-0.6.9/kornia/filters/blur.py` & `kornia-0.7.0/kornia/filters/blur.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-from typing import Tuple
+from __future__ import annotations
 
-import torch
-import torch.nn as nn
+from kornia.core import Module, Tensor
+from kornia.core.check import KORNIA_CHECK_IS_TENSOR
 
-from .filter import filter2d
-from .kernels import get_box_kernel2d, normalize_kernel2d
+from .filter import filter2d, filter2d_separable
+from .kernels import _unpack_2d_ks, get_box_kernel1d, get_box_kernel2d
 
 
 def box_blur(
-    input: torch.Tensor, kernel_size: Tuple[int, int], border_type: str = 'reflect', normalized: bool = True
-) -> torch.Tensor:
+    input: Tensor, kernel_size: tuple[int, int] | int, border_type: str = 'reflect', separable: bool = False
+) -> Tensor:
     r"""Blur an image using the box filter.
 
     .. image:: _static/img/box_blur.png
 
     The function smooths an image using the kernel:
 
     .. math::
@@ -26,36 +26,44 @@
         \end{bmatrix}
 
     Args:
         image: the image to blur with shape :math:`(B,C,H,W)`.
         kernel_size: the blurring kernel size.
         border_type: the padding mode to be applied before convolving.
           The expected modes are: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
-        normalized: if True, L1 norm of the kernel is set to 1.
+        separable: run as composition of two 1d-convolutions.
 
     Returns:
         the blurred tensor with shape :math:`(B,C,H,W)`.
 
     .. note::
        See a working example `here <https://kornia-tutorials.readthedocs.io/en/latest/
        filtering_operators.html>`__.
 
     Example:
         >>> input = torch.rand(2, 4, 5, 7)
         >>> output = box_blur(input, (3, 3))  # 2x4x5x7
         >>> output.shape
         torch.Size([2, 4, 5, 7])
     """
-    kernel: torch.Tensor = get_box_kernel2d(kernel_size)
-    if normalized:
-        kernel = normalize_kernel2d(kernel)
-    return filter2d(input, kernel, border_type)
+    KORNIA_CHECK_IS_TENSOR(input)
 
+    if separable:
+        ky, kx = _unpack_2d_ks(kernel_size)
+        kernel_y = get_box_kernel1d(ky, device=input.device, dtype=input.dtype)
+        kernel_x = get_box_kernel1d(kx, device=input.device, dtype=input.dtype)
+        out = filter2d_separable(input, kernel_x, kernel_y, border_type)
+    else:
+        kernel = get_box_kernel2d(kernel_size, device=input.device, dtype=input.dtype)
+        out = filter2d(input, kernel, border_type)
 
-class BoxBlur(nn.Module):
+    return out
+
+
+class BoxBlur(Module):
     r"""Blur an image using the box filter.
 
     The function smooths an image using the kernel:
 
     .. math::
         K = \frac{1}{\text{kernel_size}_x * \text{kernel_size}_y}
         \begin{bmatrix}
@@ -66,15 +74,15 @@
         \end{bmatrix}
 
     Args:
         kernel_size: the blurring kernel size.
         border_type: the padding mode to be applied before convolving.
           The expected modes are: ``'constant'``, ``'reflect'``,
           ``'replicate'`` or ``'circular'``. Default: ``'reflect'``.
-        normalized: if True, L1 norm of the kernel is set to 1.
+        separable: run as composition of two 1d-convolutions.
 
     Returns:
         the blurred input tensor.
 
     Shape:
         - Input: :math:`(B, C, H, W)`
         - Output: :math:`(B, C, H, W)`
@@ -83,29 +91,38 @@
         >>> input = torch.rand(2, 4, 5, 7)
         >>> blur = BoxBlur((3, 3))
         >>> output = blur(input)  # 2x4x5x7
         >>> output.shape
         torch.Size([2, 4, 5, 7])
     """
 
-    def __init__(self, kernel_size: Tuple[int, int], border_type: str = 'reflect', normalized: bool = True) -> None:
+    def __init__(
+        self, kernel_size: tuple[int, int] | int, border_type: str = 'reflect', separable: bool = False
+    ) -> None:
         super().__init__()
-        self.kernel_size: Tuple[int, int] = kernel_size
-        self.border_type: str = border_type
-        self.normalized: bool = normalized
+        self.kernel_size = kernel_size
+        self.border_type = border_type
+        self.separable = separable
+
+        if separable:
+            ky, kx = _unpack_2d_ks(self.kernel_size)
+            self.register_buffer("kernel_y", get_box_kernel1d(ky))
+            self.register_buffer("kernel_x", get_box_kernel1d(kx))
+            self.kernel_y: Tensor
+            self.kernel_x: Tensor
+        else:
+            self.register_buffer("kernel", get_box_kernel2d(kernel_size))
+            self.kernel: Tensor
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '(kernel_size='
-            + str(self.kernel_size)
-            + ', '
-            + 'normalized='
-            + str(self.normalized)
-            + ', '
-            + 'border_type='
-            + self.border_type
-            + ')'
+            f"{self.__class__.__name__}"
+            f"(kernel_size={self.kernel_size}, "
+            f"border_type={self.border_type}, "
+            f"separable={self.separable})"
         )
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
-        return box_blur(input, self.kernel_size, self.border_type, self.normalized)
+    def forward(self, input: Tensor) -> Tensor:
+        KORNIA_CHECK_IS_TENSOR(input)
+        if self.separable:
+            return filter2d_separable(input, self.kernel_x, self.kernel_y, self.border_type)
+        return filter2d(input, self.kernel, self.border_type)
```

### Comparing `kornia-0.6.9/kornia/filters/blur_pool.py` & `kornia-0.7.0/kornia/filters/blur_pool.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,25 @@
-from typing import Tuple
+from __future__ import annotations
 
-import torch
 import torch.nn.functional as F
 
-from kornia.core import Module, Tensor
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
+from kornia.core import Module, Tensor, as_tensor, pad, tensor
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 
 from .kernels import get_pascal_kernel_2d
 from .median import _compute_zero_padding  # TODO: Move to proper place
 
-__all__ = ["BlurPool2D", "MaxBlurPool2D", "blur_pool2d", "max_blur_pool2d", 'edge_aware_blur_pool2d']
+__all__ = [
+    "BlurPool2D",
+    "MaxBlurPool2D",
+    "EdgeAwareBlurPool2D",
+    "blur_pool2d",
+    "max_blur_pool2d",
+    'edge_aware_blur_pool2d',
+]
 
 
 class BlurPool2D(Module):
     r"""Compute blur (anti-aliasing) and downsample a given feature map.
 
     See :cite:`zhang2019shiftinvar` for more details.
 
@@ -39,24 +45,23 @@
         >>> bp = BlurPool2D(kernel_size=3, stride=2)
         >>> bp(input)
         tensor([[[[0.3125, 0.0625, 0.0000],
                   [0.0625, 0.3750, 0.0625],
                   [0.0000, 0.0625, 0.3125]]]])
     """
 
-    def __init__(self, kernel_size: int, stride: int = 2):
+    def __init__(self, kernel_size: tuple[int, int] | int, stride: int = 2) -> None:
         super().__init__()
         self.kernel_size = kernel_size
         self.stride = stride
-        self.register_buffer('kernel', get_pascal_kernel_2d(kernel_size, norm=True))
+        self.kernel = get_pascal_kernel_2d(kernel_size, norm=True)
 
     def forward(self, input: Tensor) -> Tensor:
-        # To align the logic with the whole lib
-        kernel = torch.as_tensor(self.kernel, device=input.device, dtype=input.dtype)
-        return _blur_pool_by_kernel2d(input, kernel.repeat((input.shape[1], 1, 1, 1)), self.stride)
+        self.kernel = as_tensor(self.kernel, device=input.device, dtype=input.dtype)
+        return _blur_pool_by_kernel2d(input, self.kernel.repeat((input.shape[1], 1, 1, 1)), self.stride)
 
 
 class MaxBlurPool2D(Module):
     r"""Compute pools and blurs and downsample a given feature map.
 
     Equivalent to ```nn.Sequential(nn.MaxPool2d(...), BlurPool2D(...))```
 
@@ -85,31 +90,47 @@
                   [0.3125, 0.8750]]]])
         >>> seq = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=1), BlurPool2D(kernel_size=3, stride=2))
         >>> seq(input)
         tensor([[[[0.5625, 0.3125],
                   [0.3125, 0.8750]]]])
     """
 
-    def __init__(self, kernel_size: int, stride: int = 2, max_pool_size: int = 2, ceil_mode: bool = False):
+    def __init__(
+        self, kernel_size: tuple[int, int] | int, stride: int = 2, max_pool_size: int = 2, ceil_mode: bool = False
+    ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
         self.stride = stride
         self.max_pool_size = max_pool_size
         self.ceil_mode = ceil_mode
-        self.register_buffer('kernel', get_pascal_kernel_2d(kernel_size, norm=True))
+        self.kernel = get_pascal_kernel_2d(kernel_size, norm=True)
 
     def forward(self, input: Tensor) -> Tensor:
-        # To align the logic with the whole lib
-        kernel = torch.as_tensor(self.kernel, device=input.device, dtype=input.dtype)
+        self.kernel = as_tensor(self.kernel, device=input.device, dtype=input.dtype)
         return _max_blur_pool_by_kernel2d(
-            input, kernel.repeat((input.size(1), 1, 1, 1)), self.stride, self.max_pool_size, self.ceil_mode
+            input, self.kernel.repeat((input.size(1), 1, 1, 1)), self.stride, self.max_pool_size, self.ceil_mode
         )
 
 
-def blur_pool2d(input: Tensor, kernel_size: int, stride: int = 2):
+class EdgeAwareBlurPool2D(Module):
+    def __init__(
+        self, kernel_size: tuple[int, int] | int, edge_threshold: float = 1.25, edge_dilation_kernel_size: int = 3
+    ) -> None:
+        super().__init__()
+        self.kernel_size = kernel_size
+        self.edge_threshold = edge_threshold
+        self.edge_dilation_kernel_size = edge_dilation_kernel_size
+
+    def forward(self, input: Tensor, epsilon: float = 1e-6) -> Tensor:
+        return edge_aware_blur_pool2d(
+            input, self.kernel_size, self.edge_threshold, self.edge_dilation_kernel_size, epsilon
+        )
+
+
+def blur_pool2d(input: Tensor, kernel_size: tuple[int, int] | int, stride: int = 2) -> Tensor:
     r"""Compute blurs and downsample a given feature map.
 
     .. image:: _static/img/blur_pool2d.png
 
     See :class:`~kornia.filters.BlurPool2D` for details.
 
     See :cite:`zhang2019shiftinvar` for more details.
@@ -143,20 +164,22 @@
     Examples:
         >>> input = torch.eye(5)[None, None]
         >>> blur_pool2d(input, 3)
         tensor([[[[0.3125, 0.0625, 0.0000],
                   [0.0625, 0.3750, 0.0625],
                   [0.0000, 0.0625, 0.3125]]]])
     """
-    kernel = get_pascal_kernel_2d(kernel_size, norm=True).repeat((input.size(1), 1, 1, 1)).to(input)
+    kernel = get_pascal_kernel_2d(kernel_size, norm=True, device=input.device, dtype=input.dtype).repeat(
+        (input.size(1), 1, 1, 1)
+    )
     return _blur_pool_by_kernel2d(input, kernel, stride)
 
 
 def max_blur_pool2d(
-    input: Tensor, kernel_size: int, stride: int = 2, max_pool_size: int = 2, ceil_mode: bool = False
+    input: Tensor, kernel_size: tuple[int, int] | int, stride: int = 2, max_pool_size: int = 2, ceil_mode: bool = False
 ) -> Tensor:
     r"""Compute pools and blurs and downsample a given feature map.
 
     .. image:: _static/img/max_blur_pool2d.png
 
     See :class:`~kornia.filters.MaxBlurPool2D` for details.
 
@@ -175,42 +198,51 @@
 
     Examples:
         >>> input = torch.eye(5)[None, None]
         >>> max_blur_pool2d(input, 3)
         tensor([[[[0.5625, 0.3125],
                   [0.3125, 0.8750]]]])
     """
-    if not len(input.shape) == 4:
-        raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
-    kernel = get_pascal_kernel_2d(kernel_size, norm=True).repeat((input.shape[1], 1, 1, 1)).to(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])
+
+    kernel = get_pascal_kernel_2d(kernel_size, norm=True, device=input.device, dtype=input.dtype).repeat(
+        (input.shape[1], 1, 1, 1)
+    )
     return _max_blur_pool_by_kernel2d(input, kernel, stride, max_pool_size, ceil_mode)
 
 
-def _blur_pool_by_kernel2d(input: Tensor, kernel: Tensor, stride: int):
+def _blur_pool_by_kernel2d(input: Tensor, kernel: Tensor, stride: int) -> Tensor:
     """Compute blur_pool by a given :math:`CxC_{out}xNxN` kernel."""
-    if not (len(kernel.shape) == 4 and kernel.shape[-1] == kernel.shape[-2]):
-        raise AssertionError(f"Invalid kernel shape. Expect CxC_outxNxN, Got {kernel.shape}")
-    padding: Tuple[int, int] = _compute_zero_padding((kernel.shape[-2], kernel.shape[-1]))
+    KORNIA_CHECK(
+        len(kernel.shape) == 4 and kernel.shape[-2] == kernel.shape[-1],
+        f"Invalid kernel shape. Expect CxC_(out, None)xNxN, Got {kernel.shape}",
+    )
+
+    padding = _compute_zero_padding((kernel.shape[-2], kernel.shape[-1]))
     return F.conv2d(input, kernel, padding=padding, stride=stride, groups=input.shape[1])
 
 
-def _max_blur_pool_by_kernel2d(input: Tensor, kernel: Tensor, stride: int, max_pool_size: int, ceil_mode: bool):
-    """Compute max_blur_pool by a given :math:`CxC_{out}xNxN` kernel."""
-    if not (len(kernel.shape) == 4 and kernel.shape[-1] == kernel.shape[-2]):
-        raise AssertionError(f"Invalid kernel shape. Expect CxC_outxNxN, Got {kernel.shape}")
+def _max_blur_pool_by_kernel2d(
+    input: Tensor, kernel: Tensor, stride: int, max_pool_size: int, ceil_mode: bool
+) -> Tensor:
+    """Compute max_blur_pool by a given :math:`CxC_(out, None)xNxN` kernel."""
+    KORNIA_CHECK(
+        len(kernel.shape) == 4 and kernel.shape[-2] == kernel.shape[-1],
+        f"Invalid kernel shape. Expect CxC_outxNxN, Got {kernel.shape}",
+    )
     # compute local maxima
     input = F.max_pool2d(input, kernel_size=max_pool_size, padding=0, stride=1, ceil_mode=ceil_mode)
     # blur and downsample
-    padding: Tuple[int, int] = _compute_zero_padding((kernel.shape[-2], kernel.shape[-1]))
+    padding = _compute_zero_padding((kernel.shape[-2], kernel.shape[-1]))
     return F.conv2d(input, kernel, padding=padding, stride=stride, groups=input.size(1))
 
 
 def edge_aware_blur_pool2d(
     input: Tensor,
-    kernel_size: int,
+    kernel_size: tuple[int, int] | int,
     edge_threshold: float = 1.25,
     edge_dilation_kernel_size: int = 3,
     epsilon: float = 1e-6,
 ) -> Tensor:
     r"""Blur the input tensor while maintaining its edges.
 
     Args:
@@ -222,22 +254,22 @@
 
     Returns:
         The blurred tensor of shape :math:`(B, C, H, W)`.
     """
     KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
     KORNIA_CHECK(edge_threshold > 0.0, f"edge threshold should be positive, but got '{edge_threshold}'")
 
-    input = F.pad(input, (2, 2, 2, 2), mode="reflect")  # pad to avoid artifacts near physical edges
+    input = pad(input, (2, 2, 2, 2), mode="reflect")  # pad to avoid artifacts near physical edges
     blurred_input = blur_pool2d(input, kernel_size=kernel_size, stride=1)  # blurry version of the input
 
     # calculate the edges (add epsilon to avoid taking the log of 0)
-    log_input, log_thresh = torch.log2(input + epsilon), torch.log2(torch.tensor(edge_threshold))
+    log_input, log_thresh = (input + epsilon).log2(), (tensor(edge_threshold)).log2()
     edges_x = log_input[..., :, 4:] - log_input[..., :, :-4]
     edges_y = log_input[..., 4:, :] - log_input[..., :-4, :]
-    edges_x, edges_y = torch.mean(edges_x, dim=-3, keepdim=True), torch.mean(edges_y, dim=-3, keepdim=True)
+    edges_x, edges_y = edges_x.mean(dim=-3, keepdim=True), edges_y.mean(dim=-3, keepdim=True)
     edges_x_mask, edges_y_mask = edges_x.abs() > log_thresh.to(edges_x), edges_y.abs() > log_thresh.to(edges_y)
     edges_xy_mask = (edges_x_mask[..., 2:-2, :] + edges_y_mask[..., :, 2:-2]).type_as(input)
 
     # dilate the content edges to have a soft mask of edges
     dilated_edges = F.max_pool3d(edges_xy_mask, edge_dilation_kernel_size, 1, edge_dilation_kernel_size // 2)
 
     # slice the padded regions
```

### Comparing `kornia-0.6.9/kornia/filters/canny.py` & `kornia-0.7.0/kornia/filters/canny.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,32 @@
+from __future__ import annotations
+
 import math
-from typing import Tuple
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
 from kornia.color import rgb_to_grayscale
+from kornia.core import Module, Tensor
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 
 from .gaussian import gaussian_blur2d
 from .kernels import get_canny_nms_kernel, get_hysteresis_kernel
 from .sobel import spatial_gradient
 
 
 def canny(
-    input: torch.Tensor,
+    input: Tensor,
     low_threshold: float = 0.1,
     high_threshold: float = 0.2,
-    kernel_size: Tuple[int, int] = (5, 5),
-    sigma: Tuple[float, float] = (1, 1),
+    kernel_size: tuple[int, int] | int = (5, 5),
+    sigma: tuple[float, float] | Tensor = (1, 1),
     hysteresis: bool = True,
     eps: float = 1e-6,
-) -> Tuple[torch.Tensor, torch.Tensor]:
+) -> tuple[Tensor, Tensor]:
     r"""Find edges of the input image and filters them using the Canny algorithm.
 
     .. image:: _static/img/canny.png
 
     Args:
         input: input image tensor with shape :math:`(B,C,H,W)`.
         low_threshold: lower threshold for the hysteresis procedure.
@@ -47,116 +49,107 @@
         >>> input = torch.rand(5, 3, 4, 4)
         >>> magnitude, edges = canny(input)  # 5x3x4x4
         >>> magnitude.shape
         torch.Size([5, 1, 4, 4])
         >>> edges.shape
         torch.Size([5, 1, 4, 4])
     """
-    if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
-
-    if not len(input.shape) == 4:
-        raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
-
-    if low_threshold > high_threshold:
-        raise ValueError(
-            "Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: {}>{}".format(
-                low_threshold, high_threshold
-            )
-        )
-
-    if low_threshold < 0 and low_threshold > 1:
-        raise ValueError(f"Invalid input threshold. low_threshold should be in range (0,1). Got: {low_threshold}")
-
-    if high_threshold < 0 and high_threshold > 1:
-        raise ValueError(f"Invalid input threshold. high_threshold should be in range (0,1). Got: {high_threshold}")
+    KORNIA_CHECK_IS_TENSOR(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])
+    KORNIA_CHECK(
+        low_threshold <= high_threshold,
+        "Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: "
+        f"{low_threshold}>{high_threshold}",
+    )
+    KORNIA_CHECK(0 < low_threshold < 1, f'Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}')
+    KORNIA_CHECK(0 < high_threshold < 1, f'Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}')
 
-    device: torch.device = input.device
-    dtype: torch.dtype = input.dtype
+    device = input.device
+    dtype = input.dtype
 
     # To Grayscale
     if input.shape[1] == 3:
         input = rgb_to_grayscale(input)
 
     # Gaussian filter
-    blurred: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma)
+    blurred: Tensor = gaussian_blur2d(input, kernel_size, sigma)
 
     # Compute the gradients
-    gradients: torch.Tensor = spatial_gradient(blurred, normalized=False)
+    gradients: Tensor = spatial_gradient(blurred, normalized=False)
 
     # Unpack the edges
-    gx: torch.Tensor = gradients[:, :, 0]
-    gy: torch.Tensor = gradients[:, :, 1]
+    gx: Tensor = gradients[:, :, 0]
+    gy: Tensor = gradients[:, :, 1]
 
     # Compute gradient magnitude and angle
-    magnitude: torch.Tensor = torch.sqrt(gx * gx + gy * gy + eps)
-    angle: torch.Tensor = torch.atan2(gy, gx)
+    magnitude: Tensor = torch.sqrt(gx * gx + gy * gy + eps)
+    angle: Tensor = torch.atan2(gy, gx)
 
     # Radians to Degrees
     angle = 180.0 * angle / math.pi
 
     # Round angle to the nearest 45 degree
     angle = torch.round(angle / 45) * 45
 
     # Non-maximal suppression
-    nms_kernels: torch.Tensor = get_canny_nms_kernel(device, dtype)
-    nms_magnitude: torch.Tensor = F.conv2d(magnitude, nms_kernels, padding=nms_kernels.shape[-1] // 2)
+    nms_kernels: Tensor = get_canny_nms_kernel(device, dtype)
+    nms_magnitude: Tensor = F.conv2d(magnitude, nms_kernels, padding=nms_kernels.shape[-1] // 2)
 
     # Get the indices for both directions
-    positive_idx: torch.Tensor = (angle / 45) % 8
+    positive_idx: Tensor = (angle / 45) % 8
     positive_idx = positive_idx.long()
 
-    negative_idx: torch.Tensor = ((angle / 45) + 4) % 8
+    negative_idx: Tensor = ((angle / 45) + 4) % 8
     negative_idx = negative_idx.long()
 
     # Apply the non-maximum suppression to the different directions
-    channel_select_filtered_positive: torch.Tensor = torch.gather(nms_magnitude, 1, positive_idx)
-    channel_select_filtered_negative: torch.Tensor = torch.gather(nms_magnitude, 1, negative_idx)
+    channel_select_filtered_positive: Tensor = torch.gather(nms_magnitude, 1, positive_idx)
+    channel_select_filtered_negative: Tensor = torch.gather(nms_magnitude, 1, negative_idx)
 
-    channel_select_filtered: torch.Tensor = torch.stack(
+    channel_select_filtered: Tensor = torch.stack(
         [channel_select_filtered_positive, channel_select_filtered_negative], 1
     )
 
-    is_max: torch.Tensor = channel_select_filtered.min(dim=1)[0] > 0.0
+    is_max: Tensor = channel_select_filtered.min(dim=1)[0] > 0.0
 
     magnitude = magnitude * is_max
 
     # Threshold
-    edges: torch.Tensor = F.threshold(magnitude, low_threshold, 0.0)
+    edges: Tensor = F.threshold(magnitude, low_threshold, 0.0)
 
-    low: torch.Tensor = magnitude > low_threshold
-    high: torch.Tensor = magnitude > high_threshold
+    low: Tensor = magnitude > low_threshold
+    high: Tensor = magnitude > high_threshold
 
     edges = low * 0.5 + high * 0.5
     edges = edges.to(dtype)
 
     # Hysteresis
     if hysteresis:
-        edges_old: torch.Tensor = -torch.ones(edges.shape, device=edges.device, dtype=dtype)
-        hysteresis_kernels: torch.Tensor = get_hysteresis_kernel(device, dtype)
+        edges_old: Tensor = -torch.ones(edges.shape, device=edges.device, dtype=dtype)
+        hysteresis_kernels: Tensor = get_hysteresis_kernel(device, dtype)
 
         while ((edges_old - edges).abs() != 0).any():
-            weak: torch.Tensor = (edges == 0.5).float()
-            strong: torch.Tensor = (edges == 1).float()
+            weak: Tensor = (edges == 0.5).float()
+            strong: Tensor = (edges == 1).float()
 
-            hysteresis_magnitude: torch.Tensor = F.conv2d(
+            hysteresis_magnitude: Tensor = F.conv2d(
                 edges, hysteresis_kernels, padding=hysteresis_kernels.shape[-1] // 2
             )
             hysteresis_magnitude = (hysteresis_magnitude == 1).any(1, keepdim=True).to(dtype)
             hysteresis_magnitude = hysteresis_magnitude * weak + strong
 
             edges_old = edges.clone()
             edges = hysteresis_magnitude + (hysteresis_magnitude == 0) * weak * 0.5
 
         edges = hysteresis_magnitude
 
     return magnitude, edges
 
 
-class Canny(nn.Module):
+class Canny(Module):
     r"""Module that finds edges of the input image and filters them using the Canny algorithm.
 
     Args:
         input: input image tensor with shape :math:`(B,C,H,W)`.
         low_threshold: lower threshold for the hysteresis procedure.
         high_threshold: upper threshold for the hysteresis procedure.
         kernel_size: the size of the kernel for the gaussian blur.
@@ -178,34 +171,30 @@
         torch.Size([5, 1, 4, 4])
     """
 
     def __init__(
         self,
         low_threshold: float = 0.1,
         high_threshold: float = 0.2,
-        kernel_size: Tuple[int, int] = (5, 5),
-        sigma: Tuple[float, float] = (1, 1),
+        kernel_size: tuple[int, int] | int = (5, 5),
+        sigma: tuple[float, float] | Tensor = (1, 1),
         hysteresis: bool = True,
         eps: float = 1e-6,
     ) -> None:
         super().__init__()
 
-        if low_threshold > high_threshold:
-            raise ValueError(
-                "Invalid input thresholds. low_threshold should be\
-                             smaller than the high_threshold. Got: {}>{}".format(
-                    low_threshold, high_threshold
-                )
-            )
-
-        if low_threshold < 0 or low_threshold > 1:
-            raise ValueError(f"Invalid input threshold. low_threshold should be in range (0,1). Got: {low_threshold}")
-
-        if high_threshold < 0 or high_threshold > 1:
-            raise ValueError(f"Invalid input threshold. high_threshold should be in range (0,1). Got: {high_threshold}")
+        KORNIA_CHECK(
+            low_threshold <= high_threshold,
+            "Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: "
+            f"{low_threshold}>{high_threshold}",
+        )
+        KORNIA_CHECK(0 < low_threshold < 1, f'Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}')
+        KORNIA_CHECK(
+            0 < high_threshold < 1, f'Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}'
+        )
 
         # Gaussian blur parameters
         self.kernel_size = kernel_size
         self.sigma = sigma
 
         # Double threshold
         self.low_threshold = low_threshold
@@ -223,11 +212,11 @@
                 ', '.join(
                     f'{name}={getattr(self, name)}' for name in sorted(self.__dict__) if not name.startswith('_')
                 ),
                 ')',
             )
         )
 
-    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+    def forward(self, input: Tensor) -> tuple[Tensor, Tensor]:
         return canny(
             input, self.low_threshold, self.high_threshold, self.kernel_size, self.sigma, self.hysteresis, self.eps
         )
```

### Comparing `kornia-0.6.9/kornia/filters/dexined.py` & `kornia-0.7.0/kornia/filters/dexined.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 # adapted from: https://github.com/xavysp/DexiNed/blob/d944b70eb6eaf40e22f8467c1e12919aa600d8e4/model.py
+
+from __future__ import annotations
+
 from collections import OrderedDict
-from typing import List
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 
 from kornia.core import Module, Tensor, concatenate
+from kornia.core.check import KORNIA_CHECK
 from kornia.utils import map_location_to_cpu
 
 url: str = "http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth"
 
 
-def weight_init(m):
+def weight_init(m: Module) -> None:
     if isinstance(m, (nn.Conv2d,)):
         # torch.nn.init.xavier_uniform_(m.weight, gain=1.0)
         torch.nn.init.xavier_normal_(m.weight, gain=1.0)
         # torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)
         if m.weight.data.shape[1] == torch.Size([1]):
             torch.nn.init.normal_(m.weight, mean=0.0)
 
@@ -32,15 +35,15 @@
         if m.weight.data.shape[1] == torch.Size([1]):
             torch.nn.init.normal_(m.weight, std=0.1)
         if m.bias is not None:
             torch.nn.init.zeros_(m.bias)
 
 
 class CoFusion(Module):
-    def __init__(self, in_ch, out_ch):
+    def __init__(self, in_ch: int, out_ch: int) -> None:
         super().__init__()
         self.conv1 = nn.Conv2d(in_ch, 64, kernel_size=3, stride=1, padding=1)
         self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)
         self.conv3 = nn.Conv2d(64, out_ch, kernel_size=3, stride=1, padding=1)
         self.relu = nn.ReLU()
         self.norm_layer1 = nn.GroupNorm(4, 64)
         self.norm_layer2 = nn.GroupNorm(4, 64)
@@ -48,105 +51,111 @@
     def forward(self, x: Tensor) -> Tensor:
         # fusecat = torch.cat(x, dim=1)
         attn = self.relu(self.norm_layer1(self.conv1(x)))
         attn = self.relu(self.norm_layer2(self.conv2(attn)))
         attn = F.softmax(self.conv3(attn), dim=1)
 
         # return ((fusecat * attn).sum(1)).unsqueeze(1)
-        return ((x * attn).sum(1)).unsqueeze(1)
+        return ((x * attn).sum(1))[:, None, ...]
 
 
 class _DenseLayer(nn.Sequential):
-    def __init__(self, input_features, out_features):
+    def __init__(self, input_features: int, out_features: int) -> None:
         super().__init__(
             OrderedDict(
                 [
                     ('relu1', nn.ReLU(inplace=True)),
                     ('conv1', nn.Conv2d(input_features, out_features, kernel_size=3, stride=1, padding=2, bias=True)),
                     ('norm1', nn.BatchNorm2d(out_features)),
                     ('relu2', nn.ReLU(inplace=True)),
                     ('conv2', nn.Conv2d(out_features, out_features, kernel_size=3, stride=1, bias=True)),
                     ('norm2', nn.BatchNorm2d(out_features)),
                 ]
             )
         )
 
-    def forward(self, x: List[Tensor]) -> List[Tensor]:
+    def forward(self, x: list[Tensor]) -> list[Tensor]:
         x1, x2 = x[0], x[1]
         x3: Tensor = x1
         for mod in self:
             x3 = mod(x3)
         return [0.5 * (x3 + x2), x2]
 
 
 class _DenseBlock(nn.Sequential):
-    def __init__(self, num_layers, input_features, out_features):
+    def __init__(self, num_layers: int, input_features: int, out_features: int) -> None:
         super().__init__()
         for i in range(num_layers):
             layer = _DenseLayer(input_features, out_features)
             self.add_module('denselayer%d' % (i + 1), layer)
             input_features = out_features
 
-    def forward(self, x: List[Tensor]) -> List[Tensor]:
+    def forward(self, x: list[Tensor]) -> list[Tensor]:
         x_out = x
         for mod in self:
             x_out = mod(x_out)
         return x_out
 
 
 class UpConvBlock(Module):
-    def __init__(self, in_features, up_scale):
+    def __init__(self, in_features: int, up_scale: int) -> None:
         super().__init__()
         self.up_factor = 2
         self.constant_features = 16
 
         layers = self.make_deconv_layers(in_features, up_scale)
-        if layers is None:
-            raise Exception("layers cannot be none")
+        KORNIA_CHECK(layers is not None, "layers cannot be none")
         self.features = nn.Sequential(*layers)
 
-    def make_deconv_layers(self, in_features: int, up_scale: int) -> List[Module]:
-        layers: List[Module] = []
+    def make_deconv_layers(self, in_features: int, up_scale: int) -> list[Module]:
+        layers: list[Module] = []
         all_pads = [0, 0, 1, 3, 7]
         for i in range(up_scale):
             kernel_size = 2**up_scale
             pad = all_pads[up_scale]  # kernel_size-1
             out_features = self.compute_out_features(i, up_scale)
             layers.append(nn.Conv2d(in_features, out_features, 1))
             layers.append(nn.ReLU(inplace=True))
             layers.append(nn.ConvTranspose2d(out_features, out_features, kernel_size, stride=2, padding=pad))
             in_features = out_features
         return layers
 
-    def compute_out_features(self, idx: int, up_scale: int):
+    def compute_out_features(self, idx: int, up_scale: int) -> int:
         return 1 if idx == up_scale - 1 else self.constant_features
 
-    def forward(self, x: Tensor, out_shape: List[int]) -> Tensor:
+    def forward(self, x: Tensor, out_shape: list[int]) -> Tensor:
         out = self.features(x)
         if out.shape[-2:] != out_shape:
             out = F.interpolate(out, out_shape, mode='bilinear')
         return out
 
 
 class SingleConvBlock(Module):
-    def __init__(self, in_features, out_features, stride, use_bs=True):
+    def __init__(self, in_features: int, out_features: int, stride: int, use_bs: bool = True) -> None:
         super().__init__()
         self.use_bn = use_bs
         self.conv = nn.Conv2d(in_features, out_features, 1, stride=stride, bias=True)
         self.bn = nn.BatchNorm2d(out_features)
 
     def forward(self, x: Tensor) -> Tensor:
         x = self.conv(x)
         if self.use_bn:
             x = self.bn(x)
         return x
 
 
 class DoubleConvBlock(nn.Sequential):
-    def __init__(self, in_features, mid_features, out_features=None, stride=1, use_act=True):
+    def __init__(
+        self,
+        in_features: int,
+        mid_features: int,
+        out_features: int | None = None,
+        stride: int = 1,
+        use_act: bool = True,
+    ) -> None:
         super().__init__()
         if out_features is None:
             out_features = mid_features
         self.add_module("conv1", nn.Conv2d(in_features, mid_features, 3, padding=1, stride=stride))
         self.add_module("bn1", nn.BatchNorm2d(mid_features))
         self.add_module("relu1", nn.ReLU(inplace=True))
         self.add_module("conv2", nn.Conv2d(mid_features, out_features, 3, padding=1))
@@ -166,15 +175,15 @@
         >>> img = torch.rand(1, 3, 320, 320)
         >>> net = DexiNed(pretrained=False)
         >>> out = net(img)
         >>> out[-1].shape
         torch.Size([1, 1, 320, 320])
     """
 
-    def __init__(self, pretrained: bool):
+    def __init__(self, pretrained: bool) -> None:
         super().__init__()
         self.block_1 = DoubleConvBlock(3, 32, 64, stride=2)
         self.block_2 = DoubleConvBlock(64, 128, use_act=False)
         self.dblock_3 = _DenseBlock(2, 128, 256)  # [128,256,100,100]
         self.dblock_4 = _DenseBlock(3, 256, 512)
         self.dblock_5 = _DenseBlock(3, 512, 512)
         self.dblock_6 = _DenseBlock(3, 512, 256)
@@ -205,21 +214,21 @@
         # self.block_cat = CoFusion(6,6)# cats fusion method
 
         if pretrained:
             self.load_from_file(url)
         else:
             self.apply(weight_init)
 
-    def load_from_file(self, path_file: str):
+    def load_from_file(self, path_file: str) -> None:
         # use torch.hub to load pretrained model
         pretrained_dict = torch.hub.load_state_dict_from_url(path_file, map_location=map_location_to_cpu)
         self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
 
-    def forward(self, x: Tensor) -> List[Tensor]:
+    def forward(self, x: Tensor) -> list[Tensor]:
         # Block 1
         block_1 = self.block_1(x)
         block_1_side = self.side_1(block_1)
 
         # Block 2
         block_2 = self.block_2(block_1)
         block_2_down = self.maxpool(block_2)
```

### Comparing `kornia-0.6.9/kornia/filters/filter.py` & `kornia-0.7.0/kornia/filters/filter.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,16 +1,22 @@
-from typing import List
+from __future__ import annotations
 
-import torch
 import torch.nn.functional as F
 
+from kornia.core import Tensor, pad
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
+
 from .kernels import normalize_kernel2d
 
+_VALID_BORDERS = {'constant', 'reflect', 'replicate', 'circular'}
+_VALID_PADDING = {'valid', 'same'}
+_VALID_BEHAVIOUR = {'conv', 'corr'}
+
 
-def _compute_padding(kernel_size: List[int]) -> List[int]:
+def _compute_padding(kernel_size: list[int]) -> list[int]:
     """Compute padding tuple."""
     # 4 or 6 ints:  (padding_left, padding_right,padding_top,padding_bottom)
     # https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad
     if len(kernel_size) < 2:
         raise AssertionError(kernel_size)
     computed = [k - 1 for k in kernel_size]
 
@@ -26,20 +32,21 @@
         out_padding[2 * i + 0] = pad_front
         out_padding[2 * i + 1] = pad_rear
 
     return out_padding
 
 
 def filter2d(
-    input: torch.Tensor,
-    kernel: torch.Tensor,
+    input: Tensor,
+    kernel: Tensor,
     border_type: str = 'reflect',
     normalized: bool = False,
     padding: str = 'same',
-) -> torch.Tensor:
+    behaviour: str = 'corr',
+) -> Tensor:
     r"""Convolve a tensor with a 2d kernel.
 
     The function applies a given kernel to a tensor. The kernel is applied
     independently at each depth channel of the tensor. Before applying the
     kernel, the function applies padding according to the specified mode so
     that the output remains in the same shape.
 
@@ -50,17 +57,20 @@
           tensor. The kernel shape must be :math:`(1, kH, kW)` or :math:`(B, kH, kW)`.
         border_type: the padding mode to be applied before convolving.
           The expected modes are: ``'constant'``, ``'reflect'``,
           ``'replicate'`` or ``'circular'``.
         normalized: If True, kernel will be L1 normalized.
         padding: This defines the type of padding.
           2 modes available ``'same'`` or ``'valid'``.
+        behaviour: defines the convolution mode -- correlation (default), using pytorch conv2d,
+        or true convolution (kernel is flipped). 2 modes available ``'corr'`` or ``'conv'``.
+
 
     Return:
-        torch.Tensor: the convolved tensor of same size and numbers of channels
+        Tensor: the convolved tensor of same size and numbers of channels
         as the input with shape :math:`(B, C, H, W)`.
 
     Example:
         >>> input = torch.tensor([[[
         ...    [0., 0., 0., 0., 0.],
         ...    [0., 0., 0., 0., 0.],
         ...    [0., 0., 5., 0., 0.],
@@ -70,56 +80,50 @@
         >>> filter2d(input, kernel, padding='same')
         tensor([[[[0., 0., 0., 0., 0.],
                   [0., 5., 5., 5., 0.],
                   [0., 5., 5., 5., 0.],
                   [0., 5., 5., 5., 0.],
                   [0., 0., 0., 0., 0.]]]])
     """
-    if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Input input is not torch.Tensor. Got {type(input)}")
-
-    if not isinstance(kernel, torch.Tensor):
-        raise TypeError(f"Input kernel is not torch.Tensor. Got {type(kernel)}")
-
-    if not isinstance(border_type, str):
-        raise TypeError(f"Input border_type is not string. Got {type(border_type)}")
-
-    if border_type not in ['constant', 'reflect', 'replicate', 'circular']:
-        raise ValueError(
-            f"Invalid border type, we expect 'constant', \
-        'reflect', 'replicate', 'circular'. Got:{border_type}"
-        )
-
-    if not isinstance(padding, str):
-        raise TypeError(f"Input padding is not string. Got {type(padding)}")
-
-    if padding not in ['valid', 'same']:
-        raise ValueError(f"Invalid padding mode, we expect 'valid' or 'same'. Got: {padding}")
-
-    if not len(input.shape) == 4:
-        raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
-
-    if (not len(kernel.shape) == 3) and not ((kernel.shape[0] == 0) or (kernel.shape[0] == input.shape[0])):
-        raise ValueError(f"Invalid kernel shape, we expect 1xHxW or BxHxW. Got: {kernel.shape}")
-
+    KORNIA_CHECK_IS_TENSOR(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])
+    KORNIA_CHECK_IS_TENSOR(kernel)
+    KORNIA_CHECK_SHAPE(kernel, ['B', 'H', 'W'])
+
+    KORNIA_CHECK(
+        str(border_type).lower() in _VALID_BORDERS,
+        f'Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}',
+    )
+    KORNIA_CHECK(
+        str(padding).lower() in _VALID_PADDING,
+        f'Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}',
+    )
+    KORNIA_CHECK(
+        str(behaviour).lower() in _VALID_BEHAVIOUR,
+        f'Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}',
+    )
     # prepare kernel
     b, c, h, w = input.shape
-    tmp_kernel: torch.Tensor = kernel.unsqueeze(1).to(input)
+    if str(behaviour).lower() == 'conv':
+        tmp_kernel = kernel.flip((-2, -1))[:, None, ...].to(device=input.device, dtype=input.dtype)
+    else:
+        tmp_kernel = kernel[:, None, ...].to(device=input.device, dtype=input.dtype)
+        #  str(behaviour).lower() == 'conv':
 
     if normalized:
         tmp_kernel = normalize_kernel2d(tmp_kernel)
 
     tmp_kernel = tmp_kernel.expand(-1, c, -1, -1)
 
     height, width = tmp_kernel.shape[-2:]
 
     # pad the input tensor
     if padding == 'same':
-        padding_shape: List[int] = _compute_padding([height, width])
-        input = F.pad(input, padding_shape, mode=border_type)
+        padding_shape: list[int] = _compute_padding([height, width])
+        input = pad(input, padding_shape, mode=border_type)
 
     # kernel and input tensor reshape to align element-wise or batch-wise params
     tmp_kernel = tmp_kernel.reshape(-1, 1, height, width)
     input = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))
 
     # convolve the tensor with the kernel.
     output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)
@@ -129,21 +133,21 @@
     else:
         out = output.view(b, c, h - height + 1, w - width + 1)
 
     return out
 
 
 def filter2d_separable(
-    input: torch.Tensor,
-    kernel_x: torch.Tensor,
-    kernel_y: torch.Tensor,
+    input: Tensor,
+    kernel_x: Tensor,
+    kernel_y: Tensor,
     border_type: str = 'reflect',
     normalized: bool = False,
     padding: str = 'same',
-) -> torch.Tensor:
+) -> Tensor:
     r"""Convolve a tensor with two 1d kernels, in x and y directions.
 
     The function applies a given kernel to a tensor. The kernel is applied
     independently at each depth channel of the tensor. Before applying the
     kernel, the function applies padding according to the specified mode so
     that the output remains in the same shape.
 
@@ -158,15 +162,15 @@
           The expected modes are: ``'constant'``, ``'reflect'``,
           ``'replicate'`` or ``'circular'``.
         normalized: If True, kernel will be L1 normalized.
         padding: This defines the type of padding.
           2 modes available ``'same'`` or ``'valid'``.
 
     Return:
-        torch.Tensor: the convolved tensor of same size and numbers of channels
+        Tensor: the convolved tensor of same size and numbers of channels
         as the input with shape :math:`(B, C, H, W)`.
 
     Example:
         >>> input = torch.tensor([[[
         ...    [0., 0., 0., 0., 0.],
         ...    [0., 0., 0., 0., 0.],
         ...    [0., 0., 5., 0., 0.],
@@ -177,22 +181,20 @@
         >>> filter2d_separable(input, kernel, kernel, padding='same')
         tensor([[[[0., 0., 0., 0., 0.],
                   [0., 5., 5., 5., 0.],
                   [0., 5., 5., 5., 0.],
                   [0., 5., 5., 5., 0.],
                   [0., 0., 0., 0., 0.]]]])
     """
-    out_x = filter2d(input, kernel_x.unsqueeze(0), border_type, normalized, padding)
-    out = filter2d(out_x, kernel_y.unsqueeze(-1), border_type, normalized, padding)
+    out_x = filter2d(input, kernel_x[..., None, :], border_type, normalized, padding)
+    out = filter2d(out_x, kernel_y[..., None], border_type, normalized, padding)
     return out
 
 
-def filter3d(
-    input: torch.Tensor, kernel: torch.Tensor, border_type: str = 'replicate', normalized: bool = False
-) -> torch.Tensor:
+def filter3d(input: Tensor, kernel: Tensor, border_type: str = 'replicate', normalized: bool = False) -> Tensor:
     r"""Convolve a tensor with a 3d kernel.
 
     The function applies a given kernel to a tensor. The kernel is applied
     independently at each depth channel of the tensor. Before applying the
     kernel, the function applies padding according to the specified mode so
     that the output remains in the same shape.
 
@@ -244,43 +246,38 @@
         <BLANKLINE>
                   [[0., 0., 0., 0., 0.],
                    [0., 5., 5., 5., 0.],
                    [0., 5., 5., 5., 0.],
                    [0., 5., 5., 5., 0.],
                    [0., 0., 0., 0., 0.]]]]])
     """
-    if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Input border_type is not torch.Tensor. Got {type(input)}")
-
-    if not isinstance(kernel, torch.Tensor):
-        raise TypeError(f"Input border_type is not torch.Tensor. Got {type(kernel)}")
-
-    if not isinstance(border_type, str):
-        raise TypeError(f"Input border_type is not string. Got {type(kernel)}")
-
-    if not len(input.shape) == 5:
-        raise ValueError(f"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}")
-
-    if not len(kernel.shape) == 4 and kernel.shape[0] != 1:
-        raise ValueError(f"Invalid kernel shape, we expect 1xDxHxW. Got: {kernel.shape}")
+    KORNIA_CHECK_IS_TENSOR(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'D', 'H', 'W'])
+    KORNIA_CHECK_IS_TENSOR(kernel)
+    KORNIA_CHECK_SHAPE(kernel, ['B', 'D', 'H', 'W'])
+
+    KORNIA_CHECK(
+        str(border_type).lower() in _VALID_BORDERS,
+        f'Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}',
+    )
 
     # prepare kernel
     b, c, d, h, w = input.shape
-    tmp_kernel: torch.Tensor = kernel.unsqueeze(1).to(input)
+    tmp_kernel = kernel[:, None, ...].to(device=input.device, dtype=input.dtype)
 
     if normalized:
         bk, dk, hk, wk = kernel.shape
         tmp_kernel = normalize_kernel2d(tmp_kernel.view(bk, dk, hk * wk)).view_as(tmp_kernel)
 
     tmp_kernel = tmp_kernel.expand(-1, c, -1, -1, -1)
 
     # pad the input tensor
     depth, height, width = tmp_kernel.shape[-3:]
-    padding_shape: List[int] = _compute_padding([depth, height, width])
-    input_pad: torch.Tensor = F.pad(input, padding_shape, mode=border_type)
+    padding_shape: list[int] = _compute_padding([depth, height, width])
+    input_pad = pad(input, padding_shape, mode=border_type)
 
     # kernel and input tensor reshape to align element-wise or batch-wise params
     tmp_kernel = tmp_kernel.reshape(-1, 1, depth, height, width)
     input_pad = input_pad.view(-1, tmp_kernel.size(0), input_pad.size(-3), input_pad.size(-2), input_pad.size(-1))
 
     # convolve the tensor with the kernel.
     output = F.conv3d(input_pad, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)
```

### Comparing `kornia-0.6.9/kornia/filters/gaussian.py` & `kornia-0.7.0/kornia/filters/gaussian.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,23 +1,26 @@
-from typing import Tuple
+from __future__ import annotations
 
-import torch
-import torch.nn as nn
+from typing import Any
+
+from kornia.core import Module, Tensor, tensor
+from kornia.core.check import KORNIA_CHECK_IS_TENSOR
+from kornia.utils import deprecated
 
 from .filter import filter2d, filter2d_separable
-from .kernels import get_gaussian_kernel1d, get_gaussian_kernel2d
+from .kernels import _unpack_2d_ks, get_gaussian_kernel1d, get_gaussian_kernel2d
 
 
 def gaussian_blur2d(
-    input: torch.Tensor,
-    kernel_size: Tuple[int, int],
-    sigma: Tuple[float, float],
+    input: Tensor,
+    kernel_size: tuple[int, int] | int,
+    sigma: tuple[float, float] | Tensor,
     border_type: str = 'reflect',
     separable: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Create an operator that blurs a tensor using a Gaussian filter.
 
     .. image:: _static/img/gaussian_blur2d.png
 
     The operator smooths the given tensor with a gaussian kernel by convolving
     it to each channel. It supports batched operation.
 
@@ -38,26 +41,41 @@
        gaussian_blur.html>`__.
 
     Examples:
         >>> input = torch.rand(2, 4, 5, 5)
         >>> output = gaussian_blur2d(input, (3, 3), (1.5, 1.5))
         >>> output.shape
         torch.Size([2, 4, 5, 5])
+
+        >>> output = gaussian_blur2d(input, (3, 3), torch.tensor([[1.5, 1.5]]))
+        >>> output.shape
+        torch.Size([2, 4, 5, 5])
     """
+    KORNIA_CHECK_IS_TENSOR(input)
+
+    if isinstance(sigma, tuple):
+        sigma = tensor([sigma], device=input.device, dtype=input.dtype)
+    else:
+        KORNIA_CHECK_IS_TENSOR(sigma)
+        sigma = sigma.to(device=input.device, dtype=input.dtype)
+
     if separable:
-        kernel_x: torch.Tensor = get_gaussian_kernel1d(kernel_size[1], sigma[1])
-        kernel_y: torch.Tensor = get_gaussian_kernel1d(kernel_size[0], sigma[0])
-        out = filter2d_separable(input, kernel_x[None], kernel_y[None], border_type)
+        ky, kx = _unpack_2d_ks(kernel_size)
+        bs = sigma.shape[0]
+        kernel_x = get_gaussian_kernel1d(kx, sigma[:, 1].view(bs, 1))
+        kernel_y = get_gaussian_kernel1d(ky, sigma[:, 0].view(bs, 1))
+        out = filter2d_separable(input, kernel_x, kernel_y, border_type)
     else:
-        kernel: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma)
-        out = filter2d(input, kernel[None], border_type)
+        kernel = get_gaussian_kernel2d(kernel_size, sigma)
+        out = filter2d(input, kernel, border_type)
+
     return out
 
 
-class GaussianBlur2d(nn.Module):
+class GaussianBlur2d(Module):
     r"""Create an operator that blurs a tensor using a Gaussian filter.
 
     The operator smooths the given tensor with a gaussian kernel by convolving
     it to each channel. It supports batched operation.
 
     Arguments:
         kernel_size: the size of the kernel.
@@ -81,36 +99,34 @@
         >>> output = gauss(input)  # 2x4x5x5
         >>> output.shape
         torch.Size([2, 4, 5, 5])
     """
 
     def __init__(
         self,
-        kernel_size: Tuple[int, int],
-        sigma: Tuple[float, float],
+        kernel_size: tuple[int, int] | int,
+        sigma: tuple[float, float] | Tensor,
         border_type: str = 'reflect',
         separable: bool = True,
     ) -> None:
         super().__init__()
-        self.kernel_size: Tuple[int, int] = kernel_size
-        self.sigma: Tuple[float, float] = sigma
+        self.kernel_size = kernel_size
+        self.sigma = sigma
         self.border_type = border_type
         self.separable = separable
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '(kernel_size='
-            + str(self.kernel_size)
-            + ', '
-            + 'sigma='
-            + str(self.sigma)
-            + ', '
-            + 'border_type='
-            + self.border_type
-            + 'separable='
-            + str(self.separable)
-            + ')'
+            f"{self.__class__.__name__}"
+            f"(kernel_size={self.kernel_size}, "
+            f"sigma={self.sigma}, "
+            f"border_type={self.border_type}, "
+            f"separable={self.separable})"
         )
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return gaussian_blur2d(input, self.kernel_size, self.sigma, self.border_type, self.separable)
+
+
+@deprecated(replace_with='gaussian_blur2d', version='6.9.10')
+def gaussian_blur2d_t(*args: Any, **kwargs: Any) -> Tensor:
+    return gaussian_blur2d(*args, **kwargs)
```

### Comparing `kornia-0.6.9/kornia/filters/kernels_geometry.py` & `kornia-0.7.0/kornia/filters/kernels_geometry.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,21 @@
-from typing import Tuple, Union
+from __future__ import annotations
 
 import torch
 
 from kornia.core import Tensor, pad, stack, tensor, zeros
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 from kornia.geometry.transform import rotate, rotate3d
 from kornia.utils import _extract_device_dtype
 
+from .kernels import _check_kernel_size, _unpack_2d_ks, _unpack_3d_ks
+
 
 def get_motion_kernel2d(
-    kernel_size: int, angle: Union[Tensor, float], direction: Union[Tensor, float] = 0.0, mode: str = 'nearest'
+    kernel_size: int, angle: Tensor | float, direction: Tensor | float = 0.0, mode: str = 'nearest'
 ) -> Tensor:
     r"""Return 2D motion blur filter.
 
     Args:
         kernel_size: motion kernel width and height. It should be odd and positive.
         angle: angle of the motion blur in degrees (anti-clockwise rotation).
         direction: forward/backward direction of the motion blur.
@@ -37,69 +40,64 @@
                  [0.0000, 0.3333, 0.0000],
                  [0.5000, 0.0000, 0.0000]]])
     """
     device, dtype = _extract_device_dtype(
         [angle if isinstance(angle, Tensor) else None, direction if isinstance(direction, Tensor) else None]
     )
 
-    if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size < 3:
-        raise TypeError("ksize must be an odd integer >= than 3")
+    # TODO: add support to kernel_size as tuple or integer
+    kernel_tuple = _unpack_2d_ks(kernel_size)
+    _check_kernel_size(kernel_size, 2)
 
     if not isinstance(angle, Tensor):
         angle = tensor([angle], device=device, dtype=dtype)
 
     if angle.dim() == 0:
-        angle = angle.unsqueeze(0)
+        angle = angle[None]
 
-    if angle.dim() != 1:
-        raise AssertionError(f"angle must be a 1-dim tensor. Got {angle}.")
+    KORNIA_CHECK_SHAPE(angle, ['B'])
 
     if not isinstance(direction, Tensor):
         direction = tensor([direction], device=device, dtype=dtype)
 
     if direction.dim() == 0:
-        direction = direction.unsqueeze(0)
-
-    if direction.dim() != 1:
-        raise AssertionError(f"direction must be a 1-dim tensor. Got {direction}.")
-
-    if direction.size(0) != angle.size(0):
-        raise AssertionError(f"direction and angle must have the same length. Got {direction} and {angle}.")
+        direction = direction[None]
 
-    kernel_tuple: Tuple[int, int] = (kernel_size, kernel_size)
+    KORNIA_CHECK_SHAPE(direction, ['B'])
+    KORNIA_CHECK(
+        direction.size(0) == angle.size(0),
+        f'direction and angle must have the same length. Got {direction} and {angle}.',
+    )
 
     # direction from [-1, 1] to [0, 1] range
     direction = (torch.clamp(direction, -1.0, 1.0) + 1.0) / 2.0
     # kernel = torch.zeros((direction.size(0), *kernel_tuple), device=device, dtype=dtype)
 
     # Element-wise linspace
     # kernel[:, kernel_size // 2, :] = torch.stack(
     #     [(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], dim=-1)
     # Alternatively
     # m = ((1 - 2 * direction)[:, None].repeat(1, kernel_size) / (kernel_size - 1))
     # kernel[:, kernel_size // 2, :] = direction[:, None].repeat(1, kernel_size) + m * torch.arange(0, kernel_size)
     k = stack([(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], -1)
     kernel = pad(k[:, None], [0, 0, kernel_size // 2, kernel_size // 2, 0, 0])
 
-    if kernel.shape != torch.Size([direction.size(0), *kernel_tuple]):
-        raise AssertionError
-    kernel = kernel.unsqueeze(1)
+    expected_shape = torch.Size([direction.size(0), *kernel_tuple])
+    KORNIA_CHECK(kernel.shape == expected_shape, f'Kernel shape should be {expected_shape}. Gotcha {kernel.shape}')
+    kernel = kernel[:, None, ...]
 
     # rotate (counterclockwise) kernel by given angle
     kernel = rotate(kernel, angle, mode=mode, align_corners=True)
     kernel = kernel[:, 0]
     kernel = kernel / kernel.sum(dim=(1, 2), keepdim=True)
     return kernel
 
 
 def get_motion_kernel3d(
-    kernel_size: int,
-    angle: Union[Tensor, Tuple[float, float, float]],
-    direction: Union[Tensor, float] = 0.0,
-    mode: str = 'nearest',
+    kernel_size: int, angle: Tensor | tuple[float, float, float], direction: Tensor | float = 0.0, mode: str = 'nearest'
 ) -> Tensor:
     r"""Return 3D motion blur filter.
 
     Args:
         kernel_size: motion kernel width, height and depth. It should be odd and positive.
         angle: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
             If tensor, it must be :math:`(B, 3)`.
@@ -136,57 +134,55 @@
                   [0.0000, 0.3333, 0.0000],
                   [0.0000, 0.0000, 0.0000]],
         <BLANKLINE>
                  [[0.0000, 0.1667, 0.0000],
                   [0.0000, 0.0000, 0.0000],
                   [0.0000, 0.0000, 0.0000]]]])
     """
-    if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size < 3:
-        raise TypeError(f"ksize must be an odd integer >= than 3. Got {kernel_size}.")
-
     device, dtype = _extract_device_dtype(
         [angle if isinstance(angle, Tensor) else None, direction if isinstance(direction, Tensor) else None]
     )
 
+    # TODO: add support to kernel_size as tuple or integer
+    kernel_tuple = _unpack_3d_ks(kernel_size)
+    _check_kernel_size(kernel_size, 2)
+
     if not isinstance(angle, Tensor):
         angle = tensor([angle], device=device, dtype=dtype)
 
     if angle.dim() == 1:
-        angle = angle.unsqueeze(0)
+        angle = angle[None]
 
-    if not (len(angle.shape) == 2 and angle.size(1) == 3):
-        raise AssertionError(f"angle must be (B, 3). Got {angle}.")
+    KORNIA_CHECK_SHAPE(angle, ['B', '3'])
 
     if not isinstance(direction, Tensor):
         direction = tensor([direction], device=device, dtype=dtype)
 
     if direction.dim() == 0:
-        direction = direction.unsqueeze(0)
-
-    if direction.dim() != 1:
-        raise AssertionError(f"direction must be a 1-dim tensor. Got {direction}.")
+        direction = direction[None]
 
-    if direction.size(0) != angle.size(0):
-        raise AssertionError(f"direction and angle must have the same length. Got {direction} and {angle}.")
-
-    kernel_tuple: Tuple[int, int, int] = (kernel_size, kernel_size, kernel_size)
+    KORNIA_CHECK_SHAPE(direction, ['B'])
+    KORNIA_CHECK(
+        direction.size(0) == angle.size(0),
+        f'direction and angle must have the same batch size. Got {direction.shape} and {angle.shape}.',
+    )
 
     # direction from [-1, 1] to [0, 1] range
     direction = (torch.clamp(direction, -1.0, 1.0) + 1.0) / 2.0
     kernel = zeros((direction.size(0), *kernel_tuple), device=device, dtype=dtype)
 
     # Element-wise linspace
     # kernel[:, kernel_size // 2, kernel_size // 2, :] = torch.stack(
     #     [(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], dim=-1)
     k = stack([(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], -1)
     kernel = pad(k[:, None, None], [0, 0, kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2, 0, 0])
 
-    if kernel.shape != torch.Size([direction.size(0), *kernel_tuple]):
-        raise AssertionError
-    kernel = kernel.unsqueeze(1)
+    expected_shape = torch.Size([direction.size(0), *kernel_tuple])
+    KORNIA_CHECK(kernel.shape == expected_shape, f'Kernel shape should be {expected_shape}. Gotcha {kernel.shape}')
+    kernel = kernel[:, None, ...]
 
     # rotate (counterclockwise) kernel by given angle
     kernel = rotate3d(kernel, angle[:, 0], angle[:, 1], angle[:, 2], mode=mode, align_corners=True)
     kernel = kernel[:, 0]
     kernel = kernel / kernel.sum(dim=(1, 2, 3), keepdim=True)
 
     return kernel
```

### Comparing `kornia-0.6.9/kornia/filters/laplacian.py` & `kornia-0.7.0/kornia/filters/laplacian.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,17 +1,18 @@
-import torch
-import torch.nn as nn
+from __future__ import annotations
+
+from kornia.core import Module, Tensor
 
 from .filter import filter2d
 from .kernels import get_laplacian_kernel2d, normalize_kernel2d
 
 
 def laplacian(
-    input: torch.Tensor, kernel_size: int, border_type: str = 'reflect', normalized: bool = True
-) -> torch.Tensor:
+    input: Tensor, kernel_size: tuple[int, int] | int, border_type: str = 'reflect', normalized: bool = True
+) -> Tensor:
     r"""Create an operator that returns a tensor using a Laplacian filter.
 
     .. image:: _static/img/laplacian.png
 
     The operator smooths the given tensor with a laplacian kernel by convolving
     it to each channel. It supports batched operation.
 
@@ -32,23 +33,23 @@
 
     Examples:
         >>> input = torch.rand(2, 4, 5, 5)
         >>> output = laplacian(input, 3)
         >>> output.shape
         torch.Size([2, 4, 5, 5])
     """
-    kernel: torch.Tensor = torch.unsqueeze(get_laplacian_kernel2d(kernel_size), dim=0)
+    kernel = get_laplacian_kernel2d(kernel_size, device=input.device, dtype=input.dtype)[None, ...]
 
     if normalized:
         kernel = normalize_kernel2d(kernel)
 
     return filter2d(input, kernel, border_type)
 
 
-class Laplacian(nn.Module):
+class Laplacian(Module):
     r"""Create an operator that returns a tensor using a Laplacian filter.
 
     The operator smooths the given tensor with a laplacian kernel by convolving
     it to each channel. It supports batched operation.
 
     Args:
         kernel_size: the size of the kernel.
@@ -65,29 +66,25 @@
         >>> input = torch.rand(2, 4, 5, 5)
         >>> laplace = Laplacian(5)
         >>> output = laplace(input)
         >>> output.shape
         torch.Size([2, 4, 5, 5])
     """
 
-    def __init__(self, kernel_size: int, border_type: str = 'reflect', normalized: bool = True) -> None:
+    def __init__(
+        self, kernel_size: tuple[int, int] | int, border_type: str = 'reflect', normalized: bool = True
+    ) -> None:
         super().__init__()
-        self.kernel_size: int = kernel_size
+        self.kernel_size = kernel_size
         self.border_type: str = border_type
         self.normalized: bool = normalized
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '(kernel_size='
-            + str(self.kernel_size)
-            + ', '
-            + 'normalized='
-            + str(self.normalized)
-            + ', '
-            + 'border_type='
-            + self.border_type
-            + ')'
+            f"{self.__class__.__name__}"
+            f"(kernel_size={self.kernel_size}, "
+            f"normalized={self.normalized}, "
+            f"border_type={self.border_type})"
         )
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return laplacian(input, self.kernel_size, self.border_type, self.normalized)
```

### Comparing `kornia-0.6.9/kornia/filters/median.py` & `kornia-0.7.0/kornia/filters/median.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,24 @@
-from typing import List, Tuple
+from __future__ import annotations
 
-import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
-from .kernels import get_binary_kernel2d
+from kornia.core import Module, Tensor
+from kornia.core.check import KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 
+from .kernels import _unpack_2d_ks, get_binary_kernel2d
 
-def _compute_zero_padding(kernel_size: Tuple[int, int]) -> Tuple[int, int]:
+
+def _compute_zero_padding(kernel_size: tuple[int, int] | int) -> tuple[int, int]:
     r"""Utility function that computes zero padding tuple."""
-    computed: List[int] = [(k - 1) // 2 for k in kernel_size]
-    return computed[0], computed[1]
+    ky, kx = _unpack_2d_ks(kernel_size)
+    return (ky - 1) // 2, (kx - 1) // 2
 
 
-def median_blur(input: torch.Tensor, kernel_size: Tuple[int, int]) -> torch.Tensor:
+def median_blur(input: Tensor, kernel_size: tuple[int, int] | int) -> Tensor:
     r"""Blur an image using the median filter.
 
     .. image:: _static/img/median_blur.png
 
     Args:
         input: the input image with shape :math:`(B,C,H,W)`.
         kernel_size: the blurring kernel size.
@@ -31,37 +32,32 @@
 
     Example:
         >>> input = torch.rand(2, 4, 5, 7)
         >>> output = median_blur(input, (3, 3))
         >>> output.shape
         torch.Size([2, 4, 5, 7])
     """
-    if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
-
-    if not len(input.shape) == 4:
-        raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
+    KORNIA_CHECK_IS_TENSOR(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])
 
-    padding: Tuple[int, int] = _compute_zero_padding(kernel_size)
+    padding = _compute_zero_padding(kernel_size)
 
     # prepare kernel
-    kernel: torch.Tensor = get_binary_kernel2d(kernel_size).to(input)
+    kernel: Tensor = get_binary_kernel2d(kernel_size, device=input.device, dtype=input.dtype)
     b, c, h, w = input.shape
 
     # map the local window to single vector
-    features: torch.Tensor = F.conv2d(input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)
+    features: Tensor = F.conv2d(input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)
     features = features.view(b, c, -1, h, w)  # BxCx(K_h * K_w)xHxW
 
     # compute the median along the feature axis
-    median: torch.Tensor = torch.median(features, dim=2)[0]
-
-    return median
+    return features.median(dim=2)[0]
 
 
-class MedianBlur(nn.Module):
+class MedianBlur(Module):
     r"""Blur an image using the median filter.
 
     Args:
         kernel_size: the blurring kernel size.
 
     Returns:
         the blurred input tensor.
@@ -74,13 +70,13 @@
         >>> input = torch.rand(2, 4, 5, 7)
         >>> blur = MedianBlur((3, 3))
         >>> output = blur(input)
         >>> output.shape
         torch.Size([2, 4, 5, 7])
     """
 
-    def __init__(self, kernel_size: Tuple[int, int]) -> None:
+    def __init__(self, kernel_size: tuple[int, int] | int) -> None:
         super().__init__()
-        self.kernel_size: Tuple[int, int] = kernel_size
+        self.kernel_size = kernel_size
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return median_blur(input, self.kernel_size)
```

### Comparing `kornia-0.6.9/kornia/filters/motion.py` & `kornia-0.7.0/kornia/filters/motion.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,71 +1,78 @@
-from typing import Tuple, Union
+from __future__ import annotations
 
-import torch
-import torch.nn as nn
+from kornia.core import Module, Tensor
+from kornia.core.check import KORNIA_CHECK
 
 from .filter import filter2d, filter3d
 from .kernels_geometry import get_motion_kernel2d, get_motion_kernel3d
 
+_VALID_BORDER = {"constant", "reflect", "replicate", "circular"}
 
-class MotionBlur(nn.Module):
+
+class MotionBlur(Module):
     r"""Blur 2D images (4D tensor) using the motion filter.
 
     Args:
         kernel_size: motion kernel width and height. It should be odd and positive.
         angle: angle of the motion blur in degrees (anti-clockwise rotation).
         direction: forward/backward direction of the motion blur.
             Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
             while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
             uniformly (but still angled) motion blur.
         border_type: the padding mode to be applied before convolving. The expected modes are:
-            ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
+             ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
+        mode: interpolation mode for rotating the kernel. ``'bilinear'`` or ``'nearest'``.
 
     Returns:
         the blurred input tensor.
 
     Shape:
         - Input: :math:`(B, C, H, W)`
         - Output: :math:`(B, C, H, W)`
 
     Examples:
         >>> input = torch.rand(2, 4, 5, 7)
         >>> motion_blur = MotionBlur(3, 35., 0.5)
         >>> output = motion_blur(input)  # 2x4x5x7
     """
 
-    def __init__(self, kernel_size: int, angle: float, direction: float, border_type: str = 'constant') -> None:
+    def __init__(
+        self, kernel_size: int, angle: float, direction: float, border_type: str = 'constant', mode: str = 'nearest'
+    ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
-        self.angle: float = angle
-        self.direction: float = direction
-        self.border_type: str = border_type
+        self.angle = angle
+        self.direction = direction
+        self.border_type = border_type
+        self.mode = mode
 
     def __repr__(self) -> str:
         return (
             f'{self.__class__.__name__} (kernel_size={self.kernel_size}, '
             f'angle={self.angle}, direction={self.direction}, border_type={self.border_type})'
         )
 
-    def forward(self, x: torch.Tensor):
+    def forward(self, x: Tensor) -> Tensor:
         return motion_blur(x, self.kernel_size, self.angle, self.direction, self.border_type)
 
 
-class MotionBlur3D(nn.Module):
+class MotionBlur3D(Module):
     r"""Blur 3D volumes (5D tensor) using the motion filter.
 
     Args:
         kernel_size: motion kernel width and height. It should be odd and positive.
         angle: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
         direction: forward/backward direction of the motion blur.
             Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
             while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
             uniformly (but still angled) motion blur.
         border_type: the padding mode to be applied before convolving. The expected modes are:
             ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
+        mode: interpolation mode for rotating the kernel. ``'bilinear'`` or ``'nearest'``.
 
     Returns:
         the blurred input tensor.
 
     Shape:
         - Input: :math:`(B, C, D, H, W)`
         - Output: :math:`(B, C, D, H, W)`
@@ -75,48 +82,52 @@
         >>> motion_blur = MotionBlur3D(3, 35., 0.5)
         >>> output = motion_blur(input)  # 2x4x5x7x9
     """
 
     def __init__(
         self,
         kernel_size: int,
-        angle: Union[float, Tuple[float, float, float]],
-        direction: float,
+        angle: float | tuple[float, float, float] | Tensor,
+        direction: float | Tensor,
         border_type: str = 'constant',
+        mode: str = 'nearest',
     ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
-        self.angle: Tuple[float, float, float]
+        KORNIA_CHECK(
+            isinstance(angle, (Tensor, float, list, tuple)),
+            f'Angle should be a Tensor, float or a sequence of floats. Got {angle}',
+        )
         if isinstance(angle, float):
             self.angle = (angle, angle, angle)
         elif isinstance(angle, (tuple, list)) and len(angle) == 3:
-            self.angle = angle
-        else:
-            raise ValueError(f"Expect angle to be either a float or a tuple of floats. Got {angle}.")
-        self.direction: float = direction
-        self.border_type: str = border_type
+            self.angle = (angle[0], angle[1], angle[2])
+
+        self.direction = direction
+        self.border_type = border_type
+        self.mode = mode
 
     def __repr__(self) -> str:
         return (
             f'{self.__class__.__name__} (kernel_size={self.kernel_size}, '
             f'angle={self.angle}, direction={self.direction}, border_type={self.border_type})'
         )
 
-    def forward(self, x: torch.Tensor):
+    def forward(self, x: Tensor) -> Tensor:
         return motion_blur3d(x, self.kernel_size, self.angle, self.direction, self.border_type)
 
 
 def motion_blur(
-    input: torch.Tensor,
+    input: Tensor,
     kernel_size: int,
-    angle: Union[float, torch.Tensor],
-    direction: Union[float, torch.Tensor],
+    angle: float | Tensor,
+    direction: float | Tensor,
     border_type: str = 'constant',
     mode: str = 'nearest',
-) -> torch.Tensor:
+) -> Tensor:
     r"""Perform motion blur on tensor images.
 
     .. image:: _static/img/motion_blur.png
 
     Args:
         input: the input tensor with shape :math:`(B, C, H, W)`.
         kernel_size: motion kernel width and height. It should be odd and positive.
@@ -141,28 +152,26 @@
         >>> torch.allclose(out_1[0], out_1[1])
         True
         >>> # perform element-wise motion blur across the batch
         >>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1., -1.]))
         >>> torch.allclose(out_1[0], out_1[1])
         False
     """
-    if border_type not in ["constant", "reflect", "replicate", "circular"]:
-        raise AssertionError
-    kernel: torch.Tensor = get_motion_kernel2d(kernel_size, angle, direction, mode)
+    kernel = get_motion_kernel2d(kernel_size, angle, direction, mode)
     return filter2d(input, kernel, border_type)
 
 
 def motion_blur3d(
-    input: torch.Tensor,
+    input: Tensor,
     kernel_size: int,
-    angle: Union[Tuple[float, float, float], torch.Tensor],
-    direction: Union[float, torch.Tensor],
+    angle: tuple[float, float, float] | Tensor,
+    direction: float | Tensor,
     border_type: str = 'constant',
     mode: str = 'nearest',
-) -> torch.Tensor:
+) -> Tensor:
     r"""Perform motion blur on 3D volumes (5D tensor).
 
     Args:
         input: the input tensor with shape :math:`(B, C, D, H, W)`.
         kernel_size: motion kernel width, height and depth. It should be odd and positive.
         angle: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
             If tensor, it must be :math:`(B, 3)`.
@@ -185,11 +194,9 @@
         >>> torch.allclose(out_1[0], out_1[1])
         True
         >>> # perform element-wise motion blur across the batch
         >>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1., -1.]))
         >>> torch.allclose(out_1[0], out_1[1])
         False
     """
-    if border_type not in ["constant", "reflect", "replicate", "circular"]:
-        raise AssertionError
-    kernel: torch.Tensor = get_motion_kernel3d(kernel_size, angle, direction, mode)
+    kernel = get_motion_kernel3d(kernel_size, angle, direction, mode)
     return filter3d(input, kernel, border_type)
```

### Comparing `kornia-0.6.9/kornia/filters/sobel.py` & `kornia-0.7.0/kornia/filters/sobel.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,14 @@
+from __future__ import annotations
+
 import torch
 import torch.nn.functional as F
 
 from kornia.core import Module, Tensor, pad
+from kornia.core.check import KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 
 from .kernels import get_spatial_gradient_kernel2d, get_spatial_gradient_kernel3d, normalize_kernel2d
 
 
 def spatial_gradient(input: Tensor, mode: str = 'sobel', order: int = 1, normalized: bool = True) -> Tensor:
     r"""Compute the first order image derivative in both x and y using a Sobel operator.
 
@@ -26,28 +29,25 @@
 
     Examples:
         >>> input = torch.rand(1, 3, 4, 4)
         >>> output = spatial_gradient(input)  # 1x3x2x4x4
         >>> output.shape
         torch.Size([1, 3, 2, 4, 4])
     """
-    if not isinstance(input, Tensor):
-        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
+    KORNIA_CHECK_IS_TENSOR(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])
 
-    if not len(input.shape) == 4:
-        raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
     # allocate kernel
-    kernel: Tensor = get_spatial_gradient_kernel2d(mode, order)
+    kernel = get_spatial_gradient_kernel2d(mode, order, device=input.device, dtype=input.dtype)
     if normalized:
         kernel = normalize_kernel2d(kernel)
 
     # prepare kernel
     b, c, h, w = input.shape
-    tmp_kernel: Tensor = kernel.to(input).detach()
-    tmp_kernel = tmp_kernel.unsqueeze(1)
+    tmp_kernel = kernel[:, None, ...]
 
     # Pad with "replicate for spatial dims, but with zeros for channel
     spatial_pad = [kernel.size(1) // 2, kernel.size(1) // 2, kernel.size(2) // 2, kernel.size(2) // 2]
     out_channels: int = 3 if order == 2 else 2
     padded_inp: Tensor = pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')
     out = F.conv2d(padded_inp, tmp_kernel, groups=1, padding=0, stride=1)
     return out.reshape(b, c, out_channels, h, w)
@@ -67,19 +67,17 @@
 
     Examples:
         >>> input = torch.rand(1, 4, 2, 4, 4)
         >>> output = spatial_gradient3d(input)
         >>> output.shape
         torch.Size([1, 4, 3, 2, 4, 4])
     """
-    if not isinstance(input, Tensor):
-        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
+    KORNIA_CHECK_IS_TENSOR(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'D', 'H', 'W'])
 
-    if not len(input.shape) == 5:
-        raise ValueError(f"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}")
     b, c, d, h, w = input.shape
     dev = input.device
     dtype = input.dtype
     if (mode == 'diff') and (order == 1):
         # we go for the special case implementation due to conv3d bad speed
         x: Tensor = pad(input, 6 * [1], 'replicate')
         center = slice(1, -1)
@@ -89,21 +87,20 @@
         out[..., 0, :, :, :] = x[..., center, center, right] - x[..., center, center, left]
         out[..., 1, :, :, :] = x[..., center, right, center] - x[..., center, left, center]
         out[..., 2, :, :, :] = x[..., right, center, center] - x[..., left, center, center]
         out = 0.5 * out
     else:
         # prepare kernel
         # allocate kernel
-        kernel: Tensor = get_spatial_gradient_kernel3d(mode, order)
+        kernel = get_spatial_gradient_kernel3d(mode, order, device=dev, dtype=dtype)
 
-        tmp_kernel: Tensor = kernel.to(input).detach()
-        tmp_kernel = tmp_kernel.repeat(c, 1, 1, 1, 1)
+        tmp_kernel = kernel.repeat(c, 1, 1, 1, 1)
 
         # convolve input tensor with grad kernel
-        kernel_flip: Tensor = tmp_kernel.flip(-3)
+        kernel_flip = tmp_kernel.flip(-3)
 
         # Pad with "replicate for spatial dims, but with zeros for channel
         spatial_pad = [
             kernel.size(2) // 2,
             kernel.size(2) // 2,
             kernel.size(3) // 2,
             kernel.size(3) // 2,
@@ -136,19 +133,16 @@
 
     Example:
         >>> input = torch.rand(1, 3, 4, 4)
         >>> output = sobel(input)  # 1x3x4x4
         >>> output.shape
         torch.Size([1, 3, 4, 4])
     """
-    if not isinstance(input, Tensor):
-        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
-
-    if not len(input.shape) == 4:
-        raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
+    KORNIA_CHECK_IS_TENSOR(input)
+    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])
 
     # comput the x/y gradients
     edges: Tensor = spatial_gradient(input, normalized=normalized)
 
     # unpack the edges
     gx: Tensor = edges[:, :, 0]
     gy: Tensor = edges[:, :, 1]
@@ -182,18 +176,15 @@
     def __init__(self, mode: str = 'sobel', order: int = 1, normalized: bool = True) -> None:
         super().__init__()
         self.normalized: bool = normalized
         self.order: int = order
         self.mode: str = mode
 
     def __repr__(self) -> str:
-        return (
-            self.__class__.__name__ + '('
-            'order=' + str(self.order) + ', ' + 'normalized=' + str(self.normalized) + ', ' + 'mode=' + self.mode + ')'
-        )
+        return f"{self.__class__.__name__}(order={self.order}, normalized={self.normalized}, mode={self.mode})"
 
     def forward(self, input: Tensor) -> Tensor:
         return spatial_gradient(input, self.mode, self.order, self.normalized)
 
 
 class SpatialGradient3d(Module):
     r"""Compute the first and second order volume derivative in x, y and d using a diff operator.
@@ -217,18 +208,17 @@
     """
 
     def __init__(self, mode: str = 'diff', order: int = 1) -> None:
         super().__init__()
         self.order: int = order
         self.mode: str = mode
         self.kernel = get_spatial_gradient_kernel3d(mode, order)
-        return
 
     def __repr__(self) -> str:
-        return self.__class__.__name__ + '(' 'order=' + str(self.order) + ', ' + 'mode=' + self.mode + ')'
+        return f"{self.__class__.__name__}(order={self.order}, mode={self.mode})"
 
     def forward(self, input: Tensor) -> Tensor:
         return spatial_gradient3d(input, self.mode, self.order)
 
 
 class Sobel(Module):
     r"""Compute the Sobel operator and returns the magnitude per channel.
@@ -251,11 +241,11 @@
 
     def __init__(self, normalized: bool = True, eps: float = 1e-6) -> None:
         super().__init__()
         self.normalized: bool = normalized
         self.eps: float = eps
 
     def __repr__(self) -> str:
-        return self.__class__.__name__ + '(' 'normalized=' + str(self.normalized) + ')'
+        return f"{self.__class__.__name__}(normalized={self.normalized})"
 
     def forward(self, input: Tensor) -> Tensor:
         return sobel(input, self.normalized, self.eps)
```

### Comparing `kornia-0.6.9/kornia/filters/unsharp.py` & `kornia-0.7.0/kornia/filters/unsharp.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,17 @@
-from typing import Tuple
+from __future__ import annotations
 
-import torch
-import torch.nn as nn
+from kornia.core import Module, Tensor
 
 from .gaussian import gaussian_blur2d
 
 
 def unsharp_mask(
-    input: torch.Tensor, kernel_size: Tuple[int, int], sigma: Tuple[float, float], border_type: str = 'reflect'
-) -> torch.Tensor:
+    input: Tensor, kernel_size: tuple[int, int] | int, sigma: tuple[float, float] | Tensor, border_type: str = 'reflect'
+) -> Tensor:
     r"""Create an operator that sharpens a tensor by applying operation out = 2 * image - gaussian_blur2d(image).
 
     .. image:: _static/img/unsharp_mask.png
 
     Args:
         input: the input tensor with shape :math:`(B,C,H,W)`.
         kernel_size: the size of the kernel.
@@ -26,20 +25,20 @@
 
     Examples:
         >>> input = torch.rand(2, 4, 5, 5)
         >>> output = unsharp_mask(input, (3, 3), (1.5, 1.5))
         >>> output.shape
         torch.Size([2, 4, 5, 5])
     """
-    data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma, border_type)
-    data_sharpened: torch.Tensor = input + (input - data_blur)
+    data_blur: Tensor = gaussian_blur2d(input, kernel_size, sigma, border_type)
+    data_sharpened: Tensor = input + (input - data_blur)
     return data_sharpened
 
 
-class UnsharpMask(nn.Module):
+class UnsharpMask(Module):
     r"""Create an operator that sharpens image with: out = 2 * image - gaussian_blur2d(image).
 
     Args:
         kernel_size: the size of the kernel.
         sigma: the standard deviation of the kernel.
         border_type: the padding mode to be applied before convolving.
           The expected modes are: ``'constant'``, ``'reflect'``,
@@ -60,15 +59,17 @@
         >>> input = torch.rand(2, 4, 5, 5)
         >>> sharpen = UnsharpMask((3, 3), (1.5, 1.5))
         >>> output = sharpen(input)
         >>> output.shape
         torch.Size([2, 4, 5, 5])
     """
 
-    def __init__(self, kernel_size: Tuple[int, int], sigma: Tuple[float, float], border_type: str = 'reflect') -> None:
+    def __init__(
+        self, kernel_size: tuple[int, int] | int, sigma: tuple[float, float] | Tensor, border_type: str = 'reflect'
+    ) -> None:
         super().__init__()
-        self.kernel_size: Tuple[int, int] = kernel_size
-        self.sigma: Tuple[float, float] = sigma
+        self.kernel_size = kernel_size
+        self.sigma = sigma
         self.border_type = border_type
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return unsharp_mask(input, self.kernel_size, self.sigma, self.border_type)
```

### Comparing `kornia-0.6.9/kornia/geometry/bbox.py` & `kornia-0.7.0/kornia/geometry/bbox.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
+from __future__ import annotations
+
 import warnings
-from typing import Optional, Tuple
 
 import torch
 
 from .linalg import transform_points
 
 __all__ = [
     "validate_bbox",
@@ -15,15 +16,14 @@
     "bbox_generator",
     "bbox_generator3d",
     "transform_bbox",
     "nms",
 ]
 
 
-@torch.jit.ignore
 def validate_bbox(boxes: torch.Tensor) -> bool:
     """Validate if a 2D bounding box usable or not. This function checks if the boxes are rectangular or not.
 
     Args:
         boxes: a tensor containing the coordinates of the bounding boxes to be extracted. The tensor must have the shape
             of Bx4x2, where each box is defined in the following ``clockwise`` order: top-left, top-right, bottom-right,
             bottom-left. The coordinates must be in the x, y order.
@@ -32,28 +32,27 @@
         raise AssertionError(f"Box shape must be (B, 4, 2) or (B, N, 4, 2). Got {boxes.shape}.")
 
     if len(boxes.shape) == 4:
         boxes = boxes.view(-1, 4, 2)
 
     if not torch.allclose((boxes[:, 1, 0] - boxes[:, 0, 0] + 1), (boxes[:, 2, 0] - boxes[:, 3, 0] + 1), atol=1e-4):
         raise ValueError(
-            "Boxes must have be rectangular, while get widths %s and %s"
-            % (str(boxes[:, 1, 0] - boxes[:, 0, 0] + 1), str(boxes[:, 2, 0] - boxes[:, 3, 0] + 1))
+            f"Boxes must have be rectangular, while get widths {boxes[:, 1, 0] - boxes[:, 0, 0] + 1!s}"
+            f" and {boxes[:, 2, 0] - boxes[:, 3, 0] + 1!s}"
         )
 
     if not torch.allclose((boxes[:, 2, 1] - boxes[:, 0, 1] + 1), (boxes[:, 3, 1] - boxes[:, 1, 1] + 1), atol=1e-4):
         raise ValueError(
-            "Boxes must have be rectangular, while get heights %s and %s"
-            % (str(boxes[:, 2, 1] - boxes[:, 0, 1] + 1), str(boxes[:, 3, 1] - boxes[:, 1, 1] + 1))
+            f"Boxes must be rectangular, while getting heights {boxes[:, 2, 1] - boxes[:, 0, 1] + 1!s}"
+            f" and {boxes[:, 3, 1] - boxes[:, 1, 1] + 1!s}"
         )
 
     return True
 
 
-@torch.jit.ignore
 def validate_bbox3d(boxes: torch.Tensor) -> bool:
     """Validate if a 3D bounding box usable or not. This function checks if the boxes are cube or not.
 
     Args:
         boxes: a tensor containing the coordinates of the bounding boxes to be extracted. The tensor must have the shape
             of Bx8x3, where each box is defined in the following ``clockwise`` order: front-top-left, front-top-right,
             front-bottom-right, front-bottom-left, back-top-left, back-top-right, back-bottom-right, back-bottom-left.
@@ -80,15 +79,15 @@
     depths = boxes[:, 4:, 2] - boxes[:, :4, 2] + 1
     if not torch.allclose(depths.permute(1, 0), depths[:, 0]):
         raise AssertionError(f"Boxes must have be cube, while get different depths {depths}.")
 
     return True
 
 
-def infer_bbox_shape(boxes: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+def infer_bbox_shape(boxes: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
     r"""Auto-infer the output sizes for the given 2D bounding boxes.
 
     Args:
         boxes: a tensor containing the coordinates of the bounding boxes to be extracted. The tensor must have the shape
             of Bx4x2, where each box is defined in the following ``clockwise`` order: top-left, top-right, bottom-right,
             bottom-left. The coordinates must be in the x, y order.
 
@@ -113,15 +112,15 @@
     """
     validate_bbox(boxes)
     width: torch.Tensor = boxes[:, 1, 0] - boxes[:, 0, 0] + 1
     height: torch.Tensor = boxes[:, 2, 1] - boxes[:, 0, 1] + 1
     return height, width
 
 
-def infer_bbox_shape3d(boxes: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+def infer_bbox_shape3d(boxes: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     r"""Auto-infer the output sizes for the given 3D bounding boxes.
 
     Args:
         boxes: a tensor containing the coordinates of the bounding boxes to be extracted. The tensor must have the shape
             of Bx8x3, where each box is defined in the following ``clockwise`` order: front-top-left, front-top-right,
             front-bottom-right, front-bottom-left, back-top-left, back-top-right, back-bottom-right, back-bottom-left.
             The coordinates must be in the x, y, z order.
@@ -193,25 +192,25 @@
                  [0., 1., 1., 1., 0.],
                  [0., 1., 1., 1., 0.],
                  [0., 0., 0., 0., 0.],
                  [0., 0., 0., 0., 0.]]])
     """
     validate_bbox(boxes)
     # zero padding the surroudings
-    mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=torch.float, device=boxes.device)
+    mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=boxes.dtype, device=boxes.device)
     # push all points one pixel off
     # in order to zero-out the fully filled rows or columns
     box_i = (boxes + 1).long()
     # set all pixels within box to 1
     for msk, bx in zip(mask, box_i):
         msk[bx[0, 1] : bx[2, 1] + 1, bx[0, 0] : bx[1, 0] + 1] = 1.0
     return mask[:, 1:-1, 1:-1]
 
 
-def bbox_to_mask3d(boxes: torch.Tensor, size: Tuple[int, int, int]) -> torch.Tensor:
+def bbox_to_mask3d(boxes: torch.Tensor, size: tuple[int, int, int]) -> torch.Tensor:
     """Convert 3D bounding boxes to masks. Covered area is 1. and the remaining is 0.
 
     Args:
         boxes: a tensor containing the coordinates of the bounding boxes to be extracted. The tensor must have the shape
             of Bx8x3, where each box is defined in the following ``clockwise`` order: front-top-left, front-top-right,
             front-bottom-right, front-bottom-left, back-top-left, back-top-right, back-bottom-right, back-bottom-left.
             The coordinates must be in the x, y, z order.
@@ -436,15 +435,15 @@
     bbox_back[:, :, -1] += depth.unsqueeze(dim=1).repeat(1, 4)
     bbox = torch.cat([bbox, bbox_back], dim=1)
 
     return bbox
 
 
 def transform_bbox(
-    trans_mat: torch.Tensor, boxes: torch.Tensor, mode: str = "xyxy", restore_coordinates: Optional[bool] = None
+    trans_mat: torch.Tensor, boxes: torch.Tensor, mode: str = "xyxy", restore_coordinates: bool | None = None
 ) -> torch.Tensor:
     r"""Apply a transformation matrix to a box or batch of boxes.
 
     Args:
         trans_mat: The transformation matrix to be applied with a shape of :math:`(3, 3)`
             or batched as :math:`(B, 3, 3)`.
         boxes: The boxes to be transformed with a common shape of :math:`(N, 4)` or batched as :math:`(B, N, 4)`, the
```

### Comparing `kornia-0.6.9/kornia/geometry/boxes.py` & `kornia-0.7.0/kornia/geometry/boxes.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,27 @@
-from typing import List, Optional, Tuple, Union, cast
+from __future__ import annotations
+
+from typing import cast
 
 import torch
+from torch import Size
 
 from kornia.core import Tensor
 from kornia.geometry.bbox import validate_bbox
 from kornia.geometry.linalg import transform_points
 from kornia.utils import eye_like
 
 __all__ = ["Boxes", "Boxes3D"]
 
 
 def _is_floating_point_dtype(dtype: torch.dtype) -> bool:
     return dtype in (torch.float16, torch.float32, torch.float64, torch.bfloat16, torch.half)
 
 
-def _merge_box_list(boxes: List[torch.Tensor], method: str = "pad") -> Tuple[torch.Tensor, List[int]]:
+def _merge_box_list(boxes: list[torch.Tensor], method: str = "pad") -> tuple[torch.Tensor, list[int]]:
     r"""Merge a list of boxes into one tensor."""
     if not all(box.shape[-2:] == torch.Size([4, 2]) and box.dim() == 3 for box in boxes):
         raise TypeError(f"Input boxes must be a list of (N, 4, 2) shaped. Got: {[box.shape for box in boxes]}.")
 
     if method == "pad":
         max_N = max(box.shape[0] for box in boxes)
         stats = [max_N - box.shape[0] for box in boxes]
@@ -38,14 +41,16 @@
         M: the transformation matrix of shape :math:`(3, 3)` or :math:`(B, 3, 3)` for 2D and :math:`(4, 4)` or
             :math:`(B, 4, 4)` for 3D hexahedron.
     """
     M = M if M.is_floating_point() else M.float()
 
     # Work with batch as kornia.transform_points only supports a batch of points.
     boxes_per_batch, n_points_per_box, coordinates_dimension = boxes.shape[-3:]
+    if boxes_per_batch == 0:
+        return boxes
     points = boxes.view(-1, n_points_per_box * boxes_per_batch, coordinates_dimension)
     M = M if M.ndim == 3 else M.unsqueeze(0)
 
     if points.shape[0] != M.shape[0]:
         raise ValueError(
             f"Batch size mismatch. Got {points.shape[0]} for boxes and {M.shape[0]} for the transformation matrix."
         )
@@ -158,69 +163,78 @@
     back_vertices = front_vertices.clone()
     back_vertices[..., 2] += depth.unsqueeze(-1) - 1
 
     polygons3d = torch.cat([front_vertices, back_vertices], dim=-2)
     return polygons3d
 
 
-# NOTE: Cannot jit with Union types with torch <= 0.10
-# @torch.jit.script
 class Boxes:
     r"""2D boxes containing N or BxN boxes.
 
     Args:
         boxes: 2D boxes, shape of :math:`(N, 4, 2)`, :math:`(B, N, 4, 2)` or a list of :math:`(N, 4, 2)`.
             See below for more details.
-        raise_if_not_floating_point: flag to control floating point casting behaviour when `boxes` is not a floating
-            point tensor. True to raise an error when `boxes` isn't a floating point tensor, False to cast to float.
+        raise_if_not_floating_point: flag to control floating point casting behaviour when `boxes` is not a
+            floating point tensor. True to raise an error when `boxes` isn't a floating point tensor, False
+            to cast to float.
         mode: the box format of the input boxes.
 
     Note:
         **2D boxes format** is defined as a floating data type tensor of shape ``Nx4x2`` or ``BxNx4x2``
-        where each box is a `quadrilateral <https://en.wikipedia.org/wiki/Quadrilateral>`_ defined by it's 4 vertices
-        coordinates (A, B, C, D). Coordinates must be in ``x, y`` order. The height and width of a box is defined as
-        ``width = xmax - xmin + 1`` and ``height = ymax - ymin + 1``. Examples of
+        where each box is a `quadrilateral <https://en.wikipedia.org/wiki/Quadrilateral>`_ defined by it's
+        4 vertices coordinates (A, B, C, D). Coordinates must be in ``x, y`` order. The height and width of
+        a box is defined as ``width = xmax - xmin + 1`` and ``height = ymax - ymin + 1``. Examples of
         `quadrilaterals <https://en.wikipedia.org/wiki/Quadrilateral>`_ are rectangles, rhombus and trapezoids.
     """
 
     def __init__(
         self,
-        boxes: Union[torch.Tensor, List[torch.Tensor]],
+        boxes: torch.Tensor | list[torch.Tensor],
         raise_if_not_floating_point: bool = True,
         mode: str = "vertices_plus",
     ) -> None:
-
-        self._N: Optional[List[int]] = None
+        self._N: list[int] | None = None
 
         if isinstance(boxes, list):
             boxes, self._N = _merge_box_list(boxes)
 
         if not isinstance(boxes, torch.Tensor):
             raise TypeError(f"Input boxes is not a Tensor. Got: {type(boxes)}.")
 
         if not boxes.is_floating_point():
             if raise_if_not_floating_point:
                 raise ValueError(f"Coordinates must be in floating point. Got {boxes.dtype}")
 
             boxes = boxes.float()
 
         if len(boxes.shape) == 0:
-            # Use reshape, so we don't end up creating a new tensor that does not depend on
-            # the inputs (and consequently confuses jit)
             boxes = boxes.reshape((-1, 4))
 
         if not (3 <= boxes.ndim <= 4 and boxes.shape[-2:] == (4, 2)):
             raise ValueError(f"Boxes shape must be (N, 4, 2) or (B, N, 4, 2). Got {boxes.shape}.")
 
         self._is_batched = False if boxes.ndim == 3 else True
 
         self._data = boxes
         self._mode = mode
 
-    def get_boxes_shape(self) -> Tuple[torch.Tensor, torch.Tensor]:
+    def __getitem__(self, key: slice | int | Tensor) -> Boxes:
+        new_box = type(self)(self._data[key], False)
+        new_box._mode = self._mode
+        return new_box
+
+    def __setitem__(self, key: slice | int | Tensor, value: Boxes) -> Boxes:
+        self._data[key] = value._data
+        return self
+
+    @property
+    def shape(self) -> tuple[int, ...] | Size:
+        return self.data.shape
+
+    def get_boxes_shape(self) -> tuple[torch.Tensor, torch.Tensor]:
         r"""Compute boxes heights and widths.
 
         Returns:
             - Boxes heights, shape of :math:`(N,)` or :math:`(B,N)`.
             - Boxes widths, shape of :math:`(N,)` or :math:`(B,N)`.
 
         Example:
@@ -229,36 +243,83 @@
             >>> boxes.get_boxes_shape()
             (tensor([[1., 1.]]), tensor([[1., 2.]]))
         """
         boxes_xywh = cast(torch.Tensor, self.to_tensor("xywh", as_padded_sequence=True))
         widths, heights = boxes_xywh[..., 2], boxes_xywh[..., 3]
         return heights, widths
 
-    def merge(self, boxes: "Boxes", inplace: bool = False) -> "Boxes":
+    def merge(self, boxes: Boxes, inplace: bool = False) -> Boxes:
         """Merges boxes.
 
         Say, current instance holds :math:`(B, N, 4, 2)` and the incoming boxes holds :math:`(B, M, 4, 2)`,
         the merge results in :math:`(B, N + M, 4, 2)`.
 
         Args:
             boxes: 2D boxes.
             inplace: do transform in-place and return self.
         """
         data = torch.cat([self._data, boxes.data], dim=1)
         if inplace:
             self._data = data
             return self
-        return Boxes(data, False)
+
+        obj = self.clone()
+        obj._data = data
+        return obj
+
+    def index_put(
+        self, indices: tuple[Tensor, ...] | list[Tensor], values: Tensor | Boxes, inplace: bool = False
+    ) -> Boxes:
+        if inplace:
+            _data = self._data
+        else:
+            _data = self._data.clone()
+
+        if isinstance(values, Boxes):
+            _data.index_put_(indices, values.data)
+        else:
+            _data.index_put_(indices, values)
+
+        if inplace:
+            return self
+
+        obj = self.clone()
+        obj._data = _data
+        return obj
+
+    def pad(self, padding_size: Tensor) -> Boxes:
+        """Pad a bounding box.
+
+        Args:
+            padding_size: (B, 4)
+        """
+        if not (len(padding_size.shape) == 2 and padding_size.size(1) == 4):
+            raise RuntimeError(f"Expected padding_size as (B, 4). Got {padding_size.shape}.")
+        self._data[..., 0] += padding_size[..., None, :1].to(device=self._data.device)  # left padding
+        self._data[..., 1] += padding_size[..., None, 2:3].to(device=self._data.device)  # top padding
+        return self
+
+    def unpad(self, padding_size: Tensor) -> Boxes:
+        """Pad a bounding box.
+
+        Args:
+            padding_size: (B, 4)
+        """
+        if not (len(padding_size.shape) == 2 and padding_size.size(1) == 4):
+            raise RuntimeError(f"Expected padding_size as (B, 4). Got {padding_size.shape}.")
+        self._data[..., 0] -= padding_size[..., None, :1].to(device=self._data.device)  # left padding
+        self._data[..., 1] -= padding_size[..., None, 2:3].to(device=self._data.device)  # top padding
+        return self
 
     def clamp(
         self,
-        topleft: Optional[Union[Tensor, Tuple[int, int]]] = None,
-        botright: Optional[Union[Tensor, Tuple[int, int]]] = None,
+        topleft: Tensor | tuple[int, int] | None = None,
+        botright: Tensor | tuple[int, int] | None = None,
         inplace: bool = False,
-    ) -> "Boxes":
+    ) -> Boxes:
         """"""
         if not (isinstance(topleft, Tensor) and isinstance(botright, Tensor)):
             raise NotImplementedError
         if inplace:
             _data = self._data
         else:
             _data = self._data.clone()
@@ -271,63 +332,78 @@
         botright_x = botright[:, None, :1].repeat(1, _data.size(1), 4)
         _data[..., 0][_data[..., 0] > botright_x] = botright_x[_data[..., 0] > botright_x]
 
         botright_y = botright[:, None, 1:].repeat(1, _data.size(1), 4)
         _data[..., 1][_data[..., 1] > botright_y] = botright_y[_data[..., 1] > botright_y]
         if inplace:
             return self
-        return Boxes(_data, False)
 
-    def trim(self, correspondence_preserve: bool = False, inplace: bool = False) -> "Boxes":
+        obj = self.clone()
+        obj._data = _data
+        return obj
+
+    def trim(self, correspondence_preserve: bool = False, inplace: bool = False) -> Boxes:
         """Trim out zero padded boxes.
 
         Given box arrangements of shape :math:`(4, 4, Box)`:
-            -- Box -- Box -- Box  -- Box --
-            --  0  --  0  -- Box  -- Box --
-            --  0  -- Box --  0   --  0  --
-            --  0  --  0  --  0   --  0  --
-
-        Nothing will change if correspondence_preserve is True. Only pure zero layers will be
-        removed, resulting in shape :math:`(4, 3, Box)`:
-            -- Box -- Box -- Box  -- Box --
-            --  0  --  0  -- Box  -- Box --
-            --  0  -- Box --  0   --  0  --
+
+            == === == === == === == === ==
+            -- Box -- Box -- Box -- Box --
+            --  0  --  0  -- Box -- Box --
+            --  0  -- Box --  0  --  0  --
+            --  0  --  0  --  0  --  0  --
+            == === == === == === == === ==
+
+        Nothing will change if correspondence_preserve is True. Only pure zero layers will be removed, resulting in
+        shape :math:`(4, 3, Box)`:
+
+            == === == === == === == === ==
+            -- Box -- Box -- Box -- Box --
+            --  0  --  0  -- Box -- Box --
+            --  0  -- Box --  0  --  0  --
+            == === == === == === == === ==
 
         Otherwise, you will get :math:`(4, 2, Box)`:
-            -- Box -- Box -- Box  -- Box --
-            --  0  -- Box -- Box  -- Box --
+
+            == === == === == === == === ==
+            -- Box -- Box -- Box -- Box --
+            --  0  -- Box -- Box -- Box --
+            == === == === == === == === ==
         """
         raise NotImplementedError
 
     def filter_boxes_by_area(
-        self, min_area: Optional[float] = None, max_area: Optional[float] = None, inplace: bool = False
-    ) -> "Boxes":
+        self, min_area: float | None = None, max_area: float | None = None, inplace: bool = False
+    ) -> Boxes:
         area = self.compute_area()
         if inplace:
             _data = self._data
         else:
             _data = self._data.clone()
         if min_area is not None:
             _data[area < min_area] = 0.0
         if max_area is not None:
             _data[area > max_area] = 0.0
         if inplace:
             return self
-        return Boxes(_data, False)
+
+        obj = self.clone()
+        obj._data = _data
+        return obj
 
     def compute_area(self) -> torch.Tensor:
         """Returns :math:`(B, N)`."""
         w = self._data[:, :, 1, 0] - self._data[:, :, 0, 0]
         h = self._data[:, :, 2, 1] - self._data[:, :, 0, 1]
         return w * h
 
     @classmethod
     def from_tensor(
-        cls, boxes: Union[torch.Tensor, List[torch.Tensor]], mode: str = "xyxy", validate_boxes: bool = True
-    ) -> "Boxes":
+        cls, boxes: torch.Tensor | list[torch.Tensor], mode: str = "xyxy", validate_boxes: bool = True
+    ) -> Boxes:
         r"""Helper method to easily create :class:`Boxes` from boxes stored in another format.
 
         Args:
             boxes: 2D boxes, shape of :math:`(N, 4)`, :math:`(B, N, 4)`, :math:`(N, 4, 2)` or :math:`(B, N, 4, 2)`.
             mode: The format in which the boxes are provided.
 
                 * 'xyxy': boxes are assumed to be in the format ``xmin, ymin, xmax, ymax`` where ``width = xmax - xmin``
@@ -361,27 +437,23 @@
                      [0., 3.]],
             <BLANKLINE>
                     [[5., 1.],
                      [7., 1.],
                      [7., 3.],
                      [5., 3.]]])
         """
-        quadrilaterals: Union[torch.Tensor, List[torch.Tensor]]
+        quadrilaterals: torch.Tensor | list[torch.Tensor]
         if isinstance(boxes, (torch.Tensor)):
             quadrilaterals = _boxes_to_quadrilaterals(boxes, mode=mode, validate_boxes=validate_boxes)
         else:
             quadrilaterals = [_boxes_to_quadrilaterals(box, mode, validate_boxes) for box in boxes]
 
-        # Due to some torch.jit.script bug (at least <= 1.9), you need to pass all arguments to __init__ when
-        # constructing the class from inside of a method.
         return cls(quadrilaterals, False, mode)
 
-    def to_tensor(
-        self, mode: Optional[str] = None, as_padded_sequence: bool = False
-    ) -> Union[torch.Tensor, List[torch.Tensor]]:
+    def to_tensor(self, mode: str | None = None, as_padded_sequence: bool = False) -> torch.Tensor | list[torch.Tensor]:
         r"""Cast :class:`Boxes` to a tensor. ``mode`` controls which 2D boxes format should be use to represent
         boxes in the tensor.
 
         Args:
             mode: the output box format. It could be:
 
                 * 'xyxy': boxes are defined as ``xmin, ymin, xmax, ymax`` where ``width = xmax - xmin`` and
@@ -392,30 +464,30 @@
                   and ``height = ymax - ymin``.
                 * 'vertices': boxes are defined by their vertices points in the following ``clockwise`` order:
                   *top-left, top-right, bottom-right, bottom-left*. Vertices coordinates are in (x,y) order. Finally,
                   box width and height are defined as ``width = xmax - xmin`` and ``height = ymax - ymin``.
                 * 'vertices_plus': similar to 'vertices' mode but where box width and length are defined as
                   ``width = xmax - xmin + 1`` and ``height = ymax - ymin + 1``. ymin + 1``.
             as_padded_sequence: whether to keep the pads for a list of boxes. This parameter is only valid
-                if the boxes are from a box list.
+                if the boxes are from a box list whilst `from_tensor`.
 
         Returns:
             Boxes tensor in the ``mode`` format. The shape depends with the ``mode`` value:
 
                 * 'vertices' or 'verticies_plus': :math:`(N, 4, 2)` or :math:`(B, N, 4, 2)`.
                 * Any other value: :math:`(N, 4)` or :math:`(B, N, 4)`.
 
         Examples:
             >>> boxes_xyxy = torch.as_tensor([[0, 3, 1, 4], [5, 1, 8, 4]])
             >>> boxes = Boxes.from_tensor(boxes_xyxy)
             >>> assert (boxes_xyxy == boxes.to_tensor(mode='xyxy')).all()
         """
         batched_boxes = self._data if self._is_batched else self._data.unsqueeze(0)
 
-        boxes: Union[torch.Tensor, List[torch.Tensor]]
+        boxes: torch.Tensor | list[torch.Tensor]
 
         # Create boxes in xyxy_plus format.
         boxes = torch.stack([batched_boxes.amin(dim=-2), batched_boxes.amax(dim=-2)], dim=-2).view(
             batched_boxes.shape[0], batched_boxes.shape[1], 4
         )
 
         if mode is None:
@@ -436,17 +508,15 @@
             offset = torch.as_tensor([0, 0, 1, 1], device=boxes.device, dtype=boxes.dtype)
             boxes = boxes + offset
 
         if mode.startswith('vertices'):
             boxes = _boxes_to_polygons(boxes[..., 0], boxes[..., 1], boxes[..., 2], boxes[..., 3])
 
         if self._N is not None and not as_padded_sequence:
-            boxes = list(
-                torch.nn.functional.pad(o, (len(o.shape) - 1) * [0, 0] + [0, -n]) for o, n in zip(boxes, self._N)
-            )
+            boxes = [torch.nn.functional.pad(o, (len(o.shape) - 1) * [0, 0] + [0, -n]) for o, n in zip(boxes, self._N)]
         else:
             boxes = boxes if self._is_batched else boxes.squeeze(0)
         return boxes
 
     def to_mask(self, height: int, width: int) -> torch.Tensor:
         """Convert 2D boxes to masks. Covered area is 1 and the remaining is 0.
 
@@ -496,41 +566,41 @@
         # Cast boxes coordinates to be integer to use them as indexes. Use round to handle decimal values.
         for mask_channel, box_xyxy in zip(mask.view(-1, height, width), clipped_boxes_xyxy.view(-1, 4).round().int()):
             # Mask channel dimensions: (height, width)
             mask_channel[box_xyxy[1] : box_xyxy[3], box_xyxy[0] : box_xyxy[2]] = 1
 
         return mask
 
-    def transform_boxes(self, M: torch.Tensor, inplace: bool = False) -> "Boxes":
+    def transform_boxes(self, M: torch.Tensor, inplace: bool = False) -> Boxes:
         r"""Apply a transformation matrix to the 2D boxes.
 
         Args:
             M: The transformation matrix to be applied, shape of :math:`(3, 3)` or :math:`(B, 3, 3)`.
             inplace: do transform in-place and return self.
 
         Returns:
             The transformed boxes.
         """
         if not 2 <= M.ndim <= 3 or M.shape[-2:] != (3, 3):
             raise ValueError(f"The transformation matrix shape must be (3, 3) or (B, 3, 3). Got {M.shape}.")
 
-        # Due to some torch.jit.script bug (at least <= 1.9), you need to pass all arguments to __init__ when
-        # constructing the class from inside of a method.
         transformed_boxes = _transform_boxes(self._data, M)
         if inplace:
             self._data = transformed_boxes
             return self
 
-        return Boxes(transformed_boxes, False)
+        obj = self.clone()
+        obj._data = transformed_boxes
+        return obj
 
-    def transform_boxes_(self, M: torch.Tensor) -> "Boxes":
+    def transform_boxes_(self, M: torch.Tensor) -> Boxes:
         """Inplace version of :func:`Boxes.transform_boxes`"""
         return self.transform_boxes(M, inplace=True)
 
-    def translate(self, size: Tensor, method: str = "warp", inplace: bool = False) -> "Boxes":
+    def translate(self, size: Tensor, method: str = "warp", inplace: bool = False) -> Boxes:
         """Translates boxes by the provided size.
 
         Args:
             size: translate size for x, y direction, shape of :math:`(B, 2)`.
             method: "warp" or "fast".
             inplace: do transform in-place and return self.
 
@@ -562,27 +632,71 @@
         return self._data.device
 
     @property
     def dtype(self) -> torch.dtype:
         """Returns boxes dtype."""
         return self._data.dtype
 
-    def to(self, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> "Boxes":
+    def to(self, device: torch.device | None = None, dtype: torch.dtype | None = None) -> Boxes:
         """Like :func:`torch.nn.Module.to()` method."""
         # In torchscript, dtype is a int and not a class. https://github.com/pytorch/pytorch/issues/51941
         if dtype is not None and not _is_floating_point_dtype(dtype):
             raise ValueError("Boxes must be in floating point")
         self._data = self._data.to(device=device, dtype=dtype)
         return self
 
-    def clone(self) -> "Boxes":
-        return Boxes(self._data.clone(), False)
+    def clone(self) -> Boxes:
+        obj = type(self)(self._data.clone(), False)
+        obj._mode = self._mode
+        obj._N = self._N
+        obj._is_batched = self._is_batched
+        return obj
+
+    def type(self, dtype: torch.dtype) -> Boxes:
+        self._data = self._data.type(dtype)
+        return self
+
+
+class VideoBoxes(Boxes):
+    temporal_channel_size: int
+
+    @classmethod
+    def from_tensor(  # type: ignore[override]
+        cls, boxes: torch.Tensor | list[torch.Tensor], validate_boxes: bool = True
+    ) -> VideoBoxes:
+        if isinstance(boxes, (list,)) or (boxes.dim() != 5 or boxes.shape[-2:] != torch.Size([4, 2])):
+            raise ValueError("Input box type is not yet supported. Please input an `BxTxNx4x2` tensor directly.")
+
+        temporal_channel_size = boxes.size(1)
+
+        quadrilaterals = _boxes_to_quadrilaterals(
+            boxes.view(boxes.size(0) * boxes.size(1), -1, boxes.size(3), boxes.size(4)),
+            mode="vertices_plus",
+            validate_boxes=validate_boxes,
+        )
+        out = cls(quadrilaterals, False, "vertices_plus")
+        out.temporal_channel_size = temporal_channel_size
+        return out
+
+    def to_tensor(self, mode: str | None = None) -> torch.Tensor | list[torch.Tensor]:  # type: ignore[override]
+        out = super().to_tensor(mode, as_padded_sequence=False)
+        if isinstance(out, Tensor):
+            return out.view(-1, self.temporal_channel_size, *out.shape[1:])
+        # If returns a list of boxes.
+        return [_out.view(-1, self.temporal_channel_size, *_out.shape[1:]) for _out in out]
+
+    def clone(self) -> VideoBoxes:
+        obj = type(self)(self._data.clone(), False)
+        obj._mode = self._mode
+        obj._N = self._N
+        obj._is_batched = self._is_batched
+        obj.temporal_channel_size = self.temporal_channel_size
+        return obj
 
 
-@torch.jit.script
 class Boxes3D:
     r"""3D boxes containing N or BxN boxes.
 
     Args:
         boxes: 3D boxes, shape of :math:`(N,8,3)` or :math:`(B,N,8,3)`. See below for more details.
         raise_if_not_floating_point: flag to control floating point casting behaviour when `boxes` is not a floating
             point tensor. True to raise an error when `boxes` isn't a floating point tensor, False to cast to float.
@@ -604,27 +718,38 @@
         if not boxes.is_floating_point():
             if raise_if_not_floating_point:
                 raise ValueError(f"Coordinates must be in floating point. Got {boxes.dtype}.")
 
             boxes = boxes.float()
 
         if len(boxes.shape) == 0:
-            # Use reshape, so we don't end up creating a new tensor that does not depend on
-            # the inputs (and consequently confuses jit)
             boxes = boxes.reshape((-1, 6))
 
         if not (3 <= boxes.ndim <= 4 and boxes.shape[-2:] == (8, 3)):
             raise ValueError(f"3D bbox shape must be (N, 8, 3) or (B, N, 8, 3). Got {boxes.shape}.")
 
         self._is_batched = False if boxes.ndim == 3 else True
 
         self._data = boxes
         self._mode = mode
 
-    def get_boxes_shape(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+    def __getitem__(self, key: slice | int | Tensor) -> Boxes3D:
+        new_box = Boxes3D(self._data[key], False, mode="xyzxyz_plus")
+        new_box._mode = self._mode
+        return new_box
+
+    def __setitem__(self, key: slice | int | Tensor, value: Boxes3D) -> Boxes3D:
+        self._data[key] = value._data
+        return self
+
+    @property
+    def shape(self) -> tuple[int, ...] | Size:
+        return self.data.shape
+
+    def get_boxes_shape(self) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
         r"""Compute boxes heights and widths.
 
         Returns:
             - Boxes depths, shape of :math:`(N,)` or :math:`(B,N)`.
             - Boxes heights, shape of :math:`(N,)` or :math:`(B,N)`.
             - Boxes widths, shape of :math:`(N,)` or :math:`(B,N)`.
 
@@ -635,15 +760,15 @@
             (tensor([30., 60.]), tensor([20., 50.]), tensor([10., 40.]))
         """
         boxes_xyzwhd = self.to_tensor(mode='xyzwhd')
         widths, heights, depths = boxes_xyzwhd[..., 3], boxes_xyzwhd[..., 4], boxes_xyzwhd[..., 5]
         return depths, heights, widths
 
     @classmethod
-    def from_tensor(cls, boxes: torch.Tensor, mode: str = "xyzxyz", validate_boxes: bool = True) -> "Boxes3D":
+    def from_tensor(cls, boxes: torch.Tensor, mode: str = "xyzxyz", validate_boxes: bool = True) -> Boxes3D:
         r"""Helper method to easily create :class:`Boxes3D` from 3D boxes stored in another format.
 
         Args:
             boxes: 3D boxes, shape of :math:`(N,6)` or :math:`(B,N,6)`.
             mode: The format in which the 3D boxes are provided.
 
                 * 'xyzxyz': boxes are assumed to be in the format ``xmin, ymin, zmin, xmax, ymax, zmax`` where
@@ -709,16 +834,14 @@
             if (height <= 0).any():
                 raise ValueError("Some boxes have negative heights or 0.")
             if (depth <= 0).any():
                 raise ValueError("Some boxes have negative depths or 0.")
 
         hexahedrons = _boxes3d_to_polygons3d(xmin, ymin, zmin, width, height, depth)
         hexahedrons = hexahedrons if batched else hexahedrons.squeeze(0)
-        # Due to some torch.jit.script bug (at least <= 1.9), you need to pass all arguments to __init__ when
-        # constructing the class from inside of a method.
         return cls(hexahedrons, raise_if_not_floating_point=False, mode=mode)
 
     def to_tensor(self, mode: str = "xyzxyz") -> torch.Tensor:
         r"""Cast :class:`Boxes3D` to a tensor. ``mode`` controls which 3D boxes format should be use to represent
         boxes in the tensor.
 
         Args:
@@ -875,37 +998,35 @@
             # Mask channel dimensions: (depth, height, width)
             mask_channel[
                 box_xyzxyz[2] : box_xyzxyz[5], box_xyzxyz[1] : box_xyzxyz[4], box_xyzxyz[0] : box_xyzxyz[3]
             ] = 1
 
         return mask
 
-    def transform_boxes(self, M: torch.Tensor, inplace: bool = False) -> "Boxes3D":
+    def transform_boxes(self, M: torch.Tensor, inplace: bool = False) -> Boxes3D:
         r"""Apply a transformation matrix to the 3D boxes.
 
         Args:
             M: The transformation matrix to be applied, shape of :math:`(4, 4)` or :math:`(B, 4, 4)`.
             inplace: do transform in-place and return self.
 
         Returns:
             The transformed boxes.
         """
         if not 2 <= M.ndim <= 3 or M.shape[-2:] != (4, 4):
             raise ValueError(f"The transformation matrix shape must be (4, 4) or (B, 4, 4). Got {M.shape}.")
 
-        # Due to some torch.jit.script bug (at least <= 1.9), you need to pass all arguments to __init__ when
-        # constructing the class from inside of a method.
         transformed_boxes = _transform_boxes(self._data, M)
         if inplace:
             self._data = transformed_boxes
             return self
 
         return Boxes3D(transformed_boxes, False, "xyzxyz_plus")
 
-    def transform_boxes_(self, M: torch.Tensor) -> "Boxes3D":
+    def transform_boxes_(self, M: torch.Tensor) -> Boxes3D:
         """Inplace version of :func:`Boxes3D.transform_boxes`"""
         return self.transform_boxes(M, inplace=True)
 
     @property
     def data(self) -> torch.Tensor:
         return self._data
 
@@ -919,14 +1040,14 @@
         return self._data.device
 
     @property
     def dtype(self) -> torch.dtype:
         """Returns boxes dtype."""
         return self._data.dtype
 
-    def to(self, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> "Boxes3D":
+    def to(self, device: torch.device | None = None, dtype: torch.dtype | None = None) -> Boxes3D:
         """Like :func:`torch.nn.Module.to()` method."""
         # In torchscript, dtype is a int and not a class. https://github.com/pytorch/pytorch/issues/51941
         if dtype is not None and not _is_floating_point_dtype(dtype):
             raise ValueError("Boxes must be in floating point")
         self._data = self._data.to(device=device, dtype=dtype)
         return self
```

### Comparing `kornia-0.6.9/kornia/geometry/calibration/distort.py` & `kornia-0.7.0/kornia/geometry/calibration/distort.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/calibration/pnp.py` & `kornia-0.7.0/kornia/geometry/calibration/pnp.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/calibration/undistort.py` & `kornia-0.7.0/kornia/geometry/calibration/undistort.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-from typing import Optional
+from __future__ import annotations
 
 import torch
 
 from kornia.geometry.linalg import transform_points
 from kornia.geometry.transform import remap
 from kornia.utils import create_meshgrid
 
 from .distort import distort_points, tilt_projection
 
 
 # Based on https://github.com/opencv/opencv/blob/master/modules/calib3d/src/undistort.dispatch.cpp#L384
 def undistort_points(
-    points: torch.Tensor, K: torch.Tensor, dist: torch.Tensor, new_K: Optional[torch.Tensor] = None, num_iters: int = 5
+    points: torch.Tensor, K: torch.Tensor, dist: torch.Tensor, new_K: torch.Tensor | None = None, num_iters: int = 5
 ) -> torch.Tensor:
     r"""Compensate for lens distortion a set of 2D image points.
 
     Radial :math:`(k_1, k_2, k_3, k_4, k_5, k_6)`,
     tangential :math:`(p_1, p_2)`, thin prism :math:`(s_1, s_2, s_3, s_4)`, and tilt :math:`(\tau_x, \tau_y)`
     distortion models are considered in this function.
```

### Comparing `kornia-0.6.9/kornia/geometry/camera/perspective.py` & `kornia-0.7.0/kornia/geometry/camera/perspective.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/camera/pinhole.py` & `kornia-0.7.0/kornia/geometry/camera/pinhole.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-from typing import Iterable, List
+from typing import Iterable, List, Union
 
 import torch
 
 from kornia.core import Device, Tensor
+from kornia.core.check import KORNIA_CHECK_SAME_DEVICE
 from kornia.geometry.conversions import convert_points_from_homogeneous, convert_points_to_homogeneous
 from kornia.geometry.linalg import inverse_transformation, transform_points
-from kornia.testing import KORNIA_CHECK_SAME_DEVICE
 from kornia.utils.helpers import _torch_inverse_cast
 
 
 class PinholeCamera:
     r"""Class that represents a Pinhole Camera model.
 
     Args:
@@ -149,45 +149,45 @@
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.extrinsics[..., 0, -1]
 
     @tx.setter
-    def tx(self, value) -> 'PinholeCamera':
+    def tx(self, value: Union[Tensor, float, int]) -> 'PinholeCamera':
         r"""Set the x-coordinate of the translation vector with the given value."""
         self.extrinsics[..., 0, -1] = value
         return self
 
     @property
     def ty(self) -> Tensor:
         r"""Return the y-coordinate of the translation vector.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.extrinsics[..., 1, -1]
 
     @ty.setter
-    def ty(self, value) -> 'PinholeCamera':
+    def ty(self, value: Union[Tensor, float, int]) -> 'PinholeCamera':
         r"""Set the y-coordinate of the translation vector with the given value."""
         self.extrinsics[..., 1, -1] = value
         return self
 
     @property
     def tz(self) -> Tensor:
         r"""Returns the z-coordinate of the translation vector.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.extrinsics[..., 2, -1]
 
     @tz.setter
-    def tz(self, value) -> 'PinholeCamera':
+    def tz(self, value: Union[Tensor, float, int]) -> 'PinholeCamera':
         r"""Set the y-coordinate of the translation vector with the given value."""
         self.extrinsics[..., 2, -1] = value
         return self
 
     @property
     def rt_matrix(self) -> Tensor:
         r"""Return the 3x4 rotation-translation matrix.
@@ -236,15 +236,15 @@
         r"""Return the inverse of the 4x4 instrisics matrix.
 
         Returns:
             tensor of shape :math:`(B, 4, 4)`.
         """
         return self.intrinsics.inverse()
 
-    def scale(self, scale_factor) -> 'PinholeCamera':
+    def scale(self, scale_factor: Tensor) -> 'PinholeCamera':
         r"""Scale the pinhole model.
 
         Args:
             scale_factor: a tensor with the scale factor. It has
               to be broadcastable with class members. The expected shape is
               :math:`(B)` or :math:`(1)`.
 
@@ -258,15 +258,15 @@
         intrinsics[..., 0, 2] *= scale_factor
         intrinsics[..., 1, 2] *= scale_factor
         # scale the image height/width
         height: Tensor = scale_factor * self.height.clone()
         width: Tensor = scale_factor * self.width.clone()
         return PinholeCamera(intrinsics, self.extrinsics, height, width)
 
-    def scale_(self, scale_factor) -> 'PinholeCamera':
+    def scale_(self, scale_factor: Union[float, int, Tensor]) -> 'PinholeCamera':
         r"""Scale the pinhole model in-place.
 
         Args:
             scale_factor: a tensor with the scale factor. It has
               to be broadcastable with class members. The expected shape is
               :math:`(B)` or :math:`(1)`.
 
@@ -303,15 +303,15 @@
             >>> pinhole = kornia.geometry.camera.PinholeCamera(K, E, h, w)
             >>> pinhole.project(X)
             tensor([[5.6088, 8.6827]])
         """
         P = self.intrinsics @ self.extrinsics
         return convert_points_from_homogeneous(transform_points(P, point_3d))
 
-    def unproject(self, point_2d: Tensor, depth: Tensor):
+    def unproject(self, point_2d: Tensor, depth: Tensor) -> Tensor:
         r"""Unproject a 2d point in 3d.
 
         Transform coordinates in the pixel frame to the world frame.
 
         Args:
             point2d: tensor containing the 2d to be projected to
                 world coordinates. The shape of the tensor can be :math:`(*, 2)`.
@@ -339,16 +339,28 @@
         P = self.intrinsics @ self.extrinsics
         P_inv = _torch_inverse_cast(P)
         return transform_points(P_inv, convert_points_to_homogeneous(point_2d) * depth)
 
     # NOTE: just for test. Decide if we keep it.
     @classmethod
     def from_parameters(
-        self, fx, fy, cx, cy, height, width, tx, ty, tz, batch_size, device: Device, dtype: torch.dtype
-    ):
+        self,
+        fx: Tensor,
+        fy: Tensor,
+        cx: Tensor,
+        cy: Tensor,
+        height: int,
+        width: int,
+        tx: Tensor,
+        ty: Tensor,
+        tz: Tensor,
+        batch_size: int,
+        device: Device,
+        dtype: torch.dtype,
+    ) -> 'PinholeCamera':
         # create the camera matrix
         intrinsics = torch.zeros(batch_size, 4, 4, device=device, dtype=dtype)
         intrinsics[..., 0, 0] += fx
         intrinsics[..., 1, 1] += fy
         intrinsics[..., 0, 2] += cx
         intrinsics[..., 1, 2] += cy
         intrinsics[..., 2, 2] += 1.0
@@ -560,15 +572,15 @@
     """
     if not (len(pinholes.shape) == 2 and pinholes.shape[1] == 12):
         raise AssertionError(pinholes.shape)
     # TODO: where is rtvec_to_pose?
     raise NotImplementedError
     # TODO: We have rtvec_to_pose in torchgeometry
     # https://github.com/whh14/torchgeometry/blob/master/torchgeometry/conversions.py#L240
-    # But it relies on angle_axis_to_rotation_matrix
+    # But it relies on axis_angle_to_rotation_matrix
     # And since then, it was changed from returning Nx4x4 matrix to Nx3x3
     # return rtvec_to_pose(optical_pose_parent)   type: ignore
 
 
 def homography_i_H_ref(pinhole_i: Tensor, pinhole_ref: Tensor) -> Tensor:
     r"""Homography from reference to ith pinhole.
```

### Comparing `kornia-0.6.9/kornia/geometry/camera/stereo.py` & `kornia-0.7.0/kornia/geometry/camera/stereo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,18 @@
+from typing import Any
+
 import torch
 
 from kornia.core import Tensor, stack, zeros
 from kornia.geometry.linalg import transform_points
 from kornia.utils.grid import create_meshgrid
 
 
 class StereoException(Exception):
-    def __init__(self, msg: str, *args, **kwargs):
+    def __init__(self, msg: str, *args: Any, **kwargs: Any) -> None:
         r"""Custom exception for the :module:`~kornia.geometry.camera.stereo` module.
 
         Adds a general helper module redirecting the user to the proper documentation site.
 
         Args:
             msg: Custom message to add to the general message.
             *args:
@@ -23,15 +25,15 @@
         final_msg = msg + doc_help
         # type ignore because of mypy error:
         # Too many arguments for "__init__" of "BaseException"
         super().__init__(final_msg, *args, **kwargs)
 
 
 class StereoCamera:
-    def __init__(self, rectified_left_camera: Tensor, rectified_right_camera: Tensor):
+    def __init__(self, rectified_left_camera: Tensor, rectified_right_camera: Tensor) -> None:
         r"""Class representing a horizontal stereo camera setup.
 
         Args:
             rectified_left_camera: The rectified left camera projection matrix
               of shape :math:`(B, 3, 4)`
             rectified_right_camera: The rectified right camera projection matrix
               of shape :math:`(B, 3, 4)`
@@ -42,15 +44,15 @@
 
         self.device = self.rectified_left_camera.device
         self.dtype = self.rectified_left_camera.dtype
 
         self._Q_matrix = self._init_Q_matrix()
 
     @staticmethod
-    def _check_stereo_camera(rectified_left_camera: Tensor, rectified_right_camera: Tensor):
+    def _check_stereo_camera(rectified_left_camera: Tensor, rectified_right_camera: Tensor) -> None:
         r"""Utility function to ensure user specified correct camera matrices.
 
         Args:
             rectified_left_camera: The rectified left camera projection matrix
               of shape :math:`(B, 3, 4)`
             rectified_right_camera: The rectified right camera projection matrix
               of shape :math:`(B, 3, 4)`
@@ -217,15 +219,15 @@
 
         Returns:
             The 3D point cloud of shape :math:`(B, H, W, 3)`
         """
         return reproject_disparity_to_3D(disparity_tensor, self.Q)
 
 
-def _check_disparity_tensor(disparity_tensor: Tensor):
+def _check_disparity_tensor(disparity_tensor: Tensor) -> None:
     r"""Utility function to ensure correct user provided correct disparity tensor.
 
     Args:
         disparity_tensor: The disparity tensor of shape :math:`(B, 1, H, W)`.
     """
     if not isinstance(disparity_tensor, Tensor):
         raise StereoException(
@@ -244,21 +246,20 @@
     if disparity_tensor.dtype not in (torch.float16, torch.float32, torch.float64):
         raise StereoException(
             f"Expected 'disparity_tensor' to have dtype torch.float16, torch.float32 or torch.float64."
             f"Got {disparity_tensor.dtype}"
         )
 
 
-def _check_Q_matrix(Q_matrix: Tensor):
+def _check_Q_matrix(Q_matrix: Tensor) -> None:
     r"""Utility function to ensure Q matrix is of correct form.
 
     Args:
         Q_matrix: The Q matrix for reprojecting disparity to a point cloud of shape :math:`(B, 4, 4)`
     """
-
     if not isinstance(Q_matrix, Tensor):
         raise StereoException(f"Expected 'Q_matrix' to be an instance of Tensor but got {type(Q_matrix)}.")
 
     if not len(Q_matrix.shape) == 3:
         raise StereoException(f"Expected 'Q_matrix' to have 3 dimensions." f"Got {Q_matrix.shape}")
 
     if not Q_matrix.shape[1:] == (4, 4):
```

### Comparing `kornia-0.6.9/kornia/geometry/conversions.py` & `kornia-0.7.0/kornia/geometry/conversions.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,33 +1,35 @@
-import enum
-import warnings
-from typing import Optional, Tuple
+from __future__ import annotations
 
 import torch
 import torch.nn.functional as F
 
 from kornia.constants import pi
 from kornia.core import Tensor, concatenate, pad, stack, tensor, where
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SHAPE
+from kornia.utils import deprecated
 from kornia.utils.helpers import _torch_inverse_cast
-from kornia.utils.misc import eye_like
 
 __all__ = [
     "rad2deg",
     "deg2rad",
     "pol2cart",
     "cart2pol",
     "convert_points_from_homogeneous",
     "convert_points_to_homogeneous",
     "convert_affinematrix_to_homography",
     "convert_affinematrix_to_homography3d",
+    "axis_angle_to_rotation_matrix",
     "angle_axis_to_rotation_matrix",
+    "axis_angle_to_quaternion",
     "angle_axis_to_quaternion",
+    "rotation_matrix_to_axis_angle",
     "rotation_matrix_to_angle_axis",
     "rotation_matrix_to_quaternion",
+    "quaternion_to_axis_angle",
     "quaternion_to_angle_axis",
     "quaternion_to_rotation_matrix",
     "quaternion_log_to_exp",
     "quaternion_exp_to_log",
     "quaternion_from_euler",
     "euler_from_quaternion",
     "denormalize_pixel_coordinates",
@@ -51,19 +53,14 @@
     "camtoworld_vision_to_graphics_4x4",
     "camtoworld_graphics_to_vision_Rt",
     "camtoworld_vision_to_graphics_Rt",
     "ARKitQTVecs_to_ColmapQTVecs",
 ]
 
 
-class QuaternionCoeffOrder(enum.Enum):
-    XYZW = 'xyzw'
-    WXYZ = 'wxyz'
-
-
 def rad2deg(tensor: Tensor) -> Tensor:
     r"""Function that converts angles from radians to degrees.
 
     Args:
         tensor: Tensor of arbitrary shape.
 
     Returns:
@@ -96,15 +93,15 @@
     """
     if not isinstance(tensor, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
 
     return tensor * pi.to(tensor.device).type(tensor.dtype) / 180.0
 
 
-def pol2cart(rho: Tensor, phi: Tensor) -> Tuple[Tensor, Tensor]:
+def pol2cart(rho: Tensor, phi: Tensor) -> tuple[Tensor, Tensor]:
     r"""Function that converts polar coordinates to cartesian coordinates.
 
     Args:
         rho: Tensor of arbitrary shape.
         phi: Tensor of same arbitrary shape.
 
     Returns:
@@ -120,15 +117,15 @@
         raise TypeError(f"Input type is not a Tensor. Got {type(rho)}, {type(phi)}")
 
     x = rho * torch.cos(phi)
     y = rho * torch.sin(phi)
     return x, y
 
 
-def cart2pol(x: Tensor, y: Tensor, eps: float = 1.0e-8) -> Tuple[Tensor, Tensor]:
+def cart2pol(x: Tensor, y: Tensor, eps: float = 1.0e-8) -> tuple[Tensor, Tensor]:
     """Function that converts cartesian coordinates to polar coordinates.
 
     Args:
         x: Tensor of arbitrary shape.
         y: Tensor of same arbitrary shape.
         eps: To avoid division by zero.
 
@@ -260,49 +257,49 @@
 
     if not (len(A.shape) == 3 and A.shape[-2:] == (3, 4)):
         raise ValueError(f"Input matrix must be a Bx3x4 tensor. Got {A.shape}")
 
     return _convert_affinematrix_to_homography_impl(A)
 
 
-def angle_axis_to_rotation_matrix(angle_axis: Tensor) -> Tensor:
+def axis_angle_to_rotation_matrix(axis_angle: Tensor) -> Tensor:
     r"""Convert 3d vector of axis-angle rotation to 3x3 rotation matrix.
 
     Args:
-        angle_axis: tensor of 3d vector of axis-angle rotations in radians with shape :math:`(N, 3)`.
+        axis_angle: tensor of 3d vector of axis-angle rotations in radians with shape :math:`(N, 3)`.
 
     Returns:
         tensor of rotation matrices of shape :math:`(N, 3, 3)`.
 
     Example:
         >>> input = tensor([[0., 0., 0.]])
-        >>> angle_axis_to_rotation_matrix(input)
+        >>> axis_angle_to_rotation_matrix(input)
         tensor([[[1., 0., 0.],
                  [0., 1., 0.],
                  [0., 0., 1.]]])
 
         >>> input = tensor([[1.5708, 0., 0.]])
-        >>> angle_axis_to_rotation_matrix(input)
+        >>> axis_angle_to_rotation_matrix(input)
         tensor([[[ 1.0000e+00,  0.0000e+00,  0.0000e+00],
                  [ 0.0000e+00, -3.6200e-06, -1.0000e+00],
                  [ 0.0000e+00,  1.0000e+00, -3.6200e-06]]])
     """
-    if not isinstance(angle_axis, Tensor):
-        raise TypeError(f"Input type is not a Tensor. Got {type(angle_axis)}")
+    if not isinstance(axis_angle, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(axis_angle)}")
 
-    if not angle_axis.shape[-1] == 3:
-        raise ValueError(f"Input size must be a (*, 3) tensor. Got {angle_axis.shape}")
+    if not axis_angle.shape[-1] == 3:
+        raise ValueError(f"Input size must be a (*, 3) tensor. Got {axis_angle.shape}")
 
-    def _compute_rotation_matrix(angle_axis, theta2, eps=1e-6):
+    def _compute_rotation_matrix(axis_angle: Tensor, theta2: Tensor, eps: float = 1e-6) -> Tensor:
         # We want to be careful to only evaluate the square root if the
-        # norm of the angle_axis vector is greater than zero. Otherwise
+        # norm of the axis_angle vector is greater than zero. Otherwise
         # we get a division by zero.
         k_one = 1.0
         theta = torch.sqrt(theta2)
-        wxyz = angle_axis / (theta + eps)
+        wxyz = axis_angle / (theta + eps)
         wx, wy, wz = torch.chunk(wxyz, 3, dim=1)
         cos_theta = torch.cos(theta)
         sin_theta = torch.sin(theta)
 
         r00 = cos_theta + wx * wx * (k_one - cos_theta)
         r10 = wz * sin_theta + wx * wy * (k_one - cos_theta)
         r20 = -wy * sin_theta + wx * wz * (k_one - cos_theta)
@@ -311,165 +308,147 @@
         r21 = wx * sin_theta + wy * wz * (k_one - cos_theta)
         r02 = wy * sin_theta + wx * wz * (k_one - cos_theta)
         r12 = -wx * sin_theta + wy * wz * (k_one - cos_theta)
         r22 = cos_theta + wz * wz * (k_one - cos_theta)
         rotation_matrix = concatenate([r00, r01, r02, r10, r11, r12, r20, r21, r22], dim=1)
         return rotation_matrix.view(-1, 3, 3)
 
-    def _compute_rotation_matrix_taylor(angle_axis):
-        rx, ry, rz = torch.chunk(angle_axis, 3, dim=1)
+    def _compute_rotation_matrix_taylor(axis_angle: Tensor) -> Tensor:
+        rx, ry, rz = torch.chunk(axis_angle, 3, dim=1)
         k_one = torch.ones_like(rx)
         rotation_matrix = concatenate([k_one, -rz, ry, rz, k_one, -rx, -ry, rx, k_one], dim=1)
         return rotation_matrix.view(-1, 3, 3)
 
     # stolen from ceres/rotation.h
 
-    _angle_axis = torch.unsqueeze(angle_axis, dim=1)
-    theta2 = torch.matmul(_angle_axis, _angle_axis.transpose(1, 2))
+    _axis_angle = torch.unsqueeze(axis_angle, dim=1)
+    theta2 = torch.matmul(_axis_angle, _axis_angle.transpose(1, 2))
     theta2 = torch.squeeze(theta2, dim=1)
 
     # compute rotation matrices
-    rotation_matrix_normal = _compute_rotation_matrix(angle_axis, theta2)
-    rotation_matrix_taylor = _compute_rotation_matrix_taylor(angle_axis)
+    rotation_matrix_normal = _compute_rotation_matrix(axis_angle, theta2)
+    rotation_matrix_taylor = _compute_rotation_matrix_taylor(axis_angle)
 
     # create mask to handle both cases
     eps = 1e-6
     mask = (theta2 > eps).view(-1, 1, 1).to(theta2.device)
     mask_pos = (mask).type_as(theta2)
     mask_neg = (~mask).type_as(theta2)
 
-    # create output pose matrix
-    rotation_matrix = eye_like(3, angle_axis, shared_memory=False)
-    # fill output matrix with masked values
-    rotation_matrix[..., :3, :3] = mask_pos * rotation_matrix_normal + mask_neg * rotation_matrix_taylor
+    # create output pose matrix with masked values
+    rotation_matrix = mask_pos * rotation_matrix_normal + mask_neg * rotation_matrix_taylor
     return rotation_matrix  # Nx3x3
 
 
-def rotation_matrix_to_angle_axis(rotation_matrix: Tensor) -> Tensor:
+@deprecated(replace_with='axis_angle_to_rotation_matrix', version='0.7.0')
+def angle_axis_to_rotation_matrix(axis_angle: Tensor) -> Tensor:
+    return axis_angle_to_rotation_matrix(axis_angle)
+
+
+def rotation_matrix_to_axis_angle(rotation_matrix: Tensor) -> Tensor:
     r"""Convert 3x3 rotation matrix to Rodrigues vector in radians.
 
     Args:
         rotation_matrix: rotation matrix of shape :math:`(N, 3, 3)`.
 
     Returns:
         Rodrigues vector transformation of shape :math:`(N, 3)`.
 
     Example:
         >>> input = tensor([[1., 0., 0.],
         ...                       [0., 1., 0.],
         ...                       [0., 0., 1.]])
-        >>> rotation_matrix_to_angle_axis(input)
+        >>> rotation_matrix_to_axis_angle(input)
         tensor([0., 0., 0.])
 
         >>> input = tensor([[1., 0., 0.],
         ...                       [0., 0., -1.],
         ...                       [0., 1., 0.]])
-        >>> rotation_matrix_to_angle_axis(input)
+        >>> rotation_matrix_to_axis_angle(input)
         tensor([1.5708, 0.0000, 0.0000])
     """
     if not isinstance(rotation_matrix, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(rotation_matrix)}")
 
     if not rotation_matrix.shape[-2:] == (3, 3):
         raise ValueError(f"Input size must be a (*, 3, 3) tensor. Got {rotation_matrix.shape}")
-    quaternion: Tensor = rotation_matrix_to_quaternion(rotation_matrix, order=QuaternionCoeffOrder.WXYZ)
-    return quaternion_to_angle_axis(quaternion, order=QuaternionCoeffOrder.WXYZ)
+    quaternion: Tensor = rotation_matrix_to_quaternion(rotation_matrix)
+    return quaternion_to_axis_angle(quaternion)
 
 
-def rotation_matrix_to_quaternion(
-    rotation_matrix: Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> Tensor:
-    r"""Convert 3x3 rotation matrix to 4d quaternion vector.
+@deprecated(replace_with='rotation_matrix_to_axis_angle', version='0.7.0')
+def rotation_matrix_to_angle_axis(rotation_matrix: Tensor) -> Tensor:
+    return rotation_matrix_to_axis_angle(rotation_matrix)
 
-    The quaternion vector has components in (w, x, y, z) or (x, y, z, w) format.
 
-    .. note::
-        The (x, y, z, w) order is going to be deprecated in favor of efficiency.
+def rotation_matrix_to_quaternion(rotation_matrix: Tensor, eps: float = 1.0e-8) -> Tensor:
+    r"""Convert 3x3 rotation matrix to 4d quaternion vector.
+
+    The quaternion vector has components in (w, x, y, z) format.
 
     Args:
         rotation_matrix: the rotation matrix to convert with shape :math:`(*, 3, 3)`.
         eps: small value to avoid zero division.
-        order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the rotation in quaternion with shape :math:`(*, 4)`.
 
     Example:
         >>> input = tensor([[1., 0., 0.],
         ...                       [0., 1., 0.],
         ...                       [0., 0., 1.]])
-        >>> rotation_matrix_to_quaternion(input, eps=torch.finfo(input.dtype).eps,
-        ...                               order=QuaternionCoeffOrder.WXYZ)
+        >>> rotation_matrix_to_quaternion(input, eps=torch.finfo(input.dtype).eps)
         tensor([1., 0., 0., 0.])
     """
     if not isinstance(rotation_matrix, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(rotation_matrix)}")
 
     if not rotation_matrix.shape[-2:] == (3, 3):
         raise ValueError(f"Input size must be a (*, 3, 3) tensor. Got {rotation_matrix.shape}")
 
-    if not torch.jit.is_scripting():
-        if order.name not in QuaternionCoeffOrder.__members__.keys():
-            raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
-
-    if order == QuaternionCoeffOrder.XYZW:
-        warnings.warn(
-            "`XYZW` quaternion coefficient order is deprecated and"
-            " will be removed after > 0.6. "
-            "Please use `QuaternionCoeffOrder.WXYZ` instead."
-        )
-
     def safe_zero_division(numerator: Tensor, denominator: Tensor) -> Tensor:
         eps: float = torch.finfo(numerator.dtype).tiny
         return numerator / torch.clamp(denominator, min=eps)
 
-    rotation_matrix_vec: Tensor = rotation_matrix.view(*rotation_matrix.shape[:-2], 9)
+    rotation_matrix_vec: Tensor = rotation_matrix.reshape(*rotation_matrix.shape[:-2], 9)
 
     m00, m01, m02, m10, m11, m12, m20, m21, m22 = torch.chunk(rotation_matrix_vec, chunks=9, dim=-1)
 
     trace: Tensor = m00 + m11 + m22
 
-    def trace_positive_cond():
+    def trace_positive_cond() -> Tensor:
         sq = torch.sqrt(trace + 1.0 + eps) * 2.0  # sq = 4 * qw.
         qw = 0.25 * sq
         qx = safe_zero_division(m21 - m12, sq)
         qy = safe_zero_division(m02 - m20, sq)
         qz = safe_zero_division(m10 - m01, sq)
-        if order == QuaternionCoeffOrder.XYZW:
-            return concatenate((qx, qy, qz, qw), dim=-1)
         return concatenate((qw, qx, qy, qz), dim=-1)
 
-    def cond_1():
+    def cond_1() -> Tensor:
         sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.0  # sq = 4 * qx.
         qw = safe_zero_division(m21 - m12, sq)
         qx = 0.25 * sq
         qy = safe_zero_division(m01 + m10, sq)
         qz = safe_zero_division(m02 + m20, sq)
-        if order == QuaternionCoeffOrder.XYZW:
-            return concatenate((qx, qy, qz, qw), dim=-1)
         return concatenate((qw, qx, qy, qz), dim=-1)
 
-    def cond_2():
+    def cond_2() -> Tensor:
         sq = torch.sqrt(1.0 + m11 - m00 - m22 + eps) * 2.0  # sq = 4 * qy.
         qw = safe_zero_division(m02 - m20, sq)
         qx = safe_zero_division(m01 + m10, sq)
         qy = 0.25 * sq
         qz = safe_zero_division(m12 + m21, sq)
-        if order == QuaternionCoeffOrder.XYZW:
-            return concatenate((qx, qy, qz, qw), dim=-1)
         return concatenate((qw, qx, qy, qz), dim=-1)
 
-    def cond_3():
+    def cond_3() -> Tensor:
         sq = torch.sqrt(1.0 + m22 - m00 - m11 + eps) * 2.0  # sq = 4 * qz.
         qw = safe_zero_division(m10 - m01, sq)
         qx = safe_zero_division(m02 + m20, sq)
         qy = safe_zero_division(m12 + m21, sq)
         qz = 0.25 * sq
-        if order == QuaternionCoeffOrder.XYZW:
-            return concatenate((qx, qy, qz, qw), dim=-1)
         return concatenate((qw, qx, qy, qz), dim=-1)
 
     where_2 = where(m11 > m22, cond_2(), cond_3())
     where_1 = where((m00 > m11) & (m00 > m22), cond_1(), where_2)
 
     quaternion: Tensor = where(trace > 0.0, trace_positive_cond(), where_1)
     return quaternion
@@ -502,61 +481,47 @@
 
 
 # based on:
 # https://github.com/matthew-brett/transforms3d/blob/8965c48401d9e8e66b6a8c37c65f2fc200a076fa/transforms3d/quaternions.py#L101
 # https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/geometry/transformation/rotation_matrix_3d.py#L247
 
 
-def quaternion_to_rotation_matrix(
-    quaternion: Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> Tensor:
+def quaternion_to_rotation_matrix(quaternion: Tensor) -> Tensor:
     r"""Convert a quaternion to a rotation matrix.
 
-    The quaternion should be in (x, y, z, w) or (w, x, y, z) format.
+    The quaternion should be in (w, x, y, z) format.
 
     Args:
         quaternion: a tensor containing a quaternion to be converted.
           The tensor can be of shape :math:`(*, 4)`.
-        order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the rotation matrix of shape :math:`(*, 3, 3)`.
 
     Example:
         >>> quaternion = tensor((0., 0., 0., 1.))
-        >>> quaternion_to_rotation_matrix(quaternion, order=QuaternionCoeffOrder.WXYZ)
+        >>> quaternion_to_rotation_matrix(quaternion)
         tensor([[-1.,  0.,  0.],
                 [ 0., -1.,  0.],
                 [ 0.,  0.,  1.]])
     """
     if not isinstance(quaternion, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 4:
         raise ValueError(f"Input must be a tensor of shape (*, 4). Got {quaternion.shape}")
 
-    if not torch.jit.is_scripting():
-        if order.name not in QuaternionCoeffOrder.__members__.keys():
-            raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
-
-    if order == QuaternionCoeffOrder.XYZW:
-        warnings.warn(
-            "`XYZW` quaternion coefficient order is deprecated and"
-            " will be removed after > 0.6. "
-            "Please use `QuaternionCoeffOrder.WXYZ` instead."
-        )
-
     # normalize the input quaternion
     quaternion_norm: Tensor = normalize_quaternion(quaternion)
 
     # unpack the normalized quaternion components
-    if order == QuaternionCoeffOrder.XYZW:
-        x, y, z, w = torch.chunk(quaternion_norm, chunks=4, dim=-1)
-    else:
-        w, x, y, z = torch.chunk(quaternion_norm, chunks=4, dim=-1)
+    w = quaternion_norm[..., 0]
+    x = quaternion_norm[..., 1]
+    y = quaternion_norm[..., 2]
+    z = quaternion_norm[..., 3]
 
     # compute the actual conversion
     tx: Tensor = 2.0 * x
     ty: Tensor = 2.0 * y
     tz: Tensor = 2.0 * z
     twx: Tensor = tx * w
     twy: Tensor = ty * w
@@ -565,302 +530,242 @@
     txy: Tensor = ty * x
     txz: Tensor = tz * x
     tyy: Tensor = ty * y
     tyz: Tensor = tz * y
     tzz: Tensor = tz * z
     one: Tensor = tensor(1.0)
 
-    matrix: Tensor = stack(
+    matrix_flat: Tensor = stack(
         (
             one - (tyy + tzz),
             txy - twz,
             txz + twy,
             txy + twz,
             one - (txx + tzz),
             tyz - twx,
             txz - twy,
             tyz + twx,
             one - (txx + tyy),
         ),
         dim=-1,
-    ).view(-1, 3, 3)
+    )
+
+    # this slightly awkward construction of the output shape is to satisfy torchscript
+    output_shape = [*list(quaternion.shape[:-1]), 3, 3]
+    matrix = matrix_flat.reshape(output_shape)
 
-    if len(quaternion.shape) == 1:
-        matrix = torch.squeeze(matrix, dim=0)
     return matrix
 
 
-def quaternion_to_angle_axis(quaternion: Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW) -> Tensor:
-    """Convert quaternion vector to angle axis of rotation in radians.
+def quaternion_to_axis_angle(quaternion: Tensor) -> Tensor:
+    """Convert quaternion vector to axis angle of rotation in radians.
 
-    The quaternion should be in (x, y, z, w) or (w, x, y, z) format.
+    The quaternion should be in (w, x, y, z) format.
 
     Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h
 
     Args:
         quaternion: tensor with quaternions.
-        order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
-        tensor with angle axis of rotation.
+        tensor with axis angle of rotation.
 
     Shape:
         - Input: :math:`(*, 4)` where `*` means, any number of dimensions
         - Output: :math:`(*, 3)`
 
     Example:
         >>> quaternion = tensor((1., 0., 0., 0.))
-        >>> quaternion_to_angle_axis(quaternion)
-        tensor([3.1416, 0.0000, 0.0000])
+        >>> quaternion_to_axis_angle(quaternion)
+        tensor([0., 0., 0.])
     """
     if not torch.is_tensor(quaternion):
         raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 4:
         raise ValueError(f"Input must be a tensor of shape Nx4 or 4. Got {quaternion.shape}")
 
-    if not torch.jit.is_scripting():
-        if order.name not in QuaternionCoeffOrder.__members__.keys():
-            raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
-
-    if order == QuaternionCoeffOrder.XYZW:
-        warnings.warn(
-            "`XYZW` quaternion coefficient order is deprecated and"
-            " will be removed after > 0.6. "
-            "Please use `QuaternionCoeffOrder.WXYZ` instead."
-        )
     # unpack input and compute conversion
     q1: Tensor = tensor([])
     q2: Tensor = tensor([])
     q3: Tensor = tensor([])
     cos_theta: Tensor = tensor([])
 
-    if order == QuaternionCoeffOrder.XYZW:
-        q1 = quaternion[..., 0]
-        q2 = quaternion[..., 1]
-        q3 = quaternion[..., 2]
-        cos_theta = quaternion[..., 3]
-    else:
-        cos_theta = quaternion[..., 0]
-        q1 = quaternion[..., 1]
-        q2 = quaternion[..., 2]
-        q3 = quaternion[..., 3]
+    cos_theta = quaternion[..., 0]
+    q1 = quaternion[..., 1]
+    q2 = quaternion[..., 2]
+    q3 = quaternion[..., 3]
 
     sin_squared_theta: Tensor = q1 * q1 + q2 * q2 + q3 * q3
 
     sin_theta: Tensor = torch.sqrt(sin_squared_theta)
     two_theta: Tensor = 2.0 * where(
         cos_theta < 0.0, torch.atan2(-sin_theta, -cos_theta), torch.atan2(sin_theta, cos_theta)
     )
 
     k_pos: Tensor = two_theta / sin_theta
     k_neg: Tensor = 2.0 * torch.ones_like(sin_theta)
     k: Tensor = where(sin_squared_theta > 0.0, k_pos, k_neg)
 
-    angle_axis: Tensor = torch.zeros_like(quaternion)[..., :3]
-    angle_axis[..., 0] += q1 * k
-    angle_axis[..., 1] += q2 * k
-    angle_axis[..., 2] += q3 * k
-    return angle_axis
+    axis_angle: Tensor = torch.zeros_like(quaternion)[..., :3]
+    axis_angle[..., 0] += q1 * k
+    axis_angle[..., 1] += q2 * k
+    axis_angle[..., 2] += q3 * k
+    return axis_angle
 
 
-def quaternion_log_to_exp(
-    quaternion: Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> Tensor:
+@deprecated(replace_with='quaternion_to_axis_angle', version='0.7.0')
+def quaternion_to_angle_axis(quaternion: Tensor) -> Tensor:
+    return quaternion_to_axis_angle(quaternion)
+
+
+def quaternion_log_to_exp(quaternion: Tensor, eps: float = 1.0e-8) -> Tensor:
     r"""Apply exponential map to log quaternion.
 
-    The quaternion should be in (x, y, z, w) or (w, x, y, z) format.
+    The quaternion should be in (w, x, y, z) format.
 
     Args:
         quaternion: a tensor containing a quaternion to be converted.
           The tensor can be of shape :math:`(*, 3)`.
         eps: a small number for clamping.
-        order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the quaternion exponential map of shape :math:`(*, 4)`.
 
     Example:
         >>> quaternion = tensor((0., 0., 0.))
-        >>> quaternion_log_to_exp(quaternion, eps=torch.finfo(quaternion.dtype).eps,
-        ...                       order=QuaternionCoeffOrder.WXYZ)
+        >>> quaternion_log_to_exp(quaternion, eps=torch.finfo(quaternion.dtype).eps)
         tensor([1., 0., 0., 0.])
     """
     if not isinstance(quaternion, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 3:
         raise ValueError(f"Input must be a tensor of shape (*, 3). Got {quaternion.shape}")
 
-    if not torch.jit.is_scripting():
-        if order.name not in QuaternionCoeffOrder.__members__.keys():
-            raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
-
-    if order == QuaternionCoeffOrder.XYZW:
-        warnings.warn(
-            "`XYZW` quaternion coefficient order is deprecated and"
-            " will be removed after > 0.6. "
-            "Please use `QuaternionCoeffOrder.WXYZ` instead."
-        )
-
     # compute quaternion norm
     norm_q: Tensor = torch.norm(quaternion, p=2, dim=-1, keepdim=True).clamp(min=eps)
 
     # compute scalar and vector
     quaternion_vector: Tensor = quaternion * torch.sin(norm_q) / norm_q
     quaternion_scalar: Tensor = torch.cos(norm_q)
 
     # compose quaternion and return
     quaternion_exp: Tensor = tensor([])
-    if order == QuaternionCoeffOrder.XYZW:
-        quaternion_exp = concatenate((quaternion_vector, quaternion_scalar), dim=-1)
-    else:
-        quaternion_exp = concatenate((quaternion_scalar, quaternion_vector), dim=-1)
+    quaternion_exp = concatenate((quaternion_scalar, quaternion_vector), dim=-1)
 
     return quaternion_exp
 
 
-def quaternion_exp_to_log(
-    quaternion: Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> Tensor:
+def quaternion_exp_to_log(quaternion: Tensor, eps: float = 1.0e-8) -> Tensor:
     r"""Apply the log map to a quaternion.
 
-    The quaternion should be in (x, y, z, w) format.
+    The quaternion should be in (w, x, y, z) format.
 
     Args:
         quaternion: a tensor containing a quaternion to be converted.
           The tensor can be of shape :math:`(*, 4)`.
         eps: a small number for clamping.
-        order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the quaternion log map of shape :math:`(*, 3)`.
 
     Example:
         >>> quaternion = tensor((1., 0., 0., 0.))
-        >>> quaternion_exp_to_log(quaternion, eps=torch.finfo(quaternion.dtype).eps,
-        ...                       order=QuaternionCoeffOrder.WXYZ)
+        >>> quaternion_exp_to_log(quaternion, eps=torch.finfo(quaternion.dtype).eps)
         tensor([0., 0., 0.])
     """
     if not isinstance(quaternion, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 4:
         raise ValueError(f"Input must be a tensor of shape (*, 4). Got {quaternion.shape}")
 
-    if not torch.jit.is_scripting():
-        if order.name not in QuaternionCoeffOrder.__members__.keys():
-            raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
-
-    if order == QuaternionCoeffOrder.XYZW:
-        warnings.warn(
-            "`XYZW` quaternion coefficient order is deprecated and"
-            " will be removed after > 0.6. "
-            "Please use `QuaternionCoeffOrder.WXYZ` instead."
-        )
-
     # unpack quaternion vector and scalar
     quaternion_vector: Tensor = tensor([])
     quaternion_scalar: Tensor = tensor([])
 
-    if order == QuaternionCoeffOrder.XYZW:
-        quaternion_vector = quaternion[..., 0:3]
-        quaternion_scalar = quaternion[..., 3:4]
-    else:
-        quaternion_scalar = quaternion[..., 0:1]
-        quaternion_vector = quaternion[..., 1:4]
+    quaternion_scalar = quaternion[..., 0:1]
+    quaternion_vector = quaternion[..., 1:4]
 
     # compute quaternion norm
     norm_q: Tensor = torch.norm(quaternion_vector, p=2, dim=-1, keepdim=True).clamp(min=eps)
 
     # apply log map
     quaternion_log: Tensor = quaternion_vector * torch.acos(torch.clamp(quaternion_scalar, min=-1.0, max=1.0)) / norm_q
 
     return quaternion_log
 
 
 # based on:
 # https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py#L138
 
 
-def angle_axis_to_quaternion(angle_axis: Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW) -> Tensor:
-    r"""Convert an angle axis to a quaternion.
+def axis_angle_to_quaternion(axis_angle: Tensor) -> Tensor:
+    r"""Convert an axis angle to a quaternion.
 
-    The quaternion vector has components in (x, y, z, w) or (w, x, y, z) format.
+    The quaternion vector has components in (w, x, y, z) format.
 
     Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h
 
     Args:
-        angle_axis: tensor with angle axis in radians.
-        order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
+        axis_angle: tensor with axis angle in radians.
 
     Return:
         tensor with quaternion.
 
     Shape:
         - Input: :math:`(*, 3)` where `*` means, any number of dimensions
         - Output: :math:`(*, 4)`
 
     Example:
-        >>> angle_axis = tensor((0., 1., 0.))
-        >>> angle_axis_to_quaternion(angle_axis, order=QuaternionCoeffOrder.WXYZ)
+        >>> axis_angle = tensor((0., 1., 0.))
+        >>> axis_angle_to_quaternion(axis_angle)
         tensor([0.8776, 0.0000, 0.4794, 0.0000])
     """
-    if not torch.is_tensor(angle_axis):
-        raise TypeError(f"Input type is not a Tensor. Got {type(angle_axis)}")
-
-    if not angle_axis.shape[-1] == 3:
-        raise ValueError(f"Input must be a tensor of shape Nx3 or 3. Got {angle_axis.shape}")
+    if not torch.is_tensor(axis_angle):
+        raise TypeError(f"Input type is not a Tensor. Got {type(axis_angle)}")
 
-    if not torch.jit.is_scripting():
-        if order.name not in QuaternionCoeffOrder.__members__.keys():
-            raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
-
-    if order == QuaternionCoeffOrder.XYZW:
-        warnings.warn(
-            "`XYZW` quaternion coefficient order is deprecated and"
-            " will be removed after > 0.6. "
-            "Please use `QuaternionCoeffOrder.WXYZ` instead."
-        )
+    if not axis_angle.shape[-1] == 3:
+        raise ValueError(f"Input must be a tensor of shape Nx3 or 3. Got {axis_angle.shape}")
 
     # unpack input and compute conversion
-    a0: Tensor = angle_axis[..., 0:1]
-    a1: Tensor = angle_axis[..., 1:2]
-    a2: Tensor = angle_axis[..., 2:3]
+    a0: Tensor = axis_angle[..., 0:1]
+    a1: Tensor = axis_angle[..., 1:2]
+    a2: Tensor = axis_angle[..., 2:3]
     theta_squared: Tensor = a0 * a0 + a1 * a1 + a2 * a2
 
     theta: Tensor = torch.sqrt(theta_squared)
     half_theta: Tensor = theta * 0.5
 
     mask: Tensor = theta_squared > 0.0
     ones: Tensor = torch.ones_like(half_theta)
 
     k_neg: Tensor = 0.5 * ones
     k_pos: Tensor = torch.sin(half_theta) / theta
     k: Tensor = where(mask, k_pos, k_neg)
     w: Tensor = where(mask, torch.cos(half_theta), ones)
 
-    quaternion: Tensor = torch.zeros(size=(*angle_axis.shape[:-1], 4), dtype=angle_axis.dtype, device=angle_axis.device)
-    if order == QuaternionCoeffOrder.XYZW:
-        quaternion[..., 0:1] = a0 * k
-        quaternion[..., 1:2] = a1 * k
-        quaternion[..., 2:3] = a2 * k
-        quaternion[..., 3:4] = w
-    else:
-        quaternion[..., 1:2] = a0 * k
-        quaternion[..., 2:3] = a1 * k
-        quaternion[..., 3:4] = a2 * k
-        quaternion[..., 0:1] = w
+    quaternion: Tensor = torch.zeros(size=(*axis_angle.shape[:-1], 4), dtype=axis_angle.dtype, device=axis_angle.device)
+    quaternion[..., 1:2] = a0 * k
+    quaternion[..., 2:3] = a1 * k
+    quaternion[..., 3:4] = a2 * k
+    quaternion[..., 0:1] = w
     return quaternion
 
 
+@deprecated(replace_with='axis_angle_to_quaternion', version='0.7.0')
+def angle_axis_to_quaternion(axis_angle: Tensor) -> Tensor:
+    return axis_angle_to_quaternion(axis_angle)
+
+
 # inspired by: https://stackoverflow.com/questions/56207448/efficient-quaternions-to-euler-transformation
 
 
-def euler_from_quaternion(w: Tensor, x: Tensor, y: Tensor, z: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
+def euler_from_quaternion(w: Tensor, x: Tensor, y: Tensor, z: Tensor) -> tuple[Tensor, Tensor, Tensor]:
     """Convert a quaternion coefficients to Euler angles.
 
     Returned angles are in radians in XYZ convention.
 
     Args:
         w: quaternion :math:`q_w` coefficient.
         x: quaternion :math:`q_x` coefficient.
@@ -887,15 +792,15 @@
     siny_cosp = 2.0 * (w * z + x * y)
     cosy_cosp = 1.0 - 2.0 * (yy + z * z)
     yaw = siny_cosp.atan2(cosy_cosp)
 
     return roll, pitch, yaw
 
 
-def quaternion_from_euler(roll: Tensor, pitch: Tensor, yaw: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
+def quaternion_from_euler(roll: Tensor, pitch: Tensor, yaw: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]:
     """Convert Euler angles to quaternion coefficients.
 
     Euler angles are assumed to be in radians in XYZ convention.
 
     Args:
         roll: the roll euler angle.
         pitch: the pitch euler angle.
@@ -1068,15 +973,15 @@
     ang_rad = deg2rad(angle)
     cos_a: Tensor = torch.cos(ang_rad)
     sin_a: Tensor = torch.sin(ang_rad)
     return stack([cos_a, sin_a, -sin_a, cos_a], dim=-1).view(*angle.shape, 2, 2)
 
 
 def normalize_homography(
-    dst_pix_trans_src_pix: Tensor, dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int]
+    dst_pix_trans_src_pix: Tensor, dsize_src: tuple[int, int], dsize_dst: tuple[int, int]
 ) -> Tensor:
     r"""Normalize a given homography in pixels to [-1, 1].
 
     Args:
         dst_pix_trans_src_pix: homography/ies from source to destination to be
           normalized. :math:`(B, 3, 3)`
         dsize_src: size of the source image (height, width).
@@ -1103,19 +1008,15 @@
 
     # compute chain transformations
     dst_norm_trans_src_norm: Tensor = dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)
     return dst_norm_trans_src_norm
 
 
 def normal_transform_pixel(
-    height: int,
-    width: int,
-    eps: float = 1e-14,
-    device: Optional[torch.device] = None,
-    dtype: Optional[torch.dtype] = None,
+    height: int, width: int, eps: float = 1e-14, device: torch.device | None = None, dtype: torch.dtype | None = None
 ) -> Tensor:
     r"""Compute the normalization matrix from image size in pixels to [-1, 1].
 
     Args:
         height image height.
         width: image width.
         eps: epsilon to prevent divide-by-zero errors
@@ -1136,16 +1037,16 @@
 
 
 def normal_transform_pixel3d(
     depth: int,
     height: int,
     width: int,
     eps: float = 1e-14,
-    device: Optional[torch.device] = None,
-    dtype: Optional[torch.dtype] = None,
+    device: torch.device | None = None,
+    dtype: torch.dtype | None = None,
 ) -> Tensor:
     r"""Compute the normalization matrix from image size in pixels to [-1, 1].
 
     Args:
         depth: image depth.
         height: image height.
         width: image width.
@@ -1169,15 +1070,15 @@
     tr_mat[1, 1] = tr_mat[1, 1] * 2.0 / height_denom
     tr_mat[2, 2] = tr_mat[2, 2] * 2.0 / depth_denom
 
     return tr_mat.unsqueeze(0)  # 1x4x4
 
 
 def denormalize_homography(
-    dst_pix_trans_src_pix: Tensor, dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int]
+    dst_pix_trans_src_pix: Tensor, dsize_src: tuple[int, int], dsize_dst: tuple[int, int]
 ) -> Tensor:
     r"""De-normalize a given homography in pixels from [-1, 1] to actual height and width.
 
     Args:
         dst_pix_trans_src_pix: homography/ies from source to destination to be
           denormalized. :math:`(B, 3, 3)`
         dsize_src: size of the source image (height, width).
@@ -1203,15 +1104,15 @@
     dst_denorm_trans_dst_pix = _torch_inverse_cast(dst_norm_trans_dst_pix)
     # compute chain transformations
     dst_norm_trans_src_norm: Tensor = dst_denorm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_norm_trans_src_pix)
     return dst_norm_trans_src_norm
 
 
 def normalize_homography3d(
-    dst_pix_trans_src_pix: Tensor, dsize_src: Tuple[int, int, int], dsize_dst: Tuple[int, int, int]
+    dst_pix_trans_src_pix: Tensor, dsize_src: tuple[int, int, int], dsize_dst: tuple[int, int, int]
 ) -> Tensor:
     r"""Normalize a given homography in pixels to [-1, 1].
 
     Args:
         dst_pix_trans_src_pix: homography/ies from source to destination to be
           normalized. :math:`(B, 4, 4)`
         dsize_src: size of the source image (depth, height, width).
@@ -1238,22 +1139,21 @@
     src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)
     dst_norm_trans_dst_pix: Tensor = normal_transform_pixel3d(dst_d, dst_h, dst_w).to(dst_pix_trans_src_pix)
     # compute chain transformations
     dst_norm_trans_src_norm: Tensor = dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)
     return dst_norm_trans_src_norm
 
 
-def normalize_points_with_intrinsics(point_2d: Tensor, camera_matrix: Tensor):
-    r"""Normalizes points with intrinsics. Useful for conversion of keypoints to be used with essential matrix.
+def normalize_points_with_intrinsics(point_2d: Tensor, camera_matrix: Tensor) -> Tensor:
+    """Normalizes points with intrinsics. Useful for conversion of keypoints to be used with essential matrix.
 
     Args:
-        point_2d: tensor containing the 2d points in the image pixel coordinates.
-        The shape of the tensor can be :math:`(*, 2)`.
-        camera_matrix: tensor containing the intrinsics camera
-            matrix. The tensor shape must be :math:`(*, 3, 3)`.
+        point_2d: tensor containing the 2d points in the image pixel coordinates. The shape of the tensor can be
+                  :math:`(*, 2)`.
+        camera_matrix: tensor containing the intrinsics camera matrix. The tensor shape must be :math:`(*, 3, 3)`.
 
     Returns:
         tensor of (u, v) cam coordinates with shape :math:`(*, 2)`.
 
     Example:
         >>> _ = torch.manual_seed(0)
         >>> X = torch.rand(1, 2)
@@ -1281,22 +1181,21 @@
     x_coord: Tensor = (u_coord - cx) / fx
     y_coord: Tensor = (v_coord - cy) / fy
 
     xy: Tensor = stack([x_coord, y_coord], dim=-1)
     return xy
 
 
-def denormalize_points_with_intrinsics(point_2d_norm: Tensor, camera_matrix: Tensor):
-    r"""Normalizes points with intrinsics. Useful for conversion of keypoints to be used with essential matrix.
+def denormalize_points_with_intrinsics(point_2d_norm: Tensor, camera_matrix: Tensor) -> Tensor:
+    """Normalizes points with intrinsics. Useful for conversion of keypoints to be used with essential matrix.
 
     Args:
-        point_2d_norm: tensor containing the 2d points in the image pixel coordinates.
-        The shape of the tensor can be :math:`(*, 2)`.
-        camera_matrix: tensor containing the intrinsics camera
-            matrix. The tensor shape must be :math:`(*, 3, 3)`.
+        point_2d_norm: tensor containing the 2d points in the image pixel coordinates. The shape of the tensor can be
+                       :math:`(*, 2)`.
+        camera_matrix: tensor containing the intrinsics camera matrix. The tensor shape must be :math:`(*, 3, 3)`.
 
     Returns:
         tensor of (u, v) cam coordinates with shape :math:`(*, 2)`.
 
     Example:
         >>> _ = torch.manual_seed(0)
         >>> X = torch.rand(1, 2)
@@ -1347,15 +1246,15 @@
     """
     KORNIA_CHECK_SHAPE(R, ["B", "3", "3"])
     KORNIA_CHECK_SHAPE(t, ["B", "3", "1"])
     Rt = concatenate([R, t], dim=2)
     return convert_affinematrix_to_homography3d(Rt)
 
 
-def matrix4x4_to_Rt(extrinsics: Tensor) -> Tuple[Tensor, Tensor]:
+def matrix4x4_to_Rt(extrinsics: Tensor) -> tuple[Tensor, Tensor]:
     r"""Converts 4x4 extrinsics into 3x3 rotation matrix R and 1x3 translation vector ts.
 
     Args:
         extrinsics: pose matrix :math:`(B, 4, 4)`.
 
     Returns:
         R: Rotation matrix, :math:`(B, 3, 3).`
@@ -1400,15 +1299,15 @@
         [[[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1.0]]],
         dtype=extrinsics_graphics.dtype,
         device=extrinsics_graphics.device,
     )
     return extrinsics_graphics @ invert_yz
 
 
-def camtoworld_graphics_to_vision_Rt(R: Tensor, t: Tensor) -> Tuple[Tensor, Tensor]:
+def camtoworld_graphics_to_vision_Rt(R: Tensor, t: Tensor) -> tuple[Tensor, Tensor]:
     r"""Converts graphics coordinate frame (e.g. OpenGL) to vision coordinate frame (e.g. OpenCV.), , i.e. flips y
     and z axis. Graphics convention: [+x, +y, +z] == [right, up, backwards]. Vision convention: [+x, +y, +z] ==
 
     [right, down, forwards]
 
     Args:
         R: Rotation matrix, :math:`(B, 3, 3).`
@@ -1457,15 +1356,15 @@
         [[[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1.0]]],
         dtype=extrinsics_vision.dtype,
         device=extrinsics_vision.device,
     )
     return extrinsics_vision @ invert_yz
 
 
-def camtoworld_vision_to_graphics_Rt(R: Tensor, t: Tensor) -> Tuple[Tensor, Tensor]:
+def camtoworld_vision_to_graphics_Rt(R: Tensor, t: Tensor) -> tuple[Tensor, Tensor]:
     r"""Converts graphics coordinate frame (e.g. OpenGL) to vision coordinate frame (e.g. OpenCV.), , i.e. flips y
     and z axis. Graphics convention: [+x, +y, +z] == [right, up, backwards]. Vision convention: [+x, +y, +z] ==
 
     [right, down, forwards]
 
     Args:
         R: Rotation matrix, :math:`(B, 3, 3).`
@@ -1486,15 +1385,15 @@
     """
     KORNIA_CHECK_SHAPE(R, ["B", "3", "3"])
     KORNIA_CHECK_SHAPE(t, ["B", "3", "1"])
     mat4x4 = camtoworld_vision_to_graphics_4x4(Rt_to_matrix4x4(R, t))
     return matrix4x4_to_Rt(mat4x4)
 
 
-def camtoworld_to_worldtocam_Rt(R: Tensor, t: Tensor) -> Tuple[Tensor, Tensor]:
+def camtoworld_to_worldtocam_Rt(R: Tensor, t: Tensor) -> tuple[Tensor, Tensor]:
     r"""Converts camtoworld, i.e. projection from camera coordinate system to world coordinate system, to worldtocam
     frame i.e. projection from world to the camera coordinate system (used in Colmap).
     See
     long-url: https://colmap.github.io/format.html#output-format
 
     Args:
         R: Rotation matrix, :math:`(B, 3, 3).`
@@ -1518,15 +1417,15 @@
 
     R_inv = R.transpose(1, 2)
     new_t: Tensor = -R_inv @ t
 
     return (R_inv, new_t)
 
 
-def worldtocam_to_camtoworld_Rt(R: Tensor, t: Tensor) -> Tuple[Tensor, Tensor]:
+def worldtocam_to_camtoworld_Rt(R: Tensor, t: Tensor) -> tuple[Tensor, Tensor]:
     r"""Converts worldtocam frame i.e. projection from world to the camera coordinate system (used in Colmap) to
     camtoworld, i.e. projection from camera coordinate system to world coordinate system.
 
     Args:
         R: Rotation matrix, :math:`(B, 3, 3).`
         t: Translation matrix :math:`(B, 3, 1)`.
 
@@ -1548,15 +1447,15 @@
 
     R_inv = R.transpose(1, 2)
     new_t: Tensor = -R_inv @ t
 
     return (R_inv, new_t)
 
 
-def ARKitQTVecs_to_ColmapQTVecs(qvec: Tensor, tvec: Tensor) -> Tuple[Tensor, Tensor]:
+def ARKitQTVecs_to_ColmapQTVecs(qvec: Tensor, tvec: Tensor) -> tuple[Tensor, Tensor]:
     r"""Converts output of Apple ARKit screen pose (in quaternion representation) to the camera-to-world
     transformation, expected by Colmap, also in quaternion representation.
 
     Args:
         qvec: ARKit rotation quaternion :math:`(B, 4)`, [x, y, z, w] format.
         tvec: translation vector :math:`(B, 3, 1)`, [x, y, z]
 
@@ -1569,13 +1468,13 @@
         >>> ARKitQTVecs_to_ColmapQTVecs(q, t)
         (tensor([[0.7071, 0.0000, 0.7071, 0.0000]]), tensor([[[-1.0000],
                  [-1.0000],
                  [ 1.0000]]]))
     """
     # ToDo:  integrate QuaterniaonAPI
 
-    Rcg = quaternion_to_rotation_matrix(qvec, order=QuaternionCoeffOrder.WXYZ)
+    Rcg = quaternion_to_rotation_matrix(qvec)
     Rcv, Tcv = camtoworld_graphics_to_vision_Rt(Rcg, tvec)
     R_colmap, t_colmap = camtoworld_to_worldtocam_Rt(Rcv, Tcv)
     t_colmap = t_colmap.reshape(-1, 3, 1)
-    q_colmap = rotation_matrix_to_quaternion(R_colmap.contiguous(), order=QuaternionCoeffOrder.WXYZ)
+    q_colmap = rotation_matrix_to_quaternion(R_colmap.contiguous())
     return q_colmap, t_colmap
```

### Comparing `kornia-0.6.9/kornia/geometry/depth.py` & `kornia-0.7.0/kornia/geometry/depth.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """Module containing operators to work on RGB-Depth images."""
 from typing import Union
 
 import torch
 import torch.nn.functional as F
 
 from kornia.core import Module, Tensor, concatenate, tensor
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.filters.sobel import spatial_gradient
 from kornia.utils import create_meshgrid
 
 from .camera import PinholeCamera, cam2pixel, pixel2cam, project_points, unproject_points
 from .conversions import normalize_pixel_coordinates
 from .linalg import compose_transformations, convert_points_to_homogeneous, inverse_transformation, transform_points
 
@@ -188,15 +189,15 @@
         self,
         pinhole_dst: PinholeCamera,
         height: int,
         width: int,
         mode: str = 'bilinear',
         padding_mode: str = 'zeros',
         align_corners: bool = True,
-    ):
+    ) -> None:
         super().__init__()
         # constructor members
         self.width: int = width
         self.height: int = height
         self.mode: str = mode
         self.padding_mode: str = padding_mode
         self.eps = 1e-6
@@ -236,24 +237,24 @@
         dst_proj_src: Tensor = torch.matmul(self._pinhole_dst.intrinsics, dst_trans_src)
 
         # update class members
         self._pinhole_src = pinhole_src
         self._dst_proj_src = dst_proj_src
         return self
 
-    def _compute_projection(self, x, y, invd):
+    def _compute_projection(self, x: Union[int, float], y: Union[int, float], invd: Union[int, float]) -> Tensor:
         if self._dst_proj_src is None or self._pinhole_src is None:
             raise ValueError("Please, call compute_projection_matrix.")
 
         point = tensor([[[x], [y], [invd], [1.0]]], device=self._dst_proj_src.device, dtype=self._dst_proj_src.dtype)
         flow = torch.matmul(self._dst_proj_src, point)
         z = 1.0 / flow[:, 2]
-        x = flow[:, 0] * z
-        y = flow[:, 1] * z
-        return concatenate([x, y], 1)
+        _x = flow[:, 0] * z
+        _y = flow[:, 1] * z
+        return concatenate([_x, _y], 1)
 
     def compute_subpixel_step(self) -> Tensor:
         """Compute the required inverse depth step to achieve sub pixel accurate sampling of the depth cost volume,
         per camera.
 
         Szeliski, Richard, and Daniel Scharstein. "Symmetric sub-pixel stereo matching." European Conference on Computer
         Vision. Springer Berlin Heidelberg, 2002.
@@ -342,15 +343,15 @@
     pinhole_dst: PinholeCamera,
     pinhole_src: PinholeCamera,
     depth_src: Tensor,
     patch_dst: Tensor,
     height: int,
     width: int,
     align_corners: bool = True,
-):
+) -> Tensor:
     r"""Function that warps a tensor from destination frame to reference given the depth in the reference frame.
 
     See :class:`~kornia.geometry.warp.DepthWarper` for details.
 
     Example:
         >>> # pinholes camera models
         >>> pinhole_dst = PinholeCamera(torch.randn(1, 4, 4), torch.randn(1, 4, 4),
@@ -361,7 +362,44 @@
         >>> depth_src = torch.ones(1, 1, 32, 32)  # Nx1xHxW
         >>> image_dst = torch.rand(1, 3, 32, 32)  # NxCxHxW
         >>> image_src = depth_warp(pinhole_dst, pinhole_src, depth_src, image_dst, 32, 32)  # NxCxHxW
     """
     warper = DepthWarper(pinhole_dst, height, width, align_corners=align_corners)
     warper.compute_projection_matrix(pinhole_src)
     return warper(depth_src, patch_dst)
+
+
+def depth_from_disparity(disparity: Tensor, baseline: Union[float, Tensor], focal: Union[float, Tensor]) -> Tensor:
+    """Computes depth from disparity.
+
+    Args:
+        disparity: Disparity tensor of shape :math:`(*, H, W)`.
+        baseline: float/tensor containing the distance between the two lenses.
+        focal: float/tensor containing the focal length.
+
+    Return:
+        Depth map of the shape :math:`(*, H, W)`.
+
+    Example:
+        >>> disparity = torch.rand(4, 1, 4, 4)
+        >>> baseline = torch.rand(1)
+        >>> focal = torch.rand(1)
+        >>> depth_from_disparity(disparity, baseline, focal).shape
+        torch.Size([4, 1, 4, 4])
+    """
+    KORNIA_CHECK_IS_TENSOR(disparity, f"Input disparity type is not a Tensor. Got {type(disparity)}.")
+    KORNIA_CHECK_SHAPE(disparity, ["*", "H", "W"])
+    KORNIA_CHECK(
+        isinstance(baseline, (float, Tensor)),
+        f"Input baseline should be either a float or Tensor. " f"Got {type(baseline)}",
+    )
+    KORNIA_CHECK(
+        isinstance(focal, (float, Tensor)), f"Input focal should be either a float or Tensor. " f"Got {type(focal)}"
+    )
+
+    if isinstance(baseline, Tensor):
+        KORNIA_CHECK_SHAPE(baseline, ["1"])
+
+    if isinstance(focal, Tensor):
+        KORNIA_CHECK_SHAPE(focal, ["1"])
+
+    return baseline * focal / disparity
```

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/__init__.py` & `kornia-0.7.0/kornia/geometry/epipolar/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/_metrics.py` & `kornia-0.7.0/kornia/geometry/epipolar/_metrics.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 """Module including useful metrics for Structure from Motion."""
 
 from torch import Tensor
 
+from kornia.core.check import KORNIA_CHECK_IS_TENSOR
 from kornia.geometry.conversions import convert_points_to_homogeneous
 from kornia.geometry.linalg import point_line_distance
-from kornia.testing import KORNIA_CHECK_IS_TENSOR
 
 
 def sampson_epipolar_distance(
     pts1: Tensor, pts2: Tensor, Fm: Tensor, squared: bool = True, eps: float = 1e-8
 ) -> Tensor:
-    r"""Return Sampson distance for correspondences given the fundamental matrix.
+    """Return Sampson distance for correspondences given the fundamental matrix.
 
     Args:
-        pts1: correspondences from the left images with shape
-          (*, N, 2 or 3). If they are not homogeneous, converted automatically.
-        pts2: correspondences from the right images with shape
-          (*, N, 2 or 3). If they are not homogeneous, converted automatically.
-        Fm: Fundamental matrices with shape :math:`(*, 3, 3)`. Called Fm to
-          avoid ambiguity with torch.nn.functional.
+        pts1: correspondences from the left images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
+              converted automatically.
+        pts2: correspondences from the right images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
+              converted automatically.
+        Fm: Fundamental matrices with shape :math:`(*, 3, 3)`. Called Fm to avoid ambiguity with torch.nn.functional.
         squared: if True (default), the squared distance is returned.
         eps: Small constant for safe sqrt.
 
     Returns:
         the computed Sampson distance with shape :math:`(*, N)`.
     """
     if not isinstance(Fm, Tensor):
@@ -58,23 +57,22 @@
         return out
     return (out + eps).sqrt()
 
 
 def symmetrical_epipolar_distance(
     pts1: Tensor, pts2: Tensor, Fm: Tensor, squared: bool = True, eps: float = 1e-8
 ) -> Tensor:
-    r"""Return symmetrical epipolar distance for correspondences given the fundamental matrix.
+    """Return symmetrical epipolar distance for correspondences given the fundamental matrix.
 
     Args:
-       pts1: correspondences from the left images with shape
-         (*, N, 2 or 3). If they are not homogeneous, converted automatically.
-       pts2: correspondences from the right images with shape
-         (*, N, 2 or 3). If they are not homogeneous, converted automatically.
-       Fm: Fundamental matrices with shape :math:`(*, 3, 3)`. Called Fm to
-         avoid ambiguity with torch.nn.functional.
+       pts1: correspondences from the left images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
+             converted automatically.
+       pts2: correspondences from the right images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
+             converted automatically.
+       Fm: Fundamental matrices with shape :math:`(*, 3, 3)`. Called Fm to avoid ambiguity with torch.nn.functional.
        squared: if True (default), the squared distance is returned.
        eps: Small constant for safe sqrt.
 
     Returns:
         the computed Symmetrical distance with shape :math:`(*, N)`.
     """
     if not isinstance(Fm, Tensor):
```

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/essential.py` & `kornia-0.7.0/kornia/geometry/epipolar/essential.py`

 * *Files 0% similar despite different names*

```diff
@@ -120,15 +120,15 @@
     return Tx @ R
 
 
 def motion_from_essential(E_mat: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
     r"""Get Motion (R's and t's ) from Essential matrix.
 
     Computes and return four possible poses exist for the decomposition of the Essential
-    matrix. The possible solutions are :math:`[R1,t], [R1,t], [R2,t], [R2,t]`.
+    matrix. The possible solutions are :math:`[R1,t], [R1,-t], [R2,t], [R2,-t]`.
 
     Args:
         E_mat: The essential matrix in the form of :math:`(*, 3, 3)`.
 
     Returns:
         The rotation and translation containing the four possible combination for the retrieved motion.
         The tuple is as following :math:`[(*, 4, 3, 3), (*, 4, 3, 1)]`.
@@ -260,15 +260,15 @@
 def relative_camera_motion(
     R1: torch.Tensor, t1: torch.Tensor, R2: torch.Tensor, t2: torch.Tensor
 ) -> Tuple[torch.Tensor, torch.Tensor]:
     r"""Compute the relative camera motion between two cameras.
 
     Given the motion parameters of two cameras, computes the motion parameters of the second
     one assuming the first one to be at the origin. If :math:`T1` and :math:`T2` are the camera motions,
-    the computed relative motion is :math:`T = T_{2}T^{1}_{1}`.
+    the computed relative motion is :math:`T = T_{2}T^{-1}_{1}`.
 
     Args:
         R1: The first camera rotation matrix with shape :math:`(*, 3, 3)`.
         t1: The first camera translation vector with shape :math:`(*, 3, 1)`.
         R2: The second camera rotation matrix with shape :math:`(*, 3, 3)`.
         t2: The second camera translation vector with shape :math:`(*, 3, 1)`.
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/fundamental.py` & `kornia-0.7.0/kornia/geometry/epipolar/fundamental.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """Module containing the functionalities for computing the Fundamental Matrix."""
 
-from typing import Optional, Tuple
+from typing import Literal, Optional, Tuple
 
 import torch
 
-from kornia.core import Tensor
+from kornia.core import Tensor, concatenate
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.geometry.conversions import convert_points_from_homogeneous, convert_points_to_homogeneous
 from kornia.geometry.linalg import transform_points
-from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.geometry.solvers import solve_cubic
 from kornia.utils.helpers import _torch_svd_cast
 
 
 def normalize_points(points: Tensor, eps: float = 1e-8) -> Tuple[Tensor, Tensor]:
     r"""Normalizes points (isotropic).
 
     Computes the transformation matrix such that the two principal moments of the set of points
@@ -66,18 +67,110 @@
     """
     if len(M.shape) < 2:
         raise AssertionError(M.shape)
     norm_val: Tensor = M[..., -1:, -1:]
     return torch.where(norm_val.abs() > eps, M / (norm_val + eps), M)
 
 
-def find_fundamental(
-    points1: torch.Tensor, points2: torch.Tensor, weights: Optional[torch.Tensor] = None
-) -> torch.Tensor:
+# Reference: Adapted from the 'run_7point' function in opencv
+# https://github.com/opencv/opencv/blob/4.x/modules/calib3d/src/fundam.cpp
+def run_7point(points1: Tensor, points2: Tensor) -> Tensor:
+    r"""Compute the fundamental matrix using the 7-point algorithm.
+
+    Args:
+        points1: A set of points in the first image with a tensor shape :math:`(B, N, 2)`.
+        points2: A set of points in the second image with a tensor shape :math:`(B, N, 2)`.
+
+    Returns:
+        the computed fundamental matrix with shape :math:`(B, 3*m, 3), Valid values of m are 1, 2 or 3`
+    """
+    KORNIA_CHECK_SHAPE(points1, ['B', '7', '2'])
+    KORNIA_CHECK_SHAPE(points2, ['B', '7', '2'])
+
+    batch_size = points1.shape[0]
+
+    points1_norm, transform1 = normalize_points(points1)
+    points2_norm, transform2 = normalize_points(points2)
+
+    x1, y1 = torch.chunk(points1_norm, dim=-1, chunks=2)  # Bx1xN
+    x2, y2 = torch.chunk(points2_norm, dim=-1, chunks=2)  # Bx1xN
+
+    ones = torch.ones_like(x1)
+    # form a linear system: which represents
+    # the equation (x2[i], 1)*F*(x1[i], 1) = 0
+    X = concatenate([x2 * x1, x2 * y1, x2, y2 * x1, y2 * y1, y2, x1, y1, ones], -1)  # BxNx9
+
+    # X * Fmat = 0 is singular (7 equations for 9 variables)
+    # solving for nullspace of X to get two F
+    ####### unstable failing gradcheck
+    # _, _, v = torch.linalg.svd(X)
+    _, _, v = _torch_svd_cast(X)
+
+    # last two singular vector as a basic of the space
+    f1 = v[..., 7].view(-1, 3, 3)
+    f2 = v[..., 8].view(-1, 3, 3)
+
+    # lambda*f1 + mu*f2 is an arbitrary fundamental matrix
+    # f ~ lambda*f1 + (1 - lambda)*f2
+    # det(f) = det(lambda*f1 + (1-lambda)*f2), find lambda
+    # form a cubic equation
+    # finding the coefficients of cubic polynomial (coeffs)
+
+    coeffs = torch.zeros((batch_size, 4), device=v.device, dtype=v.dtype)
+
+    f1_det = torch.linalg.det(f1)
+    f2_det = torch.linalg.det(f2)
+    coeffs[:, 0] = f1_det
+    coeffs[:, 1] = torch.einsum('bii->b', f2 @ torch.inverse(f1)) * f1_det
+    coeffs[:, 2] = torch.einsum('bii->b', f1 @ torch.inverse(f2)) * f2_det
+    coeffs[:, 3] = f2_det
+
+    # solve the cubic equation, there can be 1 to 3 roots
+    # roots = torch.tensor(np.roots(coeffs.numpy()))
+    roots = solve_cubic(coeffs)
+
+    fmatrix = torch.zeros((batch_size, 3, 3, 3), device=v.device, dtype=v.dtype)
+    valid_root_mask = (torch.count_nonzero(roots, dim=1) < 3) | (torch.count_nonzero(roots, dim=1) > 1)
+
+    _lambda = roots
+    _mu = torch.ones_like(_lambda)
+
+    _s = f1[valid_root_mask, 2, 2].unsqueeze(dim=1) * roots[valid_root_mask] + f2[valid_root_mask, 2, 2].unsqueeze(
+        dim=1
+    )
+    # _s_non_zero_mask = torch.abs(_s ) > 1e-16
+    _s_non_zero_mask = ~torch.isclose(_s, torch.tensor(0.0, device=v.device, dtype=v.dtype))
+
+    _mu[_s_non_zero_mask] = 1.0 / _s[_s_non_zero_mask]
+    _lambda[_s_non_zero_mask] = _lambda[_s_non_zero_mask] * _mu[_s_non_zero_mask]
+
+    f1_expanded = f1.unsqueeze(1).expand(batch_size, 3, 3, 3)
+    f2_expanded = f2.unsqueeze(1).expand(batch_size, 3, 3, 3)
+
+    fmatrix[valid_root_mask] = (
+        f1_expanded[valid_root_mask] * _lambda[valid_root_mask, :, None, None]
+        + f2_expanded[valid_root_mask] * _mu[valid_root_mask, :, None, None]
+    )
+
+    mat_ind = torch.zeros(3, 3, dtype=torch.bool)
+    mat_ind[2, 2] = True
+    fmatrix[_s_non_zero_mask, mat_ind] = 1.0
+    fmatrix[~_s_non_zero_mask, mat_ind] = 0.0
+
+    trans1_exp = transform1[valid_root_mask].unsqueeze(1).expand(-1, fmatrix.shape[2], -1, -1)
+    trans2_exp = transform2[valid_root_mask].unsqueeze(1).expand(-1, fmatrix.shape[2], -1, -1)
+
+    fmatrix[valid_root_mask] = torch.matmul(
+        trans2_exp.transpose(-2, -1), torch.matmul(fmatrix[valid_root_mask], trans1_exp)
+    )
 
+    return normalize_transformation(fmatrix)
+
+
+def run_8point(points1: Tensor, points2: Tensor, weights: Optional[Tensor] = None) -> Tensor:
     r"""Compute the fundamental matrix using the DLT formulation.
 
     The linear system is solved by using the Weighted Least Squares Solution for the 8 Points algorithm.
 
     Args:
         points1: A set of points in the first image with a tensor shape :math:`(B, N, 2), N>=8`.
         points2: A set of points in the second image with a tensor shape :math:`(B, N, 2), N>=8`.
@@ -86,15 +179,15 @@
     Returns:
         the computed fundamental matrix with shape :math:`(B, 3, 3)`.
     """
     if points1.shape != points2.shape:
         raise AssertionError(points1.shape, points2.shape)
     if points1.shape[1] < 8:
         raise AssertionError(points1.shape)
-    if not (weights is None):
+    if weights is not None:
         if not (len(weights.shape) == 2 and weights.shape[1] == points1.shape[1]):
             raise AssertionError(weights.shape)
 
     points1_norm, transform1 = normalize_points(points1)
     points2_norm, transform2 = normalize_points(points2)
 
     x1, y1 = torch.chunk(points1_norm, dim=-1, chunks=2)  # Bx1xN
@@ -125,14 +218,40 @@
 
     F_projected = U @ (torch.diag_embed(S * rank_mask) @ V.transpose(-2, -1))
     F_est = transform2.transpose(-2, -1) @ (F_projected @ transform1)
 
     return normalize_transformation(F_est)
 
 
+def find_fundamental(
+    points1: Tensor, points2: Tensor, weights: Optional[Tensor] = None, method: Literal['8POINT', '7POINT'] = '8POINT'
+) -> Tensor:
+    r"""
+    Args:
+        points1: A set of points in the first image with a tensor shape :math:`(B, N, 2), N>=8`.
+        points2: A set of points in the second image with a tensor shape :math:`(B, N, 2), N>=8`.
+        weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.
+        method: The method to use for computing the fundamental matrix. Supported methods are "7POINT" and "8POINT".
+
+    Returns:
+        the computed fundamental matrix with shape :math:`(B, 3*m, 3)`, where `m` number of fundamental matrix.
+
+    Raises:
+        ValueError: If an invalid method is provided.
+
+    """
+    if method.upper() == "7POINT":
+        result = run_7point(points1, points2)
+    elif method.upper() == "8POINT":
+        result = run_8point(points1, points2, weights)
+    else:
+        raise ValueError(f"Invalid method: {method}. Supported methods are '7POINT' and '8POINT'.")
+    return result
+
+
 def compute_correspond_epilines(points: Tensor, F_mat: Tensor) -> Tensor:
     r"""Compute the corresponding epipolar line for a given set of points.
 
     Args:
         points: tensor containing the set of points to project in the shape of :math:`(*, N, 2)` or :math:`(*, N, 3)`.
         F_mat: the fundamental to use for projection the points in the shape of :math:`(*, 3, 3)`.
 
@@ -183,23 +302,22 @@
         raise AssertionError(points.shape)
     infinity_point = lines * torch.tensor([1, 1, 0], dtype=lines.dtype, device=lines.device).view(1, 1, 3)
     perp: Tensor = points_h.cross(infinity_point, dim=2)
     return perp
 
 
 def get_closest_point_on_epipolar_line(pts1: Tensor, pts2: Tensor, Fm: Tensor) -> Tensor:
-    r"""Return closest point on the epipolar line to the correspondence, given the fundamental matrix.
+    """Return closest point on the epipolar line to the correspondence, given the fundamental matrix.
 
     Args:
-        pts1: correspondences from the left images with shape
-          (*, N, 2 or 3). If they are not homogeneous, converted automatically.
-        pts2: correspondences from the right images with shape
-          (*, N, 2 or 3). If they are not homogeneous, converted automatically.
-        Fm: Fundamental matrices with shape :math:`(*, 3, 3)`. Called Fm to
-          avoid ambiguity with torch.nn.functional.
+        pts1: correspondences from the left images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
+              converted automatically.
+        pts2: correspondences from the right images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
+              converted automatically.
+        Fm: Fundamental matrices with shape :math:`(*, 3, 3)`. Called Fm to avoid ambiguity with torch.nn.functional.
 
     Returns:
         point on epipolar line :math:`(*, N, 2)`.
     """
     if not isinstance(Fm, Tensor):
         raise TypeError(f"Fm type is not a torch.Tensor. Got {type(Fm)}")
     if (len(Fm.shape) < 3) or not Fm.shape[-2:] == (3, 3):
@@ -257,16 +375,16 @@
     if not (len(P1.shape) >= 2 and P1.shape[-2:] == (3, 4)):
         raise AssertionError(P1.shape)
     if not (len(P2.shape) >= 2 and P2.shape[-2:] == (3, 4)):
         raise AssertionError(P2.shape)
     if P1.shape[:-2] != P2.shape[:-2]:
         raise AssertionError
 
-    def vstack(x, y):
-        return torch.cat([x, y], dim=-2)
+    def vstack(x: Tensor, y: Tensor) -> Tensor:
+        return concatenate([x, y], dim=-2)
 
     X1 = P1[..., 1:, :]
     X2 = vstack(P1[..., 2:3, :], P1[..., 0:1, :])
     X3 = P1[..., :2, :]
 
     Y1 = P2[..., 1:, :]
     Y2 = vstack(P2[..., 2:3, :], P2[..., 0:1, :])
```

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/numeric.py` & `kornia-0.7.0/kornia/geometry/epipolar/numeric.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/projection.py` & `kornia-0.7.0/kornia/geometry/epipolar/projection.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 """Module for image projections."""
 from typing import Tuple, Union
 
 import torch
 from torch.linalg import qr as linalg_qr
 
+from kornia.core import Tensor, concatenate, pad
 from kornia.utils import eye_like, vec_like
 from kornia.utils.helpers import _torch_svd_cast
 
 from .numeric import cross_product_matrix
 
 
-def intrinsics_like(focal: float, input: torch.Tensor) -> torch.Tensor:
+def intrinsics_like(focal: float, input: Tensor) -> Tensor:
     r"""Return a 3x3 instrinsics matrix, with same size as the input.
 
     The center of projection will be based in the input image size.
 
     Args:
         focal: the focal length for the camera matrix.
         input: image tensor that will determine the batch size and image height
@@ -34,32 +35,32 @@
     intrinsics[..., 0, 0] *= focal
     intrinsics[..., 1, 1] *= focal
     intrinsics[..., 0, 2] += 1.0 * W / 2
     intrinsics[..., 1, 2] += 1.0 * H / 2
     return intrinsics
 
 
-def random_intrinsics(low: Union[float, torch.Tensor], high: Union[float, torch.Tensor]) -> torch.Tensor:
+def random_intrinsics(low: Union[float, Tensor], high: Union[float, Tensor]) -> Tensor:
     r"""Generate a random camera matrix based on a given uniform distribution.
 
     Args:
         low: lower range (inclusive).
         high: upper range (exclusive).
 
     Returns:
         the random camera matrix with the shape of :math:`(1, 3, 3)`.
     """
     sampler = torch.distributions.Uniform(low, high)
     fx, fy, cx, cy = (sampler.sample(torch.Size((1,))) for _ in range(4))
     zeros, ones = torch.zeros_like(fx), torch.ones_like(fx)
-    camera_matrix: torch.Tensor = torch.cat([fx, zeros, cx, zeros, fy, cy, zeros, zeros, ones])
+    camera_matrix = concatenate([fx, zeros, cx, zeros, fy, cy, zeros, zeros, ones])
     return camera_matrix.view(1, 3, 3)
 
 
-def scale_intrinsics(camera_matrix: torch.Tensor, scale_factor: Union[float, torch.Tensor]) -> torch.Tensor:
+def scale_intrinsics(camera_matrix: Tensor, scale_factor: Union[float, Tensor]) -> Tensor:
     r"""Scale a camera matrix containing the intrinsics.
 
     Applies the scaling factor to the focal length and center of projection.
 
     Args:
         camera_matrix: the camera calibration matrix containing the intrinsic
           parameters. The expected shape for the tensor is :math:`(B, 3, 3)`.
@@ -72,15 +73,15 @@
     K_scale[..., 0, 0] *= scale_factor
     K_scale[..., 1, 1] *= scale_factor
     K_scale[..., 0, 2] *= scale_factor
     K_scale[..., 1, 2] *= scale_factor
     return K_scale
 
 
-def projection_from_KRt(K: torch.Tensor, R: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
+def projection_from_KRt(K: Tensor, R: Tensor, t: Tensor) -> Tensor:
     r"""Get the projection matrix P from K, R and t.
 
     This function estimate the projection matrix by solving the following equation: :math:`P = K * [R|t]`.
 
     Args:
        K: the camera matrix with the instrinsics with shape :math:`(B, 3, 3)`.
        R: The rotation matrix with shape :math:`(B, 3, 3)`.
@@ -94,25 +95,25 @@
     if R.shape[-2:] != (3, 3):
         raise AssertionError(R.shape)
     if t.shape[-2:] != (3, 1):
         raise AssertionError(t.shape)
     if not len(K.shape) == len(R.shape) == len(t.shape):
         raise AssertionError
 
-    Rt: torch.Tensor = torch.cat([R, t], dim=-1)  # 3x4
-    Rt_h = torch.nn.functional.pad(Rt, [0, 0, 0, 1], "constant", 0.0)  # 4x4
+    Rt = concatenate([R, t], dim=-1)  # 3x4
+    Rt_h = pad(Rt, [0, 0, 0, 1], "constant", 0.0)  # 4x4
     Rt_h[..., -1, -1] += 1.0
 
-    K_h: torch.Tensor = torch.nn.functional.pad(K, [0, 1, 0, 1], "constant", 0.0)  # 4x4
+    K_h = pad(K, [0, 1, 0, 1], "constant", 0.0)  # 4x4
     K_h[..., -1, -1] += 1.0
 
     return K @ Rt
 
 
-def KRt_from_projection(P: torch.Tensor, eps: float = 1e-6) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+def KRt_from_projection(P: Tensor, eps: float = 1e-6) -> Tuple[Tensor, Tensor, Tensor]:
     r"""Decompose the Projection matrix into Camera-Matrix, Rotation Matrix and Translation vector.
 
     Args:
         P: the projection matrix with shape :math:`(B, 3, 4)`.
 
     Returns:
         - The Camera matrix with shape :math:`(B, 3, 3)`.
@@ -142,15 +143,15 @@
     K = torch.matmul(upper_mat, signs_mat)
     R = torch.matmul(signs_mat, ortho_mat)
     t = torch.matmul(torch.inverse(K), last_column)
 
     return K, R, t
 
 
-def depth_from_point(R: torch.Tensor, t: torch.Tensor, X: torch.Tensor) -> torch.Tensor:
+def depth_from_point(R: Tensor, t: Tensor, X: Tensor) -> Tensor:
     r"""Return the depth of a point transformed by a rigid transform.
 
     Args:
        R: The rotation matrix with shape :math:`(*, 3, 3)`.
        t: The translation vector with shape :math:`(*, 3, 1)`.
        X: The 3d points with shape :math:`(*, 3)`.
 
@@ -161,26 +162,24 @@
     X_out = X_tmp[..., 2, :] + t[..., 2, :]
     return X_out
 
 
 # adapted from:
 # https://github.com/opencv/opencv_contrib/blob/master/modules/sfm/src/fundamental.cpp#L61
 # https://github.com/mapillary/OpenSfM/blob/master/opensfm/multiview.py#L14
-
-
-def _nullspace(A):
+def _nullspace(A: Tensor) -> Tuple[Tensor, Tensor]:
     """Compute the null space of A.
 
     Return the smallest singular value and the corresponding vector.
     """
     _, s, v = _torch_svd_cast(A)
     return s[..., -1], v[..., -1]
 
 
-def projections_from_fundamental(F_mat: torch.Tensor) -> torch.Tensor:
+def projections_from_fundamental(F_mat: Tensor) -> Tensor:
     r"""Get the projection matrices from the Fundamental Matrix.
 
     Args:
        F_mat: the fundamental matrix with the shape :math:`(B, 3, 3)`.
 
     Returns:
         The projection matrices with shape :math:`(B, 3, 4, 2)`.
```

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/scene.py` & `kornia-0.7.0/kornia/geometry/epipolar/scene.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """Module to generate synthetic 3d scenes."""
 import math
 from typing import Dict
 
 import torch
 
-from kornia.geometry.conversions import angle_axis_to_rotation_matrix
+from kornia.geometry.conversions import axis_angle_to_rotation_matrix
 from kornia.geometry.linalg import transform_points
 
 from .projection import projection_from_KRt, random_intrinsics
 
 
 def generate_scene(num_views: int, num_points: int) -> Dict[str, torch.Tensor]:
     # Generate the 3d points
@@ -18,15 +18,15 @@
     K = random_intrinsics(0.0, 100.0)  # 1x3x3
 
     # Create random rotation per view
     ang = torch.rand(num_views, 1) * math.pi * 2.0
 
     rvec = torch.rand(num_views, 3)
     rvec = ang * rvec / torch.norm(rvec, dim=1, keepdim=True)  # Nx3
-    rot_mat = angle_axis_to_rotation_matrix(rvec)  # Nx3x3
+    rot_mat = axis_angle_to_rotation_matrix(rvec)  # Nx3x3
     # matches with cv2.Rodrigues -> yay !
 
     # Create random translation per view
     tx = torch.empty(num_views).uniform_(-0.5, 0.5)
     ty = torch.empty(num_views).uniform_(-0.5, 0.5)
     tz = torch.empty(num_views).uniform_(-1.0, 2.0)
     tvec = torch.stack([tx, ty, tz], dim=1)[..., None]
@@ -38,8 +38,8 @@
 
     # compute projection matrices
     P = projection_from_KRt(K, rot_mat, tvec)
 
     # project points3d and backproject to image plane
     points2d = transform_points(P, points3d.expand(num_views, -1, -1))
 
-    return dict(K=K, R=rot_mat, t=tvec, P=P, points3d=points3d, points2d=points2d)
+    return {"K": K, "R": rot_mat, "t": tvec, "P": P, "points3d": points3d, "points2d": points2d}
```

### Comparing `kornia-0.6.9/kornia/geometry/epipolar/triangulation.py` & `kornia-0.7.0/kornia/geometry/epipolar/triangulation.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/homography.py` & `kornia-0.7.0/kornia/geometry/homography.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import warnings
 from typing import Optional, Tuple
 
 import torch
 
 from kornia.core import Tensor
-from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.utils import _extract_device_dtype, safe_inverse_with_mask, safe_solve_with_mask
 from kornia.utils.helpers import _torch_svd_cast
 
 from .conversions import convert_points_from_homogeneous, convert_points_to_homogeneous
 from .epipolar import normalize_points
 from .linalg import transform_points
 
@@ -241,22 +241,25 @@
         torch.cross(dst_perm[..., 1:2, :], dst_perm[..., 2:3, :]) @ dst_perm[..., 0:1, :].permute(0, 1, 3, 2)
     ).sign()
     sample_is_valid = (left_sign == right_sign).view(-1, 4).min(dim=1)[0]
     return sample_is_valid
 
 
 def find_homography_lines_dlt(ls1: Tensor, ls2: Tensor, weights: Optional[Tensor] = None) -> Tensor:
-    r"""Compute the homography matrix using the DLT formulation for line correspondences.
+    """Compute the homography matrix using the DLT formulation for line correspondences.
 
     See :cite:`homolines2001` for details.
+
     The linear system is solved by using the Weighted Least Squares Solution for the 4 Line correspondences algorithm.
+
     Args:
         ls1: A set of line segments in the first image with a tensor shape :math:`(B, N, 2, 2)`.
         ls2: A set of line segments in the second image with a tensor shape :math:`(B, N, 2, 2)`.
         weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.
+
     Returns:
         the computed homography matrix with shape :math:`(B, 3, 3)`.
     """
     if len(ls1.shape) == 3:
         ls1 = ls1[None]
     if len(ls2.shape) == 3:
         ls2 = ls2[None]
```

### Comparing `kornia-0.6.9/kornia/geometry/liegroup/se2.py` & `kornia-0.7.0/kornia/geometry/liegroup/se2.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,15 +1,41 @@
 # kornia.geometry.se2 module inspired by Sophus-sympy.
 # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/se2.py
-from typing import Optional, Tuple, overload
+from __future__ import annotations
 
-from kornia.core import Module, Parameter, Tensor, concatenate, pad, rand, stack, tensor, where, zeros_like
-from kornia.geometry.liegroup._utils import check_se2_omega_shape, check_se2_r_t_shape, check_v_shape
+from typing import overload
+
+from kornia.core import (
+    Device,
+    Dtype,
+    Module,
+    Parameter,
+    Tensor,
+    concatenate,
+    pad,
+    rand,
+    stack,
+    tensor,
+    where,
+    zeros_like,
+)
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SAME_DEVICES, KORNIA_CHECK_TYPE
+from kornia.geometry.liegroup._utils import check_se2_omega_shape, check_se2_t_shape, check_v_shape
 from kornia.geometry.liegroup.so2 import So2
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SAME_DEVICES, KORNIA_CHECK_TYPE
+from kornia.geometry.vector import Vector2
+
+
+def _check_se2_r_t_shape(r: So2, t: Tensor) -> None:
+    z_shape = r.z.shape
+    if ((len(z_shape) == 1) and (len(t.shape) == 2)) or ((len(z_shape) == 0) and len(t.shape) == 1):
+        check_se2_t_shape(t)
+    else:
+        raise ValueError(
+            f"Invalid input, both the inputs should be either batched or unbatched. Got: {r.z.shape} and {t.shape}"
+        )
 
 
 class Se2(Module):
     r"""Base class to represent the Se2 group.
 
     The SE(2) is the group of rigid body transformations about the origin of two-dimensional Euclidean
     space :math:`R^2` under the operation of composition.
@@ -22,15 +48,15 @@
         >>> se2
         rotation: Parameter containing:
         tensor([1.+0.j], requires_grad=True)
         translation: Parameter containing:
         tensor([[1., 1.]], requires_grad=True)
     """
 
-    def __init__(self, rotation: So2, translation: Tensor) -> None:
+    def __init__(self, rotation: So2, translation: Vector2 | Tensor) -> None:
         """Constructor for the base class.
 
         Internally represented by a complex number `z` and a translation 2-vector.
 
         Args:
             rotation: So2 group encompassing a rotation.
             translation: translation vector with the shape of :math:`(B, 2)`.
@@ -44,33 +70,40 @@
             tensor([1.+0.j], requires_grad=True)
             translation: Parameter containing:
             tensor([[1., 1.]], requires_grad=True)
         """
         super().__init__()
         KORNIA_CHECK_TYPE(rotation, So2)
         # TODO change to KORNIA_CHECK_SHAPE once there is multiple shape support
-        check_se2_r_t_shape(rotation, translation)
-        self._rotation = rotation
-        self._translation = Parameter(translation)
+        # KORNIA_CHECK_TYPE(translation, (Vector3, Tensor))
+        if not isinstance(translation, (Vector2, Tensor)):
+            raise TypeError(f"translation type is {type(translation)}")
+        self._translation: Vector2 | Parameter
+        self._rotation: So2 = rotation
+        if isinstance(translation, Tensor):
+            _check_se2_r_t_shape(rotation, translation)  # TODO remove
+            self._translation = Parameter(translation)
+        else:
+            self._translation = translation
 
     def __repr__(self) -> str:
         return f"rotation: {self.r}\ntranslation: {self.t}"
 
-    def __getitem__(self, idx) -> 'Se2':
+    def __getitem__(self, idx: int | slice) -> Se2:
         return Se2(self._rotation[idx], self._translation[idx])
 
     @overload
-    def __mul__(self, right: 'Se2') -> 'Se2':
+    def __mul__(self, right: Se2) -> Se2:
         ...
 
     @overload
     def __mul__(self, right: Tensor) -> Tensor:
         ...
 
-    def __mul__(self, right):
+    def __mul__(self, right: Se2 | Tensor) -> Se2 | Tensor:
         """Compose two Se2 transformations.
 
         Args:
             right: the other Se2 transformation.
 
         Return:
             The resulting Se2 transformation.
@@ -78,18 +111,17 @@
         so2 = self.so2
         t = self.t
         if isinstance(right, Se2):
             KORNIA_CHECK_TYPE(right, Se2)
             _r = so2 * right.so2
             _t = t + so2 * right.t
             return Se2(_r, _t)
-        elif isinstance(right, Tensor):
-            KORNIA_CHECK_TYPE(right, Tensor)
+        elif isinstance(right, (Vector2, Tensor)):
             # TODO change to KORNIA_CHECK_SHAPE once there is multiple shape support
-            check_se2_r_t_shape(so2, right)
+            # _check_se2_r_t_shape(so2, risght)
             return so2 * right + t
         else:
             raise TypeError(f"Unsupported type: {type(right)}")
 
     @property
     def so2(self) -> So2:
         """Return the underlying rotation(So2)."""
@@ -97,30 +129,30 @@
 
     @property
     def r(self) -> So2:
         """Return the underlying rotation(So2)."""
         return self._rotation
 
     @property
-    def t(self) -> Tensor:
+    def t(self) -> Vector2 | Parameter:
         """Return the underlying translation vector of shape :math:`(B,2)`."""
         return self._translation
 
     @property
     def rotation(self) -> So2:
         """Return the underlying rotation(So2)."""
         return self._rotation
 
     @property
-    def translation(self) -> Tensor:
+    def translation(self) -> Vector2 | Parameter:
         """Return the underlying translation vector of shape :math:`(B,2)`."""
         return self._translation
 
     @staticmethod
-    def exp(v) -> 'Se2':
+    def exp(v: Tensor) -> Se2:
         """Converts elements of lie algebra to elements of lie group.
 
         Args:
             v: vector of shape :math:`(B, 3)`.
 
         Example:
             >>> v = torch.ones((1, 3))
@@ -158,19 +190,19 @@
         denom = self.so2.z.real - 1
         a = where(
             denom != 0, -(half_theta * self.so2.z.imag) / denom, tensor(0.0, device=theta.device, dtype=theta.dtype)
         )
         row0 = stack((a, half_theta), -1)
         row1 = stack((-half_theta, a), -1)
         V_inv = stack((row0, row1), -2)
-        upsilon = V_inv @ self.t[..., None]
+        upsilon = V_inv @ self.t.data[..., None]
         return stack((upsilon[..., 0, 0], upsilon[..., 1, 0], theta), -1)
 
     @staticmethod
-    def hat(v):
+    def hat(v: Tensor) -> Tensor:
         """Converts elements from vector space to lie algebra. Returns matrix of shape :math:`(B, 3, 3)`.
 
         Args:
             v: vector of shape:math:`(B, 3)`.
 
         Example:
             >>> theta = torch.tensor(3.1415/2)
@@ -182,15 +214,15 @@
         check_v_shape(v)
         upsilon = stack((v[..., 0], v[..., 1]), -1)
         theta = v[..., 2]
         col0 = concatenate((So2.hat(theta), upsilon.unsqueeze(-2)), -2)
         return pad(col0, (0, 1))
 
     @staticmethod
-    def vee(omega) -> Tensor:
+    def vee(omega: Tensor) -> Tensor:
         """Converts elements from lie algebra to vector space.
 
         Args:
             omega: 3x3-matrix representing lie algebra of shape :math:`(B, 3, 3)`.
 
         Returns:
             vector of shape :math:`(B, 3)`.
@@ -204,113 +236,138 @@
         # TODO change to KORNIA_CHECK_SHAPE once there is multiple shape support
         check_se2_omega_shape(omega)
         upsilon = omega[..., 2, :2]
         theta = So2.vee(omega[..., :2, :2])
         return concatenate((upsilon, theta[..., None]), -1)
 
     @classmethod
-    def identity(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Se2':
+    def identity(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> Se2:
         """Create a Se2 group representing an identity rotation and zero translation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
             >>> s = Se2.identity(1)
             >>> s.r
             Parameter containing:
             tensor([1.+0.j], requires_grad=True)
             >>> s.t
-            Parameter containing:
-            tensor([[0., 0.]], requires_grad=True)
+            x: tensor([0.])
+            y: tensor([0.])
         """
         t: Tensor = tensor([0.0, 0.0], device=device, dtype=dtype)
         if batch_size is not None:
             KORNIA_CHECK(batch_size >= 1, msg="batch_size must be positive")
             t = t.repeat(batch_size, 1)
-        return cls(So2.identity(batch_size, device, dtype), t)
+        return cls(So2.identity(batch_size, device, dtype), Vector2(t))
 
     def matrix(self) -> Tensor:
         """Returns the matrix representation of shape :math:`(B, 3, 3)`.
 
         Example:
             >>> s = Se2(So2.identity(1), torch.ones(1, 2))
             >>> s.matrix()
             tensor([[[1., -0., 1.],
                      [0., 1., 1.],
                      [0., 0., 1.]]], grad_fn=<CopySlices>)
         """
-        rt = concatenate((self.r.matrix(), self.t[..., None]), -1)
+        rt = concatenate((self.r.matrix(), self.t.data[..., None]), -1)
         rt_3x3 = pad(rt, (0, 0, 0, 1))  # add last row zeros
         rt_3x3[..., -1, -1] = 1.0
         return rt_3x3
 
-    def inverse(self) -> 'Se2':
+    @classmethod
+    def from_matrix(cls, matrix: Tensor) -> Se2:
+        """Create an Se2 group from a matrix.
+
+        Args:
+            matrix: tensor of shape :math:`(B, 3, 3)`.
+
+        Example:
+            >>> s = Se2.from_matrix(torch.eye(3).repeat(2, 1, 1))
+            >>> s.r
+            Parameter containing:
+            tensor([1.+0.j, 1.+0.j], requires_grad=True)
+            >>> s.t
+            Parameter containing:
+            tensor([[0., 0.],
+                    [0., 0.]], requires_grad=True)
+        """
+        # KORNIA_CHECK_SHAPE(matrix, ["B", "3", "3"])  # FIXME: resolve shape bugs. @edgarriba
+        r = So2.from_matrix(matrix[..., :2, :2])
+        t = matrix[..., :2, -1]
+        return cls(r, t)
+
+    def inverse(self) -> Se2:
         """Returns the inverse transformation.
 
         Example:
             >>> s = Se2(So2.identity(1), torch.ones(1,2))
             >>> s_inv = s.inverse()
             >>> s_inv.r
             Parameter containing:
             tensor([1.+0.j], requires_grad=True)
             >>> s_inv.t
             Parameter containing:
             tensor([[-1., -1.]], requires_grad=True)
         """
         r_inv: So2 = self.r.inverse()
-        t_inv: Tensor = r_inv * (-1 * self.t)
-        return Se2(r_inv, t_inv)
+        _t = -1 * self.t
+        if isinstance(_t, int):
+            raise TypeError('Unexpected integer from `-1 * translation`')
+
+        return Se2(r_inv, r_inv * _t)
 
     @classmethod
-    def random(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Se2':
+    def random(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> Se2:
         """Create a Se2 group representing a random transformation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
             >>> s = Se2.random()
             >>> s = Se2.random(batch_size=3)
         """
         r = So2.random(batch_size, device, dtype)
-        shape: Tuple[int, ...]
+        shape: tuple[int, ...]
         if batch_size is None:
             shape = (2,)
         else:
             KORNIA_CHECK(batch_size >= 1, msg="batch_size must be positive")
             shape = (batch_size, 2)
-        return cls(r, rand(shape, device=device, dtype=dtype))
+        return cls(r, Vector2(rand(shape, device=device, dtype=dtype)))
 
     @classmethod
-    def trans(cls, x: Tensor, y: Tensor) -> "Se2":
+    def trans(cls, x: Tensor, y: Tensor) -> Se2:
         """Construct a translation only Se2 instance.
 
         Args:
             x: the x-axis translation.
             y: the y-axis translation.
         """
         KORNIA_CHECK(x.shape == y.shape)
         KORNIA_CHECK_SAME_DEVICES([x, y])
         batch_size = x.shape[0] if len(x.shape) > 0 else None
         rotation = So2.identity(batch_size, x.device, x.dtype)
         return cls(rotation, stack((x, y), -1))
 
     @classmethod
-    def trans_x(cls, x: Tensor) -> "Se2":
+    def trans_x(cls, x: Tensor) -> Se2:
         """Construct a x-axis translation.
 
         Args:
             x: the x-axis translation.
         """
         zs = zeros_like(x)
         return cls.trans(x, zs)
 
     @classmethod
-    def trans_y(cls, y: Tensor) -> "Se2":
+    def trans_y(cls, y: Tensor) -> Se2:
         """Construct a y-axis translation.
 
         Args:
             y: the y-axis translation.
         """
         zs = zeros_like(y)
         return cls.trans(zs, y)
@@ -322,9 +379,9 @@
             >>> s = Se2.identity()
             >>> s.adjoint()
             tensor([[1., -0., 0.],
                     [0., 1., -0.],
                     [0., 0., 1.]], grad_fn=<CopySlices>)
         """
         rt = self.matrix()
-        rt[..., 0:2, 2] = stack((self.t[..., 1], -self.t[..., 0]), -1)
+        rt[..., 0:2, 2] = stack((self.t.data[..., 1], -self.t.data[..., 0]), -1)
         return rt
```

### Comparing `kornia-0.6.9/kornia/geometry/liegroup/se3.py` & `kornia-0.7.0/kornia/geometry/liegroup/so3.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,376 +1,333 @@
 # kornia.geometry.so3 module inspired by Sophus-sympy.
-# https://github.com/strasdat/Sophus/blob/master/sympy/sophus/se3.py
-from typing import Optional, Tuple
+# https://github.com/strasdat/Sophus/blob/master/sympy/sophus/so3.py
+from __future__ import annotations
 
-from kornia.core import Module, Parameter, Tensor, concatenate, eye, pad, rand, stack, tensor, where, zeros_like
-from kornia.geometry.liegroup.so3 import So3
+from kornia.core import Device, Dtype, Module, Tensor, concatenate, stack, tensor, where, zeros, zeros_like
+from kornia.core.check import KORNIA_CHECK_TYPE
 from kornia.geometry.linalg import batched_dot_product
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SAME_DEVICES, KORNIA_CHECK_TYPE
+from kornia.geometry.quaternion import Quaternion
+from kornia.geometry.vector import Vector3
 
 
-class Se3(Module):
-    r"""Base class to represent the Se3 group.
+class So3(Module):
+    r"""Base class to represent the So3 group.
 
-    The SE(3) is the group of rigid body transformations about the origin of three-dimensional Euclidean
-    space :math:`R^3` under the operation of composition.
-    See more: https://ingmec.ual.es/~jlblanco/papers/jlblanco2010geometry3D_techrep.pdf
+    The SO(3) is the group of all rotations about the origin of three-dimensional Euclidean space
+    :math:`R^3` under the operation of composition.
+    See more: https://en.wikipedia.org/wiki/3D_rotation_group
+
+    We internally represent the rotation by a unit quaternion.
 
     Example:
-        >>> from kornia.geometry.quaternion import Quaternion
         >>> q = Quaternion.identity()
-        >>> s = Se3(So3(q), torch.ones(3))
-        >>> s.r
+        >>> s = So3(q)
+        >>> s.q
         Parameter containing:
         tensor([1., 0., 0., 0.], requires_grad=True)
-        >>> s.t
-        Parameter containing:
-        tensor([1., 1., 1.], requires_grad=True)
     """
 
-    def __init__(self, rotation: So3, translation: Tensor) -> None:
+    def __init__(self, q: Quaternion) -> None:
         """Constructor for the base class.
 
-        Internally represented by a unit quaternion `q` and a translation 3-vector.
+        Internally represented by a unit quaternion `q`.
 
         Args:
-            rotation: So3 group encompassing a rotation.
-            translation: translation vector with the shape of :math:`(B, 3)`.
+            data: Quaternion with the shape of :math:`(B, 4)`.
 
         Example:
-            >>> from kornia.geometry.quaternion import Quaternion
-            >>> q = Quaternion.identity(batch_size=1)
-            >>> s = Se3(So3(q), torch.ones((1, 3)))
-            >>> s.r
-            Parameter containing:
-            tensor([[1., 0., 0., 0.]], requires_grad=True)
-            >>> s.t
+            >>> data = torch.ones((2, 4))
+            >>> q = Quaternion(data)
+            >>> So3(q)
             Parameter containing:
-            tensor([[1., 1., 1.]], requires_grad=True)
+            tensor([[1., 1., 1., 1.],
+                    [1., 1., 1., 1.]], requires_grad=True)
         """
         super().__init__()
-        KORNIA_CHECK_TYPE(rotation, So3)
-        # KORNIA_CHECK_SHAPE(t, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
-        self._rotation = rotation
-        self._translation = Parameter(translation)
+        KORNIA_CHECK_TYPE(q, Quaternion)
+        self._q = q
 
     def __repr__(self) -> str:
-        return f"rotation: {self.r}\ntranslation: {self.t}"
+        return f"{self.q}"
 
-    def __getitem__(self, idx) -> 'Se3':
-        return Se3(self._rotation[idx], self._translation[idx])
+    def __getitem__(self, idx: int | slice) -> So3:
+        return So3(self._q[idx])
 
-    def __mul__(self, right: "Se3") -> "Se3":
-        """Compose two Se3 transformations.
+    def __mul__(self, right: So3) -> So3:
+        """Compose two So3 transformations.
 
         Args:
-            right: the other Se3 transformation.
+            right: the other So3 transformation.
 
         Return:
-            The resulting Se3 transformation.
+            The resulting So3 transformation.
         """
-        so3 = self.so3
-        t = self.t
-        if isinstance(right, Se3):
-            KORNIA_CHECK_TYPE(right, Se3)
-            # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/se3.py#L97
-            _r = so3 * right.so3
-            _t = t + so3 * right.t
-            return Se3(_r, _t)
-        elif isinstance(right, Tensor):
-            KORNIA_CHECK_TYPE(right, Tensor)
-            # KORNIA_CHECK_SHAPE(right, ["B", "N"])  # FIXME: resolve shape bugs. @edgarriba
-            return so3 * right + t
+        # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/so3.py#L98
+        if isinstance(right, So3):
+            return So3(self.q * right.q)
+        elif isinstance(right, (Tensor, Vector3)):
+            # KORNIA_CHECK_SHAPE(right, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
+            w = zeros(*right.shape[:-1], 1, device=right.device, dtype=right.dtype)
+            quat = Quaternion(concatenate((w, right.data), -1))
+            out = (self.q * quat * self.q.conj()).vec
+            if isinstance(right, Tensor):
+                return out
+            elif isinstance(right, Vector3):
+                return Vector3(out)
         else:
-            raise TypeError(f"Unsupported type: {type(right)}")
-
-    @property
-    def so3(self) -> So3:
-        """Return the underlying rotation(So3)."""
-        return self._rotation
-
-    @property
-    def r(self) -> So3:
-        """Return the underlying rotation(So3)."""
-        return self._rotation
-
-    @property
-    def t(self) -> Tensor:
-        """Return the underlying translation vector of shape :math:`(B,3)`."""
-        return self._translation
+            raise TypeError(f"Not So3 or Tensor type. Got: {type(right)}")
 
     @property
-    def rotation(self) -> So3:
-        """Return the underlying rotation(So3)."""
-        return self._rotation
-
-    @property
-    def translation(self) -> Tensor:
-        """Return the underlying translation vector of shape :math:`(B,3)`."""
-        return self._translation
+    def q(self) -> Quaternion:
+        """Return the underlying data with shape :math:`(B,4)`."""
+        return self._q
 
     @staticmethod
-    def exp(v) -> 'Se3':
+    def exp(v: Tensor) -> So3:
         """Converts elements of lie algebra to elements of lie group.
 
+        See more: https://vision.in.tum.de/_media/members/demmeln/nurlanov2021so3log.pdf
+
         Args:
-            v: vector of shape :math:`(B, 6)`.
+            v: vector of shape :math:`(B,3)`.
 
         Example:
-            >>> v = torch.zeros((1, 6))
-            >>> s = Se3.exp(v)
-            >>> s.r
-            Parameter containing:
-            tensor([[1., 0., 0., 0.]], requires_grad=True)
-            >>> s.t
+            >>> v = torch.zeros((2, 3))
+            >>> s = So3.identity().exp(v)
+            >>> s
             Parameter containing:
-            tensor([[0., 0., 0.]], requires_grad=True)
+            tensor([[1., 0., 0., 0.],
+                    [1., 0., 0., 0.]], requires_grad=True)
         """
-        # KORNIA_CHECK_SHAPE(v, ["B", "6"])  # FIXME: resolve shape bugs. @edgarriba
-        upsilon = v[..., :3]
-        omega = v[..., 3:]
-        omega_hat = So3.hat(omega)
-        omega_hat_sq = omega_hat @ omega_hat
-        theta = batched_dot_product(omega, omega).sqrt()
-        R = So3.exp(omega)
-        V = (
-            eye(3, device=v.device, dtype=v.dtype)
-            + ((1 - theta.cos()) / (theta**2))[..., None, None] * omega_hat
-            + ((theta - theta.sin()) / (theta**3))[..., None, None] * omega_hat_sq
-        )
-        U = where(theta[..., None] != 0.0, (upsilon[..., None, :] * V).sum(-1), upsilon)
-        return Se3(R, U)
+        # KORNIA_CHECK_SHAPE(v, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
+        theta = batched_dot_product(v, v).sqrt()[..., None]
+        theta_nonzeros = theta != 0.0
+        theta_half = 0.5 * theta
+        # TODO: uncomment me after deprecate pytorch 10.2
+        # w = where(theta_nonzeros, theta_half.cos(), 1.0)
+        # b = where(theta_nonzeros, theta_half.sin() / theta, 0.0)
+        w = where(theta_nonzeros, theta_half.cos(), tensor(1.0, device=v.device, dtype=v.dtype))
+        b = where(theta_nonzeros, theta_half.sin() / theta, tensor(0.0, device=v.device, dtype=v.dtype))
+        xyz = b * v
+        return So3(Quaternion(concatenate((w, xyz), -1)))
 
     def log(self) -> Tensor:
         """Converts elements of lie group  to elements of lie algebra.
 
         Example:
-            >>> from kornia.geometry.quaternion import Quaternion
-            >>> q = Quaternion.identity()
-            >>> Se3(So3(q), torch.zeros(3)).log()
-            tensor([0., 0., 0., 0., 0., 0.], grad_fn=<CatBackward0>)
-        """
-        omega = self.r.log()
-        theta = batched_dot_product(omega, omega).sqrt()
-        omega_hat = So3.hat(omega)
-        omega_hat_sq = omega_hat @ omega_hat
-        V_inv = (
-            eye(3, device=omega.device, dtype=omega.dtype)
-            - 0.5 * omega_hat
-            + ((1 - theta * (theta / 2).cos() / (2 * (theta / 2).sin())) / theta.pow(2))[..., None, None] * omega_hat_sq
+            >>> data = torch.ones((2, 4))
+            >>> q = Quaternion(data)
+            >>> So3(q).log()
+            tensor([[0., 0., 0.],
+                    [0., 0., 0.]], grad_fn=<WhereBackward0>)
+        """
+        theta = batched_dot_product(self.q.vec, self.q.vec).sqrt()
+        # NOTE: this differs from https://github.com/strasdat/Sophus/blob/master/sympy/sophus/so3.py#L33
+        omega = where(
+            theta[..., None] != 0,
+            2 * self.q.real[..., None].acos() * self.q.vec / theta[..., None],
+            2 * self.q.vec / self.q.real[..., None],
         )
-        t = where(theta[..., None] != 0.0, (self.t[..., None, :] * V_inv).sum(-1), self.t)
-        return concatenate((t, omega), -1)
+        return omega
 
     @staticmethod
-    def hat(v) -> Tensor:
-        """Converts elements from vector space to lie algebra.
+    def hat(v: Vector3 | Tensor) -> Tensor:
+        """Converts elements from vector space to lie algebra. Returns matrix of shape :math:`(B,3,3)`.
 
         Args:
-            v: vector of shape :math:`(B, 6)`.
-
-        Returns:
-            matrix of shape :math:`(B, 4, 4)`.
+            v: Vector3 or tensor of shape :math:`(B,3)`.
 
         Example:
-            >>> v = torch.ones((1, 6))
-            >>> m = Se3.hat(v)
+            >>> v = torch.ones((1,3))
+            >>> m = So3.hat(v)
             >>> m
-            tensor([[[ 0., -1.,  1.,  1.],
-                     [ 1.,  0., -1.,  1.],
-                     [-1.,  1.,  0.,  1.],
-                     [ 0.,  0.,  0.,  0.]]])
-        """
-        # KORNIA_CHECK_SHAPE(v, ["B", "6"])  # FIXME: resolve shape bugs. @edgarriba
-        upsilon, omega = v[..., :3], v[..., 3:]
-        rt = concatenate((So3.hat(omega), upsilon[..., None]), -1)
-        return pad(rt, (0, 0, 0, 1))  # add zeros bottom
+            tensor([[[ 0., -1.,  1.],
+                     [ 1.,  0., -1.],
+                     [-1.,  1.,  0.]]])
+        """
+        # KORNIA_CHECK_SHAPE(v, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
+        if isinstance(v, Tensor):
+            # TODO: Figure out why mypy think `v` can be a Vector3 which didn't allow ellipsis on index
+            a, b, c = v[..., 0], v[..., 1], v[..., 2]  # type: ignore[index]
+        else:
+            a, b, c = v.x, v.y, v.z
+        z = zeros_like(a)
+        row0 = stack((z, -c, b), -1)
+        row1 = stack((c, z, -a), -1)
+        row2 = stack((-b, a, z), -1)
+        return stack((row0, row1, row2), -2)
 
     @staticmethod
-    def vee(omega) -> Tensor:
-        """Converts elements from lie algebra to vector space.
+    def vee(omega: Tensor) -> Tensor:
+        r"""Converts elements from lie algebra to vector space. Returns vector of shape :math:`(B,3)`.
+
+        .. math::
+            omega = \begin{bmatrix} 0 & -c & b \\
+            c & 0 & -a \\
+            -b & a & 0\end{bmatrix}
 
         Args:
-            omega: 4x4-matrix representing lie algebra of shape :math:`(B,4,4)`.
+            omega: 3x3-matrix representing lie algebra.
 
-        Returns:
-            vector of shape :math:`(B,6)`.
+        Example:
+            >>> v = torch.ones((1,3))
+            >>> omega = So3.hat(v)
+            >>> So3.vee(omega)
+            tensor([[1., 1., 1.]])
+        """
+        # KORNIA_CHECK_SHAPE(omega, ["B", "3", "3"])  # FIXME: resolve shape bugs. @edgarriba
+        a, b, c = omega[..., 2, 1], omega[..., 0, 2], omega[..., 1, 0]
+        return stack((a, b, c), -1)
+
+    def matrix(self) -> Tensor:
+        r"""Convert the quaternion to a rotation matrix of shape :math:`(B,3,3)`.
+
+        The matrix is of the form:
+
+        .. math::
+            \begin{bmatrix} 1-2y^2-2z^2 & 2xy-2zw & 2xy+2yw \\
+            2xy+2zw & 1-2x^2-2z^2 & 2yz-2xw \\
+            2xz-2yw & 2yz+2xw & 1-2x^2-2y^2\end{bmatrix}
 
         Example:
-            >>> v = torch.ones((1, 6))
-            >>> omega_hat = Se3.hat(v)
-            >>> Se3.vee(omega_hat)
-            tensor([[1., 1., 1., 1., 1., 1.]])
-        """
-        # KORNIA_CHECK_SHAPE(omega, ["B", "4", "4"])  # FIXME: resolve shape bugs. @edgarriba
-        head = omega[..., :3, -1]
-        tail = So3.vee(omega[..., :3, :3])
-        return concatenate((head, tail), -1)
+            >>> s = So3.identity()
+            >>> m = s.matrix()
+            >>> m
+            tensor([[1., 0., 0.],
+                    [0., 1., 0.],
+                    [0., 0., 1.]], grad_fn=<StackBackward0>)
+        """
+        w = self.q.w[..., None]
+        x, y, z = self.q.x[..., None], self.q.y[..., None], self.q.z[..., None]
+        q0 = 1 - 2 * y**2 - 2 * z**2
+        q1 = 2 * x * y - 2 * z * w
+        q2 = 2 * x * z + 2 * y * w
+        row0 = concatenate((q0, q1, q2), -1)
+        q0 = 2 * x * y + 2 * z * w
+        q1 = 1 - 2 * x**2 - 2 * z**2
+        q2 = 2 * y * z - 2 * x * w
+        row1 = concatenate((q0, q1, q2), -1)
+        q0 = 2 * x * z - 2 * y * w
+        q1 = 2 * y * z + 2 * x * w
+        q2 = 1 - 2 * x**2 - 2 * y**2
+        row2 = concatenate((q0, q1, q2), -1)
+        return stack((row0, row1, row2), -2)
 
     @classmethod
-    def identity(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Se3':
-        """Create a Se3 group representing an identity rotation and zero translation.
+    def from_matrix(cls, matrix: Tensor) -> So3:
+        """Create So3 from a rotation matrix.
 
         Args:
-            batch_size: the batch size of the underlying data.
+            matrix: the rotation matrix to convert of shape :math:`(B,3,3)`.
 
         Example:
-            >>> s = Se3.identity()
-            >>> s.r
+            >>> m = torch.eye(3)
+            >>> s = So3.from_matrix(m)
+            >>> s
             Parameter containing:
             tensor([1., 0., 0., 0.], requires_grad=True)
-            >>> s.t
+        """
+        return cls(Quaternion.from_matrix(matrix))
+
+    @classmethod
+    def from_wxyz(cls, wxyz: Tensor) -> So3:
+        """Create So3 from a tensor representing a quaternion.
+
+        Args:
+            wxyz: the quaternion to convert of shape :math:`(B,4)`.
+
+        Example:
+            >>> q = torch.tensor([1., 0., 0., 0.])
+            >>> s = So3.from_wxyz(q)
+            >>> s
             Parameter containing:
-            tensor([0., 0., 0.], requires_grad=True)
+            tensor([1., 0., 0., 0.], requires_grad=True)
         """
-        t: Tensor = tensor([0.0, 0.0, 0.0], device=device, dtype=dtype)
-        if batch_size is not None:
-            t = t.repeat(batch_size, 1)
+        # KORNIA_CHECK_SHAPE(wxyz, ["B", "4"])  # FIXME: resolve shape bugs. @edgarriba
+        return cls(Quaternion(wxyz))
 
-        return cls(So3.identity(batch_size, device, dtype), t)
+    @classmethod
+    def identity(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> So3:
+        """Create a So3 group representing an identity rotation.
 
-    def matrix(self) -> Tensor:
-        """Returns the matrix representation of shape :math:`(B, 4, 4)`.
+        Args:
+            batch_size: the batch size of the underlying data.
 
         Example:
-            >>> s = Se3(So3.identity(), torch.ones(3))
-            >>> s.matrix()
-            tensor([[1., 0., 0., 1.],
-                    [0., 1., 0., 1.],
-                    [0., 0., 1., 1.],
-                    [0., 0., 0., 1.]], grad_fn=<CopySlices>)
-        """
-        rt = concatenate((self.r.matrix(), self.t[..., None]), -1)
-        rt_4x4 = pad(rt, (0, 0, 0, 1))  # add last row zeros
-        rt_4x4[..., -1, -1] = 1.0
-        return rt_4x4
+            >>> s = So3.identity()
+            >>> s
+            Parameter containing:
+            tensor([1., 0., 0., 0.], requires_grad=True)
 
-    def inverse(self) -> 'Se3':
+            >>> s = So3.identity(batch_size=2)
+            >>> s
+            Parameter containing:
+            tensor([[1., 0., 0., 0.],
+                    [1., 0., 0., 0.]], requires_grad=True)
+        """
+        return cls(Quaternion.identity(batch_size, device, dtype))
+
+    def inverse(self) -> So3:
         """Returns the inverse transformation.
 
         Example:
-            >>> s = Se3(So3.identity(), torch.ones(3))
-            >>> s_inv = s.inverse()
-            >>> s_inv.r
+            >>> s = So3.identity()
+            >>> s.inverse()
             Parameter containing:
             tensor([1., -0., -0., -0.], requires_grad=True)
-            >>> s_inv.t
-            Parameter containing:
-            tensor([-1., -1., -1.], requires_grad=True)
         """
-        r_inv = self.r.inverse()
-        return Se3(r_inv, r_inv * (-1 * self.t))
+        return So3(self.q.conj())
 
     @classmethod
-    def random(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Se3':
-        """Create a Se3 group representing a random transformation.
+    def random(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> So3:
+        """Create a So3 group representing a random rotation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
-            >>> s = Se3.random()
-            >>> s = Se3.random(batch_size=3)
+            >>> s = So3.random()
+            >>> s = So3.random(batch_size=3)
         """
-        r = So3.random(batch_size, device, dtype)
-        shape: Tuple[int, ...]
-        if batch_size is None:
-            shape = (3,)
-        else:
-            KORNIA_CHECK(batch_size >= 1, msg="batch_size must be positive")
-            shape = (batch_size, 3)
-        return cls(r, rand(shape, device=device, dtype=dtype))
+        return cls(Quaternion.random(batch_size, device, dtype))
 
     @classmethod
-    def rot_x(cls, x: Tensor) -> "Se3":
+    def rot_x(cls, x: Tensor) -> So3:
         """Construct a x-axis rotation.
 
         Args:
             x: the x-axis rotation angle.
         """
         zs = zeros_like(x)
-        return cls(So3.rot_x(x), stack((zs, zs, zs), -1))
+        return cls.exp(stack((x, zs, zs), -1))
 
     @classmethod
-    def rot_y(cls, y: Tensor) -> "Se3":
-        """Construct a y-axis rotation.
+    def rot_y(cls, y: Tensor) -> So3:
+        """Construct a z-axis rotation.
 
         Args:
             y: the y-axis rotation angle.
         """
         zs = zeros_like(y)
-        return cls(So3.rot_y(y), stack((zs, zs, zs), -1))
+        return cls.exp(stack((zs, y, zs), -1))
 
     @classmethod
-    def rot_z(cls, z: Tensor) -> "Se3":
+    def rot_z(cls, z: Tensor) -> So3:
         """Construct a z-axis rotation.
 
         Args:
             z: the z-axis rotation angle.
         """
         zs = zeros_like(z)
-        return cls(So3.rot_z(z), stack((zs, zs, zs), -1))
-
-    @classmethod
-    def trans(cls, x: Tensor, y: Tensor, z: Tensor) -> "Se3":
-        """Construct a translation only Se3 instance.
-
-        Args:
-            x: the x-axis translation.
-            y: the y-axis translation.
-            z: the z-axis translation.
-        """
-        KORNIA_CHECK(x.shape == y.shape)
-        KORNIA_CHECK(y.shape == z.shape)
-        KORNIA_CHECK_SAME_DEVICES([x, y, z])
-        batch_size = x.shape[0] if len(x.shape) > 0 else None
-        rotation = So3.identity(batch_size, x.device, x.dtype)
-        return cls(rotation, stack((x, y, z), -1))
-
-    @classmethod
-    def trans_x(cls, x: Tensor) -> "Se3":
-        """Construct a x-axis translation.
-
-        Args:
-            x: the x-axis translation.
-        """
-        zs = zeros_like(x)
-        return cls.trans(x, zs, zs)
-
-    @classmethod
-    def trans_y(cls, y: Tensor) -> "Se3":
-        """Construct a y-axis translation.
-
-        Args:
-            y: the y-axis translation.
-        """
-        zs = zeros_like(y)
-        return cls.trans(zs, y, zs)
-
-    @classmethod
-    def trans_z(cls, z: Tensor) -> "Se3":
-        """Construct a z-axis translation.
-
-        Args:
-            z: the z-axis translation.
-        """
-        zs = zeros_like(z)
-        return cls.trans(zs, zs, z)
+        return cls.exp(stack((zs, zs, z), -1))
 
     def adjoint(self) -> Tensor:
-        """Returns the adjoint matrix of shape :math:`(B, 6, 6)`.
+        """Returns the adjoint matrix of shape :math:`(B, 3, 3)`.
 
         Example:
-            >>> s = Se3.identity()
+            >>> s = So3.identity()
             >>> s.adjoint()
-            tensor([[1., 0., 0., 0., 0., 0.],
-                    [0., 1., 0., 0., 0., 0.],
-                    [0., 0., 1., 0., 0., 0.],
-                    [0., 0., 0., 1., 0., 0.],
-                    [0., 0., 0., 0., 1., 0.],
-                    [0., 0., 0., 0., 0., 1.]], grad_fn=<CatBackward0>)
-        """
-        R = self.so3.matrix()
-        z = zeros_like(R)
-        row0 = concatenate((R, So3.hat(self.t) @ R), -1)
-        row1 = concatenate((z, R), -1)
-        return concatenate((row0, row1), -2)
+            tensor([[1., 0., 0.],
+                    [0., 1., 0.],
+                    [0., 0., 1.]], grad_fn=<StackBackward0>)
+        """
+        return self.matrix()
```

### Comparing `kornia-0.6.9/kornia/geometry/liegroup/so2.py` & `kornia-0.7.0/kornia/geometry/liegroup/so2.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,23 @@
 # kornia.geometry.so2 module inspired by Sophus-sympy.
 # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/so2.py
-from typing import Optional, overload
+from __future__ import annotations
 
-from kornia.core import Module, Parameter, Tensor, complex, rand, stack, tensor, zeros_like
+from typing import overload
+
+from kornia.core import Device, Dtype, Module, Parameter, Tensor, complex, rand, stack, tensor, zeros_like
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR
 from kornia.geometry.liegroup._utils import (
+    check_so2_matrix,
     check_so2_matrix_shape,
     check_so2_t_shape,
     check_so2_theta_shape,
     check_so2_z_shape,
 )
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR
+from kornia.geometry.vector import Vector2
 
 
 class So2(Module):
     r"""Base class to represent the So2 group.
 
     The SO(2) is the group of all rotations about the origin of two-dimensional Euclidean space
     :math:`R^2` under the operation of composition.
@@ -49,55 +53,60 @@
         # TODO change to KORNIA_CHECK_SHAPE once there is multiple shape support
         check_so2_z_shape(z)
         self._z = Parameter(z)
 
     def __repr__(self) -> str:
         return f"{self.z}"
 
-    def __getitem__(self, idx: int) -> 'So2':
+    def __getitem__(self, idx: int | slice) -> So2:
         return So2(self._z[idx])
 
     @overload
-    def __mul__(self, right: 'So2') -> 'So2':
+    def __mul__(self, right: So2) -> So2:
         ...
 
     @overload
     def __mul__(self, right: Tensor) -> Tensor:
         ...
 
-    def __mul__(self, right):
+    def __mul__(self, right: So2 | Tensor) -> So2 | Tensor:
         """Performs a left-multiplication either rotation concatenation or point-transform.
 
         Args:
             right: the other So2 transformation.
 
         Return:
             The resulting So2 transformation.
         """
         z = self.z
         if isinstance(right, So2):
             return So2(z * right.z)
-        elif isinstance(right, Tensor):
+        elif isinstance(right, (Vector2, Tensor)):
             # TODO change to KORNIA_CHECK_SHAPE once there is multiple shape support
-            check_so2_t_shape(right)
-            x = right[..., 0]
-            y = right[..., 1]
+            if isinstance(right, Tensor):
+                check_so2_t_shape(right)
+            x = right.data[..., 0]
+            y = right.data[..., 1]
             real = z.real
             imag = z.imag
-            return stack((real * x - imag * y, imag * x + real * y), -1)
+            out = stack((real * x - imag * y, imag * x + real * y), -1)
+            if isinstance(right, Tensor):
+                return out
+            else:
+                return Vector2(out)
         else:
             raise TypeError(f"Not So2 or Tensor type. Got: {type(right)}")
 
     @property
     def z(self) -> Tensor:
         """Return the underlying data with shape :math:`(B, 1)`."""
         return self._z
 
     @staticmethod
-    def exp(theta: Tensor) -> 'So2':
+    def exp(theta: Tensor) -> So2:
         """Converts elements of lie algebra to elements of lie group.
 
         Args:
             theta: angle in radians of shape :math:`(B, 1)` or :math:`(B)`.
 
         Example:
             >>> v = torch.tensor([3.1415/2])
@@ -169,34 +178,35 @@
                     [0., 1.]], grad_fn=<StackBackward0>)
         """
         row0 = stack((self.z.real, -self.z.imag), -1)
         row1 = stack((self.z.imag, self.z.real), -1)
         return stack((row0, row1), -2)
 
     @classmethod
-    def from_matrix(cls, matrix: Tensor) -> 'So2':
+    def from_matrix(cls, matrix: Tensor) -> So2:
         """Create So2 from a rotation matrix.
 
         Args:
             matrix: the rotation matrix to convert of shape :math:`(B, 2, 2)`.
 
         Example:
             >>> m = torch.eye(2)
             >>> s = So2.from_matrix(m)
             >>> s.z
             Parameter containing:
             tensor(1.+0.j, requires_grad=True)
         """
         # TODO change to KORNIA_CHECK_SHAPE once there is multiple shape support
         check_so2_matrix_shape(matrix)
+        check_so2_matrix(matrix)
         z = complex(matrix[..., 0, 0], matrix[..., 1, 0])
         return cls(z)
 
     @classmethod
-    def identity(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'So2':
+    def identity(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> So2:
         """Create a So2 group representing an identity rotation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
             >>> s = So2.identity(batch_size=2)
@@ -208,27 +218,27 @@
         imag_data = tensor(0.0, device=device, dtype=dtype)
         if batch_size is not None:
             KORNIA_CHECK(batch_size >= 1, msg="batch_size must be positive")
             real_data = real_data.repeat(batch_size)
             imag_data = imag_data.repeat(batch_size)
         return cls(complex(real_data, imag_data))
 
-    def inverse(self) -> 'So2':
+    def inverse(self) -> So2:
         """Returns the inverse transformation.
 
         Example:
             >>> s = So2.identity()
             >>> s.inverse().z
             Parameter containing:
             tensor(1.+0.j, requires_grad=True)
         """
         return So2(1 / self.z)
 
     @classmethod
-    def random(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'So2':
+    def random(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> So2:
         """Create a So2 group representing a random rotation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
             >>> s = So2.random()
```

### Comparing `kornia-0.6.9/kornia/geometry/liegroup/so3.py` & `kornia-0.7.0/kornia/geometry/liegroup/se3.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,307 +1,456 @@
 # kornia.geometry.so3 module inspired by Sophus-sympy.
-# https://github.com/strasdat/Sophus/blob/master/sympy/sophus/so3.py
-from typing import Optional
+# https://github.com/strasdat/Sophus/blob/master/sympy/sophus/se3.py
+from __future__ import annotations
 
-from kornia.core import Module, Tensor, concatenate, stack, tensor, where, zeros, zeros_like
+from kornia.core import (
+    Device,
+    Dtype,
+    Module,
+    Parameter,
+    Tensor,
+    concatenate,
+    eye,
+    pad,
+    stack,
+    tensor,
+    where,
+    zeros_like,
+)
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SAME_DEVICES
+from kornia.geometry.liegroup.so3 import So3
 from kornia.geometry.linalg import batched_dot_product
 from kornia.geometry.quaternion import Quaternion
-from kornia.testing import KORNIA_CHECK_TYPE
+from kornia.geometry.vector import Vector3
 
 
-class So3(Module):
-    r"""Base class to represent the So3 group.
+class Se3(Module):
+    r"""Base class to represent the Se3 group.
 
-    The SO(3) is the group of all rotations about the origin of three-dimensional Euclidean space
-    :math:`R^3` under the operation of composition.
-    See more: https://en.wikipedia.org/wiki/3D_rotation_group
-
-    We internally represent the rotation by a unit quaternion.
+    The SE(3) is the group of rigid body transformations about the origin of three-dimensional Euclidean
+    space :math:`R^3` under the operation of composition.
+    See more: https://ingmec.ual.es/~jlblanco/papers/jlblanco2010geometry3D_techrep.pdf
 
     Example:
         >>> q = Quaternion.identity()
-        >>> s = So3(q)
-        >>> s.q
+        >>> s = Se3(q, torch.ones(3))
+        >>> s.r
         Parameter containing:
         tensor([1., 0., 0., 0.], requires_grad=True)
+        >>> s.t
+        Parameter containing:
+        tensor([1., 1., 1.], requires_grad=True)
     """
 
-    def __init__(self, q: Quaternion) -> None:
+    def __init__(self, rotation: Quaternion | So3, translation: Vector3 | Tensor) -> None:
         """Constructor for the base class.
 
-        Internally represented by a unit quaternion `q`.
+        Internally represented by a unit quaternion `q` and a translation 3-vector.
 
         Args:
-            data: Quaternion with the shape of :math:`(B, 4)`.
+            rotation: So3 group encompassing a rotation.
+            translation: Vector3 or translation tensor with the shape of :math:`(B, 3)`.
 
         Example:
-            >>> data = torch.ones((2, 4))
-            >>> q = Quaternion(data)
-            >>> So3(q)
+            >>> from kornia.geometry.quaternion import Quaternion
+            >>> q = Quaternion.identity(batch_size=1)
+            >>> s = Se3(q, torch.ones((1, 3)))
+            >>> s.r
+            Parameter containing:
+            tensor([[1., 0., 0., 0.]], requires_grad=True)
+            >>> s.t
             Parameter containing:
-            tensor([[1., 1., 1., 1.],
-                    [1., 1., 1., 1.]], requires_grad=True)
+            tensor([[1., 1., 1.]], requires_grad=True)
         """
         super().__init__()
-        KORNIA_CHECK_TYPE(q, Quaternion)
-        self._q = q
+        # KORNIA_CHECK_TYPE(rotation, (Quaternion, So3))
+        if not isinstance(rotation, (Quaternion, So3)):
+            raise TypeError(f"rotation type is {type(rotation)}")
+        # KORNIA_CHECK_TYPE(translation, (Vector3, Tensor))
+        if not isinstance(translation, (Vector3, Tensor)):
+            raise TypeError(f"translation type is {type(translation)}")
+        # KORNIA_CHECK_SHAPE(t, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
+        self._translation: Vector3 | Parameter
+        self._rotation: So3
+        if isinstance(translation, Tensor):
+            self._translation = Parameter(translation)
+        else:
+            self._translation = translation
+        if isinstance(rotation, Quaternion):
+            self._rotation = So3(rotation)
+        else:
+            self._rotation = rotation
 
     def __repr__(self) -> str:
-        return f"{self.q}"
+        return f"rotation: {self.r}\ntranslation: {self.t}"
 
-    def __getitem__(self, idx) -> 'So3':
-        return So3(self._q[idx])
+    def __getitem__(self, idx: int | slice) -> Se3:
+        return Se3(self._rotation[idx], self._translation[idx])
 
-    def __mul__(self, right):
-        """Compose two So3 transformations.
+    def __mul__(self, right: Se3) -> Se3 | Vector3 | Tensor:
+        """Compose two Se3 transformations.
 
         Args:
-            right: the other So3 transformation.
+            right: the other Se3 transformation.
 
         Return:
-            The resulting So3 transformation.
+            The resulting Se3 transformation.
         """
-        # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/so3.py#L98
-        if isinstance(right, So3):
-            return So3(self.q * right.q)
-        elif isinstance(right, Tensor):
-            # KORNIA_CHECK_SHAPE(right, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
-            w = zeros(*right.shape[:-1], 1, device=right.device, dtype=right.dtype)
-            quat = Quaternion(concatenate((w, right), -1))
-            return (self.q * quat * self.q.conj()).vec
+        so3 = self.so3
+        t = self.t
+        if isinstance(right, Se3):
+            # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/se3.py#L97
+            _r = so3 * right.so3
+            _t = t + so3 * right.t
+            return Se3(_r, _t)
+        elif isinstance(right, (Vector3, Tensor)):
+            # KORNIA_CHECK_SHAPE(right, ["B", "N"])  # FIXME: resolve shape bugs. @edgarriba
+            return so3 * right + t.data
         else:
-            raise TypeError(f"Not So3 or Tensor type. Got: {type(right)}")
+            raise TypeError(f"Unsupported type: {type(right)}")
+
+    @property
+    def so3(self) -> So3:
+        """Return the underlying rotation(So3)."""
+        return self._rotation
+
+    @property
+    def quaternion(self) -> Quaternion:
+        """Return the underlying rotation(Quaternion)."""
+        return self._rotation.q
 
     @property
-    def q(self) -> Quaternion:
-        """Return the underlying data with shape :math:`(B,4)`."""
-        return self._q
+    def r(self) -> So3:
+        """Return the underlying rotation(So3)."""
+        return self._rotation
+
+    @property
+    def t(self) -> Vector3 | Tensor:
+        """Return the underlying translation vector of shape :math:`(B,3)`."""
+        return self._translation
+
+    @property
+    def rotation(self) -> So3:
+        """Return the underlying rotation(So3)."""
+        return self._rotation
+
+    @property
+    def translation(self) -> Vector3 | Tensor:
+        """Return the underlying translation vector of shape :math:`(B,3)`."""
+        return self._translation
 
     @staticmethod
-    def exp(v) -> 'So3':
+    def exp(v: Tensor) -> Se3:
         """Converts elements of lie algebra to elements of lie group.
 
-        See more: https://vision.in.tum.de/_media/members/demmeln/nurlanov2021so3log.pdf
-
         Args:
-            v: vector of shape :math:`(B,3)`.
+            v: vector of shape :math:`(B, 6)`.
 
         Example:
-            >>> v = torch.zeros((2, 3))
-            >>> s = So3.identity().exp(v)
-            >>> s
+            >>> v = torch.zeros((1, 6))
+            >>> s = Se3.exp(v)
+            >>> s.r
             Parameter containing:
-            tensor([[1., 0., 0., 0.],
-                    [1., 0., 0., 0.]], requires_grad=True)
+            tensor([[1., 0., 0., 0.]], requires_grad=True)
+            >>> s.t
+            Parameter containing:
+            tensor([[0., 0., 0.]], requires_grad=True)
         """
-        # KORNIA_CHECK_SHAPE(v, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
-        theta = batched_dot_product(v, v).sqrt()[..., None]
-        theta_nonzeros = theta != 0.0
-        theta_half = 0.5 * theta
-        # TODO: uncomment me after deprecate pytorch 10.2
-        # w = where(theta_nonzeros, theta_half.cos(), 1.0)
-        # b = where(theta_nonzeros, theta_half.sin() / theta, 0.0)
-        w = where(theta_nonzeros, theta_half.cos(), tensor(1.0, device=v.device, dtype=v.dtype))
-        b = where(theta_nonzeros, theta_half.sin() / theta, tensor(0.0, device=v.device, dtype=v.dtype))
-        xyz = b * v
-        return So3(Quaternion(concatenate((w, xyz), -1)))
+        # KORNIA_CHECK_SHAPE(v, ["B", "6"])  # FIXME: resolve shape bugs. @edgarriba
+        upsilon = v[..., :3]
+        omega = v[..., 3:]
+        omega_hat = So3.hat(omega)
+        omega_hat_sq = omega_hat @ omega_hat
+        theta = batched_dot_product(omega, omega).sqrt()
+        R = So3.exp(omega)
+        V = (
+            eye(3, device=v.device, dtype=v.dtype)
+            + ((1 - theta.cos()) / (theta**2))[..., None, None] * omega_hat
+            + ((theta - theta.sin()) / (theta**3))[..., None, None] * omega_hat_sq
+        )
+        U = where(theta[..., None] != 0.0, (upsilon[..., None, :] * V).sum(-1), upsilon)
+        return Se3(R, U)
 
     def log(self) -> Tensor:
         """Converts elements of lie group  to elements of lie algebra.
 
         Example:
-            >>> data = torch.ones((2, 4))
-            >>> q = Quaternion(data)
-            >>> So3(q).log()
-            tensor([[0., 0., 0.],
-                    [0., 0., 0.]], grad_fn=<WhereBackward0>)
-        """
-        theta = batched_dot_product(self.q.vec, self.q.vec).sqrt()
-        # NOTE: this differs from https://github.com/strasdat/Sophus/blob/master/sympy/sophus/so3.py#L33
-        omega = where(
-            theta[..., None] != 0,
-            2 * self.q.real[..., None].acos() * self.q.vec / theta[..., None],
-            2 * self.q.vec / self.q.real[..., None],
+            >>> from kornia.geometry.quaternion import Quaternion
+            >>> q = Quaternion.identity()
+            >>> Se3(q, torch.zeros(3)).log()
+            tensor([0., 0., 0., 0., 0., 0.], grad_fn=<CatBackward0>)
+        """
+        omega = self.r.log()
+        theta = batched_dot_product(omega, omega).sqrt()
+        t = self.t.data
+        omega_hat = So3.hat(omega)
+        omega_hat_sq = omega_hat @ omega_hat
+        V_inv = (
+            eye(3, device=omega.device, dtype=omega.dtype)
+            - 0.5 * omega_hat
+            + ((1 - theta * (theta / 2).cos() / (2 * (theta / 2).sin())) / theta.pow(2))[..., None, None] * omega_hat_sq
         )
-        return omega
+        t = where(theta[..., None] != 0.0, (t[..., None, :] * V_inv).sum(-1), t)
+        return concatenate((t, omega), -1)
 
     @staticmethod
-    def hat(v) -> Tensor:
-        """Converts elements from vector space to lie algebra. Returns matrix of shape :math:`(B,3,3)`.
+    def hat(v: Tensor) -> Tensor:
+        """Converts elements from vector space to lie algebra.
 
         Args:
-            v: vector of shape :math:`(B,3)`.
+            v: vector of shape :math:`(B, 6)`.
+
+        Returns:
+            matrix of shape :math:`(B, 4, 4)`.
 
         Example:
-            >>> v = torch.ones((1,3))
-            >>> m = So3.hat(v)
+            >>> v = torch.ones((1, 6))
+            >>> m = Se3.hat(v)
             >>> m
-            tensor([[[ 0., -1.,  1.],
-                     [ 1.,  0., -1.],
-                     [-1.,  1.,  0.]]])
-        """
-        # KORNIA_CHECK_SHAPE(v, ["B", "3"])  # FIXME: resolve shape bugs. @edgarriba
-        a, b, c = v[..., 0], v[..., 1], v[..., 2]
-        z = zeros_like(a)
-        row0 = stack((z, -c, b), -1)
-        row1 = stack((c, z, -a), -1)
-        row2 = stack((-b, a, z), -1)
-        return stack((row0, row1, row2), -2)
+            tensor([[[ 0., -1.,  1.,  1.],
+                     [ 1.,  0., -1.,  1.],
+                     [-1.,  1.,  0.,  1.],
+                     [ 0.,  0.,  0.,  0.]]])
+        """
+        # KORNIA_CHECK_SHAPE(v, ["B", "6"])  # FIXME: resolve shape bugs. @edgarriba
+        upsilon, omega = v[..., :3], v[..., 3:]
+        rt = concatenate((So3.hat(omega), upsilon[..., None]), -1)
+        return pad(rt, (0, 0, 0, 1))  # add zeros bottom
 
     @staticmethod
-    def vee(omega) -> Tensor:
-        r"""Converts elements from lie algebra to vector space. Returns vector of shape :math:`(B,3)`.
-
-        .. math::
-            omega = \begin{bmatrix} 0 & -c & b \\
-            c & 0 & -a \\
-            -b & a & 0\end{bmatrix}
+    def vee(omega: Tensor) -> Tensor:
+        """Converts elements from lie algebra to vector space.
 
         Args:
-            omega: 3x3-matrix representing lie algebra.
+            omega: 4x4-matrix representing lie algebra of shape :math:`(B,4,4)`.
+
+        Returns:
+            vector of shape :math:`(B,6)`.
 
         Example:
-            >>> v = torch.ones((1,3))
-            >>> omega = So3.hat(v)
-            >>> So3.vee(omega)
-            tensor([[1., 1., 1.]])
+            >>> v = torch.ones((1, 6))
+            >>> omega_hat = Se3.hat(v)
+            >>> Se3.vee(omega_hat)
+            tensor([[1., 1., 1., 1., 1., 1.]])
         """
-        # KORNIA_CHECK_SHAPE(omega, ["B", "3", "3"])  # FIXME: resolve shape bugs. @edgarriba
-        a, b, c = omega[..., 2, 1], omega[..., 0, 2], omega[..., 1, 0]
-        return stack((a, b, c), -1)
-
-    def matrix(self) -> Tensor:
-        r"""Convert the quaternion to a rotation matrix of shape :math:`(B,3,3)`.
+        # KORNIA_CHECK_SHAPE(omega, ["B", "4", "4"])  # FIXME: resolve shape bugs. @edgarriba
+        head = omega[..., :3, -1]
+        tail = So3.vee(omega[..., :3, :3])
+        return concatenate((head, tail), -1)
 
-        The matrix is of the form:
+    @classmethod
+    def identity(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> Se3:
+        """Create a Se3 group representing an identity rotation and zero translation.
 
-        .. math::
-            \begin{bmatrix} 1-2y^2-2z^2 & 2xy-2zw & 2xy+2yw \\
-            2xy+2zw & 1-2x^2-2z^2 & 2yz-2xw \\
-            2xz-2yw & 2yz+2xw & 1-2x^2-2y^2\end{bmatrix}
+        Args:
+            batch_size: the batch size of the underlying data.
 
         Example:
-            >>> s = So3.identity()
-            >>> m = s.matrix()
-            >>> m
-            tensor([[1., 0., 0.],
-                    [0., 1., 0.],
-                    [0., 0., 1.]], grad_fn=<StackBackward0>)
-        """
-        w = self.q.w[..., None]
-        x, y, z = self.q.x[..., None], self.q.y[..., None], self.q.z[..., None]
-        q0 = 1 - 2 * y**2 - 2 * z**2
-        q1 = 2 * x * y - 2 * z * w
-        q2 = 2 * x * z + 2 * y * w
-        row0 = concatenate((q0, q1, q2), -1)
-        q0 = 2 * x * y + 2 * z * w
-        q1 = 1 - 2 * x**2 - 2 * z**2
-        q2 = 2 * y * z - 2 * x * w
-        row1 = concatenate((q0, q1, q2), -1)
-        q0 = 2 * x * z - 2 * y * w
-        q1 = 2 * y * z + 2 * x * w
-        q2 = 1 - 2 * x**2 - 2 * y**2
-        row2 = concatenate((q0, q1, q2), -1)
-        return stack((row0, row1, row2), -2)
-
-    @classmethod
-    def from_matrix(cls, matrix: Tensor) -> 'So3':
-        """Create So3 from a rotation matrix.
-
-        Args:
-            matrix: the rotation matrix to convert of shape :math:`(B,3,3)`.
-
-        Example:
-            >>> m = torch.eye(3)
-            >>> s = So3.from_matrix(m)
-            >>> s
+            >>> s = Se3.identity()
+            >>> s.r
             Parameter containing:
             tensor([1., 0., 0., 0.], requires_grad=True)
-        """
-        return cls(Quaternion.from_matrix(matrix))
+            >>> s.t
+            x: 0.0
+            y: 0.0
+            z: 0.0
+        """
+        t = tensor([0.0, 0.0, 0.0], device=device, dtype=dtype)
+        if batch_size is not None:
+            t = t.repeat(batch_size, 1)
+
+        return cls(So3.identity(batch_size, device, dtype), Vector3(t))
+
+    def matrix(self) -> Tensor:
+        """Returns the matrix representation of shape :math:`(B, 4, 4)`.
+
+        Example:
+            >>> s = Se3(So3.identity(), torch.ones(3))
+            >>> s.matrix()
+            tensor([[1., 0., 0., 1.],
+                    [0., 1., 0., 1.],
+                    [0., 0., 1., 1.],
+                    [0., 0., 0., 1.]], grad_fn=<CopySlices>)
+        """
+        rt = concatenate((self.r.matrix(), self.t.data[..., None]), -1)
+        rt_4x4 = pad(rt, (0, 0, 0, 1))  # add last row zeros
+        rt_4x4[..., -1, -1] = 1.0
+        return rt_4x4
 
     @classmethod
-    def identity(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'So3':
-        """Create a So3 group representing an identity rotation.
+    def from_matrix(cls, matrix: Tensor) -> Se3:
+        """Create a Se3 group from a matrix.
 
         Args:
-            batch_size: the batch size of the underlying data.
+            matrix: tensor of shape :math:`(B, 4, 4)`.
 
         Example:
-            >>> s = So3.identity()
-            >>> s
+            >>> s = Se3.from_matrix(torch.eye(4))
+            >>> s.r
             Parameter containing:
             tensor([1., 0., 0., 0.], requires_grad=True)
-
-            >>> s = So3.identity(batch_size=2)
-            >>> s
+            >>> s.t
             Parameter containing:
-            tensor([[1., 0., 0., 0.],
-                    [1., 0., 0., 0.]], requires_grad=True)
+            tensor([0., 0., 0.], requires_grad=True)
         """
-        return cls(Quaternion.identity(batch_size, device, dtype))
+        # KORNIA_CHECK_SHAPE(matrix, ["B", "4", "4"])  # FIXME: resolve shape bugs. @edgarriba
+        r = So3.from_matrix(matrix[..., :3, :3])
+        t = matrix[..., :3, -1]
+        return cls(r, t)
 
-    def inverse(self) -> 'So3':
+    @classmethod
+    def from_qxyz(cls, qxyz: Tensor) -> Se3:
+        """Create a Se3 group a quaternion and translation vector.
+
+        Args:
+            qxyz: tensor of shape :math:`(B, 7)`.
+
+        Example:
+            >>> qxyz = torch.tensor([1., 2., 3., 0., 0., 0., 1.])
+            >>> s = Se3.from_qxyz(qxyz)
+            >>> s.r
+            Parameter containing:
+            tensor([1., 2., 3., 0.], requires_grad=True)
+            >>> s.t
+            x: 0.0
+            y: 0.0
+            z: 1.0
+        """
+        # KORNIA_CHECK_SHAPE(qxyz, ["B", "7"])  # FIXME: resolve shape bugs. @edgarriba
+        q, xyz = qxyz[..., :4], qxyz[..., 4:]
+        return cls(So3.from_wxyz(q), Vector3(xyz))
+
+    def inverse(self) -> Se3:
         """Returns the inverse transformation.
 
         Example:
-            >>> s = So3.identity()
-            >>> s.inverse()
+            >>> s = Se3(So3.identity(), torch.ones(3))
+            >>> s_inv = s.inverse()
+            >>> s_inv.r
             Parameter containing:
             tensor([1., -0., -0., -0.], requires_grad=True)
+            >>> s_inv.t
+            Parameter containing:
+            tensor([-1., -1., -1.], requires_grad=True)
         """
-        return So3(self.q.conj())
+        r_inv = self.r.inverse()
+        _t = -1 * self.t
+        if isinstance(_t, int):
+            raise TypeError('Unexpected integer from `-1 * translation`')
+
+        return Se3(r_inv, r_inv * _t)
 
     @classmethod
-    def random(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'So3':
-        """Create a So3 group representing a random rotation.
+    def random(cls, batch_size: int | None = None, device: Device | None = None, dtype: Dtype = None) -> Se3:
+        """Create a Se3 group representing a random transformation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
-            >>> s = So3.random()
-            >>> s = So3.random(batch_size=3)
+            >>> s = Se3.random()
+            >>> s = Se3.random(batch_size=3)
         """
-        return cls(Quaternion.random(batch_size, device, dtype))
+        shape: tuple[int, ...]
+        if batch_size is None:
+            shape = ()
+        else:
+            KORNIA_CHECK(batch_size >= 1, msg="batch_size must be positive")
+            shape = (batch_size,)
+        r = So3.random(batch_size, device, dtype)
+        t = Vector3.random(shape, device, dtype)
+        return cls(r, t)
 
     @classmethod
-    def rot_x(cls, x: Tensor) -> "So3":
+    def rot_x(cls, x: Tensor) -> Se3:
         """Construct a x-axis rotation.
 
         Args:
             x: the x-axis rotation angle.
         """
         zs = zeros_like(x)
-        return cls.exp(stack((x, zs, zs), -1))
+        return cls(So3.rot_x(x), stack((zs, zs, zs), -1))
 
     @classmethod
-    def rot_y(cls, y: Tensor) -> "So3":
-        """Construct a z-axis rotation.
+    def rot_y(cls, y: Tensor) -> Se3:
+        """Construct a y-axis rotation.
 
         Args:
             y: the y-axis rotation angle.
         """
         zs = zeros_like(y)
-        return cls.exp(stack((zs, y, zs), -1))
+        return cls(So3.rot_y(y), stack((zs, zs, zs), -1))
 
     @classmethod
-    def rot_z(cls, z: Tensor) -> "So3":
+    def rot_z(cls, z: Tensor) -> Se3:
         """Construct a z-axis rotation.
 
         Args:
             z: the z-axis rotation angle.
         """
         zs = zeros_like(z)
-        return cls.exp(stack((zs, zs, z), -1))
+        return cls(So3.rot_z(z), stack((zs, zs, zs), -1))
+
+    @classmethod
+    def trans(cls, x: Tensor, y: Tensor, z: Tensor) -> Se3:
+        """Construct a translation only Se3 instance.
+
+        Args:
+            x: the x-axis translation.
+            y: the y-axis translation.
+            z: the z-axis translation.
+        """
+        KORNIA_CHECK(x.shape == y.shape)
+        KORNIA_CHECK(y.shape == z.shape)
+        KORNIA_CHECK_SAME_DEVICES([x, y, z])
+        batch_size = x.shape[0] if len(x.shape) > 0 else None
+        rotation = So3.identity(batch_size, x.device, x.dtype)
+        return cls(rotation, stack((x, y, z), -1))
+
+    @classmethod
+    def trans_x(cls, x: Tensor) -> Se3:
+        """Construct a x-axis translation.
+
+        Args:
+            x: the x-axis translation.
+        """
+        zs = zeros_like(x)
+        return cls.trans(x, zs, zs)
+
+    @classmethod
+    def trans_y(cls, y: Tensor) -> Se3:
+        """Construct a y-axis translation.
+
+        Args:
+            y: the y-axis translation.
+        """
+        zs = zeros_like(y)
+        return cls.trans(zs, y, zs)
+
+    @classmethod
+    def trans_z(cls, z: Tensor) -> Se3:
+        """Construct a z-axis translation.
+
+        Args:
+            z: the z-axis translation.
+        """
+        zs = zeros_like(z)
+        return cls.trans(zs, zs, z)
 
     def adjoint(self) -> Tensor:
-        """Returns the adjoint matrix of shape :math:`(B, 3, 3)`.
+        """Returns the adjoint matrix of shape :math:`(B, 6, 6)`.
 
         Example:
-            >>> s = So3.identity()
+            >>> s = Se3.identity()
             >>> s.adjoint()
-            tensor([[1., 0., 0.],
-                    [0., 1., 0.],
-                    [0., 0., 1.]], grad_fn=<StackBackward0>)
-        """
-        return self.matrix()
+            tensor([[1., 0., 0., 0., 0., 0.],
+                    [0., 1., 0., 0., 0., 0.],
+                    [0., 0., 1., 0., 0., 0.],
+                    [0., 0., 0., 1., 0., 0.],
+                    [0., 0., 0., 0., 1., 0.],
+                    [0., 0., 0., 0., 0., 1.]], grad_fn=<CatBackward0>)
+        """
+        R = self.so3.matrix()
+        z = zeros_like(R)
+        row0 = concatenate((R, So3.hat(self.t) @ R), -1)
+        row1 = concatenate((z, R), -1)
+        return concatenate((row0, row1), -2)
```

### Comparing `kornia-0.6.9/kornia/geometry/linalg.py` & `kornia-0.7.0/kornia/geometry/linalg.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,29 @@
+from __future__ import annotations
+
 import torch
-from torch import Tensor
 
+from kornia.core import Tensor
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.geometry.conversions import convert_points_from_homogeneous, convert_points_to_homogeneous
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE, check_is_tensor
 
 __all__ = [
     "compose_transformations",
     "relative_transformation",
     "inverse_transformation",
     "transform_points",
     "point_line_distance",
     "squared_norm",
+    "batched_squared_norm",
     "batched_dot_product",
     "euclidean_distance",
 ]
 
 
-def compose_transformations(trans_01: torch.Tensor, trans_12: torch.Tensor) -> torch.Tensor:
+def compose_transformations(trans_01: Tensor, trans_12: Tensor) -> Tensor:
     r"""Function that composes two homogeneous transformations.
 
     .. math::
         T_0^{2} = \begin{bmatrix} R_0^1 R_1^{2} & R_0^{1} t_1^{2} + t_0^{1} \\
         \mathbf{0} & 1\end{bmatrix}
 
     Args:
@@ -35,50 +38,48 @@
         the transformation between the two frames with shape :math:`(N, 4, 4)` or :math:`(4, 4)`.
 
     Example::
         >>> trans_01 = torch.eye(4)  # 4x4
         >>> trans_12 = torch.eye(4)  # 4x4
         >>> trans_02 = compose_transformations(trans_01, trans_12)  # 4x4
     """
-    if not torch.is_tensor(trans_01):
-        raise TypeError(f"Input trans_01 type is not a torch.Tensor. Got {type(trans_01)}")
-
-    if not torch.is_tensor(trans_12):
-        raise TypeError(f"Input trans_12 type is not a torch.Tensor. Got {type(trans_12)}")
+    KORNIA_CHECK_IS_TENSOR(trans_01)
+    KORNIA_CHECK_IS_TENSOR(trans_12)
 
     if not ((trans_01.dim() in (2, 3)) and (trans_01.shape[-2:] == (4, 4))):
         raise ValueError("Input trans_01 must be a of the shape Nx4x4 or 4x4." " Got {}".format(trans_01.shape))
 
     if not ((trans_12.dim() in (2, 3)) and (trans_12.shape[-2:] == (4, 4))):
         raise ValueError("Input trans_12 must be a of the shape Nx4x4 or 4x4." " Got {}".format(trans_12.shape))
 
     if not trans_01.dim() == trans_12.dim():
         raise ValueError(f"Input number of dims must match. Got {trans_01.dim()} and {trans_12.dim()}")
 
     # unpack input data
-    rmat_01: torch.Tensor = trans_01[..., :3, :3]  # Nx3x3
-    rmat_12: torch.Tensor = trans_12[..., :3, :3]  # Nx3x3
-    tvec_01: torch.Tensor = trans_01[..., :3, -1:]  # Nx3x1
-    tvec_12: torch.Tensor = trans_12[..., :3, -1:]  # Nx3x1
+    rmat_01: Tensor = trans_01[..., :3, :3]  # Nx3x3
+    rmat_12: Tensor = trans_12[..., :3, :3]  # Nx3x3
+    tvec_01: Tensor = trans_01[..., :3, -1:]  # Nx3x1
+    tvec_12: Tensor = trans_12[..., :3, -1:]  # Nx3x1
 
     # compute the actual transforms composition
-    rmat_02: torch.Tensor = torch.matmul(rmat_01, rmat_12)
-    tvec_02: torch.Tensor = torch.matmul(rmat_01, tvec_12) + tvec_01
+    rmat_02: Tensor = torch.matmul(rmat_01, rmat_12)
+    tvec_02: Tensor = torch.matmul(rmat_01, tvec_12) + tvec_01
 
     # pack output tensor
-    trans_02: torch.Tensor = torch.zeros_like(trans_01)
+    trans_02: Tensor = torch.zeros_like(trans_01)
     trans_02[..., :3, 0:3] += rmat_02
     trans_02[..., :3, -1:] += tvec_02
     trans_02[..., -1, -1:] += 1.0
     return trans_02
 
 
-def inverse_transformation(trans_12):
-    r"""Function that inverts a 4x4 homogeneous transformation
-    :math:`T_1^{2} = \begin{bmatrix} R_1 & t_1 \\ \mathbf{0} & 1 \end{bmatrix}`
+def inverse_transformation(trans_12: Tensor) -> Tensor:
+    r"""Function that inverts a 4x4 homogeneous transformation.
+
+     :math:`T_1^{2} = \begin{bmatrix} R_1 & t_1 \\ \mathbf{0} & 1 \end{bmatrix}`
 
     The inverse transformation is computed as follows:
 
     .. math::
 
         T_2^{1} = (T_1^{2})^{-1} = \begin{bmatrix} R_1^T & -R_1^T t_1 \\
         \mathbf{0} & 1\end{bmatrix}
@@ -89,38 +90,38 @@
     Returns:
         tensor with inverted transformations with shape :math:`(N, 4, 4)` or :math:`(4, 4)`.
 
     Example:
         >>> trans_12 = torch.rand(1, 4, 4)  # Nx4x4
         >>> trans_21 = inverse_transformation(trans_12)  # Nx4x4
     """
-    if not torch.is_tensor(trans_12):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(trans_12)}")
+    KORNIA_CHECK_IS_TENSOR(trans_12)
+
     if not ((trans_12.dim() in (2, 3)) and (trans_12.shape[-2:] == (4, 4))):
         raise ValueError(f"Input size must be a Nx4x4 or 4x4. Got {trans_12.shape}")
     # unpack input tensor
-    rmat_12: torch.Tensor = trans_12[..., :3, 0:3]  # Nx3x3
-    tvec_12: torch.Tensor = trans_12[..., :3, 3:4]  # Nx3x1
+    rmat_12 = trans_12[..., :3, 0:3]  # Nx3x3
+    tvec_12 = trans_12[..., :3, 3:4]  # Nx3x1
 
     # compute the actual inverse
-    rmat_21: torch.Tensor = torch.transpose(rmat_12, -1, -2)
-    tvec_21: torch.Tensor = torch.matmul(-rmat_21, tvec_12)
+    rmat_21 = torch.transpose(rmat_12, -1, -2)
+    tvec_21 = torch.matmul(-rmat_21, tvec_12)
 
     # pack to output tensor
-    trans_21: torch.Tensor = torch.zeros_like(trans_12)
+    trans_21 = torch.zeros_like(trans_12)
     trans_21[..., :3, 0:3] += rmat_21
     trans_21[..., :3, -1:] += tvec_21
     trans_21[..., -1, -1:] += 1.0
     return trans_21
 
 
-def relative_transformation(trans_01: torch.Tensor, trans_02: torch.Tensor) -> torch.Tensor:
-    r"""Function that computes the relative homogeneous transformation from a
-    reference transformation :math:`T_1^{0} = \begin{bmatrix} R_1 & t_1 \\
-    \mathbf{0} & 1 \end{bmatrix}` to destination :math:`T_2^{0} =
+def relative_transformation(trans_01: Tensor, trans_02: Tensor) -> Tensor:
+    r"""Function that computes the relative homogeneous transformation from a reference transformation.
+
+    :math:`T_1^{0} = \begin{bmatrix} R_1 & t_1 \\ \mathbf{0} & 1 \end{bmatrix}` to destination :math:`T_2^{0} =
     \begin{bmatrix} R_2 & t_2 \\ \mathbf{0} & 1 \end{bmatrix}`.
 
     The relative transformation is computed as follows:
 
     .. math::
 
         T_1^{2} = (T_0^{1})^{-1} \cdot T_0^{2}
@@ -133,50 +134,49 @@
         the relative transformation between the transformations with shape :math:`(N, 4, 4)` or :math:`(4, 4)`.
 
     Example::
         >>> trans_01 = torch.eye(4)  # 4x4
         >>> trans_02 = torch.eye(4)  # 4x4
         >>> trans_12 = relative_transformation(trans_01, trans_02)  # 4x4
     """
-    if not torch.is_tensor(trans_01):
-        raise TypeError(f"Input trans_01 type is not a torch.Tensor. Got {type(trans_01)}")
-    if not torch.is_tensor(trans_02):
-        raise TypeError(f"Input trans_02 type is not a torch.Tensor. Got {type(trans_02)}")
+    KORNIA_CHECK_IS_TENSOR(trans_01)
+    KORNIA_CHECK_IS_TENSOR(trans_02)
     if not ((trans_01.dim() in (2, 3)) and (trans_01.shape[-2:] == (4, 4))):
         raise ValueError("Input must be a of the shape Nx4x4 or 4x4." " Got {}".format(trans_01.shape))
     if not ((trans_02.dim() in (2, 3)) and (trans_02.shape[-2:] == (4, 4))):
         raise ValueError("Input must be a of the shape Nx4x4 or 4x4." " Got {}".format(trans_02.shape))
     if not trans_01.dim() == trans_02.dim():
         raise ValueError(f"Input number of dims must match. Got {trans_01.dim()} and {trans_02.dim()}")
-    trans_10: torch.Tensor = inverse_transformation(trans_01)
-    trans_12: torch.Tensor = compose_transformations(trans_10, trans_02)
+
+    trans_10 = inverse_transformation(trans_01)
+    trans_12 = compose_transformations(trans_10, trans_02)
     return trans_12
 
 
-def transform_points(trans_01: torch.Tensor, points_1: torch.Tensor) -> torch.Tensor:
+def transform_points(trans_01: Tensor, points_1: Tensor) -> Tensor:
     r"""Function that applies transformations to a set of points.
 
     Args:
-        trans_01 (torch.Tensor): tensor for transformations of shape
+        trans_01: tensor for transformations of shape
           :math:`(B, D+1, D+1)`.
-        points_1 (torch.Tensor): tensor of points of shape :math:`(B, N, D)`.
+        points_1: tensor of points of shape :math:`(B, N, D)`.
     Returns:
-        torch.Tensor: tensor of N-dimensional points.
+        a tensor of N-dimensional points.
 
     Shape:
         - Output: :math:`(B, N, D)`
 
     Examples:
 
         >>> points_1 = torch.rand(2, 4, 3)  # BxNx3
         >>> trans_01 = torch.eye(4).view(1, 4, 4)  # Bx4x4
         >>> points_0 = transform_points(trans_01, points_1)  # BxNx3
     """
-    check_is_tensor(trans_01)
-    check_is_tensor(points_1)
+    KORNIA_CHECK_IS_TENSOR(trans_01)
+    KORNIA_CHECK_IS_TENSOR(points_1)
     if not trans_01.shape[0] == points_1.shape[0] and trans_01.shape[0] != 1:
         raise ValueError(
             "Input batch size must be the same for both tensors or 1." f"Got {trans_01.shape} and {points_1.shape}"
         )
     if not trans_01.shape[-1] == (points_1.shape[-1] + 1):
         raise ValueError("Last input dimensions must differ by one unit" f"Got{trans_01} and {points_1}")
 
@@ -210,15 +210,15 @@
 
     Returns:
         the computed distance with shape :math:`(*, N)`.
     """
     KORNIA_CHECK_IS_TENSOR(point)
     KORNIA_CHECK_IS_TENSOR(line)
 
-    if not point.shape[-1] in (2, 3):
+    if point.shape[-1] not in (2, 3):
         raise ValueError(f"pts must be a (*, 2 or 3) tensor. Got {point.shape}")
 
     if not line.shape[-1] == 3:
         raise ValueError(f"lines must be a (*, 3) tensor. Got {line.shape}")
 
     numerator = (line[..., 0] * point[..., 0] + line[..., 1] * point[..., 1] + line[..., 2]).abs()
     denominator = line[..., :2].norm(dim=-1)
@@ -238,14 +238,15 @@
     return batched_dot_product(x, x, keepdim)
 
 
 def euclidean_distance(x: Tensor, y: Tensor, keepdim: bool = False, eps: float = 1e-6) -> Tensor:
     """Compute the Euclidean distance between two set of n-dimensional points.
 
     More: https://en.wikipedia.org/wiki/Euclidean_distance
+
     Args:
         x: first set of points of shape :math:`(*, N)`.
         y: second set of points of shape :math:`(*, N)`.
         keepdim: whether to keep the dimension after reduction.
         eps: small value to have numerical stability.
     """
     KORNIA_CHECK_SHAPE(x, ["*", "N"])
```

### Comparing `kornia-0.6.9/kornia/geometry/line.py` & `kornia-0.7.0/kornia/geometry/line.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # kornia.geometry.line module inspired by Eigen::geometry::ParametrizedLine
 # https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/ParametrizedLine.h
-from typing import Optional, Tuple, Union
+from typing import Iterator, Optional, Tuple, Union
 
 import torch
 
 from kornia.core import Module, Parameter, Tensor, normalize, where
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.geometry.linalg import batched_dot_product, squared_norm
 from kornia.geometry.plane import Hyperplane
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.utils.helpers import _torch_svd_cast
 
 __all__ = ["ParametrizedLine", "fit_line"]
 
 
 class ParametrizedLine(Module):
     """Class that describes a parametrize line.
@@ -42,18 +42,18 @@
 
     def __str__(self) -> str:
         return f"Origin: {self.origin}\nDirection: {self.direction}"
 
     def __repr__(self) -> str:
         return str(self)
 
-    def __getitem__(self, idx) -> Tensor:
+    def __getitem__(self, idx: int) -> Tensor:
         return self.origin if idx == 0 else self.direction
 
-    def __iter__(self):
+    def __iter__(self) -> Iterator[Tensor]:
         yield from (self.origin, self.direction)
 
     @property
     def origin(self) -> Tensor:
         """Return the line origin point."""
         return self._origin
 
@@ -63,15 +63,15 @@
         return self._direction
 
     def dim(self) -> int:
         """Return the dimension in which the line holds."""
         return self.direction.shape[-1]
 
     @classmethod
-    def through(cls, p0, p1) -> "ParametrizedLine":
+    def through(cls, p0: Tensor, p1: Tensor) -> "ParametrizedLine":
         """Constructs a parametrized line going from a point :math:`p0` to :math:`p1`.
 
         Args:
             p0: tensor with first point :math:`(B, D)` where `D` is the point dimension.
             p1: tensor with second point :math:`(B, D)` where `D` is the point dimension.
 
         Example:
```

### Comparing `kornia-0.6.9/kornia/geometry/plane.py` & `kornia-0.7.0/kornia/geometry/plane.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # kornia.geometry.plane module inspired by Eigen::geometry::Hyperplane
 # https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/Hyperplane.h
 
 from typing import Optional
 
 from kornia.core import Module, Tensor, stack, where
-from kornia.core.tensor_wrapper import unwrap, wrap
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SHAPE, KORNIA_CHECK_TYPE
+from kornia.core.tensor_wrapper import unwrap, wrap  # type: ignore[attr-defined]
 from kornia.geometry.linalg import batched_dot_product
 from kornia.geometry.vector import Scalar, Vector3
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE, KORNIA_CHECK_TYPE
 from kornia.utils.helpers import _torch_svd_cast
 
 __all__ = ["Hyperplane", "fit_plane"]
 
 
 def normalized(v: Tensor, eps: float = 1e-6) -> Tensor:
     return v / batched_dot_product(v, v).add(eps).sqrt()
@@ -85,22 +85,22 @@
         KORNIA_CHECK(p0.shape == p1.shape)
         KORNIA_CHECK(p1.shape == p2.shape)
         v0, v1 = (p2 - p0), (p1 - p0)
         normal = v0.cross(v1)
         norm = normal.norm(-1)
 
         # https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/Hyperplane.h#L108
-        def compute_normal_svd(v0, v1):
+        def compute_normal_svd(v0: Tensor, v1: Tensor) -> 'Vector3':
             # NOTE: for reason TensorWrapper does not stack well
             m = stack((unwrap(v0), unwrap(v1)), -2)  # Bx2x3
             _, _, V = _torch_svd_cast(m)  # kornia solution lies in the last row
             return wrap(V[..., :, -1], Vector3)  # Bx3
 
         normal_mask = norm <= v0.norm(-1) * v1.norm(-1) * 1e-6
-        normal = where(normal_mask, compute_normal_svd(v0, v1), normal / (norm + 1e-6))
+        normal = where(normal_mask, compute_normal_svd(v0, v1).data, normal / (norm + 1e-6))
         offset = -batched_dot_product(p0, normal)
 
         return Hyperplane(wrap(normal, Vector3), wrap(offset, Scalar))
 
 
 # TODO: factor to avoid duplicated from line.py
 # https://github.com/strasdat/Sophus/blob/23.04-beta/cpp/sophus/geometry/fit_plane.h
```

### Comparing `kornia-0.6.9/kornia/geometry/quaternion.py` & `kornia-0.7.0/kornia/geometry/quaternion.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 # kornia.geometry.quaternion module inspired by Eigen, Sophus-sympy, and PyQuaternion.
 # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/quaternion.py
 # https://github.com/KieranWynn/pyquaternion/blob/master/pyquaternion/quaternion.py
 # https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/Quaternion.h
 from math import pi
 from typing import Optional, Tuple, Union
 
-from kornia.core import Module, Parameter, Tensor, concatenate, rand, stack, tensor, where
+from kornia.core import Device, Dtype, Module, Parameter, Tensor, concatenate, rand, stack, tensor, where
+from kornia.core.check import KORNIA_CHECK_TYPE
 from kornia.geometry.conversions import (
-    QuaternionCoeffOrder,
-    angle_axis_to_quaternion,
+    axis_angle_to_quaternion,
     normalize_quaternion,
     quaternion_to_rotation_matrix,
     rotation_matrix_to_quaternion,
 )
 from kornia.geometry.linalg import batched_dot_product
-from kornia.testing import KORNIA_CHECK_TYPE
 
 
 class Quaternion(Module):
     r"""Base class to represent a Quaternion.
 
     A quaternion is a four dimensional vector representation of a rotation transformation in 3d.
     See more: https://en.wikipedia.org/wiki/Quaternion
 
     The general definition of a quaternion is given by:
 
-    .. math ::
+    .. math::
 
         Q = a + b \cdot \mathbf{i} + c \cdot \mathbf{j} + d \cdot \mathbf{k}
 
     Thus, we represent a rotation quaternion as a contiguous tensor structure to
     perform rigid bodies transformations:
 
-    .. math ::
+    .. math::
 
         Q = \begin{bmatrix} q_w & q_x & q_y & q_z \end{bmatrix}
 
     Example:
         >>> q = Quaternion.identity(batch_size=4)
         >>> q.data
         Parameter containing:
@@ -68,15 +67,15 @@
         super().__init__()
         # KORNIA_CHECK_SHAPE(data, ["B", "4"])  # FIXME: resolve shape bugs. @edgarriba
         self._data = Parameter(data)
 
     def __repr__(self) -> str:
         return f"{self.data}"
 
-    def __getitem__(self, idx) -> 'Quaternion':
+    def __getitem__(self, idx: Union[int, slice]) -> 'Quaternion':
         return Quaternion(self.data[idx])
 
     def __neg__(self) -> 'Quaternion':
         """Inverts the sign of the quaternion data.
 
         Example:
             >>> q = Quaternion.identity()
@@ -163,15 +162,16 @@
         """Return a tuple with the underlying coefficients in WXYZ order."""
         return self.w, self.x, self.y, self.z
 
     @property
     def real(self) -> Tensor:
         """Return the real part with shape :math:`(B,)`.
 
-        Alias for :func:`~kornia.geometry.quaternion.Quaternion.w`
+        Alias for
+        :func: `~kornia.geometry.quaternion.Quaternion.w`
         """
         return self.w
 
     @property
     def vec(self) -> Tensor:
         """Return the vector with the imaginary part with shape :math:`(B, 3)`."""
         return self.data[..., 1:]
@@ -184,15 +184,16 @@
         """
         return self.data
 
     @property
     def scalar(self) -> Tensor:
         """Return a scalar with the real with shape :math:`(B,)`.
 
-        Alias for :func:`~kornia.geometry.quaternion.Quaternion.w`
+        Alias for
+        :func: `~kornia.geometry.quaternion.Quaternion.w`
         """
         return self.real
 
     @property
     def w(self) -> Tensor:
         """Return the :math:`q_w` with shape :math:`(B,)`."""
         return self.data[..., 0]
@@ -233,17 +234,17 @@
 
         Example:
             >>> q = Quaternion.identity()
             >>> m = q.matrix()
             >>> m
             tensor([[1., 0., 0.],
                     [0., 1., 0.],
-                    [0., 0., 1.]], grad_fn=<SqueezeBackward1>)
+                    [0., 0., 1.]], grad_fn=<ReshapeAliasBackward0>)
         """
-        return quaternion_to_rotation_matrix(self.data, order=QuaternionCoeffOrder.WXYZ)
+        return quaternion_to_rotation_matrix(self.data)
 
     @classmethod
     def from_matrix(cls, matrix: Tensor) -> 'Quaternion':
         """Create a quaternion from a rotation matrix.
 
         Args:
             matrix: the rotation matrix to convert of shape :math:`(B, 3, 3)`.
@@ -251,15 +252,15 @@
         Example:
             >>> m = torch.eye(3)[None]
             >>> q = Quaternion.from_matrix(m)
             >>> q.data
             Parameter containing:
             tensor([[1., 0., 0., 0.]], requires_grad=True)
         """
-        return cls(rotation_matrix_to_quaternion(matrix, order=QuaternionCoeffOrder.WXYZ))
+        return cls(rotation_matrix_to_quaternion(matrix))
 
     @classmethod
     def from_axis_angle(cls, axis_angle: Tensor) -> 'Quaternion':
         """Create a quaternion from axis-angle representation.
 
         Args:
             axis_angle: rotation vector of shape :math:`(B, 3)`.
@@ -267,30 +268,32 @@
         Example:
             >>> axis_angle = torch.tensor([[1., 0., 0.]])
             >>> q = Quaternion.from_axis_angle(axis_angle)
             >>> q.data
             Parameter containing:
             tensor([[0.8776, 0.4794, 0.0000, 0.0000]], requires_grad=True)
         """
-        return cls(angle_axis_to_quaternion(axis_angle, order=QuaternionCoeffOrder.WXYZ))
+        return cls(axis_angle_to_quaternion(axis_angle))
 
     @classmethod
-    def identity(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Quaternion':
+    def identity(
+        cls, batch_size: Optional[int] = None, device: Optional[Device] = None, dtype: Dtype = None
+    ) -> 'Quaternion':
         """Create a quaternion representing an identity rotation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
             >>> q = Quaternion.identity()
             >>> q.data
             Parameter containing:
             tensor([1., 0., 0., 0.], requires_grad=True)
         """
-        data: Tensor = tensor([1.0, 0.0, 0.0, 0.0], device=device, dtype=dtype)
+        data = tensor([1.0, 0.0, 0.0, 0.0], device=device, dtype=dtype)
         if batch_size is not None:
             data = data.repeat(batch_size, 1)
         return cls(data)
 
     @classmethod
     def from_coeffs(cls, w: float, x: float, y: float, z: float) -> 'Quaternion':
         """Create a quaternion from the data coefficients.
@@ -308,29 +311,31 @@
             tensor([1., 0., 0., 0.], requires_grad=True)
         """
         return cls(tensor([w, x, y, z]))
 
     # TODO: update signature
     # def random(cls, shape: Optional[List] = None, device = None, dtype = None) -> 'Quaternion':
     @classmethod
-    def random(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Quaternion':
+    def random(
+        cls, batch_size: Optional[int] = None, device: Optional[Device] = None, dtype: Dtype = None
+    ) -> 'Quaternion':
         """Create a random unit quaternion of shape :math:`(B, 4)`.
 
         Uniformly distributed across the rotation space as per: http://planning.cs.uiuc.edu/node198.html
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
             >>> q = Quaternion.random()
             >>> q = Quaternion.random(batch_size=2)
         """
         rand_shape = (batch_size,) if batch_size is not None else ()
 
-        r1, r2, r3 = rand((3,) + rand_shape, device=device, dtype=dtype)
+        r1, r2, r3 = rand((3, *rand_shape), device=device, dtype=dtype)
         q1 = (1.0 - r1).sqrt() * ((2 * pi * r2).sin())
         q2 = (1.0 - r1).sqrt() * ((2 * pi * r2).cos())
         q3 = r1.sqrt() * (2 * pi * r3).sin()
         q4 = r1.sqrt() * (2 * pi * r3).cos()
         return cls(stack((q1, q2, q3, q4), -1))
 
     def slerp(self, q1: 'Quaternion', t: float) -> 'Quaternion':
```

### Comparing `kornia-0.6.9/kornia/geometry/ransac.py` & `kornia-0.7.0/kornia/geometry/ransac.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 """Module containing RANSAC modules."""
 import math
 from typing import Callable, Optional, Tuple
 
 import torch
 
 from kornia.core import Device, Module, Tensor, zeros
+from kornia.core.check import KORNIA_CHECK_SHAPE
 from kornia.geometry import (
     find_fundamental,
     find_homography_dlt,
     find_homography_dlt_iterated,
     find_homography_lines_dlt,
     find_homography_lines_dlt_iterated,
     symmetrical_epipolar_distance,
 )
 from kornia.geometry.homography import (
     line_segment_transfer_error_one_way,
     oneway_transfer_error,
     sample_is_valid_for_homography,
 )
-from kornia.testing import KORNIA_CHECK_SHAPE
 
 __all__ = ["RANSAC"]
 
 
 class RANSAC(Module):
     """Module for robust geometry estimation with RANSAC. https://en.wikipedia.org/wiki/Random_sample_consensus.
 
@@ -31,26 +31,25 @@
         inliers_threshold: threshold for the correspondence to be an inlier.
         batch_size: number of generated samples at once.
         max_iterations: maximum batches to generate. Actual number of models to try is ``batch_size * max_iterations``.
         confidence: desired confidence of the result, used for the early stopping.
         max_local_iterations: number of local optimization (polishing) iterations.
     """
 
-    supported_models = ['homography', 'fundamental', 'homography_from_linesegments']
-
     def __init__(
         self,
         model_type: str = 'homography',
         inl_th: float = 2.0,
         batch_size: int = 2048,
         max_iter: int = 10,
         confidence: float = 0.99,
         max_lo_iters: int = 5,
-    ):
+    ) -> None:
         super().__init__()
+        self.supported_models = ['homography', 'fundamental', 'homography_from_linesegments']
         self.inl_th = inl_th
         self.max_iter = max_iter
         self.batch_size = batch_size
         self.model_type = model_type
         self.confidence = confidence
         self.max_lo_iters = max_lo_iters
         self.model_type = model_type
```

### Comparing `kornia-0.6.9/kornia/geometry/subpix/__init__.py` & `kornia-0.7.0/kornia/geometry/subpix/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+from __future__ import annotations
+
 from .dsnt import render_gaussian2d, spatial_expectation2d, spatial_softmax2d
 from .nms import NonMaximaSuppression2d, NonMaximaSuppression3d, nms2d, nms3d
 from .spatial_soft_argmax import (
     ConvQuadInterp3d,
     ConvSoftArgmax2d,
     ConvSoftArgmax3d,
     SpatialSoftArgmax2d,
```

### Comparing `kornia-0.6.9/kornia/geometry/subpix/dsnt.py` & `kornia-0.7.0/kornia/geometry/subpix/dsnt.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 r"""Implementation of "differentiable spatial to numerical" (soft-argmax) operations, as described in the paper
 "Numerical Coordinate Regression with Convolutional Neural Networks" by Nibali et al."""
 
-from typing import Tuple
+
+from __future__ import annotations
 
 import torch
 import torch.nn.functional as F
 
-from kornia.testing import check_is_tensor
+from kornia.core import Tensor, concatenate
+from kornia.core.check import KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.utils.grid import create_meshgrid
 
 
-def _validate_batched_image_tensor_input(tensor):
-    check_is_tensor(tensor)
-    if not len(tensor.shape) == 4:
-        raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {tensor.shape}")
+def _validate_batched_image_tensor_input(tensor: Tensor) -> None:
+    KORNIA_CHECK_IS_TENSOR(tensor)
+    KORNIA_CHECK_SHAPE(tensor, ['B', 'C', 'H', 'W'])
 
 
-def spatial_softmax2d(input: torch.Tensor, temperature: torch.Tensor = torch.tensor(1.0)) -> torch.Tensor:
+def spatial_softmax2d(input: Tensor, temperature: Tensor = torch.tensor(1.0)) -> Tensor:
     r"""Apply the Softmax function over features in each image channel.
 
     Note that this function behaves differently to :py:class:`torch.nn.Softmax2d`, which
     instead applies Softmax over features at each spatial location.
 
     Args:
         input: the input tensor with shape :math:`(B, N, H, W)`.
@@ -39,22 +40,22 @@
                   [0.0585, 0.0585, 0.0585],
                   [0.0585, 0.1589, 0.4319]]]])
     """
     _validate_batched_image_tensor_input(input)
 
     batch_size, channels, height, width = input.shape
     temperature = temperature.to(device=input.device, dtype=input.dtype)
-    x: torch.Tensor = input.view(batch_size, channels, -1)
+    x = input.view(batch_size, channels, -1)
 
-    x_soft: torch.Tensor = F.softmax(x * temperature, dim=-1)
+    x_soft = F.softmax(x * temperature, dim=-1)
 
     return x_soft.view(batch_size, channels, height, width)
 
 
-def spatial_expectation2d(input: torch.Tensor, normalized_coordinates: bool = True) -> torch.Tensor:
+def spatial_expectation2d(input: Tensor, normalized_coordinates: bool = True) -> Tensor:
     r"""Compute the expectation of coordinate values using spatial probabilities.
 
     The input heatmap is assumed to represent a valid spatial probability distribution,
     which can be achieved using :func:`~kornia.geometry.subpixel.spatial_softmax2d`.
 
     Args:
         input: the input tensor representing dense spatial probabilities with shape :math:`(B, N, H, W)`.
@@ -73,38 +74,36 @@
         tensor([[[1., 2.]]])
     """
     _validate_batched_image_tensor_input(input)
 
     batch_size, channels, height, width = input.shape
 
     # Create coordinates grid.
-    grid: torch.Tensor = create_meshgrid(height, width, normalized_coordinates, input.device)
+    grid = create_meshgrid(height, width, normalized_coordinates, input.device)
     grid = grid.to(input.dtype)
 
-    pos_x: torch.Tensor = grid[..., 0].reshape(-1)
-    pos_y: torch.Tensor = grid[..., 1].reshape(-1)
+    pos_x = grid[..., 0].reshape(-1)
+    pos_y = grid[..., 1].reshape(-1)
 
-    input_flat: torch.Tensor = input.view(batch_size, channels, -1)
+    input_flat = input.view(batch_size, channels, -1)
 
     # Compute the expectation of the coordinates.
-    expected_y: torch.Tensor = torch.sum(pos_y * input_flat, -1, keepdim=True)
-    expected_x: torch.Tensor = torch.sum(pos_x * input_flat, -1, keepdim=True)
+    expected_y = torch.sum(pos_y * input_flat, -1, keepdim=True)
+    expected_x = torch.sum(pos_x * input_flat, -1, keepdim=True)
 
-    output: torch.Tensor = torch.cat([expected_x, expected_y], -1)
+    output = concatenate([expected_x, expected_y], -1)
 
     return output.view(batch_size, channels, 2)  # BxNx2
 
 
-def _safe_zero_division(numerator: torch.Tensor, denominator: torch.Tensor, eps: float = 1e-32) -> torch.Tensor:
+def _safe_zero_division(numerator: Tensor, denominator: Tensor, eps: float = 1e-32) -> Tensor:
     return numerator / torch.clamp(denominator, min=eps)
 
 
-def render_gaussian2d(
-    mean: torch.Tensor, std: torch.Tensor, size: Tuple[int, int], normalized_coordinates: bool = True
-):
+def render_gaussian2d(mean: Tensor, std: Tensor, size: tuple[int, int], normalized_coordinates: bool = True) -> Tensor:
     r"""Render the PDF of a 2D Gaussian distribution.
 
     Args:
         mean: the mean location of the Gaussian to render, :math:`(\mu_x, \mu_y)`. Shape: :math:`(*, 2)`.
         std: the standard deviation of the Gaussian to render, :math:`(\sigma_x, \sigma_y)`.
           Shape :math:`(*, 2)`. Should be able to be broadcast with `mean`.
         size: the (height, width) of the output image.
@@ -116,18 +115,18 @@
     """
     if not (std.dtype == mean.dtype and std.device == mean.device):
         raise TypeError("Expected inputs to have the same dtype and device")
 
     height, width = size
 
     # Create coordinates grid.
-    grid: torch.Tensor = create_meshgrid(height, width, normalized_coordinates, mean.device)
+    grid = create_meshgrid(height, width, normalized_coordinates, mean.device)
     grid = grid.to(mean.dtype)
-    pos_x: torch.Tensor = grid[..., 0].view(height, width)
-    pos_y: torch.Tensor = grid[..., 1].view(height, width)
+    pos_x = grid[..., 0].view(height, width)
+    pos_y = grid[..., 1].view(height, width)
 
     # Gaussian PDF = exp(-(x - \mu)^2 / (2 \sigma^2))
     #              = exp(dists * ks),
     #                where dists = (x - \mu)^2 and ks = -1 / (2 \sigma^2)
 
     # dists <- (x - \mu)^2
     dist_x = (pos_x - mean[..., 0, None, None]) ** 2
```

### Comparing `kornia-0.6.9/kornia/geometry/subpix/nms.py` & `kornia-0.7.0/kornia/geometry/subpix/nms.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Tuple
+from __future__ import annotations
 
 import torch
 import torch.nn.functional as F
 
 from kornia.core import Module, Tensor, eye, pad, zeros
 
 
@@ -21,72 +21,76 @@
     center: int = numel // 2
     weight = eye(numel)
     weight[center, center] = 0
     return weight.view(numel, 1, kd, ky, kx)
 
 
 class NonMaximaSuppression2d(Module):
-    r"""Apply non maxima suppression to filter."""
+    r"""Apply non maxima suppression to filter.
+
+    Flag `minima_are_also_good` is useful, when you want to detect both maxima and minima, e.g. for DoG
+    """
     kernel: Tensor
 
-    def __init__(self, kernel_size: Tuple[int, int]):
+    def __init__(self, kernel_size: tuple[int, int]) -> None:
         super().__init__()
-        self.kernel_size: Tuple[int, int] = kernel_size
-        self.padding: Tuple[int, int, int, int] = self._compute_zero_padding2d(kernel_size)
+        self.kernel_size: tuple[int, int] = kernel_size
+        self.padding: tuple[int, int, int, int] = self._compute_zero_padding2d(kernel_size)
         self.register_buffer('kernel', _get_nms_kernel2d(*kernel_size))
 
     @staticmethod
-    def _compute_zero_padding2d(kernel_size: Tuple[int, int]) -> Tuple[int, int, int, int]:
+    def _compute_zero_padding2d(kernel_size: tuple[int, int]) -> tuple[int, int, int, int]:
+        # TODO: This method is duplicated with some utility function on kornia.filters
         if not isinstance(kernel_size, tuple):
             raise AssertionError(type(kernel_size))
         if len(kernel_size) != 2:
             raise AssertionError(kernel_size)
 
-        def pad(x):
+        def pad(x: int) -> int:
             return (x - 1) // 2  # zero padding function
 
         ky, kx = kernel_size  # we assume a cubic kernel
         return (pad(ky), pad(ky), pad(kx), pad(kx))
 
     def forward(self, x: Tensor, mask_only: bool = False) -> Tensor:
         if len(x.shape) != 4:
             raise AssertionError(x.shape)
         B, CH, H, W = x.size()
         # find local maximum values
         x_padded = pad(x, list(self.padding)[::-1], mode='replicate')
         B, CH, HP, WP = x_padded.size()
 
-        max_non_center = (
-            F.conv2d(x_padded.view(B * CH, 1, HP, WP), self.kernel.to(x.device, x.dtype), stride=1)
-            .view(B, CH, -1, H, W)
-            .max(dim=2)[0]
+        neighborhood = F.conv2d(x_padded.view(B * CH, 1, HP, WP), self.kernel.to(x.device, x.dtype), stride=1).view(
+            B, CH, -1, H, W
         )
+        max_non_center = neighborhood.max(dim=2)[0]
         mask = x > max_non_center
         if mask_only:
             return mask
         return x * (mask.to(x.dtype))
 
 
 class NonMaximaSuppression3d(Module):
     r"""Apply non maxima suppression to filter."""
 
-    def __init__(self, kernel_size: Tuple[int, int, int]):
+    def __init__(self, kernel_size: tuple[int, int, int]) -> None:
         super().__init__()
-        self.kernel_size: Tuple[int, int, int] = kernel_size
-        self.padding: Tuple[int, int, int, int, int, int] = self._compute_zero_padding3d(kernel_size)
+        self.kernel_size: tuple[int, int, int] = kernel_size
+        self.padding: tuple[int, int, int, int, int, int] = self._compute_zero_padding3d(kernel_size)
         self.kernel = _get_nms_kernel3d(*kernel_size)
 
     @staticmethod
-    def _compute_zero_padding3d(kernel_size: Tuple[int, int, int]) -> Tuple[int, int, int, int, int, int]:
+    def _compute_zero_padding3d(kernel_size: tuple[int, int, int]) -> tuple[int, int, int, int, int, int]:
+        # TODO: This method is duplicated with some utility function on kornia.filters
         if not isinstance(kernel_size, tuple):
             raise AssertionError(type(kernel_size))
         if len(kernel_size) != 3:
             raise AssertionError(kernel_size)
 
-        def pad(x):
+        def pad(x: int) -> int:
             return (x - 1) // 2  # zero padding function
 
         kd, ky, kx = kernel_size  # we assume a cubic kernel
         return pad(kd), pad(kd), pad(ky), pad(ky), pad(kx), pad(kx)
 
     def forward(self, x: Tensor, mask_only: bool = False) -> Tensor:
         if len(x.shape) != 5:
@@ -143,21 +147,22 @@
             return mask
         return x * (mask.to(x.dtype))
 
 
 # functional api
 
 
-def nms2d(input: Tensor, kernel_size: Tuple[int, int], mask_only: bool = False) -> Tensor:
+def nms2d(input: Tensor, kernel_size: tuple[int, int], mask_only: bool = False) -> Tensor:
     r"""Apply non maxima suppression to filter.
 
     See :class:`~kornia.geometry.subpix.NonMaximaSuppression2d` for details.
     """
     return NonMaximaSuppression2d(kernel_size)(input, mask_only)
 
 
-def nms3d(input: Tensor, kernel_size: Tuple[int, int, int], mask_only: bool = False) -> Tensor:
+def nms3d(input: Tensor, kernel_size: tuple[int, int, int], mask_only: bool = False) -> Tensor:
     r"""Apply non maxima suppression to filter.
 
-    See :class:`~kornia.feature.NonMaximaSuppression3d` for details.
+    See
+    :class: `~kornia.feature.NonMaximaSuppression3d` for details.
     """
     return NonMaximaSuppression3d(kernel_size)(input, mask_only)
```

### Comparing `kornia-0.6.9/kornia/geometry/subpix/spatial_soft_argmax.py` & `kornia-0.7.0/kornia/geometry/subpix/spatial_soft_argmax.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-from typing import Tuple, Union
+from __future__ import annotations
 
 import torch
 import torch.nn.functional as F
 
-from kornia.core import Module, Tensor, concatenate, stack, tensor, zeros
+from kornia.core import Module, Tensor, concatenate, stack, tensor, where, zeros
 from kornia.filters.sobel import spatial_gradient3d
 from kornia.geometry.conversions import normalize_pixel_coordinates, normalize_pixel_coordinates3d
 from kornia.utils import create_meshgrid, create_meshgrid3d
-from kornia.utils._compat import torch_version_geq
+from kornia.utils._compat import torch_version_ge
 from kornia.utils.helpers import safe_solve_with_mask
 
 from .dsnt import spatial_expectation2d, spatial_softmax2d
 from .nms import nms3d
 
 
 def _get_window_grid_kernel2d(h: int, w: int, device: torch.device = torch.device('cpu')) -> Tensor:
@@ -121,23 +121,24 @@
     conv_kernel = grid3d.permute(3, 0, 1, 2).unsqueeze(1)
     return conv_kernel
 
 
 class ConvSoftArgmax2d(Module):
     r"""Module that calculates soft argmax 2d per window.
 
-    See :func:`~kornia.geometry.subpix.conv_soft_argmax2d` for details.
+    See
+    :func: `~kornia.geometry.subpix.conv_soft_argmax2d` for details.
     """
 
     def __init__(
         self,
-        kernel_size: Tuple[int, int] = (3, 3),
-        stride: Tuple[int, int] = (1, 1),
-        padding: Tuple[int, int] = (1, 1),
-        temperature: Union[Tensor, float] = tensor(1.0),
+        kernel_size: tuple[int, int] = (3, 3),
+        stride: tuple[int, int] = (1, 1),
+        padding: tuple[int, int] = (1, 1),
+        temperature: Tensor | float = tensor(1.0),
         normalized_coordinates: bool = True,
         eps: float = 1e-8,
         output_value: bool = False,
     ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
         self.stride = stride
@@ -145,40 +146,25 @@
         self.temperature = temperature
         self.normalized_coordinates = normalized_coordinates
         self.eps = eps
         self.output_value = output_value
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'kernel_size='
-            + str(self.kernel_size)
-            + ', '
-            + 'stride='
-            + str(self.stride)
-            + ', '
-            + 'padding='
-            + str(self.padding)
-            + ', '
-            + 'temperature='
-            + str(self.temperature)
-            + ', '
-            + 'normalized_coordinates='
-            + str(self.normalized_coordinates)
-            + ', '
-            + 'eps='
-            + str(self.eps)
-            + ', '
-            + 'output_value='
-            + str(self.output_value)
-            + ')'
+            f"{self.__class__.__name__}"
+            f"(kernel_size={self.kernel_size}, "
+            f"stride={self.stride}, "
+            f"padding={self.padding}, "
+            f"temperature={self.temperature}, "
+            f"normalized_coordinates={self.normalized_coordinates}, "
+            f"eps={self.eps}, "
+            f"output_value={self.output_value})"
         )
 
-    def forward(self, x: Tensor):
+    def forward(self, x: Tensor) -> Tensor | tuple[Tensor, Tensor]:
         return conv_soft_argmax2d(
             x,
             self.kernel_size,
             self.stride,
             self.padding,
             self.temperature,
             self.normalized_coordinates,
@@ -186,70 +172,53 @@
             self.output_value,
         )
 
 
 class ConvSoftArgmax3d(Module):
     r"""Module that calculates soft argmax 3d per window.
 
-    See :func:`~kornia.geometry.subpix.conv_soft_argmax3d` for details.
+    See
+    :func: `~kornia.geometry.subpix.conv_soft_argmax3d` for details.
     """
 
     def __init__(
         self,
-        kernel_size: Tuple[int, int, int] = (3, 3, 3),
-        stride: Tuple[int, int, int] = (1, 1, 1),
-        padding: Tuple[int, int, int] = (1, 1, 1),
-        temperature: Union[Tensor, float] = tensor(1.0),
+        kernel_size: tuple[int, int, int] = (3, 3, 3),
+        stride: tuple[int, int, int] = (1, 1, 1),
+        padding: tuple[int, int, int] = (1, 1, 1),
+        temperature: Tensor | float = tensor(1.0),
         normalized_coordinates: bool = False,
         eps: float = 1e-8,
         output_value: bool = True,
         strict_maxima_bonus: float = 0.0,
     ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
         self.stride = stride
         self.padding = padding
         self.temperature = temperature
         self.normalized_coordinates = normalized_coordinates
         self.eps = eps
         self.output_value = output_value
         self.strict_maxima_bonus = strict_maxima_bonus
-        return
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '('
-            + 'kernel_size='
-            + str(self.kernel_size)
-            + ', '
-            + 'stride='
-            + str(self.stride)
-            + ', '
-            + 'padding='
-            + str(self.padding)
-            + ', '
-            + 'temperature='
-            + str(self.temperature)
-            + ', '
-            + 'normalized_coordinates='
-            + str(self.normalized_coordinates)
-            + ', '
-            + 'eps='
-            + str(self.eps)
-            + ', '
-            + 'strict_maxima_bonus='
-            + str(self.strict_maxima_bonus)
-            + ', '
-            + 'output_value='
-            + str(self.output_value)
-            + ')'
+            f"{self.__class__.__name__}"
+            f"(kernel_size={self.kernel_size}, "
+            f"stride={self.stride}, "
+            f"padding={self.padding}, "
+            f"temperature={self.temperature}, "
+            f"normalized_coordinates={self.normalized_coordinates}, "
+            f"eps={self.eps}, "
+            f"strict_maxima_bonus={self.strict_maxima_bonus}, "
+            f"output_value={self.output_value})"
         )
 
-    def forward(self, x: Tensor):
+    def forward(self, x: Tensor) -> Tensor | tuple[Tensor, Tensor]:
         return conv_soft_argmax3d(
             x,
             self.kernel_size,
             self.stride,
             self.padding,
             self.temperature,
             self.normalized_coordinates,
@@ -257,22 +226,22 @@
             self.output_value,
             self.strict_maxima_bonus,
         )
 
 
 def conv_soft_argmax2d(
     input: Tensor,
-    kernel_size: Tuple[int, int] = (3, 3),
-    stride: Tuple[int, int] = (1, 1),
-    padding: Tuple[int, int] = (1, 1),
-    temperature: Union[Tensor, float] = tensor(1.0),
+    kernel_size: tuple[int, int] = (3, 3),
+    stride: tuple[int, int] = (1, 1),
+    padding: tuple[int, int] = (1, 1),
+    temperature: Tensor | float = tensor(1.0),
     normalized_coordinates: bool = True,
     eps: float = 1e-8,
     output_value: bool = False,
-) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+) -> Tensor | tuple[Tensor, Tensor]:
     r"""Compute the convolutional spatial Soft-Argmax 2D over the windows of a given heatmap.
 
     .. math::
         ij(X) = \frac{\sum{(i,j)} * exp(x / T)  \in X} {\sum{exp(x / T)  \in X}}
 
     .. math::
         val(X) = \frac{\sum{x * exp(x / T)  \in X}} {\sum{exp(x / T)  \in X}}
@@ -315,21 +284,21 @@
     if not len(input.shape) == 4:
         raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
 
     if temperature <= 0:
         raise ValueError(f"Temperature should be positive float or tensor. Got: {temperature}")
 
     b, c, h, w = input.shape
-    kx, ky = kernel_size
+    ky, kx = kernel_size
     device: torch.device = input.device
     dtype: torch.dtype = input.dtype
     input = input.view(b * c, 1, h, w)
 
-    center_kernel: Tensor = _get_center_kernel2d(kx, ky, device).to(dtype)
-    window_kernel: Tensor = _get_window_grid_kernel2d(kx, ky, device).to(dtype)
+    center_kernel: Tensor = _get_center_kernel2d(ky, kx, device).to(dtype)
+    window_kernel: Tensor = _get_window_grid_kernel2d(ky, kx, device).to(dtype)
 
     # applies exponential normalization trick
     # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/
     # https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44
     x_max = F.adaptive_max_pool2d(input, (1, 1))
 
     # max is detached to prevent undesired backprop loops in the graph
@@ -370,23 +339,23 @@
     if output_value:
         return coords_max, x_softmaxpool
     return coords_max
 
 
 def conv_soft_argmax3d(
     input: Tensor,
-    kernel_size: Tuple[int, int, int] = (3, 3, 3),
-    stride: Tuple[int, int, int] = (1, 1, 1),
-    padding: Tuple[int, int, int] = (1, 1, 1),
-    temperature: Union[Tensor, float] = tensor(1.0),
+    kernel_size: tuple[int, int, int] = (3, 3, 3),
+    stride: tuple[int, int, int] = (1, 1, 1),
+    padding: tuple[int, int, int] = (1, 1, 1),
+    temperature: Tensor | float = tensor(1.0),
     normalized_coordinates: bool = False,
     eps: float = 1e-8,
     output_value: bool = True,
     strict_maxima_bonus: float = 0.0,
-) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+) -> Tensor | tuple[Tensor, Tensor]:
     r"""Compute the convolutional spatial Soft-Argmax 3D over the windows of a given heatmap.
 
     .. math::
              ijk(X) = \frac{\sum{(i,j,k)} * exp(x / T)  \in X} {\sum{exp(x / T)  \in X}}
 
     .. math::
              val(X) = \frac{\sum{x * exp(x / T)  \in X}} {\sum{exp(x / T)  \in X}}
@@ -435,21 +404,21 @@
     if not len(input.shape) == 5:
         raise ValueError(f"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}")
 
     if temperature <= 0:
         raise ValueError(f"Temperature should be positive float or tensor. Got: {temperature}")
 
     b, c, d, h, w = input.shape
-    kx, ky, kz = kernel_size
+    kz, ky, kx = kernel_size
     device: torch.device = input.device
     dtype: torch.dtype = input.dtype
     input = input.view(b * c, 1, d, h, w)
 
-    center_kernel: Tensor = _get_center_kernel3d(kx, ky, kz, device).to(dtype)
-    window_kernel: Tensor = _get_window_grid_kernel3d(kx, ky, kz, device).to(dtype)
+    center_kernel: Tensor = _get_center_kernel3d(kz, ky, kx, device).to(dtype)
+    window_kernel: Tensor = _get_window_grid_kernel3d(kz, ky, kx, device).to(dtype)
 
     # applies exponential normalization trick
     # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/
     # https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44
     x_max = F.adaptive_max_pool3d(input, (1, 1, 1))
 
     # max is detached to prevent undesired backprop loops in the graph
@@ -537,28 +506,24 @@
     def __init__(self, temperature: Tensor = tensor(1.0), normalized_coordinates: bool = True) -> None:
         super().__init__()
         self.temperature: Tensor = temperature
         self.normalized_coordinates: bool = normalized_coordinates
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '(temperature='
-            + str(self.temperature)
-            + ', '
-            + 'normalized_coordinates='
-            + str(self.normalized_coordinates)
-            + ')'
+            f"{self.__class__.__name__}"
+            f"temperature={self.temperature}, "
+            f"normalized_coordinates={self.normalized_coordinates})"
         )
 
     def forward(self, input: Tensor) -> Tensor:
         return spatial_soft_argmax2d(input, self.temperature, self.normalized_coordinates)
 
 
-def conv_quad_interp3d(input: Tensor, strict_maxima_bonus: float = 10.0, eps: float = 1e-7) -> Tuple[Tensor, Tensor]:
+def conv_quad_interp3d(input: Tensor, strict_maxima_bonus: float = 10.0, eps: float = 1e-7) -> tuple[Tensor, Tensor]:
     r"""Compute the single iteration of quadratic interpolation of the extremum (max or min).
 
     Args:
         input: the given heatmap with shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.
         strict_maxima_bonus: pixels, which are strict maxima will score (1 + strict_maxima_bonus) * value.
           This is needed for mimic behavior of strict NMS in classic local features
         eps: parameter to control the hessian matrix ill-condition number.
@@ -605,53 +570,56 @@
     dyy = A[..., 1]
     dss = A[..., 2]
     dxy = 0.25 * A[..., 3]  # normalization to match OpenCV implementation
     dys = 0.25 * A[..., 4]  # normalization to match OpenCV implementation
     dxs = 0.25 * A[..., 5]  # normalization to match OpenCV implementation
 
     Hes = stack([dxx, dxy, dxs, dxy, dyy, dys, dxs, dys, dss], -1).view(-1, 3, 3)
-    if not torch_version_geq(1, 10):
+    if not torch_version_ge(1, 10):
         # The following is needed to avoid singular cases
         Hes += torch.rand(Hes[0].size(), device=Hes.device).abs()[None] * eps
 
     nms_mask: Tensor = nms3d(input, (3, 3, 3), True)
     x_solved: Tensor = torch.zeros_like(b)
     x_solved_masked, _, solved_correctly = safe_solve_with_mask(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])
 
     #  Kill those points, where we cannot solve
     new_nms_mask = nms_mask.masked_scatter(nms_mask, solved_correctly)
 
-    x_solved.masked_scatter_(new_nms_mask.view(-1, 1, 1), x_solved_masked[solved_correctly])
+    x_solved[where(new_nms_mask.view(-1, 1, 1))[0]] = x_solved_masked[solved_correctly]
+
     dx: Tensor = -x_solved
 
     # Ignore ones, which are far from window center
     mask1 = dx.abs().max(dim=1, keepdim=True)[0] > 0.7
     dx.masked_fill_(mask1.expand_as(dx), 0)
     dy: Tensor = 0.5 * torch.bmm(b.permute(0, 2, 1), dx)
     y_max = input + dy.view(B, CH, D, H, W)
     if strict_maxima_bonus > 0:
         y_max += strict_maxima_bonus * new_nms_mask.to(input.dtype)
 
     dx_res: Tensor = dx.flip(1).reshape(B, CH, D, H, W, 3).permute(0, 1, 5, 2, 3, 4)
+    dx_res[:, :, (1, 2)] = dx_res[:, :, (2, 1)]
+
     coords_max: Tensor = grid_global.repeat(B, 1, 1, 1, 1).unsqueeze(1)
     coords_max = coords_max + dx_res
 
     return coords_max, y_max
 
 
 class ConvQuadInterp3d(Module):
     r"""Calculate soft argmax 3d per window.
 
-    See :func:`~kornia.geometry.subpix.conv_quad_interp3d` for details.
+    See
+    :func: `~kornia.geometry.subpix.conv_quad_interp3d` for details.
     """
 
     def __init__(self, strict_maxima_bonus: float = 10.0, eps: float = 1e-7) -> None:
         super().__init__()
         self.strict_maxima_bonus = strict_maxima_bonus
         self.eps = eps
-        return
 
     def __repr__(self) -> str:
-        return self.__class__.__name__ + '(' + 'strict_maxima_bonus=' + str(self.strict_maxima_bonus) + ')'
+        return f"{self.__class__.__name__}(strict_maxima_bonus={self.strict_maxima_bonus})"
 
-    def forward(self, x: Tensor):
+    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:
         return conv_quad_interp3d(x, self.strict_maxima_bonus, self.eps)
```

### Comparing `kornia-0.6.9/kornia/geometry/transform/affwarp.py` & `kornia-0.7.0/kornia/geometry/transform/affwarp.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Optional, Tuple, Union
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from kornia.filters import gaussian_blur2d
 from kornia.utils import _extract_device_dtype
 from kornia.utils.image import perform_keep_shape_image
 from kornia.utils.misc import eye_like
 
 from .imgwarp import get_affine_matrix2d, get_projective_transform, get_rotation_matrix2d, warp_affine, warp_affine3d
```

### Comparing `kornia-0.6.9/kornia/geometry/transform/crop2d.py` & `kornia-0.7.0/kornia/geometry/transform/crop2d.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/transform/crop3d.py` & `kornia-0.7.0/kornia/geometry/transform/crop3d.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/geometry/transform/flips.py` & `kornia-0.7.0/kornia/geometry/transform/flips.py`

 * *Files 13% similar despite different names*

```diff
@@ -28,15 +28,15 @@
                   [0., 0., 0.],
                   [0., 0., 0.]]]])
     """
 
     def forward(self, input: Tensor) -> Tensor:
         return vflip(input)
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return self.__class__.__name__
 
 
 class Hflip(Module):
     r"""Horizontally flip a tensor image or a batch of tensor images.
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
@@ -59,15 +59,15 @@
                   [0., 0., 0.],
                   [1., 1., 0.]]]])
     """
 
     def forward(self, input: Tensor) -> Tensor:
         return hflip(input)
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return self.__class__.__name__
 
 
 class Rot180(Module):
     r"""Rotate a tensor image or a batch of tensor images 180 degrees.
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
@@ -87,15 +87,15 @@
                   [0., 0., 0.],
                   [0., 0., 0.]]]])
     """
 
     def forward(self, input: Tensor) -> Tensor:
         return rot180(input)
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return self.__class__.__name__
 
 
 def rot180(input: Tensor) -> Tensor:
     r"""Rotate a tensor image or a batch of tensor images 180 degrees.
 
     .. image:: _static/img/rot180.png
@@ -104,15 +104,14 @@
 
     Args:
         input: input tensor.
 
     Returns:
         The rotated image tensor.
     """
-
     return torch.flip(input, [-2, -1])
 
 
 def hflip(input: Tensor) -> Tensor:
     r"""Horizontally flip a tensor image or a batch of tensor images.
 
     .. image:: _static/img/hflip.png
@@ -121,16 +120,15 @@
 
     Args:
         input: input tensor.
 
     Returns:
         The horizontally flipped image tensor.
     """
-    w = input.shape[-1]
-    return input[..., torch.arange(w - 1, -1, -1, device=input.device)]
+    return input.flip(-1).contiguous()
 
 
 def vflip(input: Tensor) -> Tensor:
     r"""Vertically flip a tensor image or a batch of tensor images.
 
     .. image:: _static/img/vflip.png
 
@@ -138,10 +136,8 @@
 
     Args:
         input: input tensor.
 
     Returns:
         The vertically flipped image tensor.
     """
-
-    h = input.shape[-2]
-    return input[..., torch.arange(h - 1, -1, -1, device=input.device), :]
+    return input.flip(-2).contiguous()
```

### Comparing `kornia-0.6.9/kornia/geometry/transform/homography_warper.py` & `kornia-0.7.0/kornia/geometry/transform/homography_warper.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,21 +1,38 @@
-from typing import Optional
+from __future__ import annotations
+
+from abc import abstractmethod
+from typing import Any
 
-import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
+from kornia.core import Module, Tensor
 from kornia.utils import create_meshgrid
 
 from .imgwarp import homography_warp, warp_grid
 
-__all__ = ["HomographyWarper"]
+__all__ = ["HomographyWarper", "BaseWarper"]
+
+
+class BaseWarper(Module):
+    def __init__(self, height: int, width: int, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+        self.height = height
+        self.width = width
+
+    @abstractmethod
+    def forward(self, patch_src: Tensor, src_homo_dst: Tensor | None = None) -> Tensor:
+        ...
+
+    @abstractmethod
+    def precompute_warp_grid(self, src_homo_dst: Tensor) -> None:
+        ...
 
 
-class HomographyWarper(nn.Module):
+class HomographyWarper(BaseWarper):
     r"""Warp tensors by homographies.
 
     .. math::
 
         X_{dst} = H_{src}^{\{dst\}} * X_{src}
 
     Args:
@@ -23,53 +40,51 @@
         width: The width of the destination tensor.
         mode: interpolation mode to calculate output values ``'bilinear'`` | ``'nearest'``.
         padding_mode: padding mode for outside grid values
           ``'zeros'`` | ``'border'`` | ``'reflection'``.
         normalized_coordinates: whether to use a grid with normalized coordinates.
         align_corners: interpolation flag.
     """
-    _warped_grid: Optional[torch.Tensor]
+    _warped_grid: Tensor | None
 
     def __init__(
         self,
         height: int,
         width: int,
         mode: str = 'bilinear',
         padding_mode: str = 'zeros',
         normalized_coordinates: bool = True,
         align_corners: bool = False,
     ) -> None:
-        super().__init__()
-        self.width: int = width
-        self.height: int = height
-        self.mode: str = mode
-        self.padding_mode: str = padding_mode
-        self.normalized_coordinates: bool = normalized_coordinates
-        self.align_corners: bool = align_corners
+        super().__init__(height, width)
+        self.mode = mode
+        self.padding_mode = padding_mode
+        self.normalized_coordinates = normalized_coordinates
+        self.align_corners = align_corners
         # create base grid to compute the flow
-        self.grid: torch.Tensor = create_meshgrid(height, width, normalized_coordinates=normalized_coordinates)
+        self.grid = create_meshgrid(height, width, normalized_coordinates=normalized_coordinates)
 
         # initialice the warped destination grid
         self._warped_grid = None
 
-    def precompute_warp_grid(self, src_homo_dst: torch.Tensor) -> None:
+    def precompute_warp_grid(self, src_homo_dst: Tensor) -> None:
         r"""Compute and store internally the transformations of the points.
 
         Useful when the same homography/homographies are reused.
 
         Args:
             src_homo_dst: Homography or homographies (stacked) to
               transform all points in the grid. Shape of the homography
               has to be :math:`(1, 3, 3)` or :math:`(N, 1, 3, 3)`.
               The homography assumes normalized coordinates [-1, 1] if
               normalized_coordinates is True.
         """
         self._warped_grid = warp_grid(self.grid, src_homo_dst)
 
-    def forward(self, patch_src: torch.Tensor, src_homo_dst: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def forward(self, patch_src: Tensor, src_homo_dst: Tensor | None = None) -> Tensor:
         r"""Warp a tensor from source into reference frame.
 
         Args:
             patch_src: The tensor to warp.
             src_homo_dst: The homography or stack of
               homographies from destination to source. The homography assumes
               normalized coordinates [-1, 1] if normalized_coordinates is True.
```

### Comparing `kornia-0.6.9/kornia/geometry/transform/image_registrator.py` & `kornia-0.7.0/kornia/geometry/transform/image_registrator.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,36 +1,50 @@
-from typing import Callable, List, Tuple, Union
+from abc import abstractmethod
+from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
-import torch.optim as optim
+from torch import nn, optim
 
-from kornia.core import Tensor
+from kornia.core import Module, Tensor
 from kornia.geometry.conversions import angle_to_rotation_matrix, convert_affinematrix_to_homography
 
-from .homography_warper import HomographyWarper
+from .homography_warper import BaseWarper, HomographyWarper
 from .pyramid import build_pyramid
 
-__all__ = ["ImageRegistrator", "Homography", "Similarity"]
+__all__ = ["ImageRegistrator", "Homography", "Similarity", "BaseModel"]
 
 
-class Homography(nn.Module):
+class BaseModel(Module):
+    @abstractmethod
+    def reset_model(self) -> None:
+        ...
+
+    @abstractmethod
+    def forward(self) -> Tensor:
+        ...
+
+    @abstractmethod
+    def forward_inverse(self) -> Tensor:
+        ...
+
+
+class Homography(BaseModel):
     r"""Homography geometric model to be used together with ImageRegistrator module for the optimization-based
     image registration."""
 
     def __init__(self) -> None:
         super().__init__()
         self.model = nn.Parameter(torch.eye(3))
         self.reset_model()
 
     def __repr__(self) -> str:
         return f'{self.__class__.__name__}({self.model})'
 
-    def reset_model(self):
+    def reset_model(self) -> None:
         """Initializes the model with identity transform."""
         torch.nn.init.eye_(self.model)
 
     def forward(self) -> Tensor:
         r"""Single-batch homography".
 
         Returns:
@@ -43,15 +57,15 @@
 
         Returns:
             Homography martix with shape :math:`(1, 3, 3)`.
         """
         return torch.unsqueeze(torch.inverse(self.model), dim=0)
 
 
-class Similarity(nn.Module):
+class Similarity(BaseModel):
     """Similarity geometric model to be used together with ImageRegistrator module for the optimization-based image
     registration.
 
     Args:
         rotation: if True, the rotation is optimizable, else constant zero.
         scale: if True, the scale is optimizable, else constant zero.
         shift: if True, the shift is optimizable, else constant one.
@@ -98,15 +112,15 @@
 
         Returns:
             Similarity with shape :math:`(1, 3, 3)`
         """
         return torch.inverse(self.forward())
 
 
-class ImageRegistrator(nn.Module):
+class ImageRegistrator(Module):
     r"""Module, which performs optimization-based image registration.
 
     Args:
         model_type: Geometrical model for registration. Can be string or Module.
         optimizer: optimizer class used for the optimization.
         loss_fn: torch loss function.
         pyramid_levels: number of scale pyramid levels.
@@ -118,54 +132,53 @@
     Example:
         >>> from kornia.geometry import ImageRegistrator
         >>> img_src = torch.rand(1, 1, 32, 32)
         >>> img_dst = torch.rand(1, 1, 32, 32)
         >>> registrator = ImageRegistrator('similarity')
         >>> homo = registrator.register(img_src, img_dst)
     """
-    known_models = ['homography', 'similarity', 'translation', 'scale', 'rotation']
 
     # TODO: resolve better type, potentially using factory.
     def __init__(
         self,
-        model_type='homography',
-        optimizer=optim.Adam,
+        model_type: Union[str, BaseModel] = 'homography',
+        optimizer: Type[optim.Optimizer] = optim.Adam,
         loss_fn: Callable[..., Tensor] = F.l1_loss,
         pyramid_levels: int = 5,
         lr: float = 1e-3,
         num_iterations: int = 100,
         tolerance: float = 1e-4,
-        warper=None,
+        warper: Optional[Type[BaseWarper]] = None,
     ) -> None:
         super().__init__()
+        self.known_models = ['homography', 'similarity', 'translation', 'scale', 'rotation']
         # We provide pre-defined combinations or allow user to supply model
         # together with warper
         if not isinstance(model_type, str):
             if warper is None:
                 raise ValueError("You must supply warper together with custom model")
             self.warper = warper
             self.model = model_type
+        elif model_type.lower() == "homography":
+            self.warper = HomographyWarper
+            self.model = Homography()
+        elif model_type.lower() == "similarity":
+            self.warper = HomographyWarper
+            self.model = Similarity(True, True, True)
+        elif model_type.lower() == "translation":
+            self.warper = HomographyWarper
+            self.model = Similarity(False, False, True)
+        elif model_type.lower() == "rotation":
+            self.warper = HomographyWarper
+            self.model = Similarity(True, False, False)
+        elif model_type.lower() == "scale":
+            self.warper = HomographyWarper
+            self.model = Similarity(False, True, False)
         else:
-            if model_type.lower() == "homography":
-                self.warper = HomographyWarper
-                self.model = Homography()
-            elif model_type.lower() == "similarity":
-                self.warper = HomographyWarper
-                self.model = Similarity(True, True, True)
-            elif model_type.lower() == "translation":
-                self.warper = HomographyWarper
-                self.model = Similarity(False, False, True)
-            elif model_type.lower() == "rotation":
-                self.warper = HomographyWarper
-                self.model = Similarity(True, False, False)
-            elif model_type.lower() == "scale":
-                self.warper = HomographyWarper
-                self.model = Similarity(False, True, False)
-            else:
-                raise ValueError(f"{model_type} is not supported. Try {self.known_models}")
+            raise ValueError(f"{model_type} is not supported. Try {self.known_models}")
         self.pyramid_levels = pyramid_levels
         self.optimizer = optimizer
         self.lr = lr
         self.loss_fn = loss_fn
         self.num_iterations = num_iterations
         self.tolerance = tolerance
 
@@ -204,15 +217,17 @@
 
         Returns:
             the transformation between two images, shape depends on the model,
             typically [1x3x3] tensor for string model_types.
         """
         self.reset_model()
         # ToDo: better parameter passing to optimizer
-        opt: optim.Optimizer = self.optimizer(self.model.parameters(), lr=self.lr)
+        _opt_args: Dict[str, Any] = {}
+        _opt_args['lr'] = self.lr
+        opt = self.optimizer(self.model.parameters(), **_opt_args)
 
         # compute the gaussian pyramids
         # [::-1] because we have to register from coarse to fine
         img_src_pyr = build_pyramid(src_img, self.pyramid_levels)[::-1]
         img_dst_pyr = build_pyramid(dst_img, self.pyramid_levels)[::-1]
         prev_loss = 1e10
         aux_models = []
```

### Comparing `kornia-0.6.9/kornia/geometry/transform/imgwarp.py` & `kornia-0.7.0/kornia/geometry/transform/imgwarp.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,38 +1,39 @@
-from typing import List, Optional, Tuple
+from __future__ import annotations
 
 import torch
 import torch.nn.functional as F
 from torch.nn.functional import grid_sample
 
 from kornia.core import Tensor, concatenate, stack, tensor, zeros
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 from kornia.geometry.conversions import (
-    angle_axis_to_rotation_matrix,
     angle_to_rotation_matrix,
+    axis_angle_to_rotation_matrix,
     convert_affinematrix_to_homography,
     convert_affinematrix_to_homography3d,
     deg2rad,
     normalize_homography,
     normalize_homography3d,
     normalize_pixel_coordinates,
 )
 from kornia.geometry.linalg import transform_points
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 from kornia.utils import create_meshgrid, create_meshgrid3d, eye_like
 from kornia.utils.helpers import _torch_inverse_cast, _torch_solve_cast
 
 __all__ = [
     "warp_perspective",
     "warp_affine",
     "get_perspective_transform",
     "get_rotation_matrix2d",
     "remap",
     "invert_affine_transform",
     "get_affine_matrix2d",
     "get_affine_matrix3d",
+    "get_translation_matrix2d",
     "get_shear_matrix2d",
     "get_shear_matrix3d",
     "warp_affine3d",
     "get_projective_transform",
     "projection_from_Rt",
     "get_perspective_transform3d",
     "warp_perspective3d",
@@ -42,15 +43,15 @@
     "homography_warp3d",
 ]
 
 
 def warp_perspective(
     src: Tensor,
     M: Tensor,
-    dsize: Tuple[int, int],
+    dsize: tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
     fill_value: Tensor = zeros(3),  # needed for jit
 ) -> Tensor:
     r"""Apply a perspective transformation to an image.
 
@@ -128,15 +129,15 @@
         return _fill_and_warp(src, grid, align_corners=align_corners, mode=mode, fill_value=fill_value)
     return F.grid_sample(src, grid, align_corners=align_corners, mode=mode, padding_mode=padding_mode)
 
 
 def warp_affine(
     src: Tensor,
     M: Tensor,
-    dsize: Tuple[int, int],
+    dsize: tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
     fill_value: Tensor = zeros(3),  # needed for jit
 ) -> Tensor:
     r"""Apply an affine transformation to a tensor.
 
@@ -289,15 +290,15 @@
 
     The algorithm is a vanilla implementation of the Direct Linear transform (DLT).
     See more: https://www.cs.cmu.edu/~16385/s17/Slides/10.2_2D_Alignment__DLT.pdf
 
     The function calculates the matrix of a perspective transform that maps from
     the source to destination points:
 
-    .. math ::
+    .. math::
 
         \begin{bmatrix}
         x^{'} \\
         y^{'} \\
         1 \\
         \end{bmatrix}
         =
@@ -463,15 +464,15 @@
 
 def remap(
     image: Tensor,
     map_x: Tensor,
     map_y: Tensor,
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
-    align_corners: Optional[bool] = None,
+    align_corners: bool | None = None,
     normalized_coordinates: bool = False,
 ) -> Tensor:
     r"""Apply a generic geometrical transformation to an image tensor.
 
     .. image:: _static/img/remap.png
 
     The function remap transforms the source tensor using the specified map:
@@ -530,23 +531,23 @@
     return grid_sample(image, map_xy, mode=mode, padding_mode=padding_mode, align_corners=align_corners)
 
 
 def invert_affine_transform(matrix: Tensor) -> Tensor:
     r"""Invert an affine transformation.
 
     The function computes an inverse affine transformation represented by
-    23 matrix:
+    2x3 matrix:
 
     .. math::
         \begin{bmatrix}
             a_{11} & a_{12} & b_{1} \\
             a_{21} & a_{22} & b_{2} \\
         \end{bmatrix}
 
-    The result is also a 23 matrix of the same type as M.
+    The result is also a 2x3 matrix of the same type as M.
 
     Args:
         matrix: original affine transform. The tensor must be
           in the shape of :math:`(B, 2, 3)`.
 
     Return:
         the reverse affine transform with shape :math:`(B, 2, 3)`.
@@ -567,16 +568,16 @@
 
 
 def get_affine_matrix2d(
     translations: Tensor,
     center: Tensor,
     scale: Tensor,
     angle: Tensor,
-    sx: Optional[Tensor] = None,
-    sy: Optional[Tensor] = None,
+    sx: Tensor | None = None,
+    sy: Tensor | None = None,
 ) -> Tensor:
     r"""Compose affine matrix from the components.
 
     Args:
         translations: tensor containing the translation vector with shape :math:`(B, 2)`.
         center: tensor containing the center vector with shape :math:`(B, 2)`.
         scale: tensor containing the scale factor with shape :math:`(B, 2)`.
@@ -599,15 +600,36 @@
     if any(s is not None for s in [sx, sy]):
         shear_mat = get_shear_matrix2d(center, sx, sy)
         transform_h = transform_h @ shear_mat
 
     return transform_h
 
 
-def get_shear_matrix2d(center: Tensor, sx: Optional[Tensor] = None, sy: Optional[Tensor] = None):
+def get_translation_matrix2d(translations: Tensor) -> Tensor:
+    r"""Compose translation matrix from the components.
+
+    Args:
+        translations: tensor containing the translation vector with shape :math:`(B, 2)`.
+
+    Returns:
+        the affine transformation matrix :math:`(B, 3, 3)`.
+
+    .. note::
+        This function is often used in conjunction with :func:`warp_affine`, :func:`warp_perspective`.
+    """
+    transform: Tensor = eye_like(3, translations)[:, :2, :]
+    transform[..., 2] += translations  # tx/ty
+
+    # pad transform to get Bx3x3
+    transform_h = convert_affinematrix_to_homography(transform)
+
+    return transform_h
+
+
+def get_shear_matrix2d(center: Tensor, sx: Tensor | None = None, sy: Tensor | None = None) -> Tensor:
     r"""Compose shear matrix Bx4x4 from the components.
 
     Note: Ordered shearing, shear x-axis then y-axis.
 
     .. math::
         \begin{bmatrix}
             1 & b \\
@@ -654,28 +676,28 @@
 
 
 def get_affine_matrix3d(
     translations: Tensor,
     center: Tensor,
     scale: Tensor,
     angles: Tensor,
-    sxy: Optional[Tensor] = None,
-    sxz: Optional[Tensor] = None,
-    syx: Optional[Tensor] = None,
-    syz: Optional[Tensor] = None,
-    szx: Optional[Tensor] = None,
-    szy: Optional[Tensor] = None,
+    sxy: Tensor | None = None,
+    sxz: Tensor | None = None,
+    syx: Tensor | None = None,
+    syz: Tensor | None = None,
+    szx: Tensor | None = None,
+    szy: Tensor | None = None,
 ) -> Tensor:
     r"""Compose 3d affine matrix from the components.
 
     Args:
         translations: tensor containing the translation vector (dx,dy,dz) with shape :math:`(B, 3)`.
         center: tensor containing the center vector (x,y,z) with shape :math:`(B, 3)`.
         scale: tensor containing the scale factor with shape :math:`(B)`.
-        angle: angle axis vector containing the rotation angles in degrees in the form
+        angle: axis angle vector containing the rotation angles in degrees in the form
             of (rx, ry, rz) with shape :math:`(B, 3)`. Internally it calls Rodrigues to compute
             the rotation matrix from axis-angle.
         sxy: tensor containing the shear factor in the xy-direction with shape :math:`(B)`.
         sxz: tensor containing the shear factor in the xz-direction with shape :math:`(B)`.
         syx: tensor containing the shear factor in the yx-direction with shape :math:`(B)`.
         syz: tensor containing the shear factor in the yz-direction with shape :math:`(B)`.
         szx: tensor containing the shear factor in the zx-direction with shape :math:`(B)`.
@@ -697,21 +719,21 @@
         transform_h = transform_h @ shear_mat
 
     return transform_h
 
 
 def get_shear_matrix3d(
     center: Tensor,
-    sxy: Optional[Tensor] = None,
-    sxz: Optional[Tensor] = None,
-    syx: Optional[Tensor] = None,
-    syz: Optional[Tensor] = None,
-    szx: Optional[Tensor] = None,
-    szy: Optional[Tensor] = None,
-):
+    sxy: Tensor | None = None,
+    sxz: Tensor | None = None,
+    syx: Tensor | None = None,
+    syz: Tensor | None = None,
+    szx: Tensor | None = None,
+    szy: Tensor | None = None,
+) -> Tensor:
     r"""Compose shear matrix Bx4x4 from the components.
     Note: Ordered shearing, shear x-axis then y-axis then z-axis.
 
     .. math::
         \begin{bmatrix}
             1 & o & r & oy + rz \\
             m & p & s & mx + py + sz -y \\
@@ -789,29 +811,31 @@
 
     shear_mat = stack([m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23], -1).view(-1, 3, 4)
     shear_mat = convert_affinematrix_to_homography3d(shear_mat)
 
     return shear_mat
 
 
-def _compute_shear_matrix_3d(sxy_tan, sxz_tan, syx_tan, syz_tan, szx_tan, szy_tan):
+def _compute_shear_matrix_3d(
+    sxy_tan: Tensor, sxz_tan: Tensor, syx_tan: Tensor, syz_tan: Tensor, szx_tan: Tensor, szy_tan: Tensor
+) -> tuple[Tensor, ...]:
     ones = torch.ones_like(sxy_tan)
 
     m00, m10, m20 = ones, sxy_tan, sxz_tan
     m01, m11, m21 = syx_tan, sxy_tan * syx_tan + ones, sxz_tan * syx_tan + syz_tan
     m02 = syx_tan * szy_tan + szx_tan
     m12 = sxy_tan * szx_tan + szy_tan * m11
     m22 = sxz_tan * szx_tan + szy_tan * m21 + ones
     return m00, m10, m20, m01, m11, m21, m02, m12, m22
 
 
 def warp_affine3d(
     src: Tensor,
     M: Tensor,
-    dsize: Tuple[int, int, int],
+    dsize: tuple[int, int, int],
     flags: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
 ) -> Tensor:
     r"""Apply a projective transformation a to 3d tensor.
 
     .. warning::
@@ -837,27 +861,27 @@
         raise AssertionError(src.shape)
     if not (len(M.shape) == 3 and M.shape[-2:] == (3, 4)):
         raise AssertionError(M.shape)
     if len(dsize) != 3:
         raise AssertionError(dsize)
     B, C, D, H, W = src.size()
 
-    size_src: Tuple[int, int, int] = (D, H, W)
-    size_out: Tuple[int, int, int] = dsize
+    size_src: tuple[int, int, int] = (D, H, W)
+    size_out: tuple[int, int, int] = dsize
 
     M_4x4 = convert_affinematrix_to_homography3d(M)  # Bx4x4
 
     # we need to normalize the transformation since grid sample needs -1/1 coordinates
     dst_norm_trans_src_norm: Tensor = normalize_homography3d(M_4x4, size_src, size_out)  # Bx4x4
 
     src_norm_trans_dst_norm = _torch_inverse_cast(dst_norm_trans_src_norm)
     P_norm: Tensor = src_norm_trans_dst_norm[:, :3]  # Bx3x4
 
     # compute meshgrid and apply to input
-    dsize_out: List[int] = [B, C] + list(size_out)
+    dsize_out: list[int] = [B, C, *list(size_out)]
     grid = F.affine_grid(P_norm, dsize_out, align_corners=align_corners)
     return grid_sample(src, grid, align_corners=align_corners, mode=flags, padding_mode=padding_mode)
 
 
 def projection_from_Rt(rmat: Tensor, tvec: Tensor) -> Tensor:
     r"""Compute the projection matrix from Rotation and translation.
 
@@ -887,15 +911,15 @@
     .. warning::
         This API signature it is experimental and might suffer some changes in the future.
 
     The function computes the projection matrix given the center and angles per axis.
 
     Args:
         center: center of the rotation (x,y,z) in the source with shape :math:`(B, 3)`.
-        angles: angle axis vector containing the rotation angles in degrees in the form
+        angles: axis angle vector containing the rotation angles in degrees in the form
             of (rx, ry, rz) with shape :math:`(B, 3)`. Internally it calls Rodrigues to compute
             the rotation matrix from axis-angle.
         scales: scale factor for x-y-z-directions with shape :math:`(B, 3)`.
 
     Returns:
         the projection matrix of 3D rotation with shape :math:`(B, 3, 4)`.
 
@@ -908,16 +932,16 @@
         raise AssertionError(angles.shape)
     if center.device != angles.device:
         raise AssertionError(center.device, angles.device)
     if center.dtype != angles.dtype:
         raise AssertionError(center.dtype, angles.dtype)
 
     # create rotation matrix
-    angle_axis_rad: Tensor = deg2rad(angles)
-    rmat: Tensor = angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3
+    axis_angle_rad: Tensor = deg2rad(angles)
+    rmat: Tensor = axis_angle_to_rotation_matrix(axis_angle_rad)  # Bx3x3
     scaling_matrix: Tensor = eye_like(3, rmat)
     scaling_matrix = scaling_matrix * scales.unsqueeze(dim=1)
     rmat = rmat @ scaling_matrix.to(rmat)
 
     # define matrix to move forth and back to origin
     from_origin_mat = eye_like(4, rmat, shared_memory=False)  # Bx4x4
     from_origin_mat[..., :3, -1] += center
@@ -936,15 +960,15 @@
 
 
 def get_perspective_transform3d(src: Tensor, dst: Tensor) -> Tensor:
     r"""Calculate a 3d perspective transform from four pairs of the corresponding points.
 
     The function calculates the matrix of a perspective transform so that:
 
-    .. math ::
+    .. math::
 
         \begin{bmatrix}
         t_{i}x_{i}^{'} \\
         t_{i}y_{i}^{'} \\
         t_{i}z_{i}^{'} \\
         t_{i} \\
         \end{bmatrix}
@@ -955,29 +979,29 @@
         y_{i} \\
         z_{i} \\
         1 \\
         \end{bmatrix}
 
     where
 
-    .. math ::
+    .. math::
         dst(i) = (x_{i}^{'},y_{i}^{'},z_{i}^{'}), src(i) = (x_{i}, y_{i}, z_{i}), i = 0,1,2,5,7
 
     Concrete math is as below:
 
-    .. math ::
+    .. math::
 
         \[ u_i =\frac{c_{00} * x_i + c_{01} * y_i + c_{02} * z_i + c_{03}}
             {c_{30} * x_i + c_{31} * y_i + c_{32} * z_i + c_{33}} \]
         \[ v_i =\frac{c_{10} * x_i + c_{11} * y_i + c_{12} * z_i + c_{13}}
             {c_{30} * x_i + c_{31} * y_i + c_{32} * z_i + c_{33}} \]
         \[ w_i =\frac{c_{20} * x_i + c_{21} * y_i + c_{22} * z_i + c_{23}}
             {c_{30} * x_i + c_{31} * y_i + c_{32} * z_i + c_{33}} \]
 
-    .. math ::
+    .. math::
 
         \begin{pmatrix}
         x_0 & y_0 & z_0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -x_0*u_0 & -y_0*u_0 & -z_0 * u_0 \\
         x_1 & y_1 & z_1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -x_1*u_1 & -y_1*u_1 & -z_1 * u_1 \\
         x_2 & y_2 & z_2 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -x_2*u_2 & -y_2*u_2 & -z_2 * u_2 \\
         x_5 & y_5 & z_5 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -x_5*u_5 & -y_5*u_5 & -z_5 * u_5 \\
         x_7 & y_7 & z_7 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -x_7*u_7 & -y_7*u_7 & -z_7 * u_7 \\
@@ -1148,15 +1172,15 @@
 
     raise NotImplementedError(f"perspective params for axis `{axis}` is not implemented.")
 
 
 def warp_perspective3d(
     src: Tensor,
     M: Tensor,
-    dsize: Tuple[int, int, int],
+    dsize: tuple[int, int, int],
     flags: str = 'bilinear',
     border_mode: str = 'zeros',
     align_corners: bool = False,
 ) -> Tensor:
     r"""Apply a perspective transformation to an image.
 
     The function warp_perspective transforms the source image using
@@ -1200,15 +1224,15 @@
     d, h, w = src.shape[-3:]
     return _transform_warp_impl3d(src, M, (d, h, w), dsize, flags, border_mode, align_corners)
 
 
 def homography_warp(
     patch_src: Tensor,
     src_homo_dst: Tensor,
-    dsize: Tuple[int, int],
+    dsize: tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = False,
     normalized_coordinates: bool = True,
     normalized_homography: bool = True,
 ) -> Tensor:
     r"""Warp image patches or tensors by normalized 2D homographies.
@@ -1240,18 +1264,16 @@
         >>> H = torch.eye(3)[None]
         >>> out = homography_warp(img, H, (4, 2), align_corners=True, normalized_homography=False)
         >>> print(out.shape)
         torch.Size([1, 4, 4, 2])
     """
     if not src_homo_dst.device == patch_src.device:
         raise TypeError(
-            "Patch and homography must be on the same device. \
-                         Got patch.device: {} src_H_dst.device: {}.".format(
-                patch_src.device, src_homo_dst.device
-            )
+            f"Patch and homography must be on the same device. Got patch.device: {patch_src.device} "
+            f"src_H_dst.device: {src_homo_dst.device}."
         )
     if normalized_homography:
         height, width = dsize
         grid = create_meshgrid(
             height, width, normalized_coordinates=normalized_coordinates, device=patch_src.device, dtype=patch_src.dtype
         )
         warped_grid = warp_grid(grid, src_homo_dst)
@@ -1261,31 +1283,31 @@
         patch_src, src_homo_dst, dsize, mode='bilinear', padding_mode=padding_mode, align_corners=True
     )
 
 
 def _transform_warp_impl3d(
     src: Tensor,
     dst_pix_trans_src_pix: Tensor,
-    dsize_src: Tuple[int, int, int],
-    dsize_dst: Tuple[int, int, int],
+    dsize_src: tuple[int, int, int],
+    dsize_dst: tuple[int, int, int],
     grid_mode: str,
     padding_mode: str,
     align_corners: bool,
 ) -> Tensor:
     """Compute the transform in normalized coordinates and perform the warping."""
     dst_norm_trans_src_norm: Tensor = normalize_homography3d(dst_pix_trans_src_pix, dsize_src, dsize_dst)
 
     src_norm_trans_dst_norm = torch.inverse(dst_norm_trans_src_norm)
     return homography_warp3d(src, src_norm_trans_dst_norm, dsize_dst, grid_mode, padding_mode, align_corners, True)
 
 
 def homography_warp3d(
     patch_src: Tensor,
     src_homo_dst: Tensor,
-    dsize: Tuple[int, int, int],
+    dsize: tuple[int, int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = False,
     normalized_coordinates: bool = True,
 ) -> Tensor:
     r"""Warp image patches or tensors by normalized 3D homographies.
 
@@ -1305,18 +1327,16 @@
     Example:
         >>> input = torch.rand(1, 3, 32, 32)
         >>> homography = torch.eye(3).view(1, 3, 3)
         >>> output = homography_warp(input, homography, (32, 32))
     """
     if not src_homo_dst.device == patch_src.device:
         raise TypeError(
-            "Patch and homography must be on the same device. \
-                         Got patch.device: {} src_H_dst.device: {}.".format(
-                patch_src.device, src_homo_dst.device
-            )
+            f"Patch and homography must be on the same device. Got patch.device: {patch_src.device} "
+            f"src_H_dst.device: {src_homo_dst.device}."
         )
 
     depth, height, width = dsize
     grid = create_meshgrid3d(
         depth, height, width, normalized_coordinates=normalized_coordinates, device=patch_src.device
     )
     warped_grid = warp_grid3d(grid, src_homo_dst)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `kornia-0.6.9/kornia/geometry/transform/pyramid.py` & `kornia-0.7.0/kornia/geometry/transform/pyramid.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,28 @@
+from __future__ import annotations
+
 import math
-from typing import List, Tuple
 
 import torch
 import torch.nn.functional as F
 
 from kornia.core import Module, Tensor, pad, stack, tensor
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.filters import filter2d, gaussian_blur2d
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 
-__all__ = ["PyrDown", "PyrUp", "ScalePyramid", "pyrdown", "pyrup", "build_pyramid", "build_laplacian_pyramid"]
+__all__ = [
+    "PyrDown",
+    "PyrUp",
+    "ScalePyramid",
+    "pyrdown",
+    "pyrup",
+    "build_pyramid",
+    "build_laplacian_pyramid",
+    "upscale_double",
+]
 
 
 def _get_pyramid_gaussian_kernel() -> Tensor:
     """Utility function that return a pre-computed gaussian kernel."""
     return (
         tensor(
             [
@@ -78,15 +88,15 @@
         - Output: :math:`(B, C, H * 2, W * 2)`
 
     Examples:
         >>> input = torch.rand(1, 2, 4, 4)
         >>> output = PyrUp()(input)  # 1x2x8x8
     """
 
-    def __init__(self, border_type: str = 'reflect', align_corners: bool = False):
+    def __init__(self, border_type: str = 'reflect', align_corners: bool = False) -> None:
         super().__init__()
         self.border_type: str = border_type
         self.align_corners: bool = align_corners
 
     def forward(self, input: Tensor) -> Tensor:
         return pyrup(input, self.border_type, self.align_corners)
 
@@ -114,83 +124,71 @@
         - Output 3rd: :math:`[(B, NL), (B, NL), (B, NL), ...]`
 
     Examples:
         >>> input = torch.rand(2, 4, 100, 100)
         >>> sp, sigmas, pds = ScalePyramid(3, 15)(input)
     """
 
-    def __init__(self, n_levels: int = 3, init_sigma: float = 1.6, min_size: int = 15, double_image: bool = False):
+    def __init__(
+        self, n_levels: int = 3, init_sigma: float = 1.6, min_size: int = 15, double_image: bool = False
+    ) -> None:
         super().__init__()
         # 3 extra levels are needed for DoG nms.
         self.n_levels = n_levels
         self.extra_levels: int = 3
         self.init_sigma = init_sigma
         self.min_size = min_size
         self.border = min_size // 2 - 1
         self.sigma_step = 2 ** (1.0 / float(self.n_levels))
         self.double_image = double_image
 
     def __repr__(self) -> str:
         return (
-            self.__class__.__name__
-            + '(n_levels='
-            + str(self.n_levels)
-            + ', '
-            + 'init_sigma='
-            + str(self.init_sigma)
-            + ', '
-            + 'min_size='
-            + str(self.min_size)
-            + ', '
-            + 'extra_levels='
-            + str(self.extra_levels)
-            + ', '
-            + 'border='
-            + str(self.border)
-            + ', '
-            + 'sigma_step='
-            + str(self.sigma_step)
-            + ', '
-            + 'double_image='
-            + str(self.double_image)
-            + ')'
+            f'{self.__class__.__name__}('
+            f'n_levels={self.n_levels}, '
+            f'init_sigma={self.init_sigma}, '
+            f'min_size={self.min_size}, '
+            f'extra_levels={self.extra_levels}, '
+            f'border={self.border}, '
+            f'sigma_step={self.sigma_step}, '
+            f'double_image={self.double_image})'
         )
 
-    def get_kernel_size(self, sigma: float):
+    def get_kernel_size(self, sigma: float) -> int:
         ksize = int(2.0 * 4.0 * sigma + 1.0)
 
         #  matches OpenCV, but may cause padding problem for small images
         #  PyTorch does not allow to pad more than original size.
         #  Therefore there is a hack in forward function
 
         if ksize % 2 == 0:
             ksize += 1
         return ksize
 
-    def get_first_level(self, input: Tensor) -> Tuple[Tensor, float, float]:
+    def get_first_level(self, input: Tensor) -> tuple[Tensor, float, float]:
         pixel_distance = 1.0
         cur_sigma = 0.5
         # Same as in OpenCV up to interpolation difference
         if self.double_image:
-            x = F.interpolate(input, scale_factor=2.0, mode='bilinear', align_corners=False)
+            x = upscale_double(input)
             pixel_distance = 0.5
             cur_sigma *= 2.0
         else:
             x = input
 
         if self.init_sigma > cur_sigma:
             sigma = max(math.sqrt(self.init_sigma**2 - cur_sigma**2), 0.01)
             ksize = self.get_kernel_size(sigma)
             cur_level = gaussian_blur2d(x, (ksize, ksize), (sigma, sigma))
             cur_sigma = self.init_sigma
         else:
             cur_level = x
         return cur_level, cur_sigma, pixel_distance
 
-    def forward(self, x: Tensor) -> Tuple[List[Tensor], List[Tensor], List[Tensor]]:
+    def forward(self, x: Tensor) -> tuple[list[Tensor], list[Tensor], list[Tensor]]:
         bs, _, _, _ = x.size()
         cur_level, cur_sigma, pixel_distance = self.get_first_level(x)
 
         sigmas = [cur_sigma * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device).to(x.dtype)]
         pixel_dists = [pixel_distance * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device).to(x.dtype)]
         pyr = [[cur_level]]
         oct_idx = 0
@@ -199,27 +197,26 @@
             for level_idx in range(1, self.n_levels + self.extra_levels):
                 sigma = cur_sigma * math.sqrt(self.sigma_step**2 - 1.0)
                 ksize = self.get_kernel_size(sigma)
 
                 # Hack, because PyTorch does not allow to pad more than original size.
                 # But for the huge sigmas, one needs huge kernel and padding...
 
-                ksize = min(ksize, min(cur_level.size(2), cur_level.size(3)))
+                ksize = min(ksize, cur_level.size(2), cur_level.size(3))
                 if ksize % 2 == 0:
                     ksize += 1
 
                 cur_level = gaussian_blur2d(cur_level, (ksize, ksize), (sigma, sigma))
                 cur_sigma *= self.sigma_step
                 pyr[-1].append(cur_level)
                 sigmas[-1][:, level_idx] = cur_sigma
                 pixel_dists[-1][:, level_idx] = pixel_distance
             _pyr = pyr[-1][-self.extra_levels]
-            nextOctaveFirstLevel = F.interpolate(
-                _pyr, size=(_pyr.size(-2) // 2, _pyr.size(-1) // 2), mode='nearest'
-            )  # Nearest matches OpenCV SIFT
+            nextOctaveFirstLevel = _pyr[:, :, ::2, ::2]
+
             pixel_distance *= 2.0
             cur_sigma = self.init_sigma
             if min(nextOctaveFirstLevel.size(2), nextOctaveFirstLevel.size(3)) <= self.min_size:
                 break
             pyr.append([nextOctaveFirstLevel])
             sigmas.append(cur_sigma * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device))
             pixel_dists.append(pixel_distance * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device))
@@ -303,15 +300,15 @@
     # blurs upsampled tensor
     x_blur: Tensor = filter2d(x_up, kernel, border_type)
     return x_blur
 
 
 def build_pyramid(
     input: Tensor, max_level: int, border_type: str = 'reflect', align_corners: bool = False
-) -> List[Tensor]:
+) -> list[Tensor]:
     r"""Construct the Gaussian pyramid for a tensor image.
 
     .. image:: _static/img/build_pyramid.png
 
     The function constructs a vector of images and builds the Gaussian pyramid
     by recursively applying pyrDown to the previously built pyramid layers.
 
@@ -331,40 +328,40 @@
     KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
     KORNIA_CHECK(
         isinstance(max_level, int) or max_level < 0,
         f"Invalid max_level, it must be a positive integer. Got: {max_level}",
     )
 
     # create empty list and append the original image
-    pyramid: List[Tensor] = []
+    pyramid: list[Tensor] = []
     pyramid.append(input)
 
     # iterate and downsample
     for _ in range(max_level - 1):
         img_curr: Tensor = pyramid[-1]
         img_down: Tensor = pyrdown(img_curr, border_type, align_corners)
         pyramid.append(img_down)
 
     return pyramid
 
 
-def is_powerof_two(x):
+def is_powerof_two(x: int) -> bool:
     # check if number x is a power of two
-    return x and (not (x & (x - 1)))
+    return bool(x) and (not (x & (x - 1)))
 
 
-def find_next_powerof_two(x):
+def find_next_powerof_two(x: int) -> int:
     # return the nearest power of 2
     n = math.ceil(math.log(x) / math.log(2))
     return 2**n
 
 
 def build_laplacian_pyramid(
     input: Tensor, max_level: int, border_type: str = 'reflect', align_corners: bool = False
-) -> List[Tensor]:
+) -> list[Tensor]:
     r"""Construct the Laplacian pyramid for a tensor image.
 
     The function constructs a vector of images and builds the Laplacian pyramid
     by recursively computing the difference after applying
     pyrUp to the adjacent layer in it's Gaussian pyramid.
 
     See :cite:`burt1987laplacian` for more details.
@@ -395,18 +392,42 @@
     if require_padding:
         # in case of arbitrary shape tensor image need to be padded.
         # Reference: https://stackoverflow.com/a/29967555
         padding = (0, find_next_powerof_two(w) - w, 0, find_next_powerof_two(h) - h)
         input = pad(input, padding, "reflect")
 
     # create gaussian pyramid
-    gaussian_pyramid: List[Tensor] = build_pyramid(input, max_level, border_type, align_corners)
+    gaussian_pyramid: list[Tensor] = build_pyramid(input, max_level, border_type, align_corners)
     # create empty list
-    laplacian_pyramid: List[Tensor] = []
+    laplacian_pyramid: list[Tensor] = []
 
     # iterate and compute difference of adjacent layers in a gaussian pyramid
     for i in range(max_level - 1):
         img_expand: Tensor = pyrup(gaussian_pyramid[i + 1], border_type, align_corners)
         laplacian: Tensor = gaussian_pyramid[i] - img_expand
         laplacian_pyramid.append(laplacian)
     laplacian_pyramid.append(gaussian_pyramid[-1])
     return laplacian_pyramid
+
+
+def upscale_double(x: Tensor) -> Tensor:
+    r"""Upscale image by the factor of 2, even indices maps to original indices.
+
+    Odd indices are linearly interpolated from the even ones.
+
+    Args:
+        x: input image.
+
+    Shape:
+        - Input: :math:`(*, H, W)`
+        - Output :math:`(*, H, W)`
+    """
+    KORNIA_CHECK_IS_TENSOR(x)
+    KORNIA_CHECK_SHAPE(x, ["*", "H", "W"])
+    double_shape = x.shape[:-2] + (x.shape[-2] * 2, x.shape[-1] * 2)
+    upscaled = torch.zeros(double_shape, device=x.device, dtype=x.dtype)
+    upscaled[..., ::2, ::2] = x
+    upscaled[..., ::2, 1::2][..., :-1] = (upscaled[..., ::2, ::2][..., :-1] + upscaled[..., ::2, 2::2]) / 2
+    upscaled[..., ::2, -1] = upscaled[..., ::2, -2]
+    upscaled[..., 1::2, :][..., :-1, :] = (upscaled[..., ::2, :][..., :-1, :] + upscaled[..., 2::2, :]) / 2
+    upscaled[..., -1, :] = upscaled[..., -2, :]
+    return upscaled
```

### Comparing `kornia-0.6.9/kornia/geometry/transform/thin_plate_spline.py` & `kornia-0.7.0/kornia/geometry/transform/thin_plate_spline.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Tuple
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from kornia.utils import create_meshgrid
 from kornia.utils.helpers import _torch_solve_cast
 
 __all__ = ["get_tps_transform", "warp_points_tps", "warp_image_tps"]
 
 # utilities for computing thin plate spline transforms
@@ -22,16 +22,17 @@
     square_dist = square_dist.clamp(min=0)  # handle possible numerical errors
     return square_dist
 
 
 def _kernel_distance(squared_distances: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
     r"""Compute the TPS kernel distance function: :math:`r^2 log(r)`, where `r` is the euclidean distance.
 
-    Since :math:`\log(r) = 1/2 \log(r^2)`, this function takes the squared distance matrix and calculates
-    :math:`0.5 r^2 log(r^2)`.
+    Since
+    :math: `\log(r) = 1/2 \log(r^2)`, this function takes the squared distance matrix and calculates
+    :math: `0.5 r^2 log(r^2)`.
     """
     # r^2 * log(r) = 1/2 * r^2 * log(r^2)
     return 0.5 * squared_distances * squared_distances.add(eps).log()
 
 
 def get_tps_transform(points_src: torch.Tensor, points_dst: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
     r"""Compute the TPS transform parameters that warp source points to target points.
```

### Comparing `kornia-0.6.9/kornia/geometry/vector.py` & `kornia-0.7.0/kornia/geometry/vector.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,28 +1,34 @@
 from typing import Optional, Tuple, Union, cast
 
-from kornia.core import Tensor, as_tensor, normalize, rand, stack
-from kornia.core.tensor_wrapper import TensorWrapper, wrap
+from kornia.core import Device, Dtype, Tensor, as_tensor, normalize, rand, stack
+from kornia.core.check import KORNIA_CHECK
+from kornia.core.tensor_wrapper import TensorWrapper, wrap  # type: ignore[attr-defined]
 from kornia.geometry.linalg import batched_dot_product, batched_squared_norm
-from kornia.testing import KORNIA_CHECK
 
-__all__ = ["Vector3", "Scalar"]
+__all__ = ["Vector3", "Vector2", "Scalar"]
 
 
 # TODO: implement more functionality to validate
 class Scalar(TensorWrapper):
     def __init__(self, data: Tensor) -> None:
         super().__init__(data)
 
 
 class Vector3(TensorWrapper):
     def __init__(self, vector: Tensor) -> None:
         super().__init__(vector)
         KORNIA_CHECK(vector.shape[-1] == 3)
 
+    def __repr__(self) -> str:
+        return f"x: {self.x}\ny: {self.y}\nz: {self.z}"
+
+    def __getitem__(self, idx: Union[slice, int, Tensor]) -> "Vector3":
+        return Vector3(self.data[idx, ...])
+
     @property
     def x(self) -> Tensor:
         return self.data[..., 0]
 
     @property
     def y(self) -> Tensor:
         return self.data[..., 1]
@@ -37,18 +43,20 @@
     def dot(self, right: "Vector3") -> Scalar:
         return Scalar(batched_dot_product(self.data, right.data))
 
     def squared_norm(self) -> Scalar:
         return Scalar(batched_squared_norm(self.data))
 
     @classmethod
-    def random(cls, shape: Optional[Tuple[int, ...]] = None, device=None, dtype=None) -> "Vector3":
+    def random(
+        cls, shape: Optional[Tuple[int, ...]] = None, device: Optional[Device] = None, dtype: Dtype = None
+    ) -> "Vector3":
         if shape is None:
             shape = ()
-        return cls(rand(shape + (3,), device=device, dtype=dtype))
+        return cls(rand((*shape, 3), device=device, dtype=dtype))
 
     # TODO: polish overload
     # @overload
     # @classmethod
     # def from_coords(
     #     cls, x: Tensor, y: Tensor, z: Tensor, device=None, dtype=None
     # ) -> "Vector3":
@@ -64,26 +72,79 @@
     # ) -> "Vector3":
     #     KORNIA_CHECK(isinstance(x, float))
     #     KORNIA_CHECK(type(x) == type(y) == type(z))
     #     return wrap(as_tensor((x, y, z), device=device, dtype=dtype), Vector3)
 
     @classmethod
     def from_coords(
-        cls, x: Union[float, Tensor], y: Union[float, Tensor], z: Union[float, Tensor], device=None, dtype=None
+        cls,
+        x: Union[float, Tensor],
+        y: Union[float, Tensor],
+        z: Union[float, Tensor],
+        device: Optional[Device] = None,
+        dtype: Dtype = None,
     ) -> "Vector3":
         KORNIA_CHECK(type(x) == type(y) == type(z))
         KORNIA_CHECK(isinstance(x, (Tensor, float)))
         if isinstance(x, float):
             return wrap(as_tensor((x, y, z), device=device, dtype=dtype), Vector3)
         # TODO: this is totally insane ...
         tensors: Tuple[Tensor, ...] = (x, cast(Tensor, y), cast(Tensor, z))
         return wrap(stack(tensors, -1), Vector3)
 
 
+class Vector2(TensorWrapper):
+    def __init__(self, vector: Tensor) -> None:
+        super().__init__(vector)
+        KORNIA_CHECK(vector.shape[-1] == 2)
+
+    def __repr__(self) -> str:
+        return f"x: {self.x}\ny: {self.y}"
+
+    def __getitem__(self, idx: Union[slice, int, Tensor]) -> "Vector2":
+        return Vector2(self.data[idx, ...])
+
+    @property
+    def x(self) -> Tensor:
+        return self.data[..., 0]
+
+    @property
+    def y(self) -> Tensor:
+        return self.data[..., 1]
+
+    def normalized(self) -> "Vector2":
+        return Vector2(normalize(self.data, p=2, dim=-1))
+
+    def dot(self, right: "Vector2") -> Scalar:
+        return Scalar(batched_dot_product(self.data, right.data))
+
+    def squared_norm(self) -> Scalar:
+        return Scalar(batched_squared_norm(self.data))
+
+    @classmethod
+    def random(cls, shape: Optional[Tuple[int, ...]] = None, device: Device = None, dtype: Dtype = None) -> "Vector2":
+        if shape is None:
+            shape = ()
+        return cls(rand((*shape, 2), device=device, dtype=dtype))
+
+    @classmethod
+    def from_coords(
+        cls, x: Union[float, Tensor], y: Union[float, Tensor], device: Device = None, dtype: Dtype = None
+    ) -> "Vector2":
+        KORNIA_CHECK(type(x) == type(y))
+        KORNIA_CHECK(isinstance(x, (Tensor, float)))
+        if isinstance(x, float):
+            return wrap(as_tensor((x, y), device=device, dtype=dtype), Vector2)
+        # TODO: this is totally insane ...
+        tensors: Tuple[Tensor, ...] = (x, cast(Tensor, y))
+        return wrap(stack(tensors, -1), Vector2)
+
+
 Vec3 = Vector3
+Vec2 = Vector2
 
 # TODO: adapt to TensorWrapper
 
 # class UnitVector(Module):
 #     def __init__(self, vector: Tensor) -> None:
 #         super().__init__()
 #         KORNIA_CHECK_SHAPE(vector, ["B", "N"])
```

### Comparing `kornia-0.6.9/kornia/grad_estimator/ste.py` & `kornia-0.7.0/kornia/grad_estimator/ste.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 from typing import Any, Callable, Optional, Tuple
 
-import torch.nn as nn
-from torch import Tensor
+from torch import Tensor, nn
 from torch.autograd import Function
 
 __all__ = ["STEFunction", "StraightThroughEstimator"]
 
 
 class STEFunction(Function):
     """Straight-Through Estimation (STE) function.
@@ -101,20 +100,20 @@
         >>> input.grad
         tensor([[[[0.0422, 0.0626, 0.0566, 0.0422],
                   [0.0566, 0.0626, 0.0626, 0.0626],
                   [0.0626, 0.0626, 0.0626, 0.0566],
                   [0.0422, 0.0566, 0.0626, 0.0422]]]])
     """
 
-    def __init__(self, target_fn: nn.Module, grad_fn: Optional[Callable[..., Any]] = None):
+    def __init__(self, target_fn: nn.Module, grad_fn: Optional[Callable[..., Any]] = None) -> None:
         super().__init__()
         self.target_fn = target_fn
         self.grad_fn = grad_fn
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return f"{self.__class__.__name__}(target_fn={self.target_fn}, grad_fn={self.grad_fn})"
 
     def forward(self, input: Tensor) -> Tensor:
         out = self.target_fn(input)
         if not isinstance(out, Tensor):
             raise NotImplementedError(
                 "Only Tensor is supported at the moment. Feel free to contribute to https://github.com/kornia/kornia."
```

### Comparing `kornia-0.6.9/kornia/io/io.py` & `kornia-0.7.0/kornia/io/io.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,74 +1,81 @@
+from __future__ import annotations
+
 try:
     import kornia_rs
 except ImportError:
     kornia_rs = None
 
 import os
 from enum import Enum
+from pathlib import Path
 
 import torch
-from torch.utils import dlpack  # TODO: remove this  if kornia relies on torch>=1.10
+from torch.utils import dlpack  # TODO: remove this if kornia relies on torch>=1.10
 
 from kornia.color import rgb_to_grayscale, rgba_to_rgb
 from kornia.color.gray import grayscale_to_rgb
 from kornia.color.rgb import rgb_to_rgba
-from kornia.core import Tensor
-from kornia.testing import KORNIA_CHECK
+from kornia.core import Device, Tensor
+from kornia.core.check import KORNIA_CHECK
 
 
 class ImageLoadType(Enum):
     r"""Enum to specify the desired image type."""
     UNCHANGED = 0
     GRAY8 = 1
     RGB8 = 2
     RGBA8 = 3
     GRAY32 = 4
     RGB32 = 5
 
 
-def load_image_to_tensor(path_file: str, device: str) -> Tensor:
+def load_image_to_tensor(path_file: str, device: Device) -> Tensor:
     # load the file and decodes using kornia_rs. Internally it uses a package that
     # combines image-rs a self maintained version of the dlpack-rs. After the decoding,
     # the obtained stream bits are encapusalted to a cv::Tensor data structure without
     # memory ownership and passed as PyCapsule from rust to python.
     cv_tensor = kornia_rs.read_image_rs(path_file)
     # for convenience use the torch dlpack parser to get a zero copy torch.Tensor
     # TODO: evaluate other potential API so that we can return in numpy, jax, mxnet since
     # the kornia_rs cv::Tensor has this ability.
     th_tensor = dlpack.from_dlpack(cv_tensor)  # HxWx3
     # move the tensor to the desired device, move the data layout to CHW and clone
     # to return an owned data tensor.
-    return th_tensor.to(torch.device(device)).permute(2, 0, 1).clone()  # CxHxW
+    dev = device if isinstance(device, torch.device) or device is None else torch.device(device)
+    return th_tensor.to(device=dev).permute(2, 0, 1).clone()  # CxHxW
 
 
 def to_float32(image: Tensor) -> Tensor:
     KORNIA_CHECK(image.dtype == torch.uint8)
     return image.float() / 255.0
 
 
 def to_uint8(image: Tensor) -> Tensor:
     KORNIA_CHECK(image.dtype == torch.float32)
     return image.mul(255.0).byte()
 
 
-def load_image(path_file: str, desired_type: ImageLoadType, device: str = "cpu") -> Tensor:
+def load_image(path_file: str | Path, desired_type: ImageLoadType, device: Device = "cpu") -> Tensor:
     """Read an image file and decode using the Kornia Rust backend.
 
     Args:
         path_file: Path to a valid image file.
         desired_type: the desired image type, defined by color space and dtype.
         device: the device where you want to get your image placed.
 
     Return:
         Image tensor with shape :math:`(3,H,W)`.
     """
     if kornia_rs is None:
         raise ModuleNotFoundError("The io API is not available: `pip install kornia_rs` in a Linux system.")
 
+    if isinstance(path_file, Path):
+        path_file = str(path_file)
+
     KORNIA_CHECK(os.path.isfile(path_file), f"Invalid file: {path_file}")
     image: Tensor = load_image_to_tensor(path_file, device)  # CxHxW
 
     if desired_type == ImageLoadType.UNCHANGED:
         return image
     elif desired_type == ImageLoadType.GRAY8:
         if image.shape[0] == 1 and image.dtype == torch.uint8:
@@ -103,7 +110,46 @@
             return to_float32(image)
         elif image.shape[0] == 1 and image.dtype == torch.uint8:
             rgb32 = grayscale_to_rgb(to_float32(image))
             return rgb32
     else:
         raise NotImplementedError(f"Unknown type: {desired_type}")
     return Tensor([])
+
+
+def write_image(path_file: str | Path, image: Tensor) -> None:
+    """Save an image file using the Kornia Rust backend.
+
+    For now, we only support the writing of JPEG of the following types: RGB8.
+
+    Args:
+        path_file: Path to a valid image file.
+        image: Image tensor with shape :math:`(3,H,W)`.
+
+    Return:
+        None.
+    """
+    if kornia_rs is None:
+        raise ModuleNotFoundError("The io API is not available: `pip install kornia_rs` in a Linux system.")
+
+    if isinstance(path_file, Path):
+        path_file = str(path_file)
+
+    KORNIA_CHECK("jpg" in path_file[-3:], f"Invalid file extension: {path_file}")
+    KORNIA_CHECK(image.dim() == 3 and image.shape[0] == 3, f"Invalid image shape: {image.shape}")
+    KORNIA_CHECK(image.dtype == torch.uint8, f"Invalid image dtype: {image.dtype}")
+
+    # create the image encoder
+    image_encoder = kornia_rs.ImageEncoder()
+    image_encoder.set_quality(100)
+
+    # move the tensor to the cpu and clone to avoid memory ownership issues.
+    image = image.cpu().clone()  # 3xHxW
+
+    # move the data layout to HWC and convert to numpy
+    image_np = image.permute(1, 2, 0).numpy()  # HxWx3
+
+    # encode the image using the kornia_rs
+    image_encoded: list[int] = image_encoder.encode(image_np.tobytes(), image_np.shape)
+
+    # save the image using the
+    kornia_rs.write_image_jpeg(path_file, image_encoded)
```

### Comparing `kornia-0.6.9/kornia/losses/__init__.py` & `kornia-0.7.0/kornia/losses/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,19 +1,26 @@
+from __future__ import annotations
+
+from .cauchy import CauchyLoss, cauchy_loss
+from .charbonnier import CharbonnierLoss, charbonnier_loss
 from .depth_smooth import InverseDepthSmoothnessLoss, inverse_depth_smoothness_loss
 from .dice import DiceLoss, dice_loss
 from .divergence import js_div_loss_2d, kl_div_loss_2d
 from .focal import BinaryFocalLossWithLogits, FocalLoss, binary_focal_loss_with_logits, focal_loss
+from .geman_mcclure import GemanMcclureLoss, geman_mcclure_loss
 from .hausdorff import HausdorffERLoss, HausdorffERLoss3D
 from .lovasz_hinge import LovaszHingeLoss, lovasz_hinge_loss
 from .lovasz_softmax import LovaszSoftmaxLoss, lovasz_softmax_loss
 from .ms_ssim import MS_SSIMLoss
 from .psnr import PSNRLoss, psnr_loss
 from .ssim import SSIMLoss, ssim_loss
+from .ssim3d import SSIM3DLoss, ssim3d_loss
 from .total_variation import TotalVariation, total_variation
 from .tversky import TverskyLoss, tversky_loss
+from .welsch import WelschLoss, welsch_loss
 
 __all__ = [
     "inverse_depth_smoothness_loss",
     "InverseDepthSmoothnessLoss",
     "dice_loss",
     "DiceLoss",
     "js_div_loss_2d",
@@ -24,17 +31,27 @@
     "FocalLoss",
     "HausdorffERLoss",
     "HausdorffERLoss3D",
     "psnr_loss",
     "PSNRLoss",
     "ssim_loss",
     "SSIMLoss",
+    "ssim3d_loss",
+    "SSIM3DLoss",
     "total_variation",
     "TotalVariation",
     "tversky_loss",
     "TverskyLoss",
     "MS_SSIMLoss",
     "LovaszHingeLoss",
     "lovasz_hinge_loss",
     "LovaszSoftmaxLoss",
     "lovasz_softmax_loss",
+    "WelschLoss",
+    "welsch_loss",
+    "CauchyLoss",
+    "cauchy_loss",
+    "GemanMcclureLoss",
+    "geman_mcclure_loss",
+    "CharbonnierLoss",
+    "charbonnier_loss",
 ]
```

### Comparing `kornia-0.6.9/kornia/losses/depth_smooth.py` & `kornia-0.7.0/kornia/losses/depth_smooth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,11 @@
+from __future__ import annotations
+
 import torch
-import torch.nn as nn
+from torch import nn
 
 # Based on
 # https://github.com/tensorflow/models/blob/master/research/struct2depth/model.py#L625-L641
 
 
 def _gradient_x(img: torch.Tensor) -> torch.Tensor:
     if len(img.shape) != 4:
```

### Comparing `kornia-0.6.9/kornia/losses/divergence.py` & `kornia-0.7.0/kornia/losses/divergence.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,39 +1,43 @@
 r"""Losses based on the divergence between probability distributions."""
 
+from __future__ import annotations
+
 import torch
 import torch.nn.functional as F
 
+from kornia.core import Tensor
+
 
-def _kl_div_2d(p: torch.Tensor, q: torch.Tensor) -> torch.Tensor:
+def _kl_div_2d(p: Tensor, q: Tensor) -> Tensor:
     # D_KL(P || Q)
     batch, chans, height, width = p.shape
     unsummed_kl = F.kl_div(
         q.reshape(batch * chans, height * width).log(), p.reshape(batch * chans, height * width), reduction='none'
     )
     kl_values = unsummed_kl.sum(-1).view(batch, chans)
     return kl_values
 
 
-def _js_div_2d(p: torch.Tensor, q: torch.Tensor) -> torch.Tensor:
+def _js_div_2d(p: Tensor, q: Tensor) -> Tensor:
     # JSD(P || Q)
     m = 0.5 * (p + q)
     return 0.5 * _kl_div_2d(p, m) + 0.5 * _kl_div_2d(q, m)
 
 
 # TODO: add this to the main module
 
 
-def _reduce_loss(losses: torch.Tensor, reduction: str) -> torch.Tensor:
+def _reduce_loss(losses: Tensor, reduction: str) -> Tensor:
     if reduction == 'none':
         return losses
     return torch.mean(losses) if reduction == 'mean' else torch.sum(losses)
 
 
-def js_div_loss_2d(input: torch.Tensor, target: torch.Tensor, reduction: str = 'mean'):
+def js_div_loss_2d(input: Tensor, target: Tensor, reduction: str = 'mean') -> Tensor:
     r"""Calculate the Jensen-Shannon divergence loss between heatmaps.
 
     Args:
         input: the input tensor with shape :math:`(B, N, H, W)`.
         target: the target tensor with shape :math:`(B, N, H, W)`.
         reduction: Specifies the reduction to apply to the
           output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction
@@ -46,15 +50,15 @@
         >>> loss = js_div_loss_2d(input, input)
         >>> loss.item()
         0.0
     """
     return _reduce_loss(_js_div_2d(target, input), reduction)
 
 
-def kl_div_loss_2d(input: torch.Tensor, target: torch.Tensor, reduction: str = 'mean'):
+def kl_div_loss_2d(input: Tensor, target: Tensor, reduction: str = 'mean') -> Tensor:
     r"""Calculate the Kullback-Leibler divergence loss between heatmaps.
 
     Args:
         input: the input tensor with shape :math:`(B, N, H, W)`.
         target: the target tensor with shape :math:`(B, N, H, W)`.
         reduction: Specifies the reduction to apply to the
           output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction
```

### Comparing `kornia-0.6.9/kornia/losses/focal.py` & `kornia-0.7.0/kornia/losses/focal.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,93 +1,104 @@
-import warnings
-from typing import Optional
+from __future__ import annotations
 
 import torch
-import torch.nn as nn
+from torch import nn
 
-from kornia.core import Tensor
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
+from kornia.core import Tensor, tensor
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.utils.one_hot import one_hot
 
 # based on:
 # https://github.com/zhezh/focalloss/blob/master/focalloss.py
 
 
 def focal_loss(
     input: Tensor,
     target: Tensor,
-    alpha: float,
+    alpha: float | None,
     gamma: float = 2.0,
     reduction: str = 'none',
-    eps: Optional[float] = None,
+    weight: Tensor | None = None,
 ) -> Tensor:
     r"""Criterion that computes Focal loss.
 
     According to :cite:`lin2018focal`, the Focal loss is computed as follows:
 
     .. math::
 
         \text{FL}(p_t) = -\alpha_t (1 - p_t)^{\gamma} \, \text{log}(p_t)
 
     Where:
        - :math:`p_t` is the model's estimated probability for each class.
 
     Args:
         input: logits tensor with shape :math:`(N, C, *)` where C = number of classes.
-        target: labels tensor with shape :math:`(N, *)` where each value is :math:`0  targets[i]  C1`.
+        target: labels tensor with shape :math:`(N, *)` where each value is an integer
+          representing correct classification :math:`target[i] \in [0, C)`.
         alpha: Weighting factor :math:`\alpha \in [0, 1]`.
         gamma: Focusing parameter :math:`\gamma >= 0`.
         reduction: Specifies the reduction to apply to the
           output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction
           will be applied, ``'mean'``: the sum of the output will be divided by
           the number of elements in the output, ``'sum'``: the output will be
           summed.
-        eps: Deprecated: scalar to enforce numerical stabiliy. This is no longer used.
+        weight: weights for classes with shape :math:`(num\_of\_classes,)`.
 
     Return:
         the computed loss.
 
     Example:
-        >>> N = 5  # num_classes
-        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)
-        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)
-        >>> output = focal_loss(input, target, alpha=0.5, gamma=2.0, reduction='mean')
+        >>> C = 5  # num_classes
+        >>> input = torch.randn(1, C, 3, 5, requires_grad=True)
+        >>> target = torch.randint(C, (1, 3, 5))
+        >>> kwargs = {"alpha": 0.5, "gamma": 2.0, "reduction": 'mean'}
+        >>> output = focal_loss(input, target, **kwargs)
         >>> output.backward()
     """
-    if eps is not None and not torch.jit.is_scripting():
-        warnings.warn(
-            "`focal_loss` has been reworked for improved numerical stability "
-            "and the `eps` argument is no longer necessary",
-            DeprecationWarning,
-            stacklevel=2,
-        )
 
     KORNIA_CHECK_SHAPE(input, ["B", "C", "*"])
-
-    n = input.shape[0]
-    out_size = (n,) + input.shape[2:]
-
-    KORNIA_CHECK(target.shape[1:] == input.shape[2:], f'Expected target size {out_size}, got {target.size()}')
+    out_size = (input.shape[0],) + input.shape[2:]
+    KORNIA_CHECK(
+        (input.shape[0] == target.shape[0] and target.shape[1:] == input.shape[2:]),
+        f'Expected target size {out_size}, got {target.shape}',
+    )
     KORNIA_CHECK(
         input.device == target.device,
         f"input and target must be in the same device. Got: {input.device} and {target.device}",
     )
 
-    # compute softmax over the classes axis
-    input_soft: Tensor = input.softmax(1)
-    log_input_soft: Tensor = input.log_softmax(1)
-
     # create the labels one hot tensor
     target_one_hot: Tensor = one_hot(target, num_classes=input.shape[1], device=input.device, dtype=input.dtype)
 
+    # compute softmax over the classes axis
+    log_input_soft: Tensor = input.log_softmax(1)
+
     # compute the actual focal loss
-    weight = torch.pow(-input_soft + 1.0, gamma)
+    loss_tmp: Tensor = -torch.pow(1.0 - log_input_soft.exp(), gamma) * log_input_soft * target_one_hot
 
-    focal = -alpha * weight * log_input_soft
-    loss_tmp = torch.einsum('bc...,bc...->b...', (target_one_hot, focal))
+    num_of_classes = input.shape[1]
+    boradcast_dims = [-1] + [1] * len(input.shape[2:])
+    if alpha is not None:
+        alpha_fac = tensor([1 - alpha] + [alpha] * (num_of_classes - 1), dtype=loss_tmp.dtype, device=loss_tmp.device)
+        alpha_fac = alpha_fac.view(boradcast_dims)
+        loss_tmp = alpha_fac * loss_tmp
+
+    if weight is not None:
+        KORNIA_CHECK_IS_TENSOR(weight, "weight must be Tensor or None.")
+        KORNIA_CHECK(
+            (weight.shape[0] == num_of_classes and weight.numel() == num_of_classes),
+            f'weight shape must be (num_of_classes,): ({num_of_classes},), got {weight.shape}',
+        )
+        KORNIA_CHECK(
+            weight.device == input.device,
+            f"weight and input must be in the same device. Got: {weight.device} and {input.device}",
+        )
+
+        weight = weight.view(boradcast_dims)
+        loss_tmp = weight * loss_tmp
 
     if reduction == 'none':
         loss = loss_tmp
     elif reduction == 'mean':
         loss = torch.mean(loss_tmp)
     elif reduction == 'sum':
         loss = torch.sum(loss_tmp)
@@ -112,114 +123,137 @@
         alpha: Weighting factor :math:`\alpha \in [0, 1]`.
         gamma: Focusing parameter :math:`\gamma >= 0`.
         reduction: Specifies the reduction to apply to the
           output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction
           will be applied, ``'mean'``: the sum of the output will be divided by
           the number of elements in the output, ``'sum'``: the output will be
           summed.
-        eps: Deprecated: scalar to enforce numerical stability. This is no longer
-          used.
+        weight: weights for classes with shape :math:`(num\_of\_classes,)`.
 
     Shape:
         - Input: :math:`(N, C, *)` where C = number of classes.
-        - Target: :math:`(N, *)` where each value is
-          :math:`0  targets[i]  C1`.
+        - Target: :math:`(N, *)` where each value is an integer
+          representing correct classification :math:`target[i] \in [0, C)`.
 
     Example:
-        >>> N = 5  # num_classes
+        >>> C = 5  # num_classes
+        >>> input = torch.randn(1, C, 3, 5, requires_grad=True)
+        >>> target = torch.randint(C, (1, 3, 5))
         >>> kwargs = {"alpha": 0.5, "gamma": 2.0, "reduction": 'mean'}
         >>> criterion = FocalLoss(**kwargs)
-        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)
-        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)
         >>> output = criterion(input, target)
         >>> output.backward()
     """
 
-    def __init__(self, alpha: float, gamma: float = 2.0, reduction: str = 'none', eps: Optional[float] = None) -> None:
+    def __init__(
+        self, alpha: float | None, gamma: float = 2.0, reduction: str = 'none', weight: Tensor | None = None
+    ) -> None:
         super().__init__()
-        self.alpha: float = alpha
+        self.alpha: float | None = alpha
         self.gamma: float = gamma
         self.reduction: str = reduction
-        self.eps: Optional[float] = eps
+        self.weight: Tensor | None = weight
 
     def forward(self, input: Tensor, target: Tensor) -> Tensor:
-        return focal_loss(input, target, self.alpha, self.gamma, self.reduction, self.eps)
+        return focal_loss(input, target, self.alpha, self.gamma, self.reduction, self.weight)
 
 
 def binary_focal_loss_with_logits(
     input: Tensor,
     target: Tensor,
-    alpha: float = 0.25,
+    alpha: float | None = 0.25,
     gamma: float = 2.0,
     reduction: str = 'none',
-    eps: Optional[float] = None,
-    pos_weight: Optional[Tensor] = None,
+    pos_weight: Tensor | None = None,
+    weight: Tensor | None = None,
 ) -> Tensor:
-    r"""Function that computes Binary Focal loss.
+    r"""Criterion that computes Binary Focal loss.
+
+    According to :cite:`lin2018focal`, the Focal loss is computed as follows:
 
     .. math::
 
         \text{FL}(p_t) = -\alpha_t (1 - p_t)^{\gamma} \, \text{log}(p_t)
 
     where:
        - :math:`p_t` is the model's estimated probability for each class.
 
     Args:
-        input: input data tensor of arbitrary shape.
-        target: the target tensor with shape matching input.
-        alpha: Weighting factor for the rare class :math:`\alpha \in [0, 1]`.
+        input: logits tensor with shape :math:`(N, C, *)` where C = number of classes.
+        target: labels tensor with the same shape as input :math:`(N, C, *)`
+          where each value is between 0 and 1.
+        alpha: Weighting factor :math:`\alpha \in [0, 1]`.
         gamma: Focusing parameter :math:`\gamma >= 0`.
         reduction: Specifies the reduction to apply to the
           output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction
           will be applied, ``'mean'``: the sum of the output will be divided by
           the number of elements in the output, ``'sum'``: the output will be
           summed.
-        eps: Deprecated: scalar for numerically stability when dividing. This is no longer used.
-        pos_weight: a weight of positive examples.
-          Its possible to trade off recall and precision by adding weights to positive examples.
-          Must be a vector with length equal to the number of classes.
+        pos_weight: a weight of positive examples with shape :math:`(num\_of\_classes,)`.
+          It is possible to trade off recall and precision by adding weights to positive examples.
+        weight: weights for classes with shape :math:`(num\_of\_classes,)`.
 
     Returns:
         the computed loss.
 
     Examples:
+        >>> C = 3  # num_classes
+        >>> input = torch.randn(1, C, 5, requires_grad=True)
+        >>> target = torch.randint(2, (1, C, 5))
         >>> kwargs = {"alpha": 0.25, "gamma": 2.0, "reduction": 'mean'}
-        >>> logits = torch.tensor([[[6.325]],[[5.26]],[[87.49]]])
-        >>> labels = torch.tensor([[[1.]],[[1.]],[[0.]]])
-        >>> binary_focal_loss_with_logits(logits, labels, **kwargs)
-        tensor(21.8725)
+        >>> output = binary_focal_loss_with_logits(input, target, **kwargs)
+        >>> output.backward()
     """
 
-    if eps is not None and not torch.jit.is_scripting():
-        warnings.warn(
-            "`binary_focal_loss_with_logits` has been reworked for improved numerical stability "
-            "and the `eps` argument is no longer necessary",
-            DeprecationWarning,
-            stacklevel=2,
-        )
-
     KORNIA_CHECK_SHAPE(input, ["B", "C", "*"])
+    KORNIA_CHECK(input.shape == target.shape, f'Expected target size {input.shape}, got {target.shape}')
     KORNIA_CHECK(
-        input.shape[0] == target.shape[0],
-        f'Expected input batch_size ({input.shape[0]}) to match target batch_size ({target.shape[0]}).',
+        input.device == target.device,
+        f"input and target must be in the same device. Got: {input.device} and {target.device}",
     )
 
-    if pos_weight is None:
-        pos_weight = torch.ones(input.shape[-1], device=input.device, dtype=input.dtype)
+    log_probs_pos: Tensor = nn.functional.logsigmoid(input)
+    log_probs_neg: Tensor = nn.functional.logsigmoid(-input)
 
-    KORNIA_CHECK_IS_TENSOR(pos_weight)
-    KORNIA_CHECK(input.shape[-1] == pos_weight.shape[0], "Expected pos_weight equals number of classes.")
+    pos_term: Tensor = -log_probs_neg.exp().pow(gamma) * target * log_probs_pos
+    neg_term: Tensor = -log_probs_pos.exp().pow(gamma) * (1.0 - target) * log_probs_neg
+    if alpha is not None:
+        pos_term = alpha * pos_term
+        neg_term = (1.0 - alpha) * neg_term
+
+    num_of_classes = input.shape[1]
+    boradcast_dims = [-1] + [1] * len(input.shape[2:])
+    if pos_weight is not None:
+        KORNIA_CHECK_IS_TENSOR(pos_weight, "pos_weight must be Tensor or None.")
+        KORNIA_CHECK(
+            (pos_weight.shape[0] == num_of_classes and pos_weight.numel() == num_of_classes),
+            f'pos_weight shape must be (num_of_classes,): ({num_of_classes},), got {pos_weight.shape}',
+        )
+        KORNIA_CHECK(
+            pos_weight.device == input.device,
+            f"pos_weight and input must be in the same device. Got: {pos_weight.device} and {input.device}",
+        )
 
-    probs_pos = input.sigmoid()
-    probs_neg = (-input).sigmoid()
+        pos_weight = pos_weight.view(boradcast_dims)
+        pos_term = pos_weight * pos_term
 
-    loss_tmp = (
-        -alpha * pos_weight * probs_neg.pow(gamma) * target * probs_pos.log()
-        - (1 - alpha) * probs_pos.pow(gamma) * (1.0 - target) * probs_neg.log()
-    )
+    loss_tmp: Tensor = pos_term + neg_term
+    if weight is not None:
+        KORNIA_CHECK_IS_TENSOR(weight, "weight must be Tensor or None.")
+        KORNIA_CHECK(
+            (weight.shape[0] == num_of_classes and weight.numel() == num_of_classes),
+            f'weight shape must be (num_of_classes,): ({num_of_classes},), got {weight.shape}',
+        )
+        KORNIA_CHECK(
+            weight.device == input.device,
+            f"weight and input must be in the same device. Got: {weight.device} and {input.device}",
+        )
+
+        weight = weight.view(boradcast_dims)
+        loss_tmp = weight * loss_tmp
 
     if reduction == 'none':
         loss = loss_tmp
     elif reduction == 'mean':
         loss = torch.mean(loss_tmp)
     elif reduction == 'sum':
         loss = torch.sum(loss_tmp)
@@ -237,44 +271,52 @@
 
         \text{FL}(p_t) = -\alpha_t (1 - p_t)^{\gamma} \, \text{log}(p_t)
 
     where:
        - :math:`p_t` is the model's estimated probability for each class.
 
     Args:
-        alpha: Weighting factor for the rare class :math:`\alpha \in [0, 1]`.
+        alpha: Weighting factor :math:`\alpha \in [0, 1]`.
         gamma: Focusing parameter :math:`\gamma >= 0`.
         reduction: Specifies the reduction to apply to the
           output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction
           will be applied, ``'mean'``: the sum of the output will be divided by
           the number of elements in the output, ``'sum'``: the output will be
           summed.
-        pos_weight: a weight of positive examples.
-          Its possible to trade off recall and precision by adding weights to positive examples.
-          Must be a vector with length equal to the number of classes.
+        pos_weight: a weight of positive examples with shape :math:`(num\_of\_classes,)`.
+          It is possible to trade off recall and precision by adding weights to positive examples.
+        weight: weights for classes with shape :math:`(num\_of\_classes,)`.
 
     Shape:
-        - Input: :math:`(N, *)`.
-        - Target: :math:`(N, *)`.
+        - Input: :math:`(N, C, *)` where C = number of classes.
+        - Target: the same shape as Input :math:`(N, C, *)`
+          where each value is between 0 and 1.
 
     Examples:
+        >>> C = 3  # num_classes
+        >>> input = torch.randn(1, C, 5, requires_grad=True)
+        >>> target = torch.randint(2, (1, C, 5))
         >>> kwargs = {"alpha": 0.25, "gamma": 2.0, "reduction": 'mean'}
-        >>> loss = BinaryFocalLossWithLogits(**kwargs)
-        >>> input = torch.randn(1, 3, 5, requires_grad=True)
-        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(2)
-        >>> output = loss(input, target)
+        >>> criterion = BinaryFocalLossWithLogits(**kwargs)
+        >>> output = criterion(input, target)
         >>> output.backward()
     """
 
     def __init__(
-        self, alpha: float, gamma: float = 2.0, reduction: str = 'none', pos_weight: Optional[Tensor] = None
+        self,
+        alpha: float | None,
+        gamma: float = 2.0,
+        reduction: str = 'none',
+        pos_weight: Tensor | None = None,
+        weight: Tensor | None = None,
     ) -> None:
         super().__init__()
-        self.alpha: float = alpha
+        self.alpha: float | None = alpha
         self.gamma: float = gamma
         self.reduction: str = reduction
-        self.pos_weight: Optional[Tensor] = pos_weight
+        self.pos_weight: Tensor | None = pos_weight
+        self.weight: Tensor | None = weight
 
     def forward(self, input: Tensor, target: Tensor) -> Tensor:
         return binary_focal_loss_with_logits(
-            input, target, self.alpha, self.gamma, self.reduction, pos_weight=self.pos_weight
+            input, target, self.alpha, self.gamma, self.reduction, self.pos_weight, self.weight
         )
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `kornia-0.6.9/kornia/losses/hausdorff.py` & `kornia-0.7.0/kornia/losses/hausdorff.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,18 @@
+from __future__ import annotations
+
 from typing import Callable
 
 import torch
-import torch.nn as nn
+from torch import nn
 
-from kornia.core import Tensor, as_tensor, stack, tensor, where, zeros_like
+from kornia.core import Module, Tensor, as_tensor, stack, tensor, where, zeros_like
 
 
-class _HausdorffERLossBase(torch.jit.ScriptModule):
+class _HausdorffERLossBase(Module):
     """Base class for binary Hausdorff loss based on morphological erosion.
 
     This is an Hausdorff Distance (HD) Loss that based on morphological erosion,which provided
     a differentiable approximation of Hausdorff distance as stated in :cite:`karimi2019reducing`.
     The code is refactored on top of `here <https://github.com/PatRyg99/HausdorffLoss/
         blob/master/hausdorff_loss.py>`__.
 
@@ -43,16 +45,15 @@
         bound = (pred - target) ** 2
 
         kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
         eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
         mask = torch.ones_like(bound, device=pred.device, dtype=torch.bool)
 
         # Same padding, assuming kernel is odd and square (cube) shaped.
-        # NOTE: int() has to be added for enabling JIT.
-        padding = int((kernel.size(-1) - 1) // 2)
+        padding = (kernel.size(-1) - 1) // 2
         for k in range(self.k):
             # compute convolution with kernel
             dilation = self.conv(bound, weight=kernel, padding=padding, groups=1)
             # apply soft thresholding at 0.5 and normalize
             erosion = dilation - 0.5
             erosion[erosion < 0] = 0
 
@@ -71,16 +72,15 @@
 
             # save erosion and add to loss
             eroded = eroded + erosion * (k + 1) ** self.alpha
             bound = erosion
 
         return eroded
 
-    # NOTE: we add type ignore because the forward pass does not work well with subclassing
-    def forward(self, pred: Tensor, target: Tensor) -> Tensor:  # type: ignore[override]
+    def forward(self, pred: Tensor, target: Tensor) -> Tensor:
         """Compute Hausdorff loss.
 
         Args:
             pred: predicted tensor with a shape of :math:`(B, C, H, W)` or :math:`(B, C, D, H, W)`.
                 Each channel is as binary as: 1 -> fg, 0 -> bg.
             target: target tensor with a shape of :math:`(B, 1, H, W)` or :math:`(B, C, D, H, W)`.
 
@@ -163,16 +163,15 @@
 
     def get_kernel(self) -> Tensor:
         """Get kernel for image morphology convolution."""
         cross = tensor([[[0, 1, 0], [1, 1, 1], [0, 1, 0]]])
         kernel = cross * 0.2
         return kernel[None]
 
-    # NOTE: we add type ignore because the forward pass does not work well with subclassing
-    def forward(self, pred: Tensor, target: Tensor) -> Tensor:  # type: ignore[override]
+    def forward(self, pred: Tensor, target: Tensor) -> Tensor:
         """Compute Hausdorff loss.
 
         Args:
             pred: predicted tensor with a shape of :math:`(B, C, H, W)`.
                 Each channel is as binary as: 1 -> fg, 0 -> bg.
             target: target tensor with a shape of :math:`(B, 1, H, W)`.
 
@@ -234,16 +233,15 @@
         bound = tensor([[[0, 0, 0], [0, 1, 0], [0, 0, 0]]])
         # NOTE: The original repo claimed it shaped as (3, 1, 3, 3)
         #    which Jian suspect it is wrongly implemented.
         # https://github.com/PatRyg99/HausdorffLoss/blob/9f580acd421af648e74b45d46555ccb7a876c27c/hausdorff_loss.py#L94
         kernel = stack([bound, cross, bound], 1) * (1 / 7)
         return kernel[None]
 
-    # NOTE: we add type ignore because the forward pass does not work well with subclassing
-    def forward(self, pred: Tensor, target: Tensor) -> Tensor:  # type: ignore[override]
+    def forward(self, pred: Tensor, target: Tensor) -> Tensor:
         """Compute 3D Hausdorff loss.
 
         Args:
             pred: predicted tensor with a shape of :math:`(B, C, D, H, W)`.
                 Each channel is as binary as: 1 -> fg, 0 -> bg.
             target: target tensor with a shape of :math:`(B, 1, D, H, W)`.
```

### Comparing `kornia-0.6.9/kornia/losses/lovasz_hinge.py` & `kornia-0.7.0/kornia/losses/lovasz_hinge.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,13 @@
+from __future__ import annotations
+
 import torch
-import torch.nn as nn
-from torch import Tensor
+from torch import Tensor, nn
 
-from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK_SHAPE
 
 # based on:
 # https://github.com/bermanmaxim/LovaszSoftmax
 
 
 def lovasz_hinge_loss(pred: Tensor, target: Tensor) -> Tensor:
     r"""Criterion that computes a surrogate binary intersection-over-union (IoU) loss.
@@ -29,15 +30,15 @@
 
         \text{loss}(x, class) = 1 - \text{IoU}(x, class)
 
     Reference:
         [1] http://proceedings.mlr.press/v37/yub15.pdf
         [2] https://arxiv.org/pdf/1705.08790.pdf
 
-    . note::
+    .. note::
         This loss function only supports binary labels. For multi-class labels please
         use the Lovasz-Softmax loss.
 
     Args:
         pred: logits tensor with shape :math:`(N, 1, H, W)`.
         labels: labels tensor with shape :math:`(N, H, W)` with binary values.
 
@@ -109,15 +110,15 @@
 
         \text{loss}(x, class) = 1 - \text{IoU}(x, class)
 
     Reference:
         [1] http://proceedings.mlr.press/v37/yub15.pdf
         [2] https://arxiv.org/pdf/1705.08790.pdf
 
-    . note::
+    .. note::
         This loss function only supports binary labels. For multi-class labels please
         use the Lovasz-Softmax loss.
 
     Args:
         pred: logits tensor with shape :math:`(N, 1, H, W)`.
         labels: labels tensor with shape :math:`(N, H, W)` with binary values.
```

### Comparing `kornia-0.6.9/kornia/losses/lovasz_softmax.py` & `kornia-0.7.0/kornia/losses/lovasz_softmax.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-from typing import List
+from __future__ import annotations
 
 import torch
-import torch.nn as nn
-from torch import Tensor
+from torch import Tensor, nn
 
-from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK_SHAPE
 
 # based on:
 # https://github.com/bermanmaxim/LovaszSoftmax
 
 
 def lovasz_softmax_loss(pred: Tensor, target: Tensor) -> Tensor:
     r"""Criterion that computes a surrogate multi-class intersection-over-union (IoU) loss.
@@ -30,22 +29,22 @@
     .. math::
 
         \text{loss}(x, class) = 1 - \text{IoU}(x, class)
 
     Reference:
         [1] https://arxiv.org/pdf/1705.08790.pdf
 
-    . note::
+    .. note::
         This loss function only supports multi-class (C > 1) labels. For binary
         labels please use the Lovasz-Hinge loss.
 
     Args:
         pred: logits tensor with shape :math:`(N, C, H, W)` where C = number of classes > 1.
         labels: labels tensor with shape :math:`(N, H, W)` where each value
-          is :math:`0  targets[i]  C1`.
+          is :math:`0  targets[i]  C-1`.
 
     Return:
         a scalar with the computed loss.
 
     Example:
         >>> N = 5  # num_classes
         >>> pred = torch.randn(1, N, 3, 5, requires_grad=True)
@@ -73,15 +72,15 @@
     # get shapes
     B, C, N = pred_flatten.shape
 
     # compute softmax over the classes axis
     pred_soft: Tensor = pred_flatten.softmax(1)
 
     # compute actual loss
-    losses: List[Tensor] = []
+    losses: list[Tensor] = []
     batch_index: Tensor = torch.arange(B, device=pred.device).reshape(-1, 1).repeat(1, N).reshape(-1)
     for c in range(C):
         foreground: Tensor = 1.0 * (target_flatten == c)
         class_pred: Tensor = pred_soft[:, c]
         errors = (class_pred - foreground).abs()
         errors_sorted, permutation = torch.sort(errors, dim=1, descending=True)
         target_sorted: Tensor = target_flatten[batch_index, permutation.view(-1)]
@@ -118,22 +117,22 @@
     .. math::
 
         \text{loss}(x, class) = 1 - \text{IoU}(x, class)
 
     Reference:
         [1] https://arxiv.org/pdf/1705.08790.pdf
 
-    . note::
+    .. note::
         This loss function only supports multi-class (C > 1) labels. For binary
         labels please use the Lovasz-Hinge loss.
 
     Args:
         pred: logits tensor with shape :math:`(N, C, H, W)` where C = number of classes > 1.
         labels: labels tensor with shape :math:`(N, H, W)` where each value
-          is :math:`0  targets[i]  C1`.
+          is :math:`0  targets[i]  C-1`.
 
     Return:
         a scalar with the computed loss.
 
     Example:
         >>> N = 5  # num_classes
         >>> criterion = LovaszSoftmaxLoss()
```

### Comparing `kornia-0.6.9/kornia/losses/ms_ssim.py` & `kornia-0.7.0/kornia/losses/ms_ssim.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,12 @@
-from typing import List, Optional, Tuple
+from __future__ import annotations
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 
 # Based on:
 # https://github.com/psyrocloud/MS-SSIM_L1_LOSS
 
 
 class MS_SSIMLoss(nn.Module):
     r"""Creates a criterion that computes MSSIM + L1 loss.
@@ -50,17 +50,17 @@
         >>> input2 = torch.rand(1, 3, 5, 5)
         >>> criterion = kornia.losses.MS_SSIMLoss()
         >>> loss = criterion(input1, input2)
     """
 
     def __init__(
         self,
-        sigmas: List[float] = [0.5, 1.0, 2.0, 4.0, 8.0],
+        sigmas: list[float] = [0.5, 1.0, 2.0, 4.0, 8.0],
         data_range: float = 1.0,
-        K: Tuple[float, float] = (0.01, 0.03),
+        K: tuple[float, float] = (0.01, 0.03),
         alpha: float = 0.025,
         compensation: float = 200.0,
         reduction: str = 'mean',
     ) -> None:
         super().__init__()
         self.DR: float = data_range
         self.C1: float = (K[0] * data_range) ** 2
@@ -79,15 +79,15 @@
             g_masks[3 * idx + 0, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)
             g_masks[3 * idx + 1, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)
             g_masks[3 * idx + 2, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)
 
         self.register_buffer('_g_masks', g_masks)
 
     def _fspecial_gauss_1d(
-        self, size: int, sigma: float, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None
+        self, size: int, sigma: float, device: torch.device | None = None, dtype: torch.dtype | None = None
     ) -> torch.Tensor:
         """Create 1-D gauss kernel.
 
         Args:
             size: the size of gauss kernel.
             sigma: sigma of normal distribution.
 
@@ -97,15 +97,15 @@
         coords = torch.arange(size, device=device, dtype=dtype)
         coords -= size // 2
         g = torch.exp(-(coords**2) / (2 * sigma**2))
         g /= g.sum()
         return g.reshape(-1)
 
     def _fspecial_gauss_2d(
-        self, size: int, sigma: float, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None
+        self, size: int, sigma: float, device: torch.device | None = None, dtype: torch.dtype | None = None
     ) -> torch.Tensor:
         """Create 2-D gauss kernel.
 
         Args:
             size: the size of gauss kernel.
             sigma: sigma of normal distribution.
```

### Comparing `kornia-0.6.9/kornia/losses/psnr.py` & `kornia-0.7.0/kornia/losses/psnr.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,13 @@
+from __future__ import annotations
+
 import torch
-import torch.nn as nn
+from torch import nn
 
-import kornia.metrics as metrics
+from kornia import metrics
 
 
 def psnr_loss(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Tensor:
     r"""Function that computes the PSNR loss.
 
     The loss is computed as follows:
 
@@ -24,15 +26,14 @@
         the computed loss as a scalar.
 
     Examples:
         >>> ones = torch.ones(1)
         >>> psnr_loss(ones, 1.2 * ones, 2.) # 10 * log(4/((1.2-1)**2)) / log(10)
         tensor(-20.0000)
     """
-
     return -1.0 * metrics.psnr(input, target, max_val)
 
 
 class PSNRLoss(nn.Module):
     r"""Create a criterion that calculates the PSNR loss.
 
     The loss is computed as follows:
```

### Comparing `kornia-0.6.9/kornia/losses/ssim.py` & `kornia-0.7.0/kornia/losses/ssim.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,13 @@
+from __future__ import annotations
+
 import torch
-import torch.nn as nn
+from torch import nn
 
-import kornia.metrics as metrics
+from kornia import metrics
 
 
 def ssim_loss(
     img1: torch.Tensor,
     img2: torch.Tensor,
     window_size: int,
     max_val: float = 1.0,
```

### Comparing `kornia-0.6.9/kornia/losses/total_variation.py` & `kornia-0.7.0/kornia/losses/total_variation.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,11 @@
+from __future__ import annotations
+
 from kornia.core import Module, Tensor
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 
 
 def total_variation(img: Tensor, reduction: str = "sum") -> Tensor:
     r"""Function that computes Total Variation according to [1].
 
     Args:
         img: the input image with shape :math:`(*, H, W)`.
@@ -72,9 +74,9 @@
                 [0., 0., 0.]])
         >>> output.sum().backward()  # grad can be implicitly created only for scalar outputs
 
     Reference:
         [1] https://en.wikipedia.org/wiki/Total_variation
     """
 
-    def forward(self, img) -> Tensor:
+    def forward(self, img: Tensor) -> Tensor:
         return total_variation(img)
```

### Comparing `kornia-0.6.9/kornia/losses/tversky.py` & `kornia-0.7.0/kornia/losses/tversky.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,12 @@
+from __future__ import annotations
+
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
+from torch import nn
 
 from kornia.utils.one_hot import one_hot
 
 # based on:
 # https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py
 
 
@@ -30,15 +32,15 @@
        - :math:`\alpha = \beta = 0.5` => dice coeff
        - :math:`\alpha = \beta = 1` => tanimoto coeff
        - :math:`\alpha + \beta = 1` => F beta coeff
 
     Args:
         input: logits tensor with shape :math:`(N, C, H, W)` where C = number of classes.
         target: labels tensor with shape :math:`(N, H, W)` where each value
-          is :math:`0  targets[i]  C1`.
+          is :math:`0  targets[i]  C-1`.
         alpha: the first coefficient in the denominator.
         beta: the second coefficient in the denominator.
         eps: scalar for numerical stability.
 
     Return:
         the computed loss.
 
@@ -105,15 +107,15 @@
         alpha: the first coefficient in the denominator.
         beta: the second coefficient in the denominator.
         eps: scalar for numerical stability.
 
     Shape:
         - Input: :math:`(N, C, H, W)` where C = number of classes.
         - Target: :math:`(N, H, W)` where each value is
-          :math:`0  targets[i]  C1`.
+          :math:`0  targets[i]  C-1`.
 
     Examples:
         >>> N = 5  # num_classes
         >>> criterion = TverskyLoss(alpha=0.5, beta=0.5)
         >>> input = torch.randn(1, N, 3, 5, requires_grad=True)
         >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)
         >>> output = criterion(input, target)
```

### Comparing `kornia-0.6.9/kornia/metrics/accuracy.py` & `kornia-0.7.0/kornia/metrics/accuracy.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-from typing import List
+from typing import List, Tuple
 
-import torch
+from kornia.core import Tensor
 
 
-def accuracy(input: torch.Tensor, target: torch.Tensor, topk=(1,)) -> List[torch.Tensor]:
+def accuracy(input: Tensor, target: Tensor, topk: Tuple[int, ...] = (1,)) -> List[Tensor]:
     """Computes the accuracy over the k top predictions for the specified values of k.
 
     Args:
         input: the input tensor with the logits to evaluate.
         target: the tensor containing the ground truth.
         topk: the expected topk ranking.
```

### Comparing `kornia-0.6.9/kornia/metrics/average_meter.py` & `kornia-0.7.0/kornia/metrics/average_meter.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,30 +6,36 @@
 class AverageMeter:
     """Computes and stores the average and current value.
 
     Example:
         >>> stats = AverageMeter()
         >>> acc1 = torch.tensor(0.99) # coming from K.metrics.accuracy
         >>> stats.update(acc1, n=1)  # where n is batch size usually
-        >>> stats.avg
-        tensor(0.9900)
+        >>> round(stats.avg, 2)
+        0.99
     """
 
     val: Union[int, float, bool, Tensor]
-    avg: Union[int, float, Tensor]
+    _avg: Union[float, Tensor]
     sum: Union[int, float, Tensor]
     count: int
 
     def __init__(self) -> None:
         self.reset()
 
-    def reset(self):
+    def reset(self) -> None:
         self.val = 0
-        self.avg = 0
+        self._avg = 0
         self.sum = 0
         self.count = 0
 
     def update(self, val: Union[int, float, bool, Tensor], n: int = 1) -> None:
         self.val = val
         self.sum += val * n
         self.count += n
-        self.avg = self.sum / self.count
+        self._avg = self.sum / self.count
+
+    @property
+    def avg(self) -> float:
+        if isinstance(self._avg, Tensor):
+            return float(self._avg.item())
+        return self._avg
```

### Comparing `kornia-0.6.9/kornia/metrics/confusion_matrix.py` & `kornia-0.7.0/kornia/metrics/confusion_matrix.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/metrics/mean_average_precision.py` & `kornia-0.7.0/kornia/metrics/mean_average_precision.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/metrics/mean_iou.py` & `kornia-0.7.0/kornia/metrics/mean_iou.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/metrics/psnr.py` & `kornia-0.7.0/kornia/metrics/psnr.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -34,16 +34,16 @@
         >>> psnr(ones, 1.2 * ones, 2.) # 10 * log(4/((1.2-1)**2)) / log(10)
         tensor(20.0000)
 
     Reference:
         https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition
     """
     if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Expected torch.Tensor but got {type(target)}.")
+        raise TypeError(f"Expected torch.Tensor but got {type(input)}.")
 
     if not isinstance(target, torch.Tensor):
-        raise TypeError(f"Expected torch.Tensor but got {type(input)}.")
+        raise TypeError(f"Expected torch.Tensor but got {type(target)}.")
 
     if input.shape != target.shape:
         raise TypeError(f"Expected tensors of equal shapes, but got {input.shape} and {target.shape}")
 
     return 10.0 * torch.log10(max_val**2 / mse(input, target, reduction='mean'))
```

### Comparing `kornia-0.6.9/kornia/metrics/ssim.py` & `kornia-0.7.0/kornia/metrics/ssim.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import List
 
 import torch
-import torch.nn as nn
+from torch import nn
 
-from kornia.filters import filter2d, get_gaussian_kernel2d
+from kornia.filters import filter2d_separable, get_gaussian_kernel1d
 from kornia.filters.filter import _compute_padding
 
 
 def _crop(img: torch.Tensor, cropping_shape: List[int]) -> torch.Tensor:
     """Crop out the part of "valid" convolution area."""
     return torch.nn.functional.pad(
         img, (-cropping_shape[2], -cropping_shape[3], -cropping_shape[0], -cropping_shape[1])
@@ -71,40 +71,40 @@
     if not len(img2.shape) == 4:
         raise ValueError(f"Invalid img2 shape, we expect BxCxHxW. Got: {img2.shape}")
 
     if not img1.shape == img2.shape:
         raise ValueError(f"img1 and img2 shapes must be the same. Got: {img1.shape} and {img2.shape}")
 
     # prepare kernel
-    kernel: torch.Tensor = get_gaussian_kernel2d((window_size, window_size), (1.5, 1.5)).unsqueeze(0)
+    kernel: torch.Tensor = get_gaussian_kernel1d(window_size, 1.5, device=img1.device, dtype=img1.dtype)
 
     # compute coefficients
     C1: float = (0.01 * max_val) ** 2
     C2: float = (0.03 * max_val) ** 2
 
     # compute local mean per channel
-    mu1: torch.Tensor = filter2d(img1, kernel)
-    mu2: torch.Tensor = filter2d(img2, kernel)
+    mu1: torch.Tensor = filter2d_separable(img1, kernel, kernel)
+    mu2: torch.Tensor = filter2d_separable(img2, kernel, kernel)
 
     cropping_shape: List[int] = []
     if padding == 'valid':
-        height, width = kernel.shape[-2:]
+        height = width = kernel.shape[-1]
         cropping_shape = _compute_padding([height, width])
         mu1 = _crop(mu1, cropping_shape)
         mu2 = _crop(mu2, cropping_shape)
     elif padding == 'same':
         pass
 
     mu1_sq = mu1**2
     mu2_sq = mu2**2
     mu1_mu2 = mu1 * mu2
 
-    mu_img1_sq = filter2d(img1**2, kernel)
-    mu_img2_sq = filter2d(img2**2, kernel)
-    mu_img1_img2 = filter2d(img1 * img2, kernel)
+    mu_img1_sq = filter2d_separable(img1**2, kernel, kernel)
+    mu_img2_sq = filter2d_separable(img2**2, kernel, kernel)
+    mu_img1_img2 = filter2d_separable(img1 * img2, kernel, kernel)
 
     if padding == 'valid':
         mu_img1_sq = _crop(mu_img1_sq, cropping_shape)
         mu_img2_sq = _crop(mu_img2_sq, cropping_shape)
         mu_img1_img2 = _crop(mu_img1_img2, cropping_shape)
     elif padding == 'same':
         pass
```

### Comparing `kornia-0.6.9/kornia/morphology/morphology.py` & `kornia-0.7.0/kornia/morphology/morphology.py`

 * *Files 1% similar despite different names*

```diff
@@ -263,14 +263,15 @@
             tensor,
             kernel=kernel,
             structuring_element=structuring_element,
             origin=origin,
             border_type=border_type,
             border_value=border_value,
             max_val=max_val,
+            engine=engine,
         ),
         kernel=kernel,
         structuring_element=structuring_element,
         origin=origin,
         border_type=border_type,
         border_value=border_value,
         max_val=max_val,
@@ -340,14 +341,15 @@
             tensor,
             kernel=kernel,
             structuring_element=structuring_element,
             origin=origin,
             border_type=border_type,
             border_value=border_value,
             max_val=max_val,
+            engine=engine,
         ),
         kernel=kernel,
         structuring_element=structuring_element,
         origin=origin,
         border_type=border_type,
         border_value=border_value,
         max_val=max_val,
```

### Comparing `kornia-0.6.9/kornia/tracking/planar_tracker.py` & `kornia-0.7.0/kornia/tracking/planar_tracker.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/utils/__init__.py` & `kornia-0.7.0/kornia/utils/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,39 +1,51 @@
 from ._compat import torch_meshgrid
-from .draw import draw_convex_polygon, draw_line, draw_rectangle
+from .draw import draw_convex_polygon, draw_line, draw_point2d, draw_rectangle
 from .grid import create_meshgrid, create_meshgrid3d
 from .helpers import (
     _extract_device_dtype,
+    deprecated,
     get_cuda_device_if_available,
+    get_cuda_or_mps_device_if_available,
+    get_mps_device_if_available,
+    is_autocast_enabled,
     map_location_to_cpu,
     safe_inverse_with_mask,
     safe_solve_with_mask,
 )
 from .image import ImageToTensor, image_list_to_tensor, image_to_tensor, tensor_to_image
+from .image_print import image_to_string, print_image
 from .memory import batched_forward
 from .misc import eye_like, vec_like
 from .one_hot import one_hot
 from .pointcloud_io import load_pointcloud_ply, save_pointcloud_ply
 
 __all__ = [
     "batched_forward",
     "one_hot",
     "create_meshgrid",
     "create_meshgrid3d",
     "get_cuda_device_if_available",
+    "get_mps_device_if_available",
+    "get_cuda_or_mps_device_if_available",
     "tensor_to_image",
     "image_to_tensor",
     "image_list_to_tensor",
     "save_pointcloud_ply",
     "load_pointcloud_ply",
     "draw_convex_polygon",
     "draw_rectangle",
     "draw_line",
+    "draw_point2d",
     "_extract_device_dtype",
     "safe_inverse_with_mask",
     "safe_solve_with_mask",
     "ImageToTensor",
     "eye_like",
     "vec_like",
     "torch_meshgrid",
     "map_location_to_cpu",
+    "is_autocast_enabled",
+    "deprecated",
+    "image_to_string",
+    "print_image",
 ]
```

### Comparing `kornia-0.6.9/kornia/utils/_compat.py` & `kornia-0.7.0/kornia/utils/_compat.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,61 +1,55 @@
 from typing import TYPE_CHECKING, Callable, ContextManager, List, Optional, Tuple, TypeVar
 
 import torch
-from torch import Tensor
-
 from packaging import version
+from torch import Tensor
 
 
 def torch_version() -> str:
     """Parse the `torch.__version__` variable and removes +cu*/cpu."""
     return torch.__version__.split('+')[0]
 
 
-# TODO: replace by torch_version_ge``
-def torch_version_geq(major, minor) -> bool:
-    _version = version.parse(torch_version())
-    return _version >= version.parse(f"{major}.{minor}")
-
-
 def torch_version_lt(major: int, minor: int, patch: int) -> bool:
     _version = version.parse(torch_version())
     return _version < version.parse(f"{major}.{minor}.{patch}")
 
 
 def torch_version_le(major: int, minor: int, patch: int) -> bool:
     _version = version.parse(torch_version())
     return _version <= version.parse(f"{major}.{minor}.{patch}")
 
 
-def torch_version_ge(major: int, minor: int, patch: int) -> bool:
+def torch_version_ge(major: int, minor: int, patch: Optional[int] = None) -> bool:
     _version = version.parse(torch_version())
-    return _version >= version.parse(f"{major}.{minor}.{patch}")
+    if patch is None:
+        return _version >= version.parse(f"{major}.{minor}")
+    else:
+        return _version >= version.parse(f"{major}.{minor}.{patch}")
 
 
 if TYPE_CHECKING:
     # TODO: remove this branch when kornia relies on torch >= 1.10.0
     def torch_meshgrid(tensors: List[Tensor], indexing: Optional[str] = None) -> Tuple[Tensor, ...]:
         ...
 
-else:
-    if torch_version_ge(1, 10, 0):
+elif torch_version_ge(1, 10, 0):
 
-        def torch_meshgrid(tensors: List[Tensor], indexing: str):
-            return torch.meshgrid(tensors, indexing=indexing)
+    def torch_meshgrid(tensors: List[Tensor], indexing: str):
+        return torch.meshgrid(tensors, indexing=indexing)
 
-    else:
-        # TODO: remove this branch when kornia relies on torch >= 1.10.0
-        def torch_meshgrid(tensors: List[Tensor], indexing: str):
-            return torch.meshgrid(tensors)
+else:
+    # TODO: remove this branch when kornia relies on torch >= 1.10.0
+    def torch_meshgrid(tensors: List[Tensor], indexing: str):
+        return torch.meshgrid(tensors)
 
 
 if TYPE_CHECKING:
     # TODO: remove this branch when kornia relies on torch >= 1.10.0
     _T = TypeVar('_T')
     torch_inference_mode: Callable[..., ContextManager[_T]]
+elif torch_version_ge(1, 10, 0):
+    torch_inference_mode = torch.inference_mode
 else:
-    if torch_version_ge(1, 10, 0):
-        torch_inference_mode = torch.inference_mode
-    else:
-        # TODO: remove this branch when kornia relies on torch >= 1.10.0
-        torch_inference_mode = torch.no_grad
+    # TODO: remove this branch when kornia relies on torch >= 1.10.0
+    torch_inference_mode = torch.no_grad
```

### Comparing `kornia-0.6.9/kornia/utils/draw.py` & `kornia-0.7.0/kornia/utils/draw.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,17 +1,44 @@
 from typing import List, Optional, Tuple, Union
 
 import torch
 from torch import Tensor
 
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
+from kornia.core.check import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 
 # TODO: implement width of the line
 
 
+def draw_point2d(image: Tensor, points: Tensor, color: Tensor) -> Tensor:
+    r"""Sets one or more coordinates in a Tensor to a color.
+
+    Args:
+        image: the input image on which to draw the points with shape :math`(C,H,W)` or :math`(H,W)`.
+        points: the [x, y] points to be drawn on the image.
+        color: the color of the pixel with :math`(C)` where :math`C` is the number of channels of the image.
+
+    Return:
+        The image with points set to the color.
+    """
+    KORNIA_CHECK(
+        (len(image.shape) == 2 and len(color.shape) == 1) or (image.shape[0] == color.shape[0]),
+        "Color dim must match the channel dims of the provided image",
+    )
+    points = points.to(dtype=torch.int64, device=image.device)
+    x, y = zip(*points)
+    if len(color.shape) == 1:
+        color = torch.unsqueeze(color, dim=1)
+    color = color.to(dtype=image.dtype, device=image.device)
+    if len(image.shape) == 2:
+        image[y, x] = color
+    else:
+        image[:, y, x] = color
+    return image
+
+
 def _draw_pixel(image: torch.Tensor, x: int, y: int, color: torch.Tensor) -> None:
     r"""Draws a pixel into an image.
 
     Args:
         image: the input image to where to draw the lines with shape :math`(C,H,W)`.
         x: the x coordinate of the pixel.
         y: the y coordinate of the pixel.
@@ -24,18 +51,17 @@
 
 
 def draw_line(image: torch.Tensor, p1: torch.Tensor, p2: torch.Tensor, color: torch.Tensor) -> torch.Tensor:
     r"""Draw a single line into an image.
 
     Args:
         image: the input image to where to draw the lines with shape :math`(C,H,W)`.
-        p1: the start point [x y] of the line with shape (2).
-        p2: the end point [x y] of the line with shape (2).
+        p1: the start point [x y] of the line with shape (2, ) or (B, 2).
+        p2: the end point [x y] of the line with shape (2, ) or (B, 2).
         color: the color of the line with shape :math`(C)` where :math`C` is the number of channels of the image.
-
     Return:
         the image with containing the line.
 
     Examples:
         >>> image = torch.zeros(1, 8, 8)
         >>> draw_line(image, torch.tensor([6, 4]), torch.tensor([1, 4]), torch.tensor([255]))
         tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],
@@ -43,96 +69,113 @@
                  [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],
                  [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],
                  [  0., 255., 255., 255., 255., 255., 255.,   0.],
                  [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],
                  [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],
                  [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]]])
     """
-
-    if (len(p1) != 2) or (len(p2) != 2):
-        raise ValueError("p1 and p2 must have length 2.")
+    if (p1.shape[0] != p2.shape[0]) or (p1.shape[-1] != 2 or p2.shape[-1] != 2):
+        raise ValueError(
+            "Input points must be 2D points with shape (2, ) or (B, 2) and must have the same batch sizes."
+        )
+    if (
+        (p1[..., 0] < 0).any()
+        or (p1[..., 0] >= image.shape[-1]).any()
+        or (p1[..., 1] < 0).any()
+        or (p1[..., 1] >= image.shape[-2]).any()
+    ):
+        raise ValueError("p1 is out of bounds.")
+    if (
+        (p2[..., 0] < 0).any()
+        or (p2[..., 0] >= image.shape[-1]).any()
+        or (p2[..., 1] < 0).any()
+        or (p2[..., 1] >= image.shape[-2]).any()
+    ):
+        raise ValueError("p2 is out of bounds.")
 
     if len(image.size()) != 3:
         raise ValueError("image must have 3 dimensions (C,H,W).")
 
     if color.size(0) != image.size(0):
         raise ValueError("color must have the same number of channels as the image.")
 
-    if (p1[0] >= image.size(2)) or (p1[1] >= image.size(1) or (p1[0] < 0) or (p1[1] < 0)):
-        raise ValueError("p1 is out of bounds.")
-
-    if (p2[0] >= image.size(2)) or (p2[1] >= image.size(1) or (p2[0] < 0) or (p2[1] < 0)):
-        raise ValueError("p2 is out of bounds.")
-
     # move p1 and p2 to the same device as the input image
     # move color to the same device and dtype as the input image
     p1 = p1.to(image.device).to(torch.int64)
     p2 = p2.to(image.device).to(torch.int64)
     color = color.to(image)
 
-    # assign points
-    x1, y1 = p1
-    x2, y2 = p2
-
-    # calcullate coefficients A,B,C of line
-    # from equation Ax + By + C = 0
-    A = y2 - y1
-    B = x1 - x2
-    C = x2 * y1 - x1 * y2
-
-    # make sure A is positive to utilize the function properly
-    if A < 0:
-        A = -A
-        B = -B
-        C = -C
-
-    # calculate the slope of the line
-    # check for division by zero
-    if B != 0:
-        m = -A / B
-
-    # make sure you start drawing in the right direction
-    x1, x2 = min(x1, x2).long(), max(x1, x2).long()
-    y1, y2 = min(y1, y2).long(), max(y1, y2).long()
-
-    # line equation that determines the distance away from the line
-    def line_equation(x, y):
-        return A * x + B * y + C
-
-    # vertical line
-    if B == 0:
-        image[:, y1 : y2 + 1, x1] = color
-    # horizontal line
-    elif A == 0:
-        image[:, y1, x1 : x2 + 1] = color
-    # slope between 0 and 1
-    elif 0 < m < 1:
-        for i in range(x1, x2 + 1):
-            _draw_pixel(image, i, y1, color)
-            if line_equation(i + 1, y1 + 0.5) > 0:
-                y1 += 1
-    # slope greater than or equal to 1
-    elif m >= 1:
-        for j in range(y1, y2 + 1):
-            _draw_pixel(image, x1, j, color)
-            if line_equation(x1 + 0.5, j + 1) < 0:
-                x1 += 1
-    # slope less then -1
-    elif m <= -1:
-        for j in range(y1, y2 + 1):
-            _draw_pixel(image, x2, j, color)
-            if line_equation(x2 - 0.5, j + 1) > 0:
-                x2 -= 1
-    # slope between -1 and 0
-    elif -1 < m < 0:
-        for i in range(x1, x2 + 1):
-            _draw_pixel(image, i, y2, color)
-            if line_equation(i + 1, y2 - 0.5) > 0:
-                y2 -= 1
-
+    x1, y1 = p1[..., 0], p1[..., 1]
+    x2, y2 = p2[..., 0], p2[..., 1]
+    dx = x2 - x1
+    dy = y2 - y1
+    dx_sign = torch.sign(dx)
+    dy_sign = torch.sign(dy)
+    dx, dy = torch.abs(dx), torch.abs(dy)
+    dx_zero_mask = dx == 0
+    dy_zero_mask = dy == 0
+    dx_gt_dy_mask = (dx > dy) & ~(dx_zero_mask | dy_zero_mask)
+    rest_mask = ~(dx_zero_mask | dy_zero_mask | dx_gt_dy_mask)
+
+    dx_zero_x_coords, dx_zero_y_coords = [], []
+    dy_zero_x_coords, dy_zero_y_coords = [], []
+    dx_gt_dy_x_coords, dx_gt_dy_y_coords = [], []
+    rest_x_coords, rest_y_coords = [], []
+
+    if dx_zero_mask.any():
+        dx_zero_x_coords = [
+            x for x_i, dy_i in zip(x1[dx_zero_mask], dy[dx_zero_mask]) for x in x_i.repeat(int(dy_i.item() + 1))
+        ]
+        dx_zero_y_coords = [
+            y
+            for y_i, s, dy_ in zip(y1[dx_zero_mask], dy_sign[dx_zero_mask], dy[dx_zero_mask])
+            for y in (y_i + s * torch.arange(0, dy_ + 1, 1, device=image.device))
+        ]
+
+    if dy_zero_mask.any():
+        dy_zero_x_coords = [
+            x
+            for x_i, s, dx_i in zip(x1[dy_zero_mask], dx_sign[dy_zero_mask], dx[dy_zero_mask])
+            for x in (x_i + s * torch.arange(0, dx_i + 1, 1, device=image.device))
+        ]
+        dy_zero_y_coords = [
+            y for y_i, dx_i in zip(y1[dy_zero_mask], dx[dy_zero_mask]) for y in y_i.repeat(int(dx_i.item() + 1))
+        ]
+
+    if dx_gt_dy_mask.any():
+        dx_gt_dy_x_coords = [
+            x
+            for x_i, s, dx_i in zip(x1[dx_gt_dy_mask], dx_sign[dx_gt_dy_mask], dx[dx_gt_dy_mask])
+            for x in (x_i + s * torch.arange(0, dx_i + 1, 1, device=image.device))
+        ]
+        dx_gt_dy_y_coords = [
+            y
+            for y_i, s, dx_i, dy_i in zip(
+                y1[dx_gt_dy_mask], dy_sign[dx_gt_dy_mask], dx[dx_gt_dy_mask], dy[dx_gt_dy_mask]
+            )
+            for y in (
+                y_i + s * torch.arange(0, dy_i + 1, dy_i / dx_i, device=image.device)[: int(dx_i.item()) + 1].ceil()
+            )
+        ]
+    if rest_mask.any():
+        rest_x_coords = [
+            x
+            for x_i, s, dx_i, dy_ in zip(x1[rest_mask], dx_sign[rest_mask], dx[rest_mask], dy[rest_mask])
+            for x in (
+                x_i + s * torch.arange(0, dx_i + 1, dx_i / dy_, device=image.device)[: int(dy_.item()) + 1].ceil()
+            )
+        ]
+        rest_y_coords = [
+            y
+            for y_i, s, dy_i in zip(y1[rest_mask], dy_sign[rest_mask], dy[rest_mask])
+            for y in (y_i + s * torch.arange(0, dy_i + 1, 1, device=image.device))
+        ]
+    x_coords = torch.tensor(dx_zero_x_coords + dy_zero_x_coords + dx_gt_dy_x_coords + rest_x_coords).long()
+    y_coords = torch.tensor(dx_zero_y_coords + dy_zero_y_coords + dx_gt_dy_y_coords + rest_y_coords).long()
+    image[:, y_coords, x_coords] = color.view(-1, 1)
     return image
 
 
 def draw_rectangle(
     image: torch.Tensor, rectangle: torch.Tensor, color: Optional[torch.Tensor] = None, fill: Optional[bool] = None
 ) -> torch.Tensor:
     r"""Draw N rectangles on a batch of image tensors.
@@ -216,15 +259,14 @@
             N is the number of points
             2 is (x, y).
         h: bottom most coordinate (top coordinate is assumed to be 0)
         w: right most coordinate (left coordinate is assumed to be 0)
     Returns:
         The left and right edges of the polygon of shape (B,B).
     """
-
     dtype = polygon.dtype
 
     # Check if polygons are in loop closed format, if not -> make it so
     if not torch.allclose(polygon[..., -1, :], polygon[..., 0, :]):
         polygon = torch.cat((polygon, polygon[..., :1, :]), dim=-2)  # (B, N+1, 2)
 
     # Partition points into edges
```

### Comparing `kornia-0.6.9/kornia/utils/grid.py` & `kornia-0.7.0/kornia/utils/grid.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/utils/image.py` & `kornia-0.7.0/kornia/utils/image.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,17 @@
 from functools import wraps
-from typing import TYPE_CHECKING, Any, Callable, List
+from typing import Any, Callable, List
 
 import torch
-import torch.nn as nn
+from torch import nn
 
 from kornia.core import Tensor
 
-if TYPE_CHECKING:
-    import numpy.typing as npt
 
-
-def image_to_tensor(image: "npt.NDArray[Any]", keepdim: bool = True) -> Tensor:
+def image_to_tensor(image: Any, keepdim: bool = True) -> Tensor:
     """Convert a numpy image to a PyTorch 4d tensor image.
 
     Args:
         image: image of the form :math:`(H, W, C)`, :math:`(H, W)` or
             :math:`(B, H, W, C)`.
         keepdim: If ``False`` unsqueeze the input image to match the shape
             :math:`(B, H, W, C)`.
@@ -54,15 +51,15 @@
         keepdim = True  # no need to unsqueeze
     else:
         raise ValueError(f"Cannot process image with shape {input_shape}")
 
     return tensor.unsqueeze(0) if not keepdim else tensor
 
 
-def image_list_to_tensor(images: List["npt.NDArray[Any]"]) -> Tensor:
+def image_list_to_tensor(images: List[Any]) -> Tensor:
     """Converts a list of numpy images to a PyTorch 4d tensor image.
 
     Args:
         images: list of images, each of the form :math:`(H, W, C)`.
         Image shapes must be consistent
 
     Returns:
@@ -135,15 +132,15 @@
 
     if len(tensor.shape) > 5:
         tensor = tensor.view(-1, tensor.shape[-4], tensor.shape[-3], tensor.shape[-2], tensor.shape[-1])
 
     return tensor
 
 
-def tensor_to_image(tensor: Tensor, keepdim: bool = False) -> "npt.NDArray[Any]":
+def tensor_to_image(tensor: Tensor, keepdim: bool = False) -> Any:
     """Converts a PyTorch tensor image to a numpy image.
 
     In case the tensor is in the GPU, it will be copied back to CPU.
 
     Args:
         tensor: image of the form :math:`(H, W)`, :math:`(C, H, W)` or
             :math:`(B, C, H, W)`.
@@ -165,15 +162,15 @@
     if not isinstance(tensor, Tensor):
         raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
 
     if len(tensor.shape) > 4 or len(tensor.shape) < 2:
         raise ValueError("Input size must be a two, three or four dimensional tensor")
 
     input_shape = tensor.shape
-    image: "npt.NDArray[Any]" = tensor.cpu().detach().numpy()
+    image = tensor.cpu().detach().numpy()
 
     if len(input_shape) == 2:
         # (H, W) -> (H, W)
         pass
     elif len(input_shape) == 3:
         # (C, H, W) -> (H, W, C)
         if input_shape[0] == 1:
@@ -197,31 +194,31 @@
 class ImageToTensor(nn.Module):
     """Converts a numpy image to a PyTorch 4d tensor image.
 
     Args:
         keepdim: If ``False`` unsqueeze the input image to match the shape :math:`(B, H, W, C)`.
     """
 
-    def __init__(self, keepdim: bool = False):
+    def __init__(self, keepdim: bool = False) -> None:
         super().__init__()
         self.keepdim = keepdim
 
-    def forward(self, x: "npt.NDArray[Any]") -> Tensor:
+    def forward(self, x: Any) -> Tensor:
         return image_to_tensor(x, keepdim=self.keepdim)
 
 
 def perform_keep_shape_image(f: Callable[..., Tensor]) -> Callable[..., Tensor]:
     """A decorator that enable `f` to be applied to an image of arbitrary leading dimensions `(*, C, H, W)`.
 
     It works by first viewing the image as `(B, C, H, W)`, applying the function and re-viewing the image as original
     shape.
     """
 
     @wraps(f)
-    def _wrapper(input: Tensor, *args, **kwargs) -> Tensor:
+    def _wrapper(input: Tensor, *args: Any, **kwargs: Any) -> Tensor:
         if not isinstance(input, Tensor):
             raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
 
         if input.numel() == 0:
             raise ValueError("Invalid input tensor, it is empty.")
 
         input_shape = input.shape
@@ -245,15 +242,15 @@
     """A decorator that enable `f` to be applied to an image of arbitrary leading dimensions `(*, C, D, H, W)`.
 
     It works by first viewing the image as `(B, C, D, H, W)`, applying the function and re-viewing the image as original
     shape.
     """
 
     @wraps(f)
-    def _wrapper(input: Tensor, *args, **kwargs) -> Tensor:
+    def _wrapper(input: Tensor, *args: Any, **kwargs: Any) -> Tensor:
         if not isinstance(input, Tensor):
             raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
 
         if input.numel() == 0:
             raise ValueError("Invalid input tensor, it is empty.")
 
         input_shape = input.shape
```

### Comparing `kornia-0.6.9/kornia/utils/memory.py` & `kornia-0.7.0/kornia/utils/memory.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
-import torch
+from typing import Any, Dict
+
+from kornia.core import Device, Module, Tensor, concatenate
 
 
 def batched_forward(
-    model: torch.nn.Module, data: torch.Tensor, device: torch.device, batch_size: int = 128, **kwargs
-) -> torch.Tensor:
+    model: Module, data: Tensor, device: Device, batch_size: int = 128, **kwargs: Dict[str, Any]
+) -> Tensor:
     r"""Convenience function, which allows to run the forward in micro-batches.
 
     When the just model.forward(data) does not fit into device memory, e.g. on laptop GPU.
     In the end, it transfers the output to the device of the input data tensor.
     E.g. running HardNet on 8000x1x32x32 tensor.
 
     Args:
@@ -41,10 +43,10 @@
                 else:
                     end = (batch_idx + 1) * bs
             else:
                 end = (batch_idx + 1) * bs
             if st >= end:
                 continue
             out_list.append(model_dev(data[st:end].to(device), **kwargs))
-        out = torch.cat(out_list, dim=0)
+        out = concatenate(out_list, 0)
         return out.to(data.device)
     return model(data, **kwargs)
```

### Comparing `kornia-0.6.9/kornia/utils/misc.py` & `kornia-0.7.0/kornia/utils/misc.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-import torch
+from kornia.core import Tensor, eye, zeros
 
 
-def eye_like(n: int, input: torch.Tensor, shared_memory: bool = False) -> torch.Tensor:
+def eye_like(n: int, input: Tensor, shared_memory: bool = False) -> Tensor:
     r"""Return a 2-D tensor with ones on the diagonal and zeros elsewhere with the same batch size as the input.
 
     Args:
         n: the number of rows :math:`(N)`.
         input: image tensor that will determine the batch size of the output matrix.
           The expected shape is :math:`(B, *)`.
         shared_memory: when set, all samples in the batch will share the same memory.
@@ -19,19 +19,20 @@
         use this method with shared_memory=False, otherwise, prefer using it with shared_memory=True.
     """
     if n <= 0:
         raise AssertionError(type(n), n)
     if len(input.shape) < 1:
         raise AssertionError(input.shape)
 
-    identity = torch.eye(n, device=input.device, dtype=input.dtype)
+    identity = eye(n, device=input.device).type(input.dtype)
+
     return identity[None].expand(input.shape[0], n, n) if shared_memory else identity[None].repeat(input.shape[0], 1, 1)
 
 
-def vec_like(n: int, tensor: torch.Tensor, shared_memory: bool = False):
+def vec_like(n: int, tensor: Tensor, shared_memory: bool = False) -> Tensor:
     r"""Return a 2-D tensor with a vector containing zeros with the same batch size as the input.
 
     Args:
         n: the number of rows :math:`(N)`.
         tensor: image tensor that will determine the batch size of the output matrix.
           The expected shape is :math:`(B, *)`.
         shared_memory: when set, all samples in the batch will share the same memory.
@@ -45,9 +46,9 @@
         use this method with shared_memory=False, otherwise, prefer using it with shared_memory=True.
     """
     if n <= 0:
         raise AssertionError(type(n), n)
     if len(tensor.shape) < 1:
         raise AssertionError(tensor.shape)
 
-    vec = torch.zeros(n, 1, device=tensor.device, dtype=tensor.dtype)
+    vec = zeros(n, 1, device=tensor.device, dtype=tensor.dtype)
     return vec[None].expand(tensor.shape[0], n, 1) if shared_memory else vec[None].repeat(tensor.shape[0], 1, 1)
```

### Comparing `kornia-0.6.9/kornia/utils/one_hot.py` & `kornia-0.7.0/kornia/utils/one_hot.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/utils/pointcloud_io.py` & `kornia-0.7.0/kornia/utils/pointcloud_io.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.9/kornia/x/callbacks.py` & `kornia-0.7.0/kornia/x/callbacks.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 from pathlib import Path
-from typing import Callable, Optional
+from typing import Callable, Dict, Optional, Union
 
 import torch
-import torch.nn as nn
+
+from kornia.core import Module
+from kornia.metrics import AverageMeter
 
 from .utils import TrainerState
 
 
 # default function to generate the filename in the model checkpoint
-def default_filename_fcn(x) -> str:
+def default_filename_fcn(x: Union[str, int]) -> str:
     return f"model_{x}.pt"
 
 
 class EarlyStopping:
     """Callback that evaluates whether there is improvement in the loss function.
 
     The module track the losses and in case of finish patience sends a termination signal to the trainer.
@@ -40,15 +42,15 @@
         self.min_delta = min_delta
         self.patience = patience
 
         self.counter: int = 0
         self.best_score: Optional[float] = None
         self.early_stop: bool = False
 
-    def __call__(self, model: nn.Module, epoch: int, valid_metric) -> TrainerState:
+    def __call__(self, model: Module, epoch: int, valid_metric: Dict[str, AverageMeter]) -> TrainerState:
         score: float = valid_metric[self.monitor].avg
 
         if self.best_score is None:
             self.best_score = score
         elif score < self.best_score + self.min_delta:
             self.counter += 1
             if self.counter >= self.patience:
@@ -91,14 +93,14 @@
 
         # track best model
         self.best_metric: float = 0.0
 
         # create directory
         Path(self.filepath).mkdir(parents=True, exist_ok=True)
 
-    def __call__(self, model: nn.Module, epoch: int, valid_metric) -> None:
+    def __call__(self, model: Module, epoch: int, valid_metric: Dict[str, AverageMeter]) -> None:
         valid_metric_value: float = valid_metric[self.monitor].avg
         if valid_metric_value > self.best_metric:
             self.best_metric = valid_metric_value
             # store old metric and save new model
             filename = Path(self.filepath) / self._filename_fcn(epoch)
             torch.save(model, filename)
```

### Comparing `kornia-0.6.9/kornia/x/trainer.py` & `kornia-0.7.0/kornia/x/trainer.py`

 * *Files 4% similar despite different names*

```diff
@@ -63,15 +63,15 @@
     def __init__(
         self,
         model: Module,
         train_dataloader: DataLoader[Any],
         valid_dataloader: DataLoader[Any],
         criterion: Optional[Module],
         optimizer: Optimizer,
-        scheduler: lr_scheduler.CosineAnnealingLR,
+        scheduler: lr_scheduler._LRScheduler,
         config: Configuration,
         callbacks: Dict[str, Callable[..., None]] = {},
     ) -> None:
         # setup the accelerator
         if Accelerator is None:
             raise ModuleNotFoundError('accelerate library is not installed: pip install "kornia[x]"')
         self.accelerator = Accelerator()
@@ -181,39 +181,39 @@
             stats.update_from_dict(self.compute_metrics(out, sample['target']), batch_size)
 
             if sample_id % 10 == 0:
                 self._logger.info(f"Test: {sample_id}/{len(self.valid_dataloader)} {stats}")
 
         return stats.as_dict()
 
-    def on_epoch_start(self, *args, **kwargs):
+    def on_epoch_start(self, *args: Any, **kwargs: Any) -> None:
         ...
 
     def preprocess(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return x
 
     def augmentations(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return x
 
-    def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
+    def compute_metrics(self, *args: Any) -> Dict[str, float]:
         """Compute metrics during the evaluation."""
         return {}
 
     def compute_loss(self, *args: Tensor) -> Tensor:
         if self.criterion is None:
             raise RuntimeError("`criterion` should not be None.")
         return self.criterion(*args)
 
     def on_before_model(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return x
 
-    def on_model(self, model, sample: Dict[str, Tensor]):
+    def on_model(self, model: Module, sample: Dict[str, Tensor]) -> Tensor:
         return model(sample["input"])
 
-    def on_after_model(self, output: Tensor, sample: Dict[str, Tensor]):
+    def on_after_model(self, output: Tensor, sample: Dict[str, Tensor]) -> None:
         ...
 
-    def on_checkpoint(self, *args, **kwargs):
+    def on_checkpoint(self, *args: Any, **kwargs: Dict[str, Any]) -> None:
         ...
 
-    def on_epoch_end(self, *args, **kwargs):
+    def on_epoch_end(self, *args: Any, **kwargs: Dict[str, Any]) -> None:
         ...
```

### Comparing `kornia-0.6.9/kornia/x/trainers.py` & `kornia-0.7.0/kornia/x/trainers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Callable, Dict, Optional
+from typing import Any, Callable, Dict, Optional, Tuple
 
 from torch.optim import Optimizer, lr_scheduler
 from torch.utils.data import DataLoader
 
 from kornia.core import Module, Tensor, stack
 from kornia.metrics import accuracy, mean_average_precision, mean_iou
 
@@ -15,91 +15,91 @@
 
     The module subclasses :py:class:`~kornia.x.Trainer` and overrides the
     :py:func:`~kornia.x.Trainer.evaluate` function implementing a standard
     :py:func:`~kornia.metrics.accuracy` topk@[1, 5].
 
     .. seealso::
         Learn how to use this class in the following
-        `example <https://github.com/kornia/kornia/blob/master/examples/train/image_classifier/>`__.
+        `example <https://github.com/kornia/tutorials/tree/master/scripts/training/image_classifier/>`__.
     """
 
     def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
         if len(args) != 2:
             raise AssertionError
         out, target = args
         acc1, acc5 = accuracy(out, target, topk=(1, 5))
-        return dict(top1=acc1.item(), top5=acc5.item())
+        return {"top1": acc1.item(), "top5": acc5.item()}
 
 
 class SemanticSegmentationTrainer(Trainer):
     """Module to be used for semantic segmentation purposes.
 
     The module subclasses :py:class:`~kornia.x.Trainer` and overrides the
     :py:func:`~kornia.x.Trainer.evaluate` function implementing IoU :py:func:`~kornia.metrics.mean_iou`.
 
     .. seealso::
         Learn how to use this class in the following
-        `example <https://github.com/kornia/kornia/blob/master/examples/train/semantic_segmentation/>`__.
+        `example <https://github.com/kornia/tutorials/tree/master/scripts/training/semantic_segmentation/>`__.
     """
 
     def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
         if len(args) != 2:
             raise AssertionError
         out, target = args
         iou = mean_iou(out.argmax(1), target, out.shape[1]).mean()
-        return dict(iou=iou.item())
+        return {"iou": iou.item()}
 
 
 class ObjectDetectionTrainer(Trainer):
     """Module to be used for object detection purposes.
 
     The module subclasses :py:class:`~kornia.x.Trainer` and overrides the
     :py:func:`~kornia.x.Trainer.evaluate` function implementing IoU :py:func:`~kornia.metrics.mean_iou`.
 
     .. seealso::
         Learn how to use this class in the following
-        `example <https://github.com/kornia/kornia/blob/master/examples/train/object_detection/>`__.
+        `example <https://github.com/kornia/tutorials/tree/master/scripts/training/object_detection/>`__.
     """
 
     def __init__(
         self,
         model: Module,
         train_dataloader: DataLoader[Any],
         valid_dataloader: DataLoader[Any],
         criterion: Optional[Module],
         optimizer: Optimizer,
-        scheduler: lr_scheduler.CosineAnnealingLR,
+        scheduler: lr_scheduler._LRScheduler,
         config: Configuration,
         num_classes: int,
         callbacks: Optional[Dict[str, Callable[..., None]]] = None,
         loss_computed_by_model: Optional[bool] = None,
     ) -> None:
         if callbacks is None:
             callbacks = {}
         super().__init__(model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, config, callbacks)
         # TODO: auto-detect if the model is from TorchVision
         self.loss_computed_by_model = loss_computed_by_model
         self.num_classes = num_classes
 
-    def on_model(self, model: Module, sample: Dict[str, Tensor]):
+    def on_model(self, model: Module, sample: Dict[str, Tensor]) -> Tensor:
         if self.loss_computed_by_model and model.training:
             return model(sample["input"], sample["target"])
         return model(sample["input"])
 
     def compute_loss(self, *args: Tensor) -> Tensor:
         if self.loss_computed_by_model:
             # Note: in case of dict losses obtained
             if isinstance(args[0], dict):
                 return stack([v for _, v in args[0].items()]).mean()
             return stack(list(args[0])).sum()
         if self.criterion is None:
             raise RuntimeError("`criterion` should not be None if `loss_computed_by_model` is False.")
         return self.criterion(*args)
 
-    def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
+    def compute_metrics(self, *args: Tuple[Dict[str, Tensor]]) -> Dict[str, float]:
         if (
             isinstance(args[0], dict)
             and "boxes" in args[0]
             and "labels" in args[0]
             and "scores" in args[0]
             and isinstance(args[1], dict)
             and "boxes" in args[1]
```

### Comparing `kornia-0.6.9/kornia/x/utils.py` & `kornia-0.7.0/kornia/x/utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from dataclasses import dataclass, field
 from enum import Enum
-from typing import Dict, Tuple
+from typing import Any, Callable, Dict, Tuple
 
-from kornia.core import Module
+from kornia.core import Module, Tensor
 from kornia.metrics.average_meter import AverageMeter
 
 # import yaml
 
 
 class TrainerState(Enum):
     STARTING = 0
@@ -48,30 +48,30 @@
         >>> import torch
         >>> import kornia as K
         >>> fcn = Lambda(lambda x: K.geometry.resize(x, (32, 16)))
         >>> fcn(torch.rand(1, 4, 64, 32)).shape
         torch.Size([1, 4, 32, 16])
     """
 
-    def __init__(self, fcn):
+    def __init__(self, fcn: Callable[..., Any]) -> None:
         super().__init__()
         self.fcn = fcn
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Any:
         return self.fcn(x)
 
 
 class StatsTracker:
     """Stats tracker for computing metrics on the fly."""
 
     def __init__(self) -> None:
         self._stats: Dict[str, AverageMeter] = {}
 
     @property
-    def stats(self):
+    def stats(self) -> Dict[str, AverageMeter]:
         return self._stats
 
     def update(self, key: str, val: float, batch_size: int) -> None:
         """Update the stats by the key value pair."""
         if key not in self._stats:
             self._stats[key] = AverageMeter()
         self._stats[key].update(val, batch_size)
```

### Comparing `kornia-0.6.9/kornia.egg-info/PKG-INFO` & `kornia-0.7.0/kornia.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,36 +1,38 @@
 Metadata-Version: 2.1
 Name: kornia
-Version: 0.6.9
+Version: 0.7.0
 Summary: Open Source Differentiable Computer Vision Library for PyTorch
-Home-page: https://www.kornia.org
-Download-URL: https://github.com/kornia/kornia
-Author: Edgar Riba
-Author-email: edgar@kornia.org
+Author-email: Edgar Riba <edgar@kornia.org>
 License: Apache-2.0
 Project-URL: Bug Tracker, https://github.com/kornia/kornia/issues
 Project-URL: Documentation, https://kornia.readthedocs.io/en/latest
+Project-URL: Download, https://github.com/kornia/kornia
+Project-URL: Homepage, https://www.kornia.org
 Project-URL: Source Code, https://github.com/kornia/kornia
 Keywords: computer vision,deep learning,pytorch
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
 Classifier: Environment :: GPU
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Natural Language :: English
 Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Processing
 Classifier: Topic :: Software Development :: Libraries
-Requires-Python: >=3.7
+Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Provides-Extra: dev
 Provides-Extra: docs
 Provides-Extra: x
 License-File: LICENSE
 
 <div align="center">
@@ -54,16 +56,18 @@
 [![PyPI python](https://img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia)
 [![PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/kornia)
 [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/project/kornia)
 [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENCE)
 [![Slack](https://img.shields.io/badge/Slack-4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)
 [![Twitter](https://img.shields.io/twitter/follow/kornia_foss?style=social)](https://twitter.com/kornia_foss)
 
-[![tests-cpu](https://github.com/kornia/kornia/actions/workflows/tests_cpu.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cpu.yml)
+[![tests-cpu](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml)
+[![tests-cpu-nightly](https://github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml)
 [![tests-cuda](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml)
+[![tests-cpu-float16](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml/badge.svg?event=schedule&&branch=master)](https://github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml)
 [![codecov](https://codecov.io/gh/kornia/kornia/branch/master/graph/badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/kornia)
 [![Documentation Status](https://readthedocs.org/projects/kornia/badge/?version=latest)](https://kornia.readthedocs.io/en/latest/?badge=latest)
 [![pre-commit.ci status](https://results.pre-commit.ci/badge/github/kornia/kornia/master.svg)](https://results.pre-commit.ci/latest/github/kornia/kornia/master)
 
 <a href="https://www.producthunt.com/posts/kornia?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-kornia" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=306439&theme=light" alt="Kornia - Computer vision library for deep learning | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /></a>
 
 </p>
@@ -146,28 +150,28 @@
 
 :triangular_flag_on_post: **Updates**
 - :white_check_mark: [Image Matching](https://kornia.readthedocs.io/en/latest/applications/image_matching.html) Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR).
 - :white_check_mark: [Face Detection](https://kornia.readthedocs.io/en/latest/applications/face_detection.html) Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/frapochetti/blurry-faces).
 
 ## Cite
 
-If you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](https://github.com/kornia/kornia/blob/master/CITATION.md).
+If you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](./CITATION.md).
 
   ```bibtex
   @inproceedings{eriba2019kornia,
     author    = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},
     title     = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},
     booktitle = {Winter Conference on Applications of Computer Vision},
     year      = {2020},
     url       = {https://arxiv.org/pdf/1910.02190.pdf}
   }
   ```
 
 ## Contributing
-We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](https://github.com/kornia/kornia/blob/master/CONTRIBUTING.rst) notes. The participation in this open source project is subject to [Code of Conduct](https://github.com/kornia/kornia/blob/master/CODE_OF_CONDUCT.md).
+We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes. The participation in this open source project is subject to [Code of Conduct](./CODE_OF_CONDUCT.md).
 
 
 ## Community
 - **Forums:** discuss implementations, research, etc. [GitHub Forums](https://github.com/kornia/kornia/discussions)
 - **GitHub Issues:** bug reports, feature requests, install issues, RFCs, thoughts, etc. [OPEN](https://github.com/kornia/kornia/issues/new/choose)
 - **Slack:** Join our workspace to keep in touch with our core contributors and be part of our community. [JOIN HERE](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)
 - For general information, please visit our website at www.kornia.org
```

#### html2text {}

```diff
@@ -1,44 +1,53 @@
-Metadata-Version: 2.1 Name: kornia Version: 0.6.9 Summary: Open Source
-Differentiable Computer Vision Library for PyTorch Home-page: https://
-www.kornia.org Download-URL: https://github.com/kornia/kornia Author: Edgar
-Riba Author-email: edgar@kornia.org License: Apache-2.0 Project-URL: Bug
-Tracker, https://github.com/kornia/kornia/issues Project-URL: Documentation,
-https://kornia.readthedocs.io/en/latest Project-URL: Source Code, https://
-github.com/kornia/kornia Keywords: computer vision,deep learning,pytorch
-Classifier: Development Status :: 4 - Beta Classifier: Environment :: Console
-Classifier: Environment :: GPU Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Education Classifier: Intended Audience ::
-Information Technology Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: Apache Software License Classifier:
-Natural Language :: English Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3 Classifier: Programming
-Language :: Python :: 3 :: Only Classifier: Topic :: Scientific/Engineering ::
-Artificial Intelligence Classifier: Topic :: Scientific/Engineering :: Image
-Processing Classifier: Topic :: Software Development :: Libraries Requires-
-Python: >=3.7 Description-Content-Type: text/markdown Provides-Extra: dev
-Provides-Extra: docs Provides-Extra: x License-File: LICENSE
+Metadata-Version: 2.1 Name: kornia Version: 0.7.0 Summary: Open Source
+Differentiable Computer Vision Library for PyTorch Author-email: Edgar Riba
+kornia.org> License: Apache-2.0 Project-URL: Bug Tracker, https://github.com/
+kornia/kornia/issues Project-URL: Documentation, https://kornia.readthedocs.io/
+en/latest Project-URL: Download, https://github.com/kornia/kornia Project-URL:
+Homepage, https://www.kornia.org Project-URL: Source Code, https://github.com/
+kornia/kornia Keywords: computer vision,deep learning,pytorch Classifier:
+Development Status :: 4 - Beta Classifier: Environment :: Console Classifier:
+Environment :: GPU Classifier: Intended Audience :: Developers Classifier:
+Intended Audience :: Education Classifier: Intended Audience :: Information
+Technology Classifier: Intended Audience :: Science/Research Classifier:
+License :: OSI Approved :: Apache Software License Classifier: Natural Language
+:: English Classifier: Operating System :: OS Independent Classifier:
+Programming Language :: Python :: 3 :: Only Classifier: Programming Language ::
+Python :: 3.8 Classifier: Programming Language :: Python :: 3.9 Classifier:
+Programming Language :: Python :: 3.10 Classifier: Programming Language ::
+Python :: 3.11 Classifier: Topic :: Scientific/Engineering :: Artificial
+Intelligence Classifier: Topic :: Scientific/Engineering :: Image Processing
+Classifier: Topic :: Software Development :: Libraries Requires-Python: >=3.8
+Description-Content-Type: text/markdown Provides-Extra: dev Provides-Extra:
+docs Provides-Extra: x License-File: LICENSE
        [https://github.com/kornia/data/raw/main/kornia_banner_pixie.png]
 --- English | [](README_zh-CN.md)  Website  Docs  Try_it_Now
    Tutorials  Examples  Blog  Community [![PyPI python](https://
   img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia) [!
 [PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/
    kornia) [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/
 project/kornia) [![License](https://img.shields.io/badge/License-Apache%202.0-
        blue.svg)](LICENCE) [![Slack](https://img.shields.io/badge/Slack-
      4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/
     shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA) [![Twitter](https://
  img.shields.io/twitter/follow/kornia_foss?style=social)](https://twitter.com/
 kornia_foss) [![tests-cpu](https://github.com/kornia/kornia/actions/workflows/
- tests_cpu.yml/badge.svg)](https://github.com/kornia/kornia/actions/workflows/
-    tests_cpu.yml) [![tests-cuda](https://github.com/kornia/kornia/actions/
-workflows/tests_cuda.yml/badge.svg)](https://github.com/kornia/kornia/actions/
-  workflows/tests_cuda.yml) [![codecov](https://codecov.io/gh/kornia/kornia/
-branch/master/graph/badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/
-kornia) [![Documentation Status](https://readthedocs.org/projects/kornia/badge/
+   scheduled_test_cpu.yml/badge.svg?event=schedule&&branch=master)](https://
+ github.com/kornia/kornia/actions/workflows/scheduled_test_cpu.yml) [![tests-
+       cpu-nightly](https://github.com/kornia/kornia/actions/workflows/
+ scheduled_test_nightly.yml/badge.svg?event=schedule&&branch=master)](https://
+   github.com/kornia/kornia/actions/workflows/scheduled_test_nightly.yml) [!
+[tests-cuda](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml/
+badge.svg)](https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml)
+   [![tests-cpu-float16](https://github.com/kornia/kornia/actions/workflows/
+scheduled_test_cpu_half.yml/badge.svg?event=schedule&&branch=master)](https://
+  github.com/kornia/kornia/actions/workflows/scheduled_test_cpu_half.yml) [!
+      [codecov](https://codecov.io/gh/kornia/kornia/branch/master/graph/
+     badge.svg?token=FzCb7e0Bso)](https://codecov.io/gh/kornia/kornia) [!
+     [Documentation Status](https://readthedocs.org/projects/kornia/badge/
   ?version=latest)](https://kornia.readthedocs.io/en/latest/?badge=latest) [!
    [pre-commit.ci status](https://results.pre-commit.ci/badge/github/kornia/
 kornia/master.svg)](https://results.pre-commit.ci/latest/github/kornia/kornia/
   master) [Kornia_-_Computer_vision_library_for_deep_learning_|_Product_Hunt]
 *Kornia* is a differentiable computer vision library for [PyTorch](https://
 pytorch.org). It consists of a set of routines and differentiable modules to
 solve generic computer vision problems. At its core, the package uses *PyTorch*
@@ -90,28 +99,26 @@
 Integrated to [Huggingface Spaces](https://huggingface.co/spaces). See [Gradio
 Web Demo](https://huggingface.co/spaces/akhaliq/Kornia-LoFTR). - :
 white_check_mark: [Face Detection](https://kornia.readthedocs.io/en/latest/
 applications/face_detection.html) Integrated to [Huggingface Spaces](https://
 huggingface.co/spaces). See [Gradio Web Demo](https://huggingface.co/spaces/
 frapochetti/blurry-faces). ## Cite If you are using kornia in your research-
 related documents, it is recommended that you cite the paper. See more in
-[CITATION](https://github.com/kornia/kornia/blob/master/CITATION.md). ```bibtex
-@inproceedings{eriba2019kornia, author = {E. Riba, D. Mishkin, D. Ponsa, E.
-Rublee and G. Bradski}, title = {Kornia: an Open Source Differentiable Computer
-Vision Library for PyTorch}, booktitle = {Winter Conference on Applications of
-Computer Vision}, year = {2020}, url = {https://arxiv.org/pdf/1910.02190.pdf} }
-``` ## Contributing We appreciate all contributions. If you are planning to
-contribute back bug-fixes, please do so without any further discussion. If you
-plan to contribute new features, utility functions or extensions, please first
-open an issue and discuss the feature with us. Please, consider reading the
-[CONTRIBUTING](https://github.com/kornia/kornia/blob/master/CONTRIBUTING.rst)
-notes. The participation in this open source project is subject to [Code of
-Conduct](https://github.com/kornia/kornia/blob/master/CODE_OF_CONDUCT.md). ##
-Community - **Forums:** discuss implementations, research, etc. [GitHub Forums]
-(https://github.com/kornia/kornia/discussions) - **GitHub Issues:** bug
-reports, feature requests, install issues, RFCs, thoughts, etc. [OPEN](https://
-github.com/kornia/kornia/issues/new/choose) - **Slack:** Join our workspace to
-keep in touch with our core contributors and be part of our community. [JOIN
-HERE](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-
-2AQRi~X9Uu6PLMuUZdvfjA) - For general information, please visit our website at
-www.kornia.org [https://contrib.rocks/image?repo=Kornia/kornia] Made with
-[contrib.rocks](https://contrib.rocks).
+[CITATION](./CITATION.md). ```bibtex @inproceedings{eriba2019kornia, author =
+{E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski}, title = {Kornia: an
+Open Source Differentiable Computer Vision Library for PyTorch}, booktitle =
+{Winter Conference on Applications of Computer Vision}, year = {2020}, url =
+{https://arxiv.org/pdf/1910.02190.pdf} } ``` ## Contributing We appreciate all
+contributions. If you are planning to contribute back bug-fixes, please do so
+without any further discussion. If you plan to contribute new features, utility
+functions or extensions, please first open an issue and discuss the feature
+with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes.
+The participation in this open source project is subject to [Code of Conduct]
+(./CODE_OF_CONDUCT.md). ## Community - **Forums:** discuss implementations,
+research, etc. [GitHub Forums](https://github.com/kornia/kornia/discussions) -
+**GitHub Issues:** bug reports, feature requests, install issues, RFCs,
+thoughts, etc. [OPEN](https://github.com/kornia/kornia/issues/new/choose) -
+**Slack:** Join our workspace to keep in touch with our core contributors and
+be part of our community. [JOIN HERE](https://join.slack.com/t/kornia/
+shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA) - For general information,
+please visit our website at www.kornia.org [https://contrib.rocks/
+image?repo=Kornia/kornia] Made with [contrib.rocks](https://contrib.rocks).
```

### Comparing `kornia-0.6.9/kornia.egg-info/SOURCES.txt` & `kornia-0.7.0/kornia.egg-info/SOURCES.txt`

 * *Files 16% similar despite different names*

```diff
@@ -1,12 +1,10 @@
 LICENSE
 README.md
 pyproject.toml
-setup.cfg
-setup.py
 kornia/__init__.py
 kornia/constants.py
 kornia/py.typed
 kornia.egg-info/PKG-INFO
 kornia.egg-info/SOURCES.txt
 kornia.egg-info/dependency_links.txt
 kornia.egg-info/requires.txt
@@ -25,17 +23,20 @@
 kornia/augmentation/_2d/geometric/fisheye.py
 kornia/augmentation/_2d/geometric/horizontal_flip.py
 kornia/augmentation/_2d/geometric/pad.py
 kornia/augmentation/_2d/geometric/perspective.py
 kornia/augmentation/_2d/geometric/resize.py
 kornia/augmentation/_2d/geometric/resized_crop.py
 kornia/augmentation/_2d/geometric/rotation.py
+kornia/augmentation/_2d/geometric/shear.py
 kornia/augmentation/_2d/geometric/thin_plate_spline.py
+kornia/augmentation/_2d/geometric/translate.py
 kornia/augmentation/_2d/geometric/vertical_flip.py
 kornia/augmentation/_2d/intensity/__init__.py
+kornia/augmentation/_2d/intensity/auto_contrast.py
 kornia/augmentation/_2d/intensity/base.py
 kornia/augmentation/_2d/intensity/box_blur.py
 kornia/augmentation/_2d/intensity/brightness.py
 kornia/augmentation/_2d/intensity/channel_shuffle.py
 kornia/augmentation/_2d/intensity/color_jiggle.py
 kornia/augmentation/_2d/intensity/color_jitter.py
 kornia/augmentation/_2d/intensity/contrast.py
@@ -44,81 +45,106 @@
 kornia/augmentation/_2d/intensity/erasing.py
 kornia/augmentation/_2d/intensity/gamma.py
 kornia/augmentation/_2d/intensity/gaussian_blur.py
 kornia/augmentation/_2d/intensity/gaussian_noise.py
 kornia/augmentation/_2d/intensity/grayscale.py
 kornia/augmentation/_2d/intensity/hue.py
 kornia/augmentation/_2d/intensity/invert.py
+kornia/augmentation/_2d/intensity/median_blur.py
 kornia/augmentation/_2d/intensity/motion_blur.py
 kornia/augmentation/_2d/intensity/normalize.py
 kornia/augmentation/_2d/intensity/planckian_jitter.py
 kornia/augmentation/_2d/intensity/plasma.py
 kornia/augmentation/_2d/intensity/posterize.py
+kornia/augmentation/_2d/intensity/random_rain.py
 kornia/augmentation/_2d/intensity/random_rgb_shift.py
+kornia/augmentation/_2d/intensity/random_snow.py
 kornia/augmentation/_2d/intensity/saturation.py
 kornia/augmentation/_2d/intensity/sharpness.py
 kornia/augmentation/_2d/intensity/solarize.py
 kornia/augmentation/_2d/mix/__init__.py
 kornia/augmentation/_2d/mix/base.py
 kornia/augmentation/_2d/mix/cutmix.py
 kornia/augmentation/_2d/mix/jigsaw.py
 kornia/augmentation/_2d/mix/mixup.py
 kornia/augmentation/_2d/mix/mosaic.py
 kornia/augmentation/_3d/__init__.py
 kornia/augmentation/_3d/base.py
 kornia/augmentation/_3d/geometric/__init__.py
 kornia/augmentation/_3d/geometric/affine.py
+kornia/augmentation/_3d/geometric/base.py
 kornia/augmentation/_3d/geometric/center_crop.py
 kornia/augmentation/_3d/geometric/crop.py
 kornia/augmentation/_3d/geometric/depthical_flip.py
 kornia/augmentation/_3d/geometric/horizontal_flip.py
 kornia/augmentation/_3d/geometric/perspective.py
 kornia/augmentation/_3d/geometric/rotation.py
 kornia/augmentation/_3d/geometric/vertical_flip.py
 kornia/augmentation/_3d/intensity/__init__.py
+kornia/augmentation/_3d/intensity/base.py
 kornia/augmentation/_3d/intensity/equalize.py
 kornia/augmentation/_3d/intensity/motion_blur.py
+kornia/augmentation/auto/__init__.py
+kornia/augmentation/auto/base.py
+kornia/augmentation/auto/autoaugment/__init__.py
+kornia/augmentation/auto/autoaugment/autoaugment.py
+kornia/augmentation/auto/autoaugment/ops.py
+kornia/augmentation/auto/operations/__init__.py
+kornia/augmentation/auto/operations/base.py
+kornia/augmentation/auto/operations/ops.py
+kornia/augmentation/auto/operations/policy.py
+kornia/augmentation/auto/rand_augment/__init__.py
+kornia/augmentation/auto/rand_augment/ops.py
+kornia/augmentation/auto/rand_augment/rand_augment.py
+kornia/augmentation/auto/trivial_augment/__init__.py
+kornia/augmentation/auto/trivial_augment/trivial_augment.py
 kornia/augmentation/container/__init__.py
 kornia/augmentation/container/augment.py
 kornia/augmentation/container/base.py
 kornia/augmentation/container/dispatcher.py
 kornia/augmentation/container/image.py
+kornia/augmentation/container/ops.py
+kornia/augmentation/container/params.py
 kornia/augmentation/container/patch.py
-kornia/augmentation/container/utils.py
 kornia/augmentation/container/video.py
 kornia/augmentation/random_generator/__init__.py
 kornia/augmentation/random_generator/base.py
 kornia/augmentation/random_generator/utils.py
 kornia/augmentation/random_generator/_2d/__init__.py
 kornia/augmentation/random_generator/_2d/affine.py
 kornia/augmentation/random_generator/_2d/color_jiggle.py
 kornia/augmentation/random_generator/_2d/color_jitter.py
 kornia/augmentation/random_generator/_2d/crop.py
 kornia/augmentation/random_generator/_2d/cutmix.py
+kornia/augmentation/random_generator/_2d/gaussian_blur.py
 kornia/augmentation/random_generator/_2d/jigsaw.py
 kornia/augmentation/random_generator/_2d/mixup.py
 kornia/augmentation/random_generator/_2d/mosaic.py
 kornia/augmentation/random_generator/_2d/motion_blur.py
 kornia/augmentation/random_generator/_2d/perspective.py
 kornia/augmentation/random_generator/_2d/plain_uniform.py
 kornia/augmentation/random_generator/_2d/planckian_jitter.py
 kornia/augmentation/random_generator/_2d/posterize.py
 kornia/augmentation/random_generator/_2d/probability.py
+kornia/augmentation/random_generator/_2d/random_rain.py
 kornia/augmentation/random_generator/_2d/rectangle_earase.py
 kornia/augmentation/random_generator/_2d/resize.py
+kornia/augmentation/random_generator/_2d/shear.py
+kornia/augmentation/random_generator/_2d/translate.py
 kornia/augmentation/random_generator/_3d/__init__.py
 kornia/augmentation/random_generator/_3d/affine.py
 kornia/augmentation/random_generator/_3d/crop.py
 kornia/augmentation/random_generator/_3d/motion_blur.py
 kornia/augmentation/random_generator/_3d/perspective.py
 kornia/augmentation/random_generator/_3d/rotation.py
 kornia/augmentation/utils/__init__.py
 kornia/augmentation/utils/helpers.py
 kornia/augmentation/utils/param_validation.py
 kornia/color/__init__.py
+kornia/color/colormap.py
 kornia/color/gray.py
 kornia/color/hls.py
 kornia/color/hsv.py
 kornia/color/lab.py
 kornia/color/luv.py
 kornia/color/raw.py
 kornia/color/rgb.py
@@ -131,50 +157,84 @@
 kornia/contrib/connected_components.py
 kornia/contrib/diamond_square.py
 kornia/contrib/distance_transform.py
 kornia/contrib/edge_detection.py
 kornia/contrib/extract_patches.py
 kornia/contrib/face_detection.py
 kornia/contrib/histogram_matching.py
+kornia/contrib/image_prompter.py
 kornia/contrib/image_stitching.py
 kornia/contrib/lambda_module.py
+kornia/contrib/object_detection.py
+kornia/contrib/visual_prompter.py
 kornia/contrib/vit.py
 kornia/contrib/vit_mobile.py
+kornia/contrib/models/__init__.py
+kornia/contrib/models/base.py
+kornia/contrib/models/common.py
+kornia/contrib/models/structures.py
+kornia/contrib/models/tiny_vit.py
+kornia/contrib/models/rt_detr/__init__.py
+kornia/contrib/models/rt_detr/model.py
+kornia/contrib/models/rt_detr/post_processor.py
+kornia/contrib/models/rt_detr/architecture/__init__.py
+kornia/contrib/models/rt_detr/architecture/hgnetv2.py
+kornia/contrib/models/rt_detr/architecture/hybrid_encoder.py
+kornia/contrib/models/rt_detr/architecture/resnet_d.py
+kornia/contrib/models/rt_detr/architecture/rtdetr_head.py
+kornia/contrib/models/sam/__init__.py
+kornia/contrib/models/sam/model.py
+kornia/contrib/models/sam/architecture/__init__.py
+kornia/contrib/models/sam/architecture/common.py
+kornia/contrib/models/sam/architecture/image_encoder.py
+kornia/contrib/models/sam/architecture/mask_decoder.py
+kornia/contrib/models/sam/architecture/prompt_encoder.py
+kornia/contrib/models/sam/architecture/transformer.py
 kornia/core/__init__.py
 kornia/core/_backend.py
+kornia/core/check.py
 kornia/core/tensor_wrapper.py
 kornia/enhance/__init__.py
 kornia/enhance/adjust.py
 kornia/enhance/core.py
 kornia/enhance/equalization.py
 kornia/enhance/histogram.py
+kornia/enhance/integral.py
 kornia/enhance/normalize.py
 kornia/enhance/shift_rgb.py
 kornia/enhance/zca.py
 kornia/feature/__init__.py
 kornia/feature/affine_shape.py
 kornia/feature/defmo.py
 kornia/feature/hardnet.py
 kornia/feature/hynet.py
 kornia/feature/integrated.py
 kornia/feature/keynet.py
 kornia/feature/laf.py
+kornia/feature/lightglue.py
 kornia/feature/matching.py
 kornia/feature/mkd.py
 kornia/feature/orientation.py
 kornia/feature/responses.py
 kornia/feature/scale_space_detector.py
 kornia/feature/siftdesc.py
 kornia/feature/sosnet.py
 kornia/feature/tfeat.py
 kornia/feature/adalam/__init__.py
 kornia/feature/adalam/adalam.py
 kornia/feature/adalam/core.py
 kornia/feature/adalam/ransac.py
 kornia/feature/adalam/utils.py
+kornia/feature/disk/__init__.py
+kornia/feature/disk/detector.py
+kornia/feature/disk/disk.py
+kornia/feature/disk/structs.py
+kornia/feature/disk/_unets/__init__.py
+kornia/feature/disk/_unets/blocks.py
+kornia/feature/disk/_unets/unet.py
 kornia/feature/loftr/__init__.py
 kornia/feature/loftr/loftr.py
 kornia/feature/loftr/backbone/__init__.py
 kornia/feature/loftr/backbone/resnet_fpn.py
 kornia/feature/loftr/loftr_module/__init__.py
 kornia/feature/loftr/loftr_module/fine_preprocess.py
 kornia/feature/loftr/loftr_module/linear_attention.py
@@ -186,33 +246,36 @@
 kornia/feature/loftr/utils/position_encoding.py
 kornia/feature/loftr/utils/supervision.py
 kornia/feature/sold2/__init__.py
 kornia/feature/sold2/backbones.py
 kornia/feature/sold2/sold2.py
 kornia/feature/sold2/sold2_detector.py
 kornia/filters/__init__.py
+kornia/filters/bilateral.py
 kornia/filters/blur.py
 kornia/filters/blur_pool.py
 kornia/filters/canny.py
 kornia/filters/dexined.py
 kornia/filters/filter.py
 kornia/filters/gaussian.py
+kornia/filters/guided.py
 kornia/filters/kernels.py
 kornia/filters/kernels_geometry.py
 kornia/filters/laplacian.py
 kornia/filters/median.py
 kornia/filters/motion.py
 kornia/filters/sobel.py
 kornia/filters/unsharp.py
 kornia/geometry/__init__.py
 kornia/geometry/bbox.py
 kornia/geometry/boxes.py
 kornia/geometry/conversions.py
 kornia/geometry/depth.py
 kornia/geometry/homography.py
+kornia/geometry/keypoints.py
 kornia/geometry/linalg.py
 kornia/geometry/line.py
 kornia/geometry/plane.py
 kornia/geometry/quaternion.py
 kornia/geometry/ransac.py
 kornia/geometry/ray.py
 kornia/geometry/vector.py
@@ -234,14 +297,16 @@
 kornia/geometry/epipolar/triangulation.py
 kornia/geometry/liegroup/__init__.py
 kornia/geometry/liegroup/_utils.py
 kornia/geometry/liegroup/se2.py
 kornia/geometry/liegroup/se3.py
 kornia/geometry/liegroup/so2.py
 kornia/geometry/liegroup/so3.py
+kornia/geometry/solvers/__init__.py
+kornia/geometry/solvers/polynomial_solver.py
 kornia/geometry/subpix/__init__.py
 kornia/geometry/subpix/dsnt.py
 kornia/geometry/subpix/nms.py
 kornia/geometry/subpix/spatial_soft_argmax.py
 kornia/geometry/transform/__init__.py
 kornia/geometry/transform/affwarp.py
 kornia/geometry/transform/crop2d.py
@@ -251,52 +316,70 @@
 kornia/geometry/transform/homography_warper.py
 kornia/geometry/transform/image_registrator.py
 kornia/geometry/transform/imgwarp.py
 kornia/geometry/transform/pyramid.py
 kornia/geometry/transform/thin_plate_spline.py
 kornia/grad_estimator/__init__.py
 kornia/grad_estimator/ste.py
+kornia/image/__init__.py
+kornia/image/base.py
+kornia/image/image.py
 kornia/io/__init__.py
 kornia/io/io.py
 kornia/losses/__init__.py
+kornia/losses/cauchy.py
+kornia/losses/charbonnier.py
 kornia/losses/depth_smooth.py
 kornia/losses/dice.py
 kornia/losses/divergence.py
 kornia/losses/focal.py
+kornia/losses/geman_mcclure.py
 kornia/losses/hausdorff.py
 kornia/losses/lovasz_hinge.py
 kornia/losses/lovasz_softmax.py
 kornia/losses/ms_ssim.py
 kornia/losses/psnr.py
 kornia/losses/ssim.py
+kornia/losses/ssim3d.py
 kornia/losses/total_variation.py
 kornia/losses/tversky.py
+kornia/losses/welsch.py
 kornia/metrics/__init__.py
 kornia/metrics/accuracy.py
 kornia/metrics/average_meter.py
 kornia/metrics/confusion_matrix.py
 kornia/metrics/mean_average_precision.py
 kornia/metrics/mean_iou.py
 kornia/metrics/psnr.py
 kornia/metrics/ssim.py
+kornia/metrics/ssim3d.py
 kornia/morphology/__init__.py
 kornia/morphology/morphology.py
+kornia/sensors/__init__.py
+kornia/sensors/camera/__init__.py
+kornia/sensors/camera/camera_model.py
+kornia/sensors/camera/distortion_model.py
+kornia/sensors/camera/projection_model.py
 kornia/testing/__init__.py
 kornia/tracking/__init__.py
 kornia/tracking/planar_tracker.py
 kornia/utils/__init__.py
 kornia/utils/_compat.py
 kornia/utils/draw.py
 kornia/utils/grid.py
 kornia/utils/helpers.py
 kornia/utils/image.py
+kornia/utils/image_print.py
 kornia/utils/memory.py
 kornia/utils/misc.py
 kornia/utils/one_hot.py
 kornia/utils/pointcloud_io.py
 kornia/x/__init__.py
 kornia/x/callbacks.py
 kornia/x/trainer.py
 kornia/x/trainers.py
 kornia/x/utils.py
-test/test_contrib.py
+requirements/requirements-dev.txt
+requirements/requirements-docs.txt
+requirements/requirements-x.txt
+requirements/requirements.txt
 test/test_metrics.py
```

### Comparing `kornia-0.6.9/test/test_metrics.py` & `kornia-0.7.0/test/test_metrics.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import pytest
 import torch
 
 import kornia
-from kornia.testing import assert_close
+from kornia.testing import BaseTester, assert_close, tensor_to_gradcheck_var
 
 
 class TestMeanIoU:
     def test_two_classes_perfect(self, device, dtype):
         batch_size = 1
         num_classes = 2
         actual = torch.tensor([[1, 1, 1, 1, 0, 0, 0, 0]], device=device, dtype=torch.long)
@@ -216,7 +216,139 @@
         scores = torch.tensor([0.7], device=device, dtype=dtype)
 
         gt_boxes = torch.tensor([[100, 50, 150, 100.0]], device=device, dtype=dtype)
         gt_labels = torch.tensor([1], device=device, dtype=torch.long)
 
         with pytest.raises(AssertionError):
             _ = kornia.metrics.mean_average_precision(boxes[0], [labels], [scores], [gt_boxes], [gt_labels], 2)
+
+
+class TestSSIM3d(BaseTester):
+    @pytest.mark.parametrize(
+        "shape,padding,window_size,max_value",
+        [
+            ((1, 1, 3, 3, 3), 'same', 5, 1.0),
+            ((1, 1, 3, 3, 3), 'same', 3, 2.0),
+            ((1, 1, 3, 3, 3), 'same', 3, 0.5),
+            ((1, 1, 3, 3, 3), 'valid', 3, 1.0),
+            ((2, 4, 3, 3, 3), 'same', 3, 1.0),
+        ],
+    )
+    def test_smoke(self, shape, padding, window_size, max_value, device, dtype):
+        img_a = (torch.ones(shape, device=device, dtype=dtype) * max_value).clamp(0.0, max_value)
+        img_b = torch.zeros(shape, device=device, dtype=dtype)
+
+        actual = kornia.metrics.ssim3d(img_a, img_b, window_size, max_value, padding=padding)
+        expected = torch.ones_like(actual, device=device, dtype=dtype)
+
+        self.assert_close(actual, expected * 0.0001)
+
+        actual = kornia.metrics.ssim3d(img_a, img_a, window_size, max_value, padding=padding)
+        self.assert_close(actual, expected)
+
+    @pytest.mark.parametrize(
+        "shape,padding,window_size,expected",
+        [
+            ((1, 1, 2, 2, 3), 'same', 3, (1, 1, 2, 2, 3)),
+            ((1, 1, 3, 3, 3), 'same', 5, (1, 1, 3, 3, 3)),
+            ((1, 1, 3, 3, 3), 'valid', 3, (1, 1, 1, 1, 1)),
+            ((2, 4, 3, 3, 3), 'same', 3, (2, 4, 3, 3, 3)),
+        ],
+    )
+    def test_cardinality(self, shape, padding, window_size, expected, device, dtype):
+        img = torch.rand(shape, device=device, dtype=dtype)
+
+        actual = kornia.metrics.ssim3d(img, img, window_size, padding=padding)
+
+        assert actual.shape == expected
+
+    def test_exception(self, device, dtype):
+        img = torch.rand(1, 1, 3, 3, 3, device=device, dtype=dtype)
+
+        # Check if both are tensors
+        with pytest.raises(TypeError) as errinfo:
+            kornia.metrics.ssim3d(1.0, img, 3)
+        assert 'Not a Tensor type. Got:' in str(errinfo)
+
+        with pytest.raises(TypeError) as errinfo:
+            kornia.metrics.ssim3d(img, 1.0, 3)
+        assert 'Not a Tensor type. Got:' in str(errinfo)
+
+        # Check both shapes
+        img_wrong_shape = torch.rand(3, 3, device=device, dtype=dtype)
+        with pytest.raises(TypeError) as errinfo:
+            kornia.metrics.ssim3d(img, img_wrong_shape, 3)
+        assert 'shape must be [' in str(errinfo)
+
+        with pytest.raises(TypeError) as errinfo:
+            kornia.metrics.ssim3d(img_wrong_shape, img, 3)
+        assert 'shape must be [' in str(errinfo)
+
+        # Check if same shape
+        img_b = torch.rand(1, 1, 3, 3, 4, device=device, dtype=dtype)
+        with pytest.raises(Exception) as errinfo:
+            kornia.metrics.ssim3d(img, img_b, 3)
+        assert 'img1 and img2 shapes must be the same. Got:' in str(errinfo)
+
+    def test_unit(self, device, dtype):
+        img_a = torch.tensor(
+            [
+                [
+                    [
+                        [[0.7, 1.0, 0.5], [1.0, 0.3, 1.0], [0.2, 1.0, 0.1]],
+                        [[0.2, 1.0, 0.1], [1.0, 0.3, 1.0], [0.7, 1.0, 0.5]],
+                        [[1.0, 0.3, 1.0], [0.7, 1.0, 0.5], [0.2, 1.0, 0.1]],
+                    ]
+                ]
+            ],
+            device=device,
+            dtype=dtype,
+        )
+
+        img_b = torch.ones(1, 1, 3, 3, 3, device=device, dtype=dtype) * 0.5
+
+        actual = kornia.metrics.ssim3d(img_a, img_b, 3, padding='same')
+
+        expected = torch.tensor(
+            [
+                [
+                    [
+                        [[0.0093, 0.0080, 0.0075], [0.0075, 0.0068, 0.0063], [0.0067, 0.0060, 0.0056]],
+                        [[0.0077, 0.0070, 0.0065], [0.0077, 0.0069, 0.0064], [0.0075, 0.0066, 0.0062]],
+                        [[0.0075, 0.0069, 0.0064], [0.0078, 0.0070, 0.0065], [0.0077, 0.0067, 0.0064]],
+                    ]
+                ]
+            ],
+            device=device,
+            dtype=dtype,
+        )
+
+        self.assert_close(actual, expected, atol=1e-4, rtol=1e-4)
+
+    @pytest.mark.parametrize(
+        "shape,padding,window_size,max_value",
+        [
+            ((1, 1, 3, 3, 3), 'same', 5, 1.0),
+            ((1, 1, 3, 3, 3), 'same', 3, 2.0),
+            ((1, 1, 3, 3, 3), 'same', 3, 0.5),
+            ((1, 1, 3, 3, 3), 'valid', 3, 1.0),
+        ],
+    )
+    def test_module(self, shape, padding, window_size, max_value, device, dtype):
+        img_a = torch.rand(shape, device=device, dtype=dtype).clamp(0.0, max_value)
+        img_b = torch.rand(shape, device=device, dtype=dtype).clamp(0.0, max_value)
+
+        ops = kornia.metrics.ssim3d
+        mod = kornia.metrics.SSIM3D(window_size, max_value, padding=padding)
+
+        ops_out = ops(img_a, img_b, window_size, max_value, padding=padding)
+        mod_out = mod(img_a, img_b)
+
+        self.assert_close(ops_out, mod_out)
+
+    def test_gradcheck(self, device):
+        img = torch.rand(1, 1, 3, 3, 3, device=device)
+
+        op = kornia.metrics.ssim3d
+        img = tensor_to_gradcheck_var(img)
+
+        assert self.gradcheck(op, (img, img, 3), nondet_tol=1e-8)
```

