# Comparing `tmp/pyBPPIBridge-0.4.10-py3-none-any.whl.zip` & `tmp/pyBPPIBridge-0.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,51 +1,51 @@
-Zip file size: 44358 bytes, number of entries: 49
--rw-rw-r--  2.0 unx      147 b- defN 23-Jul-25 10:13 bppibridge.py
--rw-rw-r--  2.0 unx      401 b- defN 23-Jul-25 10:13 bppibridgesq.py
--rw-rw-r--  2.0 unx      439 b- defN 23-Jul-25 10:13 bppibridge/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-07 16:35 config/__init__.py
--rw-rw-r--  2.0 unx     4005 b- defN 23-Jul-25 10:13 config/appConfig.py
--rw-rw-r--  2.0 unx     5639 b- defN 23-Jul-25 10:13 config/cmdLineConfig.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-07 16:35 pipelines/__init__.py
--rw-rw-r--  2.0 unx     2417 b- defN 23-Jul-28 09:56 pipelines/pipeline.py
--rw-rw-r--  2.0 unx     4736 b- defN 23-Jul-25 10:13 pipelines/pipelineFactory.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-05 07:02 pipelines/bppi/__init__.py
--rw-rw-r--  2.0 unx     8041 b- defN 23-Jul-28 16:23 pipelines/bppi/bppiPipeline.py
--rw-rw-r--  2.0 unx     1334 b- defN 23-Jul-27 09:35 pipelines/bppi/uploadConfig.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-07 16:35 pipelines/bppi/project/__init__.py
--rw-rw-r--  2.0 unx      930 b- defN 23-Jul-25 10:13 pipelines/bppi/project/bppiApiProjectWrapper.py
--rw-rw-r--  2.0 unx      508 b- defN 23-Jul-27 20:44 pipelines/bppi/project/bppiProject.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-07 16:35 pipelines/bppi/repository/__init__.py
--rw-rw-r--  2.0 unx     8770 b- defN 23-Jul-28 16:23 pipelines/bppi/repository/bppiApiRepositoryWrapper.py
--rw-rw-r--  2.0 unx     7064 b- defN 23-Jul-28 16:17 pipelines/bppi/repository/bppiRepository.py
--rw-rw-r--  2.0 unx     2126 b- defN 23-Jul-25 10:13 pipelines/bppi/repository/repConfig.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-07 16:35 pipelines/project/__init__.py
--rw-rw-r--  2.0 unx      650 b- defN 23-Jul-27 09:42 pipelines/readers/Reader.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-24 15:41 pipelines/readers/__init__.py
--rw-rw-r--  2.0 unx      519 b- defN 23-Jul-27 10:18 pipelines/readers/bpAPIReader.py
--rw-rw-r--  2.0 unx     1016 b- defN 23-Jul-27 10:08 pipelines/readers/csvFileReader.py
--rw-rw-r--  2.0 unx     1068 b- defN 23-Jul-27 10:06 pipelines/readers/excelFileReader.py
--rw-rw-r--  2.0 unx     1261 b- defN 23-Jul-27 09:55 pipelines/readers/odbcReader.py
--rw-rw-r--  2.0 unx     3812 b- defN 23-Jul-27 09:42 pipelines/readers/sapRFCTableReader.py
--rw-rw-r--  2.0 unx     3881 b- defN 23-Jul-27 09:42 pipelines/readers/xesFileReader.py
--rw-rw-r--  2.0 unx     2147 b- defN 23-Jul-25 10:13 pipelines/readers/builders/SQLBuilder.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-07 16:35 pipelines/readers/builders/__init__.py
--rw-rw-r--  2.0 unx     3105 b- defN 23-Jul-25 10:13 pipelines/readers/builders/blueprismSQLBuilder.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-28 16:09 pipelines/repository/__init__.py
--rw-rw-r--  2.0 unx    10473 b- defN 23-Jul-28 16:24 pipelines/repository/bppiPLRBluePrismApi.py
--rw-rw-r--  2.0 unx    11539 b- defN 23-Jul-28 16:08 pipelines/repository/bppiPLRBluePrismRepo.py
--rw-rw-r--  2.0 unx     1365 b- defN 23-Jul-28 16:21 pipelines/repository/bppiPLRCSVFile.py
--rw-rw-r--  2.0 unx      831 b- defN 23-Jul-28 10:04 pipelines/repository/bppiPLRChorusExtract.py
--rw-rw-r--  2.0 unx     1492 b- defN 23-Jul-28 16:24 pipelines/repository/bppiPLRExcelFile.py
--rw-rw-r--  2.0 unx     1726 b- defN 23-Jul-28 16:09 pipelines/repository/bppiPLRODBC.py
--rw-rw-r--  2.0 unx     2811 b- defN 23-Jul-28 16:24 pipelines/repository/bppiPLRSAPRfcTable.py
--rw-rw-r--  2.0 unx     1264 b- defN 23-Jul-28 16:24 pipelines/repository/bppiPLRXESFile.py
--rw-rw-r--  2.0 unx      537 b- defN 23-Jul-25 10:13 utils/__init__.py
--rw-rw-r--  2.0 unx     8863 b- defN 23-Jul-25 10:13 utils/constants.py
--rw-rw-r--  2.0 unx     2016 b- defN 23-Jul-25 10:13 utils/log.py
--rw-rw-r--  2.0 unx     1070 b- defN 23-Jul-31 08:22 pyBPPIBridge-0.4.10.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3753 b- defN 23-Jul-31 08:22 pyBPPIBridge-0.4.10.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jul-31 08:22 pyBPPIBridge-0.4.10.dist-info/WHEEL
--rw-rw-r--  2.0 unx       47 b- defN 23-Jul-31 08:22 pyBPPIBridge-0.4.10.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       47 b- defN 23-Jul-31 08:22 pyBPPIBridge-0.4.10.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4309 b- defN 23-Jul-31 08:22 pyBPPIBridge-0.4.10.dist-info/RECORD
-49 files, 116251 bytes uncompressed, 37392 bytes compressed:  67.8%
+Zip file size: 44306 bytes, number of entries: 49
+-rw-rw-rw-  2.0 fat      154 b- defN 23-Jul-25 17:11 bppibridge.py
+-rw-rw-rw-  2.0 fat      413 b- defN 23-Jul-25 17:11 bppibridgesq.py
+-rw-rw-rw-  2.0 fat      569 b- defN 23-Aug-01 14:12 bppibridge/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 config/__init__.py
+-rw-rw-rw-  2.0 fat     4112 b- defN 23-Jul-25 17:11 config/appConfig.py
+-rw-rw-rw-  2.0 fat     3059 b- defN 23-Aug-02 07:20 config/cmdLineConfig.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 pipelines/__init__.py
+-rw-rw-rw-  2.0 fat     3437 b- defN 23-Aug-01 12:53 pipelines/pipeline.py
+-rw-rw-rw-  2.0 fat     4644 b- defN 23-Aug-01 14:13 pipelines/pipelineFactory.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/bppi/__init__.py
+-rw-rw-rw-  2.0 fat     5058 b- defN 23-Aug-01 08:49 pipelines/bppi/bppiPipeline.py
+-rw-rw-rw-  2.0 fat     1384 b- defN 23-Aug-01 08:08 pipelines/bppi/uploadConfig.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/bppi/project/__init__.py
+-rw-rw-rw-  2.0 fat      969 b- defN 23-Aug-01 08:08 pipelines/bppi/project/bppiApiProjectWrapper.py
+-rw-rw-rw-  2.0 fat      523 b- defN 23-Aug-01 08:08 pipelines/bppi/project/bppiProject.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/bppi/repository/__init__.py
+-rw-rw-rw-  2.0 fat     8963 b- defN 23-Aug-01 08:08 pipelines/bppi/repository/bppiApiRepositoryWrapper.py
+-rw-rw-rw-  2.0 fat     9166 b- defN 23-Aug-01 12:53 pipelines/bppi/repository/bppiRepository.py
+-rw-rw-rw-  2.0 fat     2195 b- defN 23-Aug-01 08:08 pipelines/bppi/repository/repConfig.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 pipelines/project/__init__.py
+-rw-rw-rw-  2.0 fat      679 b- defN 23-Aug-01 08:08 pipelines/readers/Reader.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-25 17:11 pipelines/readers/__init__.py
+-rw-rw-rw-  2.0 fat     9613 b- defN 23-Aug-02 08:30 pipelines/readers/bpAPIReader.py
+-rw-rw-rw-  2.0 fat     1052 b- defN 23-Aug-01 08:08 pipelines/readers/csvFileReader.py
+-rw-rw-rw-  2.0 fat     1105 b- defN 23-Aug-01 08:08 pipelines/readers/excelFileReader.py
+-rw-rw-rw-  2.0 fat     1422 b- defN 23-Aug-02 12:19 pipelines/readers/odbcReader.py
+-rw-rw-rw-  2.0 fat     3913 b- defN 23-Aug-01 08:08 pipelines/readers/sapRFCTableReader.py
+-rw-rw-rw-  2.0 fat     3989 b- defN 23-Aug-01 08:08 pipelines/readers/xesFileReader.py
+-rw-rw-rw-  2.0 fat     2209 b- defN 23-Aug-02 11:46 pipelines/readers/builders/SQLBuilder.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/readers/builders/__init__.py
+-rw-rw-rw-  2.0 fat     3676 b- defN 23-Aug-02 12:06 pipelines/readers/builders/blueprismSQLBuilder.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 pipelines/repository/__init__.py
+-rw-rw-rw-  2.0 fat     2164 b- defN 23-Aug-02 08:23 pipelines/repository/bppiPLRBluePrismApi.py
+-rw-rw-rw-  2.0 fat    12377 b- defN 23-Aug-02 12:36 pipelines/repository/bppiPLRBluePrismRepo.py
+-rw-rw-rw-  2.0 fat     1338 b- defN 23-Aug-01 12:54 pipelines/repository/bppiPLRCSVFile.py
+-rw-rw-rw-  2.0 fat      788 b- defN 23-Aug-01 12:57 pipelines/repository/bppiPLRChorusExtract.py
+-rw-rw-rw-  2.0 fat     1467 b- defN 23-Aug-01 12:57 pipelines/repository/bppiPLRExcelFile.py
+-rw-rw-rw-  2.0 fat     1888 b- defN 23-Aug-02 11:48 pipelines/repository/bppiPLRODBC.py
+-rw-rw-rw-  2.0 fat     2590 b- defN 23-Aug-01 13:43 pipelines/repository/bppiPLRSAPRfcTable.py
+-rw-rw-rw-  2.0 fat     1303 b- defN 23-Aug-01 12:58 pipelines/repository/bppiPLRXESFile.py
+-rw-rw-rw-  2.0 fat      555 b- defN 23-Jul-25 17:11 utils/__init__.py
+-rw-rw-rw-  2.0 fat     8001 b- defN 23-Aug-02 07:13 utils/constants.py
+-rw-rw-rw-  2.0 fat     2071 b- defN 23-Jul-25 17:11 utils/log.py
+-rw-rw-rw-  2.0 fat     1091 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     3868 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       47 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       47 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4303 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/RECORD
+49 files, 116294 bytes uncompressed, 37352 bytes compressed:  67.9%
```

## zipnote {}

```diff
@@ -123,26 +123,26 @@
 
 Filename: utils/constants.py
 Comment: 
 
 Filename: utils/log.py
 Comment: 
 
-Filename: pyBPPIBridge-0.4.10.dist-info/LICENSE
+Filename: pyBPPIBridge-0.5.0.dist-info/LICENSE
 Comment: 
 
-Filename: pyBPPIBridge-0.4.10.dist-info/METADATA
+Filename: pyBPPIBridge-0.5.0.dist-info/METADATA
 Comment: 
 
-Filename: pyBPPIBridge-0.4.10.dist-info/WHEEL
+Filename: pyBPPIBridge-0.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: pyBPPIBridge-0.4.10.dist-info/entry_points.txt
+Filename: pyBPPIBridge-0.5.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: pyBPPIBridge-0.4.10.dist-info/top_level.txt
+Filename: pyBPPIBridge-0.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: pyBPPIBridge-0.4.10.dist-info/RECORD
+Filename: pyBPPIBridge-0.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## bppibridge.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-from bppibridge import main
-
-if __name__ == "__main__":
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+from bppibridge import main
+
+if __name__ == "__main__":
 	main()
```

## bppibridgesq.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import argparse
-from pipelines.pipelineFactory import pipelineFactory
-from config.cmdLineConfig import cmdLineConfig
-
-if __name__ == "__main__":
-	# Get configuration from cmdline & ini file
-	config, src = cmdLineConfig.readSqlite(argparse.ArgumentParser())
-	# Process 
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import argparse
+from pipelines.pipelineFactory import pipelineFactory
+from config.cmdLineConfig import cmdLineConfig
+
+if __name__ == "__main__":
+	# Get configuration from cmdline & ini file
+	config, src = cmdLineConfig.readSqlite(argparse.ArgumentParser())
+	# Process 
 	pipelineFactory(src, config).createAndExecute()
```

## bppibridge/__init__.py

```diff
@@ -1,15 +1,19 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import argparse
-from pipelines.pipelineFactory import pipelineFactory
-from config.cmdLineConfig import cmdLineConfig
-
-def main() -> None:
-	"""Entry point for the application script"""
-	
-	# Get configuration from cmdline & ini file
-	config, src = cmdLineConfig.readIni(argparse.ArgumentParser())
-	# Process 
-	pipelineFactory(src, config).createAndExecute()
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import argparse
+from pipelines.pipelineFactory import pipelineFactory
+from config.cmdLineConfig import cmdLineConfig
+from utils.log import log
+import utils.constants as C
+
+def main() -> None:
+	"""Entry point for the application script"""
+	
+	# Get configuration from cmdline & ini file
+	config = cmdLineConfig.readIni(argparse.ArgumentParser())
+	# Get the logger
+	log = pipelineFactory.getLogger(config)
+	# Execute the pipeline 
+	pipelineFactory(config, log).process()
```

## config/appConfig.py

 * *Ordering differences only*

```diff
@@ -1,108 +1,108 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import configparser
-import utils.constants as C
-from utils import cursorByField 
-import sqlite3
-
-SECTION_PARAM_SEP = "."
-
-class appConfig():
-    """This class contains all the configuration needed and loaded mainly from the INI file
-    """
-
-    def __init__(self):
-        self.__parameters = {}
-        return
-
-    def addParameter(self, name, value):
-        """Add a new parameter in the list
-        Args:
-            name (str): paramter name
-            value (str): parameter value
-        """
-        try:
-            self.__parameters[name] = value
-        except Exception as e:
-            print("addParameter() -> " + str(e))
-
-    def loadFromINIFile(self, filename) -> bool:
-        """ Load the configuration from the INI file in parameter
-        Args:
-            filename (str): INI file name
-        Returns:
-            bool: False if error
-        """
-        try:
-            myConfig = configparser.ConfigParser()
-            myConfig.read(filename)
-            for section in myConfig:
-                for param in myConfig[section]:
-                    try :
-                        self.__parameters[section + SECTION_PARAM_SEP + param] = myConfig[section][param]
-                    except:
-                        self.__parameters[section + SECTION_PARAM_SEP + param] = C.EMPTY
-            return True
-        except Exception as e:
-            print("loadFromINIFile() -> " + str(e))
-            return False
-
-    def loadFromSQLite(self, db_file, id) -> bool:
-        """ Load the configuration from the sqlite file in parameter. ALl the parameters comes from a VIEW (VIEW_GET_FULLCONFIG_BLUEPRISM_REPO) 
-            in SQLite and must have the format:
-            * section_param (with a _ instead of a .), the _ is replaced in the parameter list automatically
-        Args:
-            filename (str): sqlite3 file name
-            id (str): id of the configuration
-        Returns:
-            bool: False if error
-        """
-        try:
-            conn = sqlite3.connect(db_file)
-            cur = conn.cursor()
-            cur.execute(C.SQLITE_GETCONFIG.format(id))
-            rows = cur.fetchall()
-            params = [d[0] for d in cur.description]
-            if (len(rows) == 1):
-                r = cursorByField(cur, rows[0])
-                for item in params:
-                    self.__parameters[item.replace("_", ".")] = str(r.get(item))
-            else:
-                raise Exception ("There are more than one configuration (ore none) available.")
-
-            return True
-        except Exception as e:
-            print("loadFromSQLite() -> " + str(e))
-            return False
-
-    def getParameter(self, parameter, default="") -> str:
-        """ Returns the Parameter value based on the INI Section & parameter name.
-            If the parameter comes from the INI file we use the SECTION_PARAM_SEP to separate the section with the parameter
-        Args:
-            parameter (str): INI parameter name (section.parameter)
-            default (str): default value if not found (by default empty string)
-        Returns:
-            str: parameter value, empty if not found
-        """
-        try:
-            splitParams = parameter.split(SECTION_PARAM_SEP)
-            if (len(splitParams) == 2): # with section
-                param = self.__parameters[splitParams[0] + SECTION_PARAM_SEP + splitParams[1]]
-            else:
-                param =  self.__parameters[parameter]
-            return default if (param == None or param == "") else param
-        except Exception as e:
-            return default
-
-    def setParameter(self, parameter, value):
-        """ Surcharge the existing paramter with a new (but not persistent) value
-        Args:
-            parameter (str): parameter name
-            value (str): new value
-        """
-        try:
-            self.__parameters[parameter] = str(value)
-        except Exception as e:
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import configparser
+import utils.constants as C
+from utils import cursorByField 
+import sqlite3
+
+SECTION_PARAM_SEP = "."
+
+class appConfig():
+    """This class contains all the configuration needed and loaded mainly from the INI file
+    """
+
+    def __init__(self):
+        self.__parameters = {}
+        return
+
+    def addParameter(self, name, value):
+        """Add a new parameter in the list
+        Args:
+            name (str): paramter name
+            value (str): parameter value
+        """
+        try:
+            self.__parameters[name] = value
+        except Exception as e:
+            print("addParameter() -> " + str(e))
+
+    def loadFromINIFile(self, filename) -> bool:
+        """ Load the configuration from the INI file in parameter
+        Args:
+            filename (str): INI file name
+        Returns:
+            bool: False if error
+        """
+        try:
+            myConfig = configparser.ConfigParser()
+            myConfig.read(filename)
+            for section in myConfig:
+                for param in myConfig[section]:
+                    try :
+                        self.__parameters[section + SECTION_PARAM_SEP + param] = myConfig[section][param]
+                    except:
+                        self.__parameters[section + SECTION_PARAM_SEP + param] = C.EMPTY
+            return True
+        except Exception as e:
+            print("loadFromINIFile() -> " + str(e))
+            return False
+
+    def loadFromSQLite(self, db_file, id) -> bool:
+        """ Load the configuration from the sqlite file in parameter. ALl the parameters comes from a VIEW (VIEW_GET_FULLCONFIG_BLUEPRISM_REPO) 
+            in SQLite and must have the format:
+            * section_param (with a _ instead of a .), the _ is replaced in the parameter list automatically
+        Args:
+            filename (str): sqlite3 file name
+            id (str): id of the configuration
+        Returns:
+            bool: False if error
+        """
+        try:
+            conn = sqlite3.connect(db_file)
+            cur = conn.cursor()
+            cur.execute(C.SQLITE_GETCONFIG.format(id))
+            rows = cur.fetchall()
+            params = [d[0] for d in cur.description]
+            if (len(rows) == 1):
+                r = cursorByField(cur, rows[0])
+                for item in params:
+                    self.__parameters[item.replace("_", ".")] = str(r.get(item))
+            else:
+                raise Exception ("There are more than one configuration (ore none) available.")
+
+            return True
+        except Exception as e:
+            print("loadFromSQLite() -> " + str(e))
+            return False
+
+    def getParameter(self, parameter, default="") -> str:
+        """ Returns the Parameter value based on the INI Section & parameter name.
+            If the parameter comes from the INI file we use the SECTION_PARAM_SEP to separate the section with the parameter
+        Args:
+            parameter (str): INI parameter name (section.parameter)
+            default (str): default value if not found (by default empty string)
+        Returns:
+            str: parameter value, empty if not found
+        """
+        try:
+            splitParams = parameter.split(SECTION_PARAM_SEP)
+            if (len(splitParams) == 2): # with section
+                param = self.__parameters[splitParams[0] + SECTION_PARAM_SEP + splitParams[1]]
+            else:
+                param =  self.__parameters[parameter]
+            return default if (param == None or param == "") else param
+        except Exception as e:
+            return default
+
+    def setParameter(self, parameter, value):
+        """ Surcharge the existing paramter with a new (but not persistent) value
+        Args:
+            parameter (str): parameter name
+            value (str): new value
+        """
+        try:
+            self.__parameters[parameter] = str(value)
+        except Exception as e:
             pass
```

## config/cmdLineConfig.py

```diff
@@ -1,149 +1,98 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from config.appConfig import appConfig
-
-class cmdLineConfig:
-	
-	@staticmethod
-	def readDatabase(parser):
-		return None, None
-
-	@staticmethod
-	def readSqlite(parser):
-		""" This function gather the arguments sent in the CLI and build the configuration object / USE FOR SQLITE FILE CONFIGURATION FILE ONLY
-		Args:
-			parser (argparse.ArgumentParser): CLI arguments
-		Raises:
-			Exception: Unable to gather the CLI args
-		Returns:
-			utils.appConfig: config object
-			string: Data Source Tag (command line)
-		"""
-		try:
-			config = appConfig()
-			# Parser CLI arguments
-			parser.add_argument("-" + C.PARAM_FILENAME, help="SQLite 3 data file", required=True)
-			parser.add_argument("-" + C.PARAM_SQ_ID, help="Pipeline Configuration ID inside the configuration file", required=True)
-			args = vars(parser.parse_args())
-			# Load configuration via the INI file
-			config.loadFromSQLite(args[C.PARAM_FILENAME], args[C.PARAM_SQ_ID])
-
-			src = config.getParameter(C.PARAM_SRCTYPE)
-			# Config "exceptions" ...
-			file_management = (src == C.PARAM_SRCTYPE_VALCSV or 
-							src == C.PARAM_SRCTYPE_VALXLS or 
-							src == C.PARAM_SRCTYPE_VALXES or 
-							src == C.PARAM_SRCTYPE_CHORUSFILE)
-			if (file_management):
-				# For File (CSV/XES/Excel) load only, takes the CLI args and put them in the config object
-				config.addParameter(C.PARAM_FILENAME, args[C.PARAM_FILENAME])
-				if (src == C.PARAM_SRCTYPE_VALCSV or src == C.PARAM_SRCTYPE_CHORUSFILE):
-					config.addParameter(C.PARAM_CSV_SEPARATOR, args[C.PARAM_CSV_SEPARATOR])
-				if (src == C.PARAM_SRCTYPE_VALXLS):
-					config.addParameter(C.PARAM_EXCELSHEETNAME, args[C.PARAM_EXCELSHEETNAME])
-			return config, src
-
-		except Exception as e:
-			print(e)
-			parser.print_help()
-			return None, None
-
-
-	@staticmethod
-	def manageArgs(args):
-		""" manage the arguments in command line with the ini config file
-		Args:
-			args (_type_): command line arguments
-		Returns:
-			appConfig: cinfiguration object
-			str: source type
-		"""
-		config = appConfig()
-		src = args[C.PARAM_SRCTYPE]
-		config.setParameter(C.PARAM_SRCTYPE, src)
-		config.setParameter(C.CONFIG_SOURCE_NAME, C.CONFIG_SOURCE_INI)
-		if (not(src in C.PARAM_SRCTYPE_SUPPORTED)):
-			raise Exception("Missing Data Source type {csv|xes|excel|odbc|bprepo|bpapi|saptable}")
-		# Load configuration via the INI file
-		if (args[C.PARAM_CONFIGFILE] != 0):
-			config.loadFromINIFile(args[C.PARAM_CONFIGFILE])
-		else:
-			raise Exception("Missing config file argument {}".format(C.PARAM_CONFIGFILE))
-		# Config "exceptions" ...
-		file_management = (src == C.PARAM_SRCTYPE_VALCSV or 
-						src == C.PARAM_SRCTYPE_VALXLS or 
-						src == C.PARAM_SRCTYPE_VALXES or 
-						src == C.PARAM_SRCTYPE_CHORUSFILE)
-		if (file_management):
-			# For File (CSV/XES/Excel) load only, takes the CLI args and put them in the config object
-			config.addParameter(C.PARAM_FILENAME, args[C.PARAM_FILENAME])
-			if (src == C.PARAM_SRCTYPE_VALCSV or src == C.PARAM_SRCTYPE_CHORUSFILE):
-				config.addParameter(C.PARAM_CSV_SEPARATOR, args[C.PARAM_CSV_SEPARATOR])
-			if (src == C.PARAM_SRCTYPE_VALXLS):
-				config.addParameter(C.PARAM_EXCELSHEETNAME, args[C.PARAM_EXCELSHEETNAME])
-
-		return config, src
-
-	@staticmethod
-	def readIni(parser):
-		""" This function gather the arguments sent in the CLI and build the configuration object / USE FOR INI FILE CONFIGURATION FILE ONLY
-		Args:
-			parser (argparse.ArgumentParser): CLI arguments
-		Raises:
-			Exception: Unable to gather the CLI args
-		Returns:
-			utils.appConfig: config object
-			string: Data Source Tag (command line)
-		"""
-		try:
-			# Parser CLI arguments
-			parser.add_argument("-" + C.PARAM_SRCTYPE, help="(All) Data source type {csv|xes|excel|odbc|bprepo|bpapi|saptable}", required=True)
-			parser.add_argument("-" + C.PARAM_CONFIGFILE, help="(All) Config file with all configuration details (INI format)", required=True)
-			parser.add_argument("-" + C.PARAM_FILENAME, help="(csv|xes|excel) File name and path to import", default=C.EMPTY)
-			parser.add_argument("-" + C.PARAM_CSV_SEPARATOR, help="(csv) CSV file field separator (comma by default)", default=C.DEFCSVSEP)
-			parser.add_argument("-" + C.PARAM_EXCELSHEETNAME, help="(excel) Excel Sheet name", default="0")
-			parser.add_argument("-" + C.PARAM_FROMDATE, help="(bprepo) FROM date -> Delta extraction (Format YYYY-MM-DD HH:MM:SS)", default=C.EMPTY)
-			parser.add_argument("-" + C.PARAM_TODATE, help="(bprepo) TO date -> Delta extraction (Format YYYY-MM-DD HH:MM:SS)", default=C.EMPTY)
-			args = vars(parser.parse_args())
-			config, src = cmdLineConfig.manageArgs(args)
-			return config, src
-
-		except Exception as e:
-			print("ERROR> " + str(e))
-			parser.print_help()
-			return None, None
-		
-	@staticmethod
-	def emulate_readIni(sourcetype, 
-		     			configfile, 
-		     			filename="", 
-						sep="", 
-						sheet="", 
-						fromdate="", 
-						todate=""):
-		""" This function gather the arguments sent in the CLI and build the configuration object / USE FOR INI FILE CONFIGURATION FILE ONLY
-		Args:
-			parser (argparse.ArgumentParser): CLI arguments
-		Raises:
-			Exception: Unable to gather the CLI args
-		Returns:
-			utils.appConfig: config object
-			string: Data Source Tag (command line)
-		"""
-		try:
-			config = appConfig()
-			# Check Data Source Type
-			args = dict(sourcetype=sourcetype, 
-	       				configfile=configfile,
-						filename=filename,
-						sep = sep,
-						sheet=sheet)
-			config, src = cmdLineConfig.manageArgs(args)
-			return config, src
-
-		except Exception as e:
-			print("ERROR> " + str(e))
-			return None, None
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from config.appConfig import appConfig
+
+class cmdLineConfig:
+	
+	@staticmethod
+	def readDatabase(parser):
+		return None, None
+
+	@staticmethod
+	def readSqlite(parser):
+		""" This function gather the arguments sent in the CLI and build the configuration object / USE FOR SQLITE FILE CONFIGURATION FILE ONLY
+		Args:
+			parser (argparse.ArgumentParser): CLI arguments
+		Raises:
+			Exception: Unable to gather the CLI args
+		Returns:
+			utils.appConfig: config object
+			string: Data Source Tag (command line)
+		"""
+		try:
+			config = appConfig()
+			# Parser CLI arguments
+			parser.add_argument("-" + C.PARAM_FILENAME, help="SQLite 3 data file", required=True)
+			parser.add_argument("-" + C.PARAM_SQ_ID, help="Pipeline Configuration ID inside the configuration file", required=True)
+			args = vars(parser.parse_args())
+			# Load configuration via the INI file
+			config.loadFromSQLite(args[C.PARAM_FILENAME], args[C.PARAM_SQ_ID])
+			return config
+		except Exception as e:
+			print(e)
+			parser.print_help()
+			return None, None
+
+	@staticmethod
+	def manageArgs(args):
+		""" manage the arguments in command line with the ini config file
+		Args:
+			args (_type_): command line arguments
+		Returns:
+			appConfig: cinfiguration object
+		"""
+		config = appConfig()
+		# Load configuration via the INI file
+		if (args[C.PARAM_CONFIGFILE] != 0):
+			config.loadFromINIFile(args[C.PARAM_CONFIGFILE])
+		else:
+			raise Exception("Missing config file argument {}".format(C.PARAM_CONFIGFILE))
+		return config
+
+	@staticmethod
+	def readIni(parser):
+		""" This function gather the arguments sent in the CLI and build the configuration object / USE FOR INI FILE CONFIGURATION FILE ONLY
+		Args:
+			parser (argparse.ArgumentParser): CLI arguments
+		Raises:
+			Exception: Unable to gather the CLI args
+		Returns:
+			utils.appConfig: config object
+			string: Data Source Tag (command line)
+		"""
+		try:
+			# Parser CLI arguments
+			parser.add_argument("-" + C.PARAM_CONFIGFILE, help="(All) Config file with all configuration details (INI format)", required=True)
+			args = vars(parser.parse_args())
+			config = cmdLineConfig.manageArgs(args)
+			return config
+
+		except Exception as e:
+			print("ERROR> " + str(e))
+			parser.print_help()
+			return None, None
+		
+	@staticmethod
+	def emulate_readIni(configfile):
+		""" This function gather the arguments sent in the CLI and build the configuration object / USE FOR INI FILE CONFIGURATION FILE ONLY
+		Args:
+			parser (argparse.ArgumentParser): CLI arguments
+		Raises:
+			Exception: Unable to gather the CLI args
+		Returns:
+			utils.appConfig: config object
+			string: Data Source Tag (command line)
+		"""
+		try:
+			config = appConfig()
+			# Check Data Source Type
+			args = dict(configfile=configfile)
+			config = cmdLineConfig.manageArgs(args)
+			return config
+
+		except Exception as e:
+			print("ERROR> " + str(e))
+			return None
```

## pipelines/pipeline.py

```diff
@@ -1,80 +1,101 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-from config.appConfig import appConfig
-import utils.constants as C
-from utils.log import log
-import pandas as pd
-
-class pipeline:
-    def __init__(self, config):
-        self.__config = config          # All the configuration parameters
-        self.__trace = None             # Logger
-
-    # Contains all the config parameters (from the INI file)
-    @property
-    def config(self) -> appConfig:
-        return self.__config
-    @config.setter   
-    def config(self, value):
-        self.__config = value
-
-    @property
-    def mandatoryParameters(self) -> str:
-        return C.EMPTY
-    
-    @property
-    def log(self) -> log:
-        return self.__trace
-    @log.setter   
-    def log(self, value):
-        self.__trace = value
-
-    def checkParameters(self) -> bool:
-        """Check the mandatory parameters
-        Returns:
-            bool: False si at least one mandatory param is missing
-        """
-        return True
-    
-    def initialize(self) -> bool:
-        """Initialize the Class instance for the pipeline
-            * initialize the logger
-            * check the mandatory parameters
-            * iother inits ...
-        Returns:
-            bool: False if error
-        """
-        return True
-
-    def terminate(self) -> bool:
-        # For surcharge
-        self.log.info("*** End of Job treatment ***")
-        return True
-    
-    def extract(self) -> pd.DataFrame: 
-        """This method must be surchaged and aims to collect the data from the datasource to provides the corresponding dataframe
-        Returns:
-            pd.DataFrame: Dataset in a pd.Dataframe object
-        """
-        return pd.DataFrame()
-
-    def transform(self, df) -> pd.DataFrame: 
-        """ Surcharge this method to enable modification in the Dataset after gathering the data and before uploding them in BPPI
-            By default just manage the event mapping.
-        Args:
-            df (pd.DataFrame): source dataset
-        Returns:
-            pd.DataFrame: altered dataset
-        """
-        return self.eventMap(df)
-
-    def load(self, dfDataset) -> bool:
-        """ Surcharge this method to upload a dataset (Pandas DataFrame) into BPPI
-        Args:
-            dfDataset (pd.DataFrame): DataFrame with the Data to upload
-        Returns:
-            bool: False if error
-        """
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+from config.appConfig import appConfig
+import utils.constants as C
+from utils.log import log
+import pandas as pd
+MANDATORY_PARAM_LIST = []
+
+class pipeline:
+    def __init__(self, config, log):
+        self.__config = config          # All the configuration parameters
+        self.__log = log                # Logger
+
+    # Contains all the config parameters (from the INI file)
+    @property
+    def config(self) -> appConfig:
+        return self.__config
+    @config.setter   
+    def config(self, value):
+        self.__config = value
+
+    @property
+    def mandatoryParameters(self) -> str:
+        return C.EMPTY
+    
+    @property
+    def log(self) -> log:
+        return self.__log
+    @log.setter   
+    def log(self, value):
+        self.__log = value
+
+    def checkParameters(self) -> bool:
+        """Check the mandatory parameters
+        Returns:
+            bool: False si at least one mandatory param is missing
+        """
+        self.log.info("*** Check parameters treatment ***")
+        return True
+    
+    def initialize(self) -> bool:
+        """Initialize the Class instance by gathering the BPPI repository infos.
+            * initialize the logger
+            * check the mandatory parameters
+            * init the API (get the BPPI Repository infos)
+        Returns:
+            bool: False if error
+        """
+        try:
+            # Checking parameters
+            self.log.info("*** Beggining of Job treatment ***")
+            if (not self.checkParameters()):
+                raise Exception("Some mandatory parameters are missing")
+            return True
+        except Exception as e:
+            self.log.error("initialize() Error -> " + str(e))
+            return False
+
+    def terminate(self) -> bool:
+        # For surcharge
+        self.log.info("*** End of Job treatment ***")
+        return True
+    
+    def extract(self) -> pd.DataFrame: 
+        """This method must be surchaged and aims to collect the data from the datasource to provides the corresponding dataframe
+        Returns:
+            pd.DataFrame: Dataset in a pd.Dataframe object
+        """
+        self.log.info("*** Extraction treatment ***")
+        return pd.DataFrame()
+
+    def transform(self, df) -> pd.DataFrame: 
+        """ Surcharge this method to enable modification in the Dataset after gathering the data and before uploding them in BPPI
+            By default just manage the event mapping.
+        Args:
+            df (pd.DataFrame): source dataset
+        Returns:
+            pd.DataFrame: altered dataset
+        """
+        self.log.info("*** Data Transformation treatment ***")
+        return self.eventMap(df)
+
+    def load(self, dfDataset) -> bool:
+        """ Surcharge this method to upload a dataset (Pandas DataFrame) into BPPI
+        Args:
+            dfDataset (pd.DataFrame): DataFrame with the Data to upload
+        Returns:
+            bool: False if error
+        """
+        self.log.info("*** Loading treatment ***")
+        return True
+    
+    def afterLoad(self) -> bool:
+        """ Surcharge this method to manage tasks after loading the dataset into BPPI
+        Returns:
+            bool: False if error
+        """
+        self.log.info("*** After Loading treatment ***")
         return True
```

## pipelines/pipelineFactory.py

```diff
@@ -1,108 +1,118 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-import importlib
-
-class pipelineFactory:
-	def __init__(self, datasource, config):
-		self.__config = config
-		self.__datasource = datasource
-    
-	@property
-	def config(self):
-		return self.__config
-	@property
-	def datasource(self):
-		return self.__datasource
-	
-	def createAndExecute(self):
-		""" Initialize the process and execute
-		Returns:
-			int: Number of rows read
-			int: Number of rows transformed
-			int: Number of rows loaded
-		"""
-		E_counts, T_counts, L_counts = 0, 0, 0
-		try:
-			# INSTANCIATE ONLY THE NEEDED CLASS / DATA SOURCE TYPE
-			print("Info> BPPI Bridge initialisation ...")
-			pipeline = self.create()
-			if (pipeline == None):
-				raise Exception ("The Data pipeline cannot be created")
-		except Exception as e:
-			print("Error> pipelineFactory.createAndExecute(): The bridge cannot be initialized: {}".format(str(e)))
-		
-		try:
-			# PROCESS THE DATA
-			if (pipeline.initialize()):
-				pipeline.log.info("The BPPI Bridge has been initialized successfully")
-				pipeline.log.info("Extract data from Data Source ...")
-				df = pipeline.extract()	# EXTRACT (E of ETL)
-				E_counts = df.shape[0]
-				pipeline.log.info("Data extracted successfully, {} rows to import into BPPI".format(E_counts))
-				if (df.shape[0] == 0):
-					pipeline.log.info("** There are no data to process, terminate here **")
-				else:
-					pipeline.log.info("Transform imported data ...")
-					df = pipeline.transform(df)	# TRANSFORM (T of ETL)
-					T_counts = df.shape[0]
-					pipeline.log.info("Data transformed successfully, {} rows - after transformation - to import into BPPI".format(T_counts))
-					if (df.empty != True): 
-						# LOAD (L of ETL)
-						pipeline.log.info("Load data into the BPPI Repository table ...")
-						if pipeline.load(df):
-							L_counts = T_counts
-							pipeline.log.info("Data loaded successfully")
-							if (self.config.getParameter(C.PARAM_BPPITODOACTIVED, C.NO) == C.YES):
-								pipeline.log.info("Execute BPPI To Do ...")
-								if (pipeline.executeToDo()):
-									pipeline.log.info("BPPI To Do executed successfully")
-				pipeline.terminate()
-			else:
-				print("pipelineFactory.createAndExecute(): The Data pipeline has not been initialized properly")
-			return E_counts, T_counts, L_counts
-		
-		except Exception as e:
-			pipeline.log.error("pipelineFactory.createAndExecute(): Error when processing the data: {}".format(str(e)))
-			return E_counts, T_counts, L_counts
-
-	def create(self):
-		""" This function dynamically instanciate the right data pipeline (manages ETL) class to create a pipeline object. 
-			This to avoid in loading all the connectors (if any of them failed for example) when making a global import, 
-			by this way only the needed import is done on the fly
-			Args:
-				pipeline (str): Datasource type
-				config (config): Configuration set
-			Returns:
-				Object: Data Source Object
-		"""
-		try:
-			if (self.config == None): 
-				raise Exception("The configuration is not available or is invalid.")
-			if (self.datasource == None): 
-				raise Exception("The datasource is not correctly specified or is invalid.")
-			if (self.datasource == C.PARAM_SRCTYPE_VALCSV):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRCSVFile").bppiPLRCSVFile
-			elif (self.datasource == C.PARAM_SRCTYPE_VALXES):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRXESFile").bppiPLRXESFile
-			elif (self.datasource == C.PARAM_SRCTYPE_VALXLS):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRExcelFile").bppiPLRExcelFile
-			elif (self.datasource == C.PARAM_SRCTYPE_VALODBC):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRODBC").bppiPLRODBC
-			elif (self.datasource == C.PARAM_SRCTYPE_VALBP):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRBluePrismRepo").bppiPLRBluePrismRepo
-			elif (self.datasource == C.PARAM_SRCTYPE_VALBPAPI):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRBluePrismApi").bppiPLRBluePrismApi
-			elif (self.datasource == C.PARAM_SRCTYPE_VALSAPTABLE):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRSAPRfcTable").bppiPLRSAPRfcTable
-			elif (self.datasource == C.PARAM_SRCTYPE_CHORUSFILE):
-				datasourceObject = importlib.import_module(C.PIPELINE_FOLDER + "bppiPLRChorusExtract").bppiPLRChorusExtract
-			else:
-				raise Exception ("Error when loading the Data Source Factory in pipeline folder")
-			return datasourceObject(self.config)
-		
-		except Exception as e:
-			print("pipelineFactory.create(): Error when loading the Data Source Factory: {}".format(str(e)))
-			return None
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+import importlib
+from .pipeline import pipeline
+from utils.log import log
+
+class pipelineFactory:
+	def __init__(self, config, log):
+		self.__config = config
+		self.__log = log
+
+	@property
+	def config(self):
+		return self.__config
+	@property
+	def log(self) -> log:
+		return self.__log
+	
+	@staticmethod
+	def getLogger(config) -> log:
+		if (config != None):
+			# Init logger
+			logfilename = config.getParameter(C.PARAM_LOGFOLDER, "") + config.getParameter(C.PARAM_LOGFILENAME, C.TRACE_FILENAME)
+			print("Log file: {}".format(logfilename))
+			level = config.getParameter(C.PARAM_LOGLEVEL, C.TRACE_DEFAULT_LEVEL)
+			format = config.getParameter(C.PARAM_LOGFORMAT, C.TRACE_DEFAULT_FORMAT)
+			return log(__name__, logfilename, level, format)
+		else:
+			raise Exception ("Configuration failed, impossible to create the logger.")
+
+	def process(self):
+		""" Initialize the process and execute the pipeline
+		Returns:
+			int: Number of rows read
+			int: Number of rows transformed
+			int: Number of rows loaded
+		"""
+		try:
+			# INSTANCIATE ONLY THE NEEDED CLASS / DATA SOURCE TYPE
+			self.log.info("BPPI Bridge initialisation ...")
+			pipeline = self.create()
+			if (pipeline == None):
+				raise Exception ("The Data pipeline cannot be created")
+		except Exception as e:
+			self.log.error("Error> pipelineFactory.process(): The bridge cannot be initialized: {}".format(str(e)))
+	
+		return self.execute(pipeline=pipeline)
+	
+	def execute(self, pipeline):
+		""" Execute the pipeline
+		Returns:
+			int: Number of rows read
+			int: Number of rows transformed
+			int: Number of rows loaded
+		"""
+		E_counts, T_counts, L_counts = 0, 0, 0
+		try:
+			# PROCESS THE DATA
+			if (pipeline.initialize()): # init logs here ...
+				pipeline.log.info("The BPPI Bridge has been initialized successfully")
+				pipeline.log.info("Extract data from Data Source ...")
+				df = pipeline.extract()	# EXTRACT (E of ETL)
+				E_counts = df.shape[0]
+				pipeline.log.info("Data extracted successfully, {} rows to import into BPPI".format(E_counts))
+				if (df.shape[0] == 0):
+					pipeline.log.info("** There are no data to process, terminate here **")
+				else:
+					pipeline.log.info("Transform imported data ...")
+					df = pipeline.transform(df)	# TRANSFORM (T of ETL)
+					T_counts = df.shape[0]
+					pipeline.log.info("Data transformed successfully, {} rows - after transformation - to import into BPPI".format(T_counts))
+					if (df.empty != True): 
+						# LOAD (L of ETL)
+						pipeline.log.info("Load data into the BPPI Repository table ...")
+						if pipeline.load(df): # LOAD (L of ETL)
+							L_counts = T_counts
+							pipeline.log.info("Data loaded successfully")
+							if (self.config.getParameter(C.PARAM_BPPITODOACTIVED, C.NO) == C.YES):
+								pipeline.log.info("Execute one or more BPPI <ToDo> ...")
+								if (pipeline.afterLoad()):
+									pipeline.log.info("BPPI To Do executed successfully")
+				pipeline.log.info("Data Counts -> E:{} T:{} L:{}".format(E_counts, T_counts, L_counts))
+			else:
+				self.log.error("pipelineFactory.createAndExecute(): The Data pipeline has not been initialized properly")
+			
+			pipeline.terminate()
+			return E_counts, T_counts, L_counts
+		
+		except Exception as e:
+			pipeline.log.error("pipelineFactory.createAndExecute(): Error when processing the data: {}".format(str(e)))
+			return E_counts, T_counts, L_counts
+
+	def create(self) -> pipeline:
+		""" This function dynamically instanciate the right data pipeline (manages ETL) class to create a pipeline object. 
+			This to avoid in loading all the connectors (if any of them failed for example) when making a global import, 
+			by this way only the needed import is done on the fly
+			Args:
+				pipeline (str): Datasource type
+				config (config): Configuration set
+			Returns:
+				Object: Data Source Object
+		"""
+		try:
+			# Get the pipeline class to instantiate from the config
+			pipelinePath = self.config.getParameter(C.PARAM_PIPELINE_PATH, C.PIPELINE_FOLDER)
+			pipelineClass = self.config.getParameter(C.PARAM_PIPELINE_CLASSNAME, C.PIPELINE_FOLDER)
+
+			# Instantiate the pipeline object
+			datasourceObject = importlib.import_module(pipelinePath + "." + pipelineClass)
+			pipelineClass = getattr(datasourceObject, pipelineClass)
+			return pipelineClass(self.config, self.log)
+		
+		except Exception as e:
+			self.log.error("pipelineFactory.create(): Error when loading the Data Source Factory: {}".format(str(e)))
+			return None
```

## pipelines/bppi/bppiPipeline.py

```diff
@@ -1,167 +1,99 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-from pipelines.bppi.repository.bppiApiRepositoryWrapper import bppiApiRepositoryWrapper
-from utils.log import log
-import pandas as pd
-import utils.constants as C
-import time
-from pipelines.pipeline import pipeline
-
-MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
-                        C.PARAM_BPPIURL]
-
-class bppiPipeline(pipeline):
-
-    @property
-    def url(self) -> str:
-        return self.__serverURL
-    @property
-    def token(self) -> str:
-        return self.__token
-    
-    def checkParameters(self) -> bool:
-        """Check the mandatory parameters
-        Returns:
-            bool: False si at least one mandatory param is missing
-        """
-        try:
-            for param in self.mandatoryParameters:
-                if (self.config.getParameter(param, "") == ""):
-                    self.log.error("Parameter <{}> is missing".format(param))
-                    return False 
-            return True
-        except Exception as e:
-            self.log.error("checkParameters() Error -> " + str(e))
-            return False
-        
-    def initialize(self) -> bool:
-        """Initialize the Class instance by gathering the BPPI repository infos.
-            * initialize the logger
-            * check the mandatory parameters
-            * init the API (get the BPPI Repository infos)
-        Returns:
-            bool: False if error
-        """
-        try:
-            # Init logger
-            logfilename = self.config.getParameter(C.PARAM_LOGFOLDER, "") + self.config.getParameter(C.PARAM_LOGFILENAME, C.TRACE_FILENAME)
-            print("Log file: {}".format(logfilename))
-            level = self.config.getParameter(C.PARAM_LOGLEVEL, C.TRACE_DEFAULT_LEVEL)
-            format = self.config.getParameter(C.PARAM_LOGFORMAT, C.TRACE_DEFAULT_FORMAT)
-            self.log = log(__name__, logfilename, level, format)
-            # Init BPPI APIs
-            self.log.info("*** Beggining of Job treatment ***")
-            if (not self.checkParameters()):
-                raise Exception("Some mandatory parameters are missing")
-            return True
-        except Exception as e:
-            self.log.error("initialize() Error -> " + str(e))
-            return False
-    
-    def getStatus(self, processingId) -> str:
-        """Return the status of a process launched on the BPPI server
-        Args:
-            processingId (_type_): ID of the BPPI Process
-        Returns:
-            str: Process status (from BPPI server)
-        """
-        try:
-            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
-                                            self.config.getParameter(C.PARAM_BPPIURL))
-            api.log = self.log
-            return api.getProcessingStatus(processingId)
-        except Exception as e:
-            self.log.error("getStatus() Error -> " + str(e))
-            return C.API_STATUS_ERROR
-
-    def waitForEndOfProcessing(self, processId) -> str:
-        """Wait for the end of the BPPI process execution
-        Args:
-            processId (_type_): ID of the BPPI Process
-        Returns:
-            str: Final Status
-        """
-        try:
-            self.log.info("Wait for the end of a process execution")
-            EndOfWait = True
-            nbIterations = 0
-            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
-                                            self.config.getParameter(C.PARAM_BPPIURL))
-            api.log = self.log
-            while (EndOfWait):
-                # 5 - Check the status to veriify if the task is finished
-                status = self.getStatus(processId)
-                if ((status != C.API_STATUS_IN_PROGRESS) or (nbIterations > C.API_DEF_NB_ITERATION_MAX)):
-                    EndOfWait = False
-                time.sleep(C.API_DEF_WAIT_DURATION_SEC)
-                nbIterations += 1
-            return status
-        except Exception as e:
-            self.log.error("waitForEndOfProcessing() Error -> " + str(e))
-            return C.API_STATUS_ERROR
-    
-    def eventMap(self, df) -> pd.DataFrame:
-        """ Map the events with the dataset (in parameter df). 
-            Event Map file:
-                * CSV format + Header
-                * Name in the C.PARAM_EVENTMAPTABLE
-                * Column to map with the event map file  in the C.PARAM_EVENTMAPNAME field (orginal dataset)
-                * Only 2 columns in the event map file: 
-                    - col 1: source event name (the one to map with the source dataset)
-                    - col 2: new event name (the one to use for event replacement)
-            Mapping Rules:
-                * Replace the Col1 per col2 every time (event name replacement)
-                * If Col2 empty -> remove the row (remove not necessary events)
-                * If Name has not match with Col1 -> remove the row
-            If the mapping file does not exists just create a template one with col1 = col2 (so that the user can update himself the column 2)
-        Args:
-            df (pd.DataFrame): Data Source
-        Returns:
-            pd.DataFrame: Data altered with the new events & remove the unecesserary ones
-        """
-        try:
-            dfAltered = df
-            if (self.config.getParameter(C.PARAM_EVENTMAP, C.NO) == C.YES):
-                # Get parameters
-                self.log.info("Map the events with the original dataset and the event map table")
-                evtMapFilename = self.config.getParameter(C.PARAM_EVENTMAPTABLE)
-                if (evtMapFilename == ""):
-                    raise Exception("No Event map filename (CSV) was specified")
-                evtMapColumnname = self.config.getParameter(C.PARAM_EVENTMAPNAME)
-                if (evtMapColumnname == ""):
-                    raise Exception("No Event column name (in the data source) was specified")
-                # Open the event map file (assuming 1st col -> Original Event, 2nd col -> event altered or if nothing to remove)
-                try:
-                    dfevtMap = pd.read_csv(evtMapFilename, encoding=C.ENCODING)
-                except FileNotFoundError as e:
-                    self.log.warning("{} does not exist, create a event map template file instead".format(evtMapFilename))
-                    # Create the file template
-                    colName = df[evtMapColumnname].value_counts().index
-                    dfevtMap = pd.DataFrame(columns=["Source", "Target"])
-                    dfevtMap["Source"] = colName
-                    dfevtMap["Target"] = colName
-                    dfevtMap = dfevtMap.sort_values(by=['Source'])
-                    dfevtMap.to_csv(evtMapFilename, encoding=C.ENCODING, index=False)
-                    return df # No map to do !
-                # Manage the event mapping
-                if (dfevtMap.shape[1] != 2):
-                    raise Exception("There are more than 2 columns in the event map file.")
-                dfevtMap.rename(columns={dfevtMap.columns[0]:evtMapColumnname}, inplace=True)
-                originalRecCount = df.shape[0]
-                self.log.debug("There are {} records in the original dataset".format(originalRecCount))
-                dfAltered = pd.merge(df, dfevtMap, on=evtMapColumnname, how ="inner")
-                # Drop rows with a bad/No join (lookup) --> when the Target column is equal to NaN
-                dfAltered = dfAltered.dropna(subset=["Target"])
-                # Reshape the dataset (columns changes)
-                del dfAltered[evtMapColumnname]
-                dfAltered.rename(columns={dfevtMap.columns[1]: evtMapColumnname}, inplace=True)
-                iNbRemoved = originalRecCount - dfAltered.shape[0]
-                if (iNbRemoved != 0):
-                    self.log.warning("{} records have been removed ".format(iNbRemoved))
-            return dfAltered
-        
-        except Exception as e:
-            self.log.error("eventMap() Error -> {}".format(str(e)))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+from utils.log import log
+import pandas as pd
+import utils.constants as C
+from pipelines.pipeline import pipeline
+
+MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
+                        C.PARAM_BPPIURL]
+
+class bppiPipeline(pipeline):
+
+    @property
+    def url(self) -> str:
+        return self.__serverURL
+    @property
+    def token(self) -> str:
+        return self.__token
+    
+    def checkParameters(self) -> bool:
+        """Check the mandatory parameters
+        Returns:
+            bool: False si at least one mandatory param is missing
+        """
+        try:
+            for param in self.mandatoryParameters:
+                if (self.config.getParameter(param, "") == ""):
+                    self.log.error("Parameter <{}> is missing".format(param))
+                    return False 
+            return True
+        except Exception as e:
+            self.log.error("checkParameters() Error -> " + str(e))
+            return False
+    
+    def eventMap(self, df) -> pd.DataFrame:
+        """ Map the events with the dataset (in parameter df). 
+            Event Map file:
+                * CSV format + Header
+                * Name in the C.PARAM_EVENTMAPTABLE
+                * Column to map with the event map file  in the C.PARAM_EVENTMAPNAME field (orginal dataset)
+                * Only 2 columns in the event map file: 
+                    - col 1: source event name (the one to map with the source dataset)
+                    - col 2: new event name (the one to use for event replacement)
+            Mapping Rules:
+                * Replace the Col1 per col2 every time (event name replacement)
+                * If Col2 empty -> remove the row (remove not necessary events)
+                * If Name has not match with Col1 -> remove the row
+            If the mapping file does not exists just create a template one with col1 = col2 (so that the user can update himself the column 2)
+        Args:
+            df (pd.DataFrame): Data Source
+        Returns:
+            pd.DataFrame: Data altered with the new events & remove the unecesserary ones
+        """
+        try:
+            dfAltered = df
+            if (self.config.getParameter(C.PARAM_EVENTMAP, C.NO) == C.YES):
+                # Get parameters
+                self.log.info("Map the events with the original dataset and the event map table")
+                evtMapFilename = self.config.getParameter(C.PARAM_EVENTMAPTABLE)
+                if (evtMapFilename == ""):
+                    raise Exception("No Event map filename (CSV) was specified")
+                evtMapColumnname = self.config.getParameter(C.PARAM_EVENTMAPNAME)
+                if (evtMapColumnname == ""):
+                    raise Exception("No Event column name (in the data source) was specified")
+                # Open the event map file (assuming 1st col -> Original Event, 2nd col -> event altered or if nothing to remove)
+                try:
+                    dfevtMap = pd.read_csv(evtMapFilename, encoding=C.ENCODING)
+                except FileNotFoundError as e:
+                    self.log.warning("{} does not exist, create a event map template file instead".format(evtMapFilename))
+                    # Create the file template
+                    colName = df[evtMapColumnname].value_counts().index
+                    dfevtMap = pd.DataFrame(columns=["Source", "Target"])
+                    dfevtMap["Source"] = colName
+                    dfevtMap["Target"] = colName
+                    dfevtMap = dfevtMap.sort_values(by=['Source'])
+                    dfevtMap.to_csv(evtMapFilename, encoding=C.ENCODING, index=False)
+                    return df # No map to do !
+                # Manage the event mapping
+                if (dfevtMap.shape[1] != 2):
+                    raise Exception("There are more than 2 columns in the event map file.")
+                dfevtMap.rename(columns={dfevtMap.columns[0]:evtMapColumnname}, inplace=True)
+                originalRecCount = df.shape[0]
+                self.log.debug("There are {} records in the original dataset".format(originalRecCount))
+                dfAltered = pd.merge(df, dfevtMap, on=evtMapColumnname, how ="inner")
+                # Drop rows with a bad/No join (lookup) --> when the Target column is equal to NaN
+                dfAltered = dfAltered.dropna(subset=["Target"])
+                # Reshape the dataset (columns changes)
+                del dfAltered[evtMapColumnname]
+                dfAltered.rename(columns={dfevtMap.columns[1]: evtMapColumnname}, inplace=True)
+                iNbRemoved = originalRecCount - dfAltered.shape[0]
+                if (iNbRemoved != 0):
+                    self.log.warning("{} records have been removed ".format(iNbRemoved))
+            return dfAltered
+        
+        except Exception as e:
+            self.log.error("eventMap() Error -> {}".format(str(e)))
             return df
```

## pipelines/bppi/uploadConfig.py

 * *Ordering differences only*

```diff
@@ -1,51 +1,51 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import json
-
-class uploadConfig():
-    """ This class contains all the informations gathered from the BPPI server.
-    -> From the user configuration when creating a token.
-    """
-    def __init__(self, httpResponse = None):
-        self.__loaded = False
-        if (httpResponse == None):
-            self.__url = ""
-            self.__key = ""
-            self.__headers = ""
-            self.__jsonContent = json.dumps({}).encode("utf8")
-        else:
-            self.load(httpResponse)
-        return
-    
-    def load(self, httpResponse):
-        try:
-            self.__jsonContent = httpResponse
-            j = json.loads(httpResponse)
-            self.__url = j['url']
-            self.__headers = j['headers']
-            self.__key = j['key']
-            self.__loaded = True
-        except:
-            self.__loaded = False
-
-    @property
-    def jsonContent(self):
-        return self.__jsonContent
-    @property
-    def url(self):
-        return self.__url
-    @property
-    def key(self):
-        return self.__key
-    @property
-    def headers(self):
-        return self.__headers
-    
-    @property
-    def loaded(self):
-        return self.__loaded
-    @loaded.setter   
-    def loaded(self, value):
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import json
+
+class uploadConfig():
+    """ This class contains all the informations gathered from the BPPI server.
+    -> From the user configuration when creating a token.
+    """
+    def __init__(self, httpResponse = None):
+        self.__loaded = False
+        if (httpResponse == None):
+            self.__url = ""
+            self.__key = ""
+            self.__headers = ""
+            self.__jsonContent = json.dumps({}).encode("utf8")
+        else:
+            self.load(httpResponse)
+        return
+    
+    def load(self, httpResponse):
+        try:
+            self.__jsonContent = httpResponse
+            j = json.loads(httpResponse)
+            self.__url = j['url']
+            self.__headers = j['headers']
+            self.__key = j['key']
+            self.__loaded = True
+        except:
+            self.__loaded = False
+
+    @property
+    def jsonContent(self):
+        return self.__jsonContent
+    @property
+    def url(self):
+        return self.__url
+    @property
+    def key(self):
+        return self.__key
+    @property
+    def headers(self):
+        return self.__headers
+    
+    @property
+    def loaded(self):
+        return self.__loaded
+    @loaded.setter   
+    def loaded(self, value):
         self.__loaded = value
```

## pipelines/bppi/project/bppiApiProjectWrapper.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import requests
-import json
-import utils.constants as C
-
-class bppiApiProjectWrapper:
-    """This class acts as a gateway for the BPPI API calls
-    """
-    def __init__(self, token, serverURL):
-        self.__token = token
-        self.__serverURL = serverURL
-        self.__log = None
-
-    @property
-    def log(self):
-        return self.__log
-    @log.setter   
-    def log(self, value):
-        self.__log = value
-
-    @property
-    def apiRootPath(self):
-        return self.__serverURL + C.API_1_0
-    @property
-    def URL(self):
-        return self.__serverURL
-    @property
-    def Token(self):
-        return self.__token
-    
-    def getProjectsList(self):
-        try: 
-            return True
-        except Exception as e:
-            self.log.error("getRepositoryConfiguration Error | " + str(e))
-            return False
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import requests
+import json
+import utils.constants as C
+
+class bppiApiProjectWrapper:
+    """This class acts as a gateway for the BPPI API calls
+    """
+    def __init__(self, token, serverURL):
+        self.__token = token
+        self.__serverURL = serverURL
+        self.__log = None
+
+    @property
+    def log(self):
+        return self.__log
+    @log.setter   
+    def log(self, value):
+        self.__log = value
+
+    @property
+    def apiRootPath(self):
+        return self.__serverURL + C.API_1_0
+    @property
+    def URL(self):
+        return self.__serverURL
+    @property
+    def Token(self):
+        return self.__token
+    
+    def getProjectsList(self):
+        try: 
+            return True
+        except Exception as e:
+            self.log.error("getRepositoryConfiguration Error | " + str(e))
+            return False
```

## pipelines/bppi/project/bppiProject.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-from bppi.repository.bppiApiRepositoryWrapper import bppiApiRepositoryWrapper
-from bppi.bppiPipeline import bppiPipeline
-import utils.constants as C
-
-MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
-                        C.PARAM_BPPIURL]
-
-class bppiProject(bppiPipeline):
-    def __init__(self, config):
-        super().__init__(config)
-        self.__projectInfos = None   # BPPI Project infos (gathered from the bppi server)
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+from bppi.repository.bppiApiRepositoryWrapper import bppiApiRepositoryWrapper
+from bppi.bppiPipeline import bppiPipeline
+import utils.constants as C
+
+MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
+                        C.PARAM_BPPIURL]
+
+class bppiProject(bppiPipeline):
+    def __init__(self, config):
+        super().__init__(config)
+        self.__projectInfos = None   # BPPI Project infos (gathered from the bppi server)
```

## pipelines/bppi/repository/bppiApiRepositoryWrapper.py

 * *Ordering differences only*

```diff
@@ -1,194 +1,194 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import requests
-import json
-from pipelines.bppi.repository.repConfig import repConfig
-from pipelines.bppi.uploadConfig import uploadConfig
-from urllib import request
-import utils.constants as C
-
-class bppiApiRepositoryWrapper:
-    """This class acts as a gateway for the BPPI API calls
-    """
-    def __init__(self, token, serverURL):
-        self.__token = token
-        self.__serverURL = serverURL
-        self.__log = None
-
-    @property
-    def log(self):
-        return self.__log
-    @log.setter   
-    def log(self, value):
-        self.__log = value
-
-    @property
-    def apiRootPath(self):
-        return self.__serverURL + C.API_1_0
-    @property
-    def URL(self):
-        return self.__serverURL
-    @property
-    def Token(self):
-        return self.__token
-
-    def getRepositoryConfiguration(self) -> repConfig:
-        """ HTTP GET / Gather the repository details & config from the server
-        Returns:
-            repConfig: BPPI repository config
-        """
-        try: 
-            # Get Api call for getting Repository informations
-            self.log.info("BPPI API - Get Api call for getting Repository informations ...")
-            url = self.apiRootPath + C.API_REPOSITORY_CONFIG
-            self.log.debug("BPPI API - HTTP GET Request sent: " + url)
-            headers = {}
-            headers["Authorization"] = "Bearer " + self.Token
-            headers["content-type"] = "application/json"
-            httpResponse = requests.get(url , headers=headers) 
-            repositoryCfg = repConfig(httpResponse) # content in repositoryCfg.jsonContent
-            if (repositoryCfg.loaded):
-                self.log.info("BPPI API - Informations from BPPI Repository collected successfully")
-            else:
-                raise Exception ("Impossible to collect repository informations.")
-            return repositoryCfg
-        
-        except Exception as e:
-            self.log.error("bppiApiRepositoryWrapper.getRepositoryConfiguration() - " + str(e))
-            return repConfig()
-
-    def prepareUpload(self, repositoryId) -> uploadConfig:
-        """ HTTP POST Call / get the Server info for upload / timeline.getUploadData
-        Args:
-            repositoryId (_type_): BPPI Repository ID
-        Returns:
-            uploadConfig: Upload details configuration
-        """
-        try: 
-            self.log.info("BPPI API - Get the Server info for upload ...")
-            url = self.apiRootPath + C.API_SERVER_UPLOAD_INFOS.format(repositoryId)
-            self.log.debug("BPPI API - HTTP POST Request " + url)
-            jsondata = json.dumps({"fileName": "timeline.csv"}).encode("utf8")
-            self.log.debug("BPPI API - HTTP POST Data sent: ", jsondata)
-            req = request.Request(url)
-            req.add_header('Content-Type', 'application/json; charset=utf-8')
-            req.add_header('Authorization', 'Bearer ' + self.Token)
-            httpResponse = request.urlopen(req, jsondata).read().decode("utf8")
-            cfg = uploadConfig(httpResponse) # see cfg.jsonContent
-            if (cfg.loaded):
-                self.log.info("BPPI API - Upload prepared successfully")
-            else:
-                raise Exception ("Impossible to prepare the upload")
-            return cfg
-        
-        except Exception as e:
-            self.log.error("bppiApiRepositoryWrapper.prepareUpload() - " + str(e))
-            return uploadConfig()
-
-    def uploadData(self, csvData, url, headersAcl) -> bool:
-        """HTTP PUT Call / Upload data (csv format) to the server
-        Args:
-            csvData (_type_): Data (CSV format)
-            url (_type_): BPPI URL (upload destination <- uploadConfig)
-            headersAcl (_type_): Header ACL (<- uploadConfig)
-        Returns:
-            bool: _description_
-        """
-        try:
-            self.log.info("BPPI API - Upload CSV formatted data to the BPPI Server")
-            headers = {}
-            headers["Authorization"] = "Bearer " + self.Token
-            headers["content-type"] = "text/csv"
-            headers.update(headersAcl)
-            self.log.debug("BPPI API - HTTP PUT Request " + url)
-            response = requests.put(url , data=csvData, headers=headers)
-            self.log.debug("BPPI API - HTTP Response {}".format(str(response)))
-            return response.ok
-        except Exception as e:
-            self.log.error("bppiApiRepositoryWrapper.uploadData() - UploadData Error | " + str(e))
-            return False
-
-    def loadFileToBPPIRepository(self, repositoryId, fkeys, repositoryTable) -> str:
-        """ HTTP POST Call / Upload the file in BPPI repo / timeline.loadFileIntoRepositoryTable
-        Args:
-            repositoryId (_type_): BPPI Repository ID
-            fkeys (_type_): Keys
-            repositoryTable (_type_): Table to create/append in the Repository
-        Returns:
-            str: ID of the Process execution
-        """
-        try:
-            self.log.info("BPPI API - Load the file to the BPPI repository")
-            url = self.apiRootPath + C.API_SERVER_LOAD_2_REPO.format(repositoryId)
-            self.log.debug("BPPI API - HTTP POST Request " + url)
-            req = request.Request(url)
-            req.add_header('Content-Type', 'application/json; charset=utf-8')
-            req.add_header('Authorization', 'Bearer ' + self.Token)
-            js = {}
-            js["fileKeys"] = json.loads(fkeys)
-            js["tableName"] = repositoryTable
-            jsondata = json.dumps(js).encode("utf8")
-            self.log.debug("HTTP POST Data sent: ", jsondata)
-            httpResponse = request.urlopen(req, jsondata).read()
-            jres2 = json.loads(httpResponse.decode("utf8"))
-            self.log.debug("BPPI API - HTTP Response {}".format(jres2))
-            self.log.info("BPPI API - Loading the file with process ID {} ".format(jres2["processingId"]))
-            return jres2["processingId"]
-        
-        except Exception as e:
-            self.log.error("bppiApiRepositoryWrapper.loadFileToBPPIRepository() - loadFileToBPPIRepository Error | " + str(e))
-            return str(-1)
-
-    def getProcessingStatus(self, processID) -> str:
-        """ HTTP Returns the processing Status 
-        Args:
-            processID (_type_): Process ID
-        Raises:
-            Exception: Exception / Error with HTTP dump
-        Returns:
-            str: Status
-        """
-        try:
-            self.log.info("BPPI API - Check status for the BPPI Task {}".format(processID))
-            url = self.apiRootPath + C.API_PROCESSING_STATUS + "/" + processID
-            self.log.debug("HTTP GET Request " + url)
-            response = requests.get(url, headers={ 'Authorization': 'Bearer ' + self.Token, 'content-type': 'application/json' })
-            jres = json.loads(response.content)
-            self.log.debug("BPPI API - HTTP Response {}".format(response.content))
-            self.log.info("BPPI API - BPPI Task {} status is {} ".format(processID, jres["status"]))
-            if (jres["status"] == C.API_STATUS_ERROR):
-                raise Exception(json.dumps(jres))
-            return jres["status"]
-        
-        except Exception as e:
-            self.log.error("bppiApiRepositoryWrapper.getProcessingStatus() - getProcessingStatus Error | " + str(e))
-            return C.API_STATUS_ERROR
-
-    def executeTODO(self, repositoryId, todo, tableName) -> str:
-        """ HTTPPOST Call / get the Server info for upload / timeline.getUploadData
-        Args:
-            repositoryId (_type_): Repository ID
-            todo (_type_): TO DO Name
-            tableName (_type_): Table name
-        Returns:
-            str: Process ID
-        """
-        try: 
-            self.log.info("BPPI API - Execute a To Do in BPPI repository")
-            url = self.apiRootPath + C.API_EXECUTE_TODO.format(repositoryId)
-            self.log.debug("bBPPI API - HTTP POST Request " + url)
-            jsondata = json.dumps({"todoListNames": todo, "tableName" : tableName}).encode("utf8")
-            self.log.debug("BPPI API - HTTP POST Data sent: ", jsondata)
-            req = request.Request(url)
-            req.add_header('Content-Type', 'application/json; charset=utf-8')
-            req.add_header('Authorization', 'Bearer ' + self.Token)
-            httpResponse = request.urlopen(req, jsondata).read()
-            jres2 = json.loads(httpResponse.decode("utf8"))
-            self.log.debug("BPPI API - HTTP Response {}".format(jres2))
-            self.log.info("BPPI API - Loading the file with process ID: " + jres2["processingId"])
-            return jres2["processingId"]
-        except Exception as e:
-            self.log.error(e)
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import requests
+import json
+from pipelines.bppi.repository.repConfig import repConfig
+from pipelines.bppi.uploadConfig import uploadConfig
+from urllib import request
+import utils.constants as C
+
+class bppiApiRepositoryWrapper:
+    """This class acts as a gateway for the BPPI API calls
+    """
+    def __init__(self, token, serverURL):
+        self.__token = token
+        self.__serverURL = serverURL
+        self.__log = None
+
+    @property
+    def log(self):
+        return self.__log
+    @log.setter   
+    def log(self, value):
+        self.__log = value
+
+    @property
+    def apiRootPath(self):
+        return self.__serverURL + C.API_1_0
+    @property
+    def URL(self):
+        return self.__serverURL
+    @property
+    def Token(self):
+        return self.__token
+
+    def getRepositoryConfiguration(self) -> repConfig:
+        """ HTTP GET / Gather the repository details & config from the server
+        Returns:
+            repConfig: BPPI repository config
+        """
+        try: 
+            # Get Api call for getting Repository informations
+            self.log.info("BPPI API - Get Api call for getting Repository informations ...")
+            url = self.apiRootPath + C.API_REPOSITORY_CONFIG
+            self.log.debug("BPPI API - HTTP GET Request sent: " + url)
+            headers = {}
+            headers["Authorization"] = "Bearer " + self.Token
+            headers["content-type"] = "application/json"
+            httpResponse = requests.get(url , headers=headers) 
+            repositoryCfg = repConfig(httpResponse) # content in repositoryCfg.jsonContent
+            if (repositoryCfg.loaded):
+                self.log.info("BPPI API - Informations from BPPI Repository collected successfully")
+            else:
+                raise Exception ("Impossible to collect repository informations.")
+            return repositoryCfg
+        
+        except Exception as e:
+            self.log.error("bppiApiRepositoryWrapper.getRepositoryConfiguration() - " + str(e))
+            return repConfig()
+
+    def prepareUpload(self, repositoryId) -> uploadConfig:
+        """ HTTP POST Call / get the Server info for upload / timeline.getUploadData
+        Args:
+            repositoryId (_type_): BPPI Repository ID
+        Returns:
+            uploadConfig: Upload details configuration
+        """
+        try: 
+            self.log.info("BPPI API - Get the Server info for upload ...")
+            url = self.apiRootPath + C.API_SERVER_UPLOAD_INFOS.format(repositoryId)
+            self.log.debug("BPPI API - HTTP POST Request " + url)
+            jsondata = json.dumps({"fileName": "timeline.csv"}).encode("utf8")
+            self.log.debug("BPPI API - HTTP POST Data sent: ", jsondata)
+            req = request.Request(url)
+            req.add_header('Content-Type', 'application/json; charset=utf-8')
+            req.add_header('Authorization', 'Bearer ' + self.Token)
+            httpResponse = request.urlopen(req, jsondata).read().decode("utf8")
+            cfg = uploadConfig(httpResponse) # see cfg.jsonContent
+            if (cfg.loaded):
+                self.log.info("BPPI API - Upload prepared successfully")
+            else:
+                raise Exception ("Impossible to prepare the upload")
+            return cfg
+        
+        except Exception as e:
+            self.log.error("bppiApiRepositoryWrapper.prepareUpload() - " + str(e))
+            return uploadConfig()
+
+    def uploadData(self, csvData, url, headersAcl) -> bool:
+        """HTTP PUT Call / Upload data (csv format) to the server
+        Args:
+            csvData (_type_): Data (CSV format)
+            url (_type_): BPPI URL (upload destination <- uploadConfig)
+            headersAcl (_type_): Header ACL (<- uploadConfig)
+        Returns:
+            bool: _description_
+        """
+        try:
+            self.log.info("BPPI API - Upload CSV formatted data to the BPPI Server")
+            headers = {}
+            headers["Authorization"] = "Bearer " + self.Token
+            headers["content-type"] = "text/csv"
+            headers.update(headersAcl)
+            self.log.debug("BPPI API - HTTP PUT Request " + url)
+            response = requests.put(url , data=csvData, headers=headers)
+            self.log.debug("BPPI API - HTTP Response {}".format(str(response)))
+            return response.ok
+        except Exception as e:
+            self.log.error("bppiApiRepositoryWrapper.uploadData() - UploadData Error | " + str(e))
+            return False
+
+    def loadFileToBPPIRepository(self, repositoryId, fkeys, repositoryTable) -> str:
+        """ HTTP POST Call / Upload the file in BPPI repo / timeline.loadFileIntoRepositoryTable
+        Args:
+            repositoryId (_type_): BPPI Repository ID
+            fkeys (_type_): Keys
+            repositoryTable (_type_): Table to create/append in the Repository
+        Returns:
+            str: ID of the Process execution
+        """
+        try:
+            self.log.info("BPPI API - Load the file to the BPPI repository")
+            url = self.apiRootPath + C.API_SERVER_LOAD_2_REPO.format(repositoryId)
+            self.log.debug("BPPI API - HTTP POST Request " + url)
+            req = request.Request(url)
+            req.add_header('Content-Type', 'application/json; charset=utf-8')
+            req.add_header('Authorization', 'Bearer ' + self.Token)
+            js = {}
+            js["fileKeys"] = json.loads(fkeys)
+            js["tableName"] = repositoryTable
+            jsondata = json.dumps(js).encode("utf8")
+            self.log.debug("HTTP POST Data sent: ", jsondata)
+            httpResponse = request.urlopen(req, jsondata).read()
+            jres2 = json.loads(httpResponse.decode("utf8"))
+            self.log.debug("BPPI API - HTTP Response {}".format(jres2))
+            self.log.info("BPPI API - Loading the file with process ID {} ".format(jres2["processingId"]))
+            return jres2["processingId"]
+        
+        except Exception as e:
+            self.log.error("bppiApiRepositoryWrapper.loadFileToBPPIRepository() - loadFileToBPPIRepository Error | " + str(e))
+            return str(-1)
+
+    def getProcessingStatus(self, processID) -> str:
+        """ HTTP Returns the processing Status 
+        Args:
+            processID (_type_): Process ID
+        Raises:
+            Exception: Exception / Error with HTTP dump
+        Returns:
+            str: Status
+        """
+        try:
+            self.log.info("BPPI API - Check status for the BPPI Task {}".format(processID))
+            url = self.apiRootPath + C.API_PROCESSING_STATUS + "/" + processID
+            self.log.debug("HTTP GET Request " + url)
+            response = requests.get(url, headers={ 'Authorization': 'Bearer ' + self.Token, 'content-type': 'application/json' })
+            jres = json.loads(response.content)
+            self.log.debug("BPPI API - HTTP Response {}".format(response.content))
+            self.log.info("BPPI API - BPPI Task {} status is {} ".format(processID, jres["status"]))
+            if (jres["status"] == C.API_STATUS_ERROR):
+                raise Exception(json.dumps(jres))
+            return jres["status"]
+        
+        except Exception as e:
+            self.log.error("bppiApiRepositoryWrapper.getProcessingStatus() - getProcessingStatus Error | " + str(e))
+            return C.API_STATUS_ERROR
+
+    def executeTODO(self, repositoryId, todo, tableName) -> str:
+        """ HTTPPOST Call / get the Server info for upload / timeline.getUploadData
+        Args:
+            repositoryId (_type_): Repository ID
+            todo (_type_): TO DO Name
+            tableName (_type_): Table name
+        Returns:
+            str: Process ID
+        """
+        try: 
+            self.log.info("BPPI API - Execute a To Do in BPPI repository")
+            url = self.apiRootPath + C.API_EXECUTE_TODO.format(repositoryId)
+            self.log.debug("bBPPI API - HTTP POST Request " + url)
+            jsondata = json.dumps({"todoListNames": todo, "tableName" : tableName}).encode("utf8")
+            self.log.debug("BPPI API - HTTP POST Data sent: ", jsondata)
+            req = request.Request(url)
+            req.add_header('Content-Type', 'application/json; charset=utf-8')
+            req.add_header('Authorization', 'Bearer ' + self.Token)
+            httpResponse = request.urlopen(req, jsondata).read()
+            jres2 = json.loads(httpResponse.decode("utf8"))
+            self.log.debug("BPPI API - HTTP Response {}".format(jres2))
+            self.log.info("BPPI API - Loading the file with process ID: " + jres2["processingId"])
+            return jres2["processingId"]
+        except Exception as e:
+            self.log.error(e)
             return str(-1)
```

## pipelines/bppi/repository/bppiRepository.py

```diff
@@ -1,142 +1,188 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-from pipelines.bppi.repository.bppiApiRepositoryWrapper import bppiApiRepositoryWrapper
-from pipelines.bppi.bppiPipeline import bppiPipeline
-import utils.constants as C
-from pipelines.bppi.repository.repConfig import repConfig
-import json
-
-MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
-                        C.PARAM_BPPIURL]
-
-class bppiRepository(bppiPipeline):
-    def __init__(self, config):
-        super().__init__(config)
-        self.__repositoryInfos = None   # BPPI Repository infos (gathered from the bppi server)
-
-    @property
-    def repositoryConfig(self) -> repConfig:
-        return self.__repositoryInfos
-
-    @property
-    def bppiTable(self) -> str:
-        # Priority on what in inside the config file
-        ini = self.config.getParameter(C.PARAM_BPPITABLE, C.EMPTY)
-        return ini if (ini != C.EMPTY) else self.repositoryConfig.repositoryTableName
-    
-    @property
-    def bppiTodos(self) -> str:
-        # Priority on what in inside the config file
-        ini = self.config.getParameter(C.PARAM_BPPITODOS, C.EMPTY).split(',')
-        return ini if (ini != C.EMPTY) else self.repositoryConfig.todoLists
-    
-    def initialize(self) -> bool:
-        """Initialize the Class instance by gathering the BPPI repository infos.
-            * initialize the logger
-            * check the mandatory parameters
-            * init the API (get the BPPI Repository infos)
-        Returns:
-            bool: False if error
-        """
-        try:
-            super().initialize()
-            # Get the repository configuration infos
-            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
-                                           self.config.getParameter(C.PARAM_BPPIURL))
-            api.log = super().log
-            self.__repositoryInfos = api.getRepositoryConfiguration()
-            return True
-        except Exception as e:
-            self.log.error("initialize() Error -> " + str(e))
-            return False
-
-    def executeToDo(self) -> bool:
-        """Execute a BPPI TO DO (be careful as this TO DO must exists)
-        Returns:
-            bool: False if error or the TO DO does not exists
-        """
-        try:
-            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
-                                            self.config.getParameter(C.PARAM_BPPIURL))
-            api.log = self.log
-            self.log.info("Execute these TO DO: {}".format(",".join(self.bppiTodos)))
-            if (self.repositoryConfig.loaded):
-                if (len(self.bppiTodos) > 0):
-                    processId = api.executeTODO(self.repositoryConfig.repositoryId, 
-                                                self.bppiTodos, 
-                                                self.bppiTable)
-                    self.waitForEndOfProcessing(processId)
-                    self.log.info("To Do executed successfully")
-                    return True
-                else:
-                    self.log.info("No configured To Do to execute")
-                    return False
-        except Exception as e:
-            self.log.error("executeToDo() Error -> " + str(e))
-            return False
-
-    def load(self, dfDataset) -> bool:
-        """ Upload a dataset (Pandas DataFrame) in the BPPI repository (in one transaction)
-        Args:
-            dfDataset (pd.DataFrame): DataFrame with the Data to upload
-        Returns:
-            bool: False if error
-        """
-        try:
-            self.log.info("Upload the data into the BPPI repository in one transaction")
-            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
-                                            self.config.getParameter(C.PARAM_BPPIURL))
-            api.log = self.log
-            if (self.repositoryConfig.loaded):
-                fileKeys = []
-                blocIdx, blocIdxEnd = 0, 0
-                datasize = dfDataset.shape[0]
-                if (datasize > C.API_BLOC_SIZE_LIMIT):
-                    self.log.info("Data (all) size (Nb Lines= {}) is larger than the upload limit {}, split the data in several data blocs".format(datasize , C.API_BLOC_SIZE_LIMIT))
-                    blocNum = 1
-                    while (blocIdxEnd < len(dfDataset)-1):
-                        # Create the blocs (Nb of line to API_BLOC_SIZE_LIMIT)
-                        blocIdxEnd = blocIdx + C.API_BLOC_SIZE_LIMIT - 1
-                        if (blocIdxEnd >= len(dfDataset)-1):
-                            blocIdxEnd = len(dfDataset)-1
-                        self.log.debug("Data bloc N{}, Index from {} -> {}".format(blocNum, blocIdx, blocIdxEnd))
-                        blocData = dfDataset.iloc[blocIdx:blocIdxEnd:,:]
-                        blocIdx += C.API_BLOC_SIZE_LIMIT 
-                        # 2 - Prepare the upload
-                        uploadCfg = api.prepareUpload(self.repositoryConfig.repositoryId)
-                        # 3 - Upload the file to the server
-                        blocData_toupload = blocData.to_csv(header=True, encoding=C.ENCODING, index=False)
-                        uploadOK = api.uploadData(blocData_toupload, uploadCfg.url, uploadCfg.headers)
-                        fileKeys.append(uploadCfg.key)
-                        if (uploadOK):
-                            self.log.info("Data bloc N{} was uploaded successfully".format(blocNum))
-                        else:
-                            self.log.warning("Data bloc N{} was NOT uploaded successfully".format(blocNum))
-                            break
-                else:
-                    self.log.debug("The data can be uploaded in one unique bloc")
-                    # 2 - Prepare the complete file upload
-                    uploadCfg = api.prepareUpload(self.repositoryConfig.repositoryId)
-                    fileKeys.append(uploadCfg.key)
-                    blocData_toupload = dfDataset.to_csv(header=True, encoding=C.ENCODING, index=False)
-                    uploadOK = api.uploadData(blocData_toupload, uploadCfg.url, uploadCfg.headers)
-                    keys = uploadCfg.key
-                    if (uploadOK):
-                        self.log.info("Data was uploaded successfully")
-                    else:
-                        self.log.warning("Data was NOT uploaded successfully")
-                keys = json.dumps(fileKeys)
-                if (uploadOK):
-                    self.log.info("Load the uploaded data/bloc(s) into the BPPI repository")
-                    # 4 - Load the file into the BPPI repository
-                    processId = api.loadFileToBPPIRepository(self.repositoryConfig.repositoryId, keys, self.bppiTable)
-                    self.waitForEndOfProcessing(processId)
-                else:
-                    self.log.error("The data have not been loaded successfully")
-            return True
-        
-        except Exception as e:
-            self.log.error("upload() Error -> " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+from pipelines.bppi.repository.bppiApiRepositoryWrapper import bppiApiRepositoryWrapper
+from pipelines.bppi.bppiPipeline import bppiPipeline
+import utils.constants as C
+from pipelines.bppi.repository.repConfig import repConfig
+import json
+import time
+
+MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
+                        C.PARAM_BPPIURL]
+
+class bppiRepository(bppiPipeline):
+    def __init__(self, config, log):
+        super().__init__(config, log)
+        self.__repositoryInfos = None   # BPPI Repository infos (gathered from the bppi server)
+
+    @property
+    def repositoryConfig(self) -> repConfig:
+        return self.__repositoryInfos
+
+    @property
+    def bppiTable(self) -> str:
+        # Priority on what in inside the config file
+        ini = self.config.getParameter(C.PARAM_BPPITABLE, C.EMPTY)
+        return ini if (ini != C.EMPTY) else self.repositoryConfig.repositoryTableName
+    
+    @property
+    def bppiTodos(self) -> str:
+        # Priority on what in inside the config file
+        ini = self.config.getParameter(C.PARAM_BPPITODOS, C.EMPTY).split(',')
+        return ini if (ini != C.EMPTY) else self.repositoryConfig.todoLists
+    
+    def initialize(self) -> bool:
+        """Initialize the Class instance by gathering the BPPI repository infos.
+            * initialize the logger
+            * check the mandatory parameters
+            * init the API (get the BPPI Repository infos)
+        Returns:
+            bool: False if error
+        """
+        try:
+            super().initialize()
+            # Get the repository configuration infos
+            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
+                                           self.config.getParameter(C.PARAM_BPPIURL))
+            api.log = super().log
+            self.__repositoryInfos = api.getRepositoryConfiguration()
+            return True
+        except Exception as e:
+            self.log.error("initialize() Error -> " + str(e))
+            return False
+
+    def getStatus(self, processingId) -> str:
+        """Return the status of a process launched on the BPPI server
+        Args:
+            processingId (_type_): ID of the BPPI Process
+        Returns:
+            str: Process status (from BPPI server)
+        """
+        try:
+            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
+                                            self.config.getParameter(C.PARAM_BPPIURL))
+            api.log = self.log
+            return api.getProcessingStatus(processingId)
+        except Exception as e:
+            self.log.error("getStatus() Error -> " + str(e))
+            return C.API_STATUS_ERROR
+
+    def waitForEndOfProcessing(self, processId) -> str:
+        """Wait for the end of the BPPI process execution
+        Args:
+            processId (_type_): ID of the BPPI Process
+        Returns:
+            str: Final Status
+        """
+        try:
+            self.log.info("Wait for the end of a process execution")
+            EndOfWait = True
+            nbIterations = 0
+            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
+                                            self.config.getParameter(C.PARAM_BPPIURL))
+            api.log = self.log
+            while (EndOfWait):
+                # 5 - Check the status to veriify if the task is finished
+                status = self.getStatus(processId)
+                if ((status != C.API_STATUS_IN_PROGRESS) or (nbIterations > C.API_DEF_NB_ITERATION_MAX)):
+                    EndOfWait = False
+                time.sleep(C.API_DEF_WAIT_DURATION_SEC)
+                nbIterations += 1
+            return status
+        except Exception as e:
+            self.log.error("waitForEndOfProcessing() Error -> " + str(e))
+            return C.API_STATUS_ERROR
+
+    def afterLoad(self) -> bool:
+        return self.executeToDo()
+
+    def executeToDo(self) -> bool:
+        """Execute a BPPI TO DO (be careful as this TO DO must exists)
+        Returns:
+            bool: False if error or the TO DO does not exists
+        """
+        try:
+            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
+                                            self.config.getParameter(C.PARAM_BPPIURL))
+            api.log = self.log
+            self.log.info("Execute these TO DO: {}".format(",".join(self.bppiTodos)))
+            if (self.repositoryConfig.loaded):
+                if (len(self.bppiTodos) > 0):
+                    processId = api.executeTODO(self.repositoryConfig.repositoryId, 
+                                                self.bppiTodos, 
+                                                self.bppiTable)
+                    self.waitForEndOfProcessing(processId)
+                    self.log.info("To Do executed successfully")
+                    return True
+                else:
+                    self.log.info("No configured To Do to execute")
+                    return False
+        except Exception as e:
+            self.log.error("executeToDo() Error -> " + str(e))
+            return False
+
+    def load(self, dfDataset) -> bool:
+        """ Upload a dataset (Pandas DataFrame) in the BPPI repository (in one transaction)
+        Args:
+            dfDataset (pd.DataFrame): DataFrame with the Data to upload
+        Returns:
+            bool: False if error
+        """
+        try:
+            self.log.info("Upload the data into the BPPI repository in one transaction")
+            api = bppiApiRepositoryWrapper(self.config.getParameter(C.PARAM_BPPITOKEN), 
+                                            self.config.getParameter(C.PARAM_BPPIURL))
+            api.log = self.log
+            if (self.repositoryConfig.loaded):
+                fileKeys = []
+                blocIdx, blocIdxEnd = 0, 0
+                datasize = dfDataset.shape[0]
+                if (datasize > C.API_BLOC_SIZE_LIMIT):
+                    self.log.info("Data (all) size (Nb Lines= {}) is larger than the upload limit {}, split the data in several data blocs".format(datasize , C.API_BLOC_SIZE_LIMIT))
+                    blocNum = 1
+                    while (blocIdxEnd < len(dfDataset)-1):
+                        # Create the blocs (Nb of line to API_BLOC_SIZE_LIMIT)
+                        blocIdxEnd = blocIdx + C.API_BLOC_SIZE_LIMIT - 1
+                        if (blocIdxEnd >= len(dfDataset)-1):
+                            blocIdxEnd = len(dfDataset)-1
+                        self.log.debug("Data bloc N{}, Index from {} -> {}".format(blocNum, blocIdx, blocIdxEnd))
+                        blocData = dfDataset.iloc[blocIdx:blocIdxEnd:,:]
+                        blocIdx += C.API_BLOC_SIZE_LIMIT 
+                        # 2 - Prepare the upload
+                        uploadCfg = api.prepareUpload(self.repositoryConfig.repositoryId)
+                        # 3 - Upload the file to the server
+                        blocData_toupload = blocData.to_csv(header=True, encoding=C.ENCODING, index=False)
+                        uploadOK = api.uploadData(blocData_toupload, uploadCfg.url, uploadCfg.headers)
+                        fileKeys.append(uploadCfg.key)
+                        if (uploadOK):
+                            self.log.info("Data bloc N{} was uploaded successfully".format(blocNum))
+                        else:
+                            self.log.warning("Data bloc N{} was NOT uploaded successfully".format(blocNum))
+                            break
+                else:
+                    self.log.debug("The data can be uploaded in one unique bloc")
+                    # 2 - Prepare the complete file upload
+                    uploadCfg = api.prepareUpload(self.repositoryConfig.repositoryId)
+                    fileKeys.append(uploadCfg.key)
+                    blocData_toupload = dfDataset.to_csv(header=True, encoding=C.ENCODING, index=False)
+                    uploadOK = api.uploadData(blocData_toupload, uploadCfg.url, uploadCfg.headers)
+                    keys = uploadCfg.key
+                    if (uploadOK):
+                        self.log.info("Data was uploaded successfully")
+                    else:
+                        self.log.warning("Data was NOT uploaded successfully")
+                keys = json.dumps(fileKeys)
+                if (uploadOK):
+                    self.log.info("Load the uploaded data/bloc(s) into the BPPI repository")
+                    # 4 - Load the file into the BPPI repository
+                    processId = api.loadFileToBPPIRepository(self.repositoryConfig.repositoryId, keys, self.bppiTable)
+                    self.waitForEndOfProcessing(processId)
+                else:
+                    self.log.error("The data have not been loaded successfully")
+            return True
+        
+        except Exception as e:
+            self.log.error("upload() Error -> " + str(e))
             return False
```

## pipelines/bppi/repository/repConfig.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import json
-
-class repConfig():
-    """ This class contains all the informations from the BPPI Repository Configuration page (Data Source).
-    """
-    def __init__(self, httpResponse = None):
-        super().__init__()
-        if (httpResponse == None):
-            self.__httpResponse = None
-            self.__repositoryId = ""
-            self.__repositoryTableName = ""
-            self.__username = ""
-            self.__password = ""
-            self.__dbConnectionString = ""
-            self.__query = ""
-            self.__todoLists = []
-            super().loaded = False
-            self.__jsonContent = json.dumps({}).encode("utf8")
-        else:
-            self.__jsonContent = httpResponse.content
-            self.__httpResponse = httpResponse
-            self.parse(httpResponse.content)
-        return
-    
-    def parse(self, httpResponse):
-        try:
-            j = json.loads(httpResponse)
-            self.__repositoryId = j['repositoryId']
-            self.__repositoryTableName = j['repositoryTableName']
-            self.__todoLists = j['todoLists']
-            self.__username = j['username']
-            self.__password = j['password']
-            self.__dbConnectionString = j['dbConnectionString']
-            self.__query = j['query']
-            self.__loaded = True
-        except:
-            self.__loaded = False
-
-    @property
-    def jsonContent(self):
-        return self.__jsonContent
-    
-    @property
-    def repositoryId(self):
-        return self.__repositoryId
-    
-    @property
-    def repositoryTableName(self):
-        return self.__repositoryTableName
-    @repositoryTableName.setter   
-    def repositoryTableName(self, value):
-        self.__repositoryTableName = value
-        
-    @property
-    def todoLists(self):
-        return self.__todoLists
-    @todoLists.setter   
-    def todoLists(self, value):
-        self.__todoLists = value
-
-    @property
-    def loaded(self):
-        return self.__loaded
-    @loaded.setter   
-    def loaded(self, value):
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import json
+
+class repConfig():
+    """ This class contains all the informations from the BPPI Repository Configuration page (Data Source).
+    """
+    def __init__(self, httpResponse = None):
+        super().__init__()
+        if (httpResponse == None):
+            self.__httpResponse = None
+            self.__repositoryId = ""
+            self.__repositoryTableName = ""
+            self.__username = ""
+            self.__password = ""
+            self.__dbConnectionString = ""
+            self.__query = ""
+            self.__todoLists = []
+            super().loaded = False
+            self.__jsonContent = json.dumps({}).encode("utf8")
+        else:
+            self.__jsonContent = httpResponse.content
+            self.__httpResponse = httpResponse
+            self.parse(httpResponse.content)
+        return
+    
+    def parse(self, httpResponse):
+        try:
+            j = json.loads(httpResponse)
+            self.__repositoryId = j['repositoryId']
+            self.__repositoryTableName = j['repositoryTableName']
+            self.__todoLists = j['todoLists']
+            self.__username = j['username']
+            self.__password = j['password']
+            self.__dbConnectionString = j['dbConnectionString']
+            self.__query = j['query']
+            self.__loaded = True
+        except:
+            self.__loaded = False
+
+    @property
+    def jsonContent(self):
+        return self.__jsonContent
+    
+    @property
+    def repositoryId(self):
+        return self.__repositoryId
+    
+    @property
+    def repositoryTableName(self):
+        return self.__repositoryTableName
+    @repositoryTableName.setter   
+    def repositoryTableName(self, value):
+        self.__repositoryTableName = value
+        
+    @property
+    def todoLists(self):
+        return self.__todoLists
+    @todoLists.setter   
+    def todoLists(self, value):
+        self.__todoLists = value
+
+    @property
+    def loaded(self):
+        return self.__loaded
+    @loaded.setter   
+    def loaded(self, value):
         self.__loaded = value
```

## pipelines/readers/Reader.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import pandas as pd
-from utils.log import log
-
-class Reader:
-    def __init__(self, log = None):
-        self.__content = pd.DataFrame()
-        self.__log = log
-
-    @property
-    def log(self) -> log:
-        return self.__log
-
-    @property
-    def content(self):
-        return self.__content
-    @content.setter   
-    def content(self, value):
-        self.__content = value
-
-    def read(self) -> bool:
-        """ Returns all the data in a DataFrame format
-        Returns:
-            bool: False is any trouble when reading
-        """
-        return True
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import pandas as pd
+from utils.log import log
+
+class Reader:
+    def __init__(self, log = None):
+        self.__content = pd.DataFrame()
+        self.__log = log
+
+    @property
+    def log(self) -> log:
+        return self.__log
+
+    @property
+    def content(self):
+        return self.__content
+    @content.setter   
+    def content(self, value):
+        self.__content = value
+
+    def read(self) -> bool:
+        """ Returns all the data in a DataFrame format
+        Returns:
+            bool: False is any trouble when reading
+        """
+        return True
```

## pipelines/readers/bpAPIReader.py

```diff
@@ -1,20 +1,201 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-import pandas as pd
-from .Reader import Reader 
-
-class bpAPIReader(Reader):
-    def read(self) -> bool:
-        """ Returns all the BP Repository data in a df
-        Returns:
-            bool: False is any trouble when reading
-        """
-        try:
-            return True
-        
-        except Exception as e:
-            self.log.error("bpAPIReader.read() Error: " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+import pandas as pd
+from .Reader import Reader 
+import requests 
+import urllib.parse
+
+AUTH_TOKEN_SUFFIX_URL = "/connect/token"
+
+class bpAPIReader(Reader):
+
+    def setConnectionParams(self, urlAuth, urlApi, sslCheck, pageSize, clientID, secret, bpProcessName):
+        self.__urlAuth = urlAuth
+        self.__sslCheck = sslCheck
+        self.__pageSize = pageSize
+        self.__clientID = clientID
+        self.__secret = secret
+        self.__urlApi = urlApi 
+        self.__bpProcessName = bpProcessName
+
+    def __buildAPIURL(self):
+        return self.__urlApi + C.PBAPI_VER
+
+    def __getSSLVerification(self):
+        return (self.__sslCheck == C.YES)
+
+    def __getPageSize(self):
+        return self.__pageSize
+
+    def __getAccessToken(self):
+        """ OAuth2 protocol usage with the Blue Prism API to get the access token
+        Returns:
+            str: Blue Prism API Access Token
+        """
+        try:
+            self.log.debug("BP API - Get the Blue Prism API access token")
+            # Obtain an access token using client credentials grant
+            token_params = {
+                "grant_type": "client_credentials",
+                "client_id": self.__clientID,
+                "client_secret": self.__secret,
+            }
+            token_response = requests.post(self.__urlAuth + AUTH_TOKEN_SUFFIX_URL, 
+                                           data=token_params, 
+                                           verify=self.__getSSLVerification())
+            token_data = token_response.json()
+            self.log.debug("BP API - Blue Prism Access Token has been returned successfully")
+            # The access token can be extracted from the response
+            return token_data["access_token"]
+        
+        except Exception as e:
+            self.log.error("bpAPIReader.__getAccessToken() -> Unable to get the Blue Prism API Access Token, " + str(e))
+            return None
+
+    def __getSessionIDList(self, access_token):
+        """ Get the list of Blue Prism Sessions,by using the access token for making authorized API requests
+        Args:
+            access_token (str): Blue Prism API Token access
+        Returns:
+            DataFrame: List of Session ID
+        """
+        try:
+            self.log.debug("BP API - Get the Blue Prism session list")
+            headers = {
+                "Authorization": "Bearer " + access_token,
+            }
+            api_endpoint = self.__buildAPIURL() + C.BPAPI_SESSIONS_LIST
+            params = { 'sessionParameters.processName.eq': self.__bpProcessName }
+            api_endpoint += "?" + urllib.parse.urlencode(params, quote_via=urllib.parse.quote)
+            api_response = requests.get(api_endpoint, 
+                                        headers=headers, 
+                                        verify=self.__getSSLVerification())
+            if (api_response.status_code == C.HTTP_API_OK):
+                df = pd.DataFrame.from_dict(api_response.json()["items"], orient='columns')
+                self.log.debug("BP API - {} sessions have been returned.".format(len(df)))
+                return df["sessionId"]
+            else:
+                self.log.error("bpAPIReader.__getSessionIDList() -> API Call error, {}".format((api_response.status_code)))
+                return pd.DataFrame()
+            
+        except Exception as e:
+            self.log.error("bpAPIReader.__getSessionIDList() -> Unable to get the Blue Prism session list, " + str(e))
+            return pd.DataFrame()
+ 
+    def __getSessionDetails(self, access_token, sessionID):
+        """ Returns the global informations on a Blue Prism Session (header)
+        Args:
+            access_token (str): Blue Prism API Token access
+            sessionID (str): Blue Prism Session ID
+        Returns:
+            DataFrame: Session details
+        """
+        try:
+            self.log.debug("BP API - Get the Blue Prism session information (header)")
+            api_endpoint = (self.__buildAPIURL() + C.BPAPI_SESSION_HEAD).format(sessionID)
+            headers = {
+                "Authorization": "Bearer " + access_token,
+            }
+            api_response = requests.get(api_endpoint, 
+                                        headers=headers, 
+                                        verify=self.__getSSLVerification())
+            if (api_response.status_code == C.HTTP_API_OK):
+                return api_response.json()
+            else:
+                raise Exception("API Call error, {}".format((api_response.status_code)))
+            
+        except Exception as e:
+            self.log.error("bpAPIReader.__getSessionDetails() -> Unable to get the Blue Prism session global info, " + str(e))
+            return pd.DataFrame()
+        
+    def __getSessionLogs(self, access_token, sessionID):
+        """ Returns the all the sessions logs. The API works with pages (Max 1000 logs per page), so we've to loop into the returned pages.
+        Args:
+            access_token (str): Blue Prism API Token access
+            sessionID (str): Blue Prism Session ID
+        Returns:
+            DataFrame: Session logs
+        """
+        try:
+            self.log.debug("BP API - Get the Blue Prism session [{}] details".format(sessionID))
+            loop_on_page = True
+            all_logs = pd.DataFrame()
+            next_page_token = ""
+            iteration = 1
+            # The API returns logs per pages (Max 1000 logs per page)
+            while (loop_on_page):
+                self.log.debug("BP API - Get logs per page, iteration N{}".format(iteration))
+                # Build URL API Call
+                api_endpoint = (self.__buildAPIURL() + C.BPAPI_SESSION_LOGS).format(sessionID)
+                params = { 'sessionLogsParameters.itemsPerPage': self.__getPageSize() }
+                if (next_page_token != ""):
+                    params.update( { "sessionLogsParameters.pagingToken" : next_page_token })
+                api_endpoint += "?" + urllib.parse.urlencode(params, quote_via=urllib.parse.quote)
+
+                headers = {  "Authorization": "Bearer " + access_token }
+                api_response = requests.get(api_endpoint, 
+                                            headers=headers, 
+                                            verify=self.__getSSLVerification())
+                # Aggregate the logs (all pages)
+                if (api_response.status_code == C.HTTP_API_OK):
+                    df = pd.DataFrame.from_dict(api_response.json()["items"], orient='columns')
+                    all_logs = pd.concat([all_logs, df]) 
+                    next_page_token = api_response.json()["pagingToken"]
+                if (next_page_token == None or api_response.status_code != C.HTTP_API_OK):
+                    self.log.debug("bpAPIReader.__getSessionInfos() -> No more pages")
+                    loop_on_page = False
+                iteration += 1
+            self.log.debug("BP API - {} sessions details (steps/stages) have been returned.".format(len(all_logs)))
+            return all_logs
+        
+        except Exception as e:
+            self.log.error("bpAPIReader.__getSessionLogs() -> Unable to get the Blue Prism session [{}] details, {}".format(sessionID, str(e)))
+            return pd.DataFrame()
+
+    def __getSessionParameters(self, access_token, sessionID):
+        """ Returns the all the sessions parameters. 
+            *** In progress **
+        Args:
+            access_token (str): Blue Prism API Token access
+            sessionID (str): Blue Prism Session ID
+        Returns:
+            json: parameters
+        """
+        ssl_verification = self.__getSSLVerification()
+        api_endpoint = (self.__buildAPIURL() + C.BPAPI_SESSION_PARAMS).format(sessionID)
+        headers = {
+            "Authorization": "Bearer " + access_token,
+        }
+        api_response = requests.get(api_endpoint, headers=headers, verify=ssl_verification)
+        return api_response.json()
+
+    def read(self) -> bool:
+        """ Returns all the BP Repository data in a df
+        Returns:
+            bool: False is any trouble when reading
+        """
+        try:
+            access_token = self.__getAccessToken()
+            if (access_token != None):
+                sessionIDList = self.__getSessionIDList(access_token)
+                logs = pd.DataFrame()
+                # Aggregate the logs from all the sessions
+                for session in sessionIDList:
+                    self.log.debug("BP API - Collect logs from session {} ...".format(session))
+                    session_info = self.__getSessionDetails(access_token, session)
+                    session_logs = self.__getSessionLogs(access_token, session)
+                    # Add Session log data
+                    session_logs["ResourceName"] = session_info['resourceName']
+                    session_logs["status"] = session_info['status']
+                    session_logs["SessionID"] = session
+                    logs = pd.concat([logs, session_logs]) 
+                    self.log.debug("BP API - session {} logs collected successfully, Total: {} rows/logs".format(session, logs.shape[0]))
+            self.content = logs
+            return True
+        
+        except Exception as e:
+            self.log.error("bpAPIReader.read() Error: " + str(e))
             return False
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## pipelines/readers/csvFileReader.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import pandas as pd
-from .Reader import Reader 
-import utils.constants as C
-
-class csvFileReader(Reader):
-    @property
-    def filename(self):
-        return self.__filename
-    @filename.setter   
-    def filename(self, value):
-        self.__filename = value
-
-    @property
-    def separator(self):
-        return self.__separator
-    @separator.setter   
-    def separator(self, value):
-        self.__separator = value
-
-    def read(self) -> bool:
-        """ Returns all the BP Repository data in a df
-        Returns:
-            bool: False is any trouble when reading
-        """
-        try:
-            self.content = pd.read_csv(self.filename, 
-                                       encoding=C.ENCODING, 
-                                       delimiter=self.separator)
-            return True
-        
-        except Exception as e:
-            self.log.error("bpRepo.read() Error: " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import pandas as pd
+from .Reader import Reader 
+import utils.constants as C
+
+class csvFileReader(Reader):
+    @property
+    def filename(self):
+        return self.__filename
+    @filename.setter   
+    def filename(self, value):
+        self.__filename = value
+
+    @property
+    def separator(self):
+        return self.__separator
+    @separator.setter   
+    def separator(self, value):
+        self.__separator = value
+
+    def read(self) -> bool:
+        """ Returns all the BP Repository data in a df
+        Returns:
+            bool: False is any trouble when reading
+        """
+        try:
+            self.content = pd.read_csv(self.filename, 
+                                       encoding=C.ENCODING, 
+                                       delimiter=self.separator)
+            return True
+        
+        except Exception as e:
+            self.log.error("bpRepo.read() Error: " + str(e))
             return False
```

## pipelines/readers/excelFileReader.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import pandas as pd
-from .Reader import Reader 
-
-class excelFileReader(Reader):
-    @property
-    def filename(self):
-        return self.__filename
-    @filename.setter   
-    def filename(self, value):
-        self.__filename = value
-
-    @property
-    def sheet(self):
-        return self.__sheet
-    @sheet.setter   
-    def sheet(self, value):
-        self.__sheet = value
-
-    def read(self) -> bool:
-        """ Returns all the BP Repository data in a df
-        Returns:
-            bool: False is any trouble when reading
-        """
-        try:
-            if (self.sheet == "0" or self.sheet == ""):
-                self.sheet = 0
-            # Read the Excel file and provides a DataFrame
-            self.content = pd.read_excel(self.filename, 
-                               sheet_name=self.sheet) #, engine='openpyxl')
-            return True
-        
-        except Exception as e:
-            self.log.error("bpRepo.read() Error: " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import pandas as pd
+from .Reader import Reader 
+
+class excelFileReader(Reader):
+    @property
+    def filename(self):
+        return self.__filename
+    @filename.setter   
+    def filename(self, value):
+        self.__filename = value
+
+    @property
+    def sheet(self):
+        return self.__sheet
+    @sheet.setter   
+    def sheet(self, value):
+        self.__sheet = value
+
+    def read(self) -> bool:
+        """ Returns all the BP Repository data in a df
+        Returns:
+            bool: False is any trouble when reading
+        """
+        try:
+            if (self.sheet == "0" or self.sheet == ""):
+                self.sheet = 0
+            # Read the Excel file and provides a DataFrame
+            self.content = pd.read_excel(self.filename, 
+                               sheet_name=self.sheet) #, engine='openpyxl')
+            return True
+        
+        except Exception as e:
+            self.log.error("bpRepo.read() Error: " + str(e))
             return False
```

## pipelines/readers/odbcReader.py

```diff
@@ -1,37 +1,44 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-import pandas as pd
-from .Reader import Reader 
-import pyodbc
-
-class odbcReader(Reader):
-    def setConnectionParams(self, connectionstring, query):
-        self.__connString = connectionstring
-        self.__query = query
-
-    def read(self) -> bool:
-        """ Returns all the BP Repository data in a df
-        Returns:
-            bool: False is any trouble when reading
-        """
-        try:
-            self.log.info("Connect to thr ODBC Datasource ...")
-            odbcConnection = pyodbc.connect(self.__connString)
-            self.log.info("Connected to ODBC Data source")
-            if (not odbcConnection.closed):
-                self.log.debug("Execute the query: {}".format(self.__query))
-                self.content = pd.read_sql(self.__query, odbcConnection)
-                odbcConnection.close()
-                self.log.debug("<{}> rows read".format(self.content.shape[0]))
-            return True
-        
-        except Exception as e:
-            self.log.error("odbcReader.read() Error: " + str(e))
-            try:
-                odbcConnection.close()
-            except:
-                pass
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+import pandas as pd
+from .Reader import Reader 
+import pyodbc
+import warnings
+
+warnings.filterwarnings('ignore')
+
+class odbcReader(Reader):
+    def setConnectionParams(self, connectionstring, query):
+        self.__connString = connectionstring
+        self.__query = query
+
+    @property
+    def query(self) -> str:
+        return self.__query
+
+    def read(self) -> bool:
+        """ Returns all the BP Repository data in a df
+        Returns:
+            bool: False is any trouble when reading
+        """
+        try:
+            self.log.info("Connect to the ODBC Datasource ...")
+            odbcConnection = pyodbc.connect(self.__connString)
+            self.log.info("Connected to ODBC Data source")
+            if (not odbcConnection.closed):
+                self.log.debug("Execute the query: {}".format(self.query))
+                self.content = pd.read_sql(self.query, odbcConnection)
+                odbcConnection.close()
+                self.log.debug("<{}> rows read".format(self.content.shape[0]))
+            return True
+        
+        except Exception as e:
+            self.log.error("odbcReader.read() Error: " + str(e))
+            try:
+                odbcConnection.close()
+            except:
+                pass
             return False
```

## pipelines/readers/sapRFCTableReader.py

 * *Ordering differences only*

```diff
@@ -1,102 +1,102 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import pandas as pd
-from pyrfc import Connection, ABAPApplicationError, ABAPRuntimeError, LogonError, CommunicationError, RFCError
-from .Reader import Reader 
-"""
-    SE37 check in SAP
-    RFC_READ_TABLE (function module)
-
-"""
-
-class sapRFCTableReader(Reader):
-    def setConnectionParams(self, ahost, client, sysnr, user, pwd, router):
-        self.__ahost = ahost
-        self.__client = client
-        self.__sysnr = sysnr
-        self.__user = user
-        self.__pwd = pwd
-        self.__router = router
-
-    def setImportParameters(self, rfcfields, rfctable, rowcount):
-        self.__rfcfields = rfcfields
-        self.__rfctable = rfctable
-        self.__rowcount = rowcount
-
-    def __connectToSAP(self) -> Connection:
-        """ Connect to the SAP instance via RFC
-        Returns:
-            connection: SAP Connection
-        """
-        try:
-            # Get the SAP parmaters first
-            self.log.info("Connect to SAP via RFC")
-            conn = Connection(ashost=self.__ahost, 
-                              sysnr=self.__sysnr, 
-                              client=self.__client, 
-                              user=self.__user, 
-                              passwd=self.__pwd, 
-                              saprouter=self.__router)
-            return conn
-        except CommunicationError:
-            self.log.error("sapRFCTable.__connectToSAP() Could not connect to server.")
-        except LogonError:
-            self.log.error("sapRFCTable.__connectToSAP() Could not log in. Wrong credentials?")
-            print("Could not log in. Wrong credentials?")
-        except (ABAPApplicationError, ABAPRuntimeError):
-            self.log.error("sapRFCTable.__connectToSAP(): An error occurred")
-        return None
-
-    def __callRFCReadTable(self, conn) -> pd.DataFrame:
-        """ Call the RFC_READ_TABLE BAPI and get the dataset as result
-        Args:
-            conn (_type_): SAP Connection via pyrfc
-        Returns:
-            pd.DataFrame: DataFrame with the dataset
-        """
-        try:
-            # Get the list of fields to gather
-            # Call RFC_READ_TABLE
-            self.log.info("Gather data from the SAP Table")
-            result = conn.call("RFC_READ_TABLE",
-                                ROWCOUNT=self.__rowcount,
-                                QUERY_TABLE=self.__rfctable,
-                                FIELDS=self.__rfcfields)
-
-            # Get the data & create the dataFrame
-            data = result["DATA"]
-            self.log.info("<{}> rows has been read from SAP".format(len(data)))
-            fields = result["FIELDS"]
-
-            records = []
-            for entry in data:
-                record = {}
-                for i, field in enumerate(fields):
-                    field_name = field["FIELDNAME"]
-                    idx = int(field["OFFSET"])
-                    length = int(field["LENGTH"])
-                    field_value = str(entry["WA"][idx:idx+length])
-                    record[field_name] = field_value
-                records.append(record)
-            return pd.DataFrame(records, dtype=str)
-
-        except Exception as e:
-            self.log.error("sapRFCTable.__callRFCReadTable() Exception -> " + str(e))
-            return pd.DataFrame()
-        
-    def read(self) -> bool:
-        """ Returns all the SAP Table data in a df
-        Returns:
-            bool: False is any trouble when reading
-        """
-        try:
-            sapConn = self.__connectToSAP()
-            if (sapConn != None):
-                self.content = self.__callRFCReadTable(sapConn)
-            return True
-        
-        except Exception as e:
-            self.log.error("sapRFCTable.read() Error: " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import pandas as pd
+from pyrfc import Connection, ABAPApplicationError, ABAPRuntimeError, LogonError, CommunicationError, RFCError
+from .Reader import Reader 
+"""
+    SE37 check in SAP
+    RFC_READ_TABLE (function module)
+
+"""
+
+class sapRFCTableReader(Reader):
+    def setConnectionParams(self, ahost, client, sysnr, user, pwd, router):
+        self.__ahost = ahost
+        self.__client = client
+        self.__sysnr = sysnr
+        self.__user = user
+        self.__pwd = pwd
+        self.__router = router
+
+    def setImportParameters(self, rfcfields, rfctable, rowcount):
+        self.__rfcfields = rfcfields
+        self.__rfctable = rfctable
+        self.__rowcount = rowcount
+
+    def __connectToSAP(self) -> Connection:
+        """ Connect to the SAP instance via RFC
+        Returns:
+            connection: SAP Connection
+        """
+        try:
+            # Get the SAP parmaters first
+            self.log.info("Connect to SAP via RFC")
+            conn = Connection(ashost=self.__ahost, 
+                              sysnr=self.__sysnr, 
+                              client=self.__client, 
+                              user=self.__user, 
+                              passwd=self.__pwd, 
+                              saprouter=self.__router)
+            return conn
+        except CommunicationError:
+            self.log.error("sapRFCTable.__connectToSAP() Could not connect to server.")
+        except LogonError:
+            self.log.error("sapRFCTable.__connectToSAP() Could not log in. Wrong credentials?")
+            print("Could not log in. Wrong credentials?")
+        except (ABAPApplicationError, ABAPRuntimeError):
+            self.log.error("sapRFCTable.__connectToSAP(): An error occurred")
+        return None
+
+    def __callRFCReadTable(self, conn) -> pd.DataFrame:
+        """ Call the RFC_READ_TABLE BAPI and get the dataset as result
+        Args:
+            conn (_type_): SAP Connection via pyrfc
+        Returns:
+            pd.DataFrame: DataFrame with the dataset
+        """
+        try:
+            # Get the list of fields to gather
+            # Call RFC_READ_TABLE
+            self.log.info("Gather data from the SAP Table")
+            result = conn.call("RFC_READ_TABLE",
+                                ROWCOUNT=self.__rowcount,
+                                QUERY_TABLE=self.__rfctable,
+                                FIELDS=self.__rfcfields)
+
+            # Get the data & create the dataFrame
+            data = result["DATA"]
+            self.log.info("<{}> rows has been read from SAP".format(len(data)))
+            fields = result["FIELDS"]
+
+            records = []
+            for entry in data:
+                record = {}
+                for i, field in enumerate(fields):
+                    field_name = field["FIELDNAME"]
+                    idx = int(field["OFFSET"])
+                    length = int(field["LENGTH"])
+                    field_value = str(entry["WA"][idx:idx+length])
+                    record[field_name] = field_value
+                records.append(record)
+            return pd.DataFrame(records, dtype=str)
+
+        except Exception as e:
+            self.log.error("sapRFCTable.__callRFCReadTable() Exception -> " + str(e))
+            return pd.DataFrame()
+        
+    def read(self) -> bool:
+        """ Returns all the SAP Table data in a df
+        Returns:
+            bool: False is any trouble when reading
+        """
+        try:
+            sapConn = self.__connectToSAP()
+            if (sapConn != None):
+                self.content = self.__callRFCReadTable(sapConn)
+            return True
+        
+        except Exception as e:
+            self.log.error("sapRFCTable.read() Error: " + str(e))
             return False
```

## pipelines/readers/xesFileReader.py

 * *Ordering differences only*

```diff
@@ -1,108 +1,108 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import xmltodict    # MIT License
-from json import dumps, loads
-import pandas as pd
-from .Reader import Reader 
-
-# Inspired by https://github.com/FrankBGao/read_xes/tree/master
-DATATYPES = ['string',  'int', 'date', 'float', 'boolean', 'id']
-CASE_KEY = 'concept-name-attr'
-
-class xesFileReader(Reader):
-    @property
-    def filename(self):
-        return self.__filename
-    @filename.setter   
-    def filename(self, value):
-        self.__filename = value
-        
-    def __getEventDetails(self, event, id):
-        """ returns all columns for one event (in a list)
-        Args:
-            event (_type_): event details
-            id (_type_): trace id
-        Returns:
-            list: events details
-        """
-        one_event_attri = list(event.keys())
-        one_event_dict = {}
-        for i in DATATYPES:
-            if i in one_event_attri:
-                if type(event[i]) == list:
-                    for j in event[i]:
-                        one_event_dict[j['@key']] = j['@value']
-                else:
-                    one_event_dict[event[i]['@key']] = event[i]['@value']
-        one_event_dict[CASE_KEY] = id
-        return one_event_dict
-
-    def __ExtractOneTrace(self, trace_item):
-        """ extract logs and attributes from 1 trace
-        Args:
-            trace_item (_type_): 1 trace (contains attrs + several logs)
-        Returns:
-            dict: trace attributes
-            list: events
-        """
-
-        # Build atributes / trace
-        attrs = list(trace_item.keys())
-        attrs_dict = {}
-        for i in DATATYPES:
-            if i in attrs:
-                if type(trace_item[i]) == list:
-                    for j in trace_item[i]:
-                        attrs_dict[j['@key']] = j['@value']
-                else:
-                    attrs_dict[trace_item[i]['@key']] = trace_item[i]['@value']
-        # build events / trace
-        events = []
-        if type(trace_item['event']) == dict:
-            trace_item['event'] = [trace_item['event']]
-
-        for i in trace_item['event']:
-            inter_event = self.__getEventDetails(i, attrs_dict['concept:name'])
-            events.append(inter_event)
-        return attrs_dict, events
-
-    def __extractAll(self, xml):
-        """ This functions reads the XES file and extract all the events and attributes
-        Args:
-            xml (str): XML flow (XES)
-        Returns:
-            list: event list
-            list: attributes
-        """
-        traces = loads(dumps(xmltodict.parse(xml)))['log']['trace']
-        self.log.debug("xesFile.__extractAll(): {} traces to manage".format(len(traces)))
-        attributes_list = []
-        event_list = []
-        # reads the traces tags one by one and get all the events & attrs
-        traceIdx = 1
-        for trace in traces:
-            trace_item = self.__ExtractOneTrace(trace)
-            attributes_list.append(trace_item[0]) # Attributes
-            event_list = event_list + trace_item[1] # Event details
-            self.log.debug("xesFile.__extractAll(): {}) {} -> {} evts".format(traceIdx, trace_item[0]['concept:name'], len(trace_item[1])))
-            traceIdx += 1
-        return event_list, attributes_list
-    
-    def read(self) -> bool:
-        """ Returns all the XES events in a DataFrame format
-        Returns:
-            bool: False is any trouble when reading
-        """
-        try:
-            if (self.filename == ""):
-                raise Exception ("No XES file specified.")
-            xmldata = open(self.filename, mode='r').read()
-            events, attributes = self.__extractAll(xmldata)
-            self.content = pd.DataFrame(events)
-            return True
-        
-        except Exception as e:
-            self.log.error("xesFile.getEvents() Error: " + str(e))
-            return False
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import xmltodict    # MIT License
+from json import dumps, loads
+import pandas as pd
+from .Reader import Reader 
+
+# Inspired by https://github.com/FrankBGao/read_xes/tree/master
+DATATYPES = ['string',  'int', 'date', 'float', 'boolean', 'id']
+CASE_KEY = 'concept-name-attr'
+
+class xesFileReader(Reader):
+    @property
+    def filename(self):
+        return self.__filename
+    @filename.setter   
+    def filename(self, value):
+        self.__filename = value
+        
+    def __getEventDetails(self, event, id):
+        """ returns all columns for one event (in a list)
+        Args:
+            event (_type_): event details
+            id (_type_): trace id
+        Returns:
+            list: events details
+        """
+        one_event_attri = list(event.keys())
+        one_event_dict = {}
+        for i in DATATYPES:
+            if i in one_event_attri:
+                if type(event[i]) == list:
+                    for j in event[i]:
+                        one_event_dict[j['@key']] = j['@value']
+                else:
+                    one_event_dict[event[i]['@key']] = event[i]['@value']
+        one_event_dict[CASE_KEY] = id
+        return one_event_dict
+
+    def __ExtractOneTrace(self, trace_item):
+        """ extract logs and attributes from 1 trace
+        Args:
+            trace_item (_type_): 1 trace (contains attrs + several logs)
+        Returns:
+            dict: trace attributes
+            list: events
+        """
+
+        # Build atributes / trace
+        attrs = list(trace_item.keys())
+        attrs_dict = {}
+        for i in DATATYPES:
+            if i in attrs:
+                if type(trace_item[i]) == list:
+                    for j in trace_item[i]:
+                        attrs_dict[j['@key']] = j['@value']
+                else:
+                    attrs_dict[trace_item[i]['@key']] = trace_item[i]['@value']
+        # build events / trace
+        events = []
+        if type(trace_item['event']) == dict:
+            trace_item['event'] = [trace_item['event']]
+
+        for i in trace_item['event']:
+            inter_event = self.__getEventDetails(i, attrs_dict['concept:name'])
+            events.append(inter_event)
+        return attrs_dict, events
+
+    def __extractAll(self, xml):
+        """ This functions reads the XES file and extract all the events and attributes
+        Args:
+            xml (str): XML flow (XES)
+        Returns:
+            list: event list
+            list: attributes
+        """
+        traces = loads(dumps(xmltodict.parse(xml)))['log']['trace']
+        self.log.debug("xesFile.__extractAll(): {} traces to manage".format(len(traces)))
+        attributes_list = []
+        event_list = []
+        # reads the traces tags one by one and get all the events & attrs
+        traceIdx = 1
+        for trace in traces:
+            trace_item = self.__ExtractOneTrace(trace)
+            attributes_list.append(trace_item[0]) # Attributes
+            event_list = event_list + trace_item[1] # Event details
+            self.log.debug("xesFile.__extractAll(): {}) {} -> {} evts".format(traceIdx, trace_item[0]['concept:name'], len(trace_item[1])))
+            traceIdx += 1
+        return event_list, attributes_list
+    
+    def read(self) -> bool:
+        """ Returns all the XES events in a DataFrame format
+        Returns:
+            bool: False is any trouble when reading
+        """
+        try:
+            if (self.filename == ""):
+                raise Exception ("No XES file specified.")
+            xmldata = open(self.filename, mode='r').read()
+            events, attributes = self.__extractAll(xmldata)
+            self.content = pd.DataFrame(events)
+            return True
+        
+        except Exception as e:
+            self.log.error("xesFile.getEvents() Error: " + str(e))
+            return False
```

## pipelines/readers/builders/SQLBuilder.py

```diff
@@ -1,64 +1,60 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-import pathlib
-from string import Template
-
-class SQLBuilder():
-    def __init__(self, log, config):
-        self.__log = log
-        self.__config = config
-        pass
-
-    @property
-    def log(self):
-        return self.__log
-    @property
-    def config(self):
-        return self.__config
-    
-    def getTemplate(self) -> Template:
-        """ returns the template SQL file
-        Args:
-            filename (_type_): filename (from the INI database.query parameter)
-        Returns:
-            Template: Return the String template
-        """
-        try:
-            content = self.config.getParameter(C.PARAM_QUERY)
-            if (self.config.getParameter(C.CONFIG_SOURCE_NAME, C.EMPTY) == C.CONFIG_SOURCE_SQ3):
-                # If config from SQLite or DB, the content is inside the field
-                return Template(content)
-            else:
-                # If config from INI file, the content is inside a file
-                return Template(pathlib.Path(content).read_text())
-        except Exception as e:
-            self.log.error("getTemplate() -> Error when reading the SQL template " + str(e))
-            return ""
-
-    def setSubstDict(self) -> dict:
-        """ returns a dictionnary with all the values to substitute in the SQL query.
-            By default no values to substitute
-        Returns:
-            dict: dictionnary with values
-        """
-        return {}
-
-    def build(self) -> str:
-        """Build the SQL Query based on a string template (stored in a file)
-        Returns:
-            str: built SQL Query
-        """
-        try: 
-            # Get the query skeleton in the sql file
-            sqlTemplate = self.getTemplate()
-            # Create the Substitute dict
-            valuesToReplace = self.setSubstDict()
-            # replace the values in the template
-            return sqlTemplate.substitute(valuesToReplace)
-
-        except Exception as e:
-            self.log.error("build() -> Unable to build the Blue Prism Query -> " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+import pathlib
+from string import Template
+
+class SQLBuilder():
+    def __init__(self, log, query, configtype=C.CONFIG_SOURCE_INI):
+        self.__log = log
+        self.__query = query    # self.config.getParameter(C.PARAM_QUERY)
+        self.__configtype = configtype # self.config.getParameter(C.CONFIG_SOURCE_NAME, C.EMPTY)
+
+    @property
+    def log(self):
+        return self.__log
+    
+    def getTemplate(self) -> Template:
+        """ returns the template SQL file
+        Args:
+            filename (_type_): filename (from the INI database.query parameter)
+        Returns:
+            Template: Return the String template
+        """
+        try:
+            if (self.__configtype == C.CONFIG_SOURCE_SQ3):
+                # If config from SQLite or DB, the content is inside the field
+                return Template(self.__query)
+            else:
+                # If config from INI file, the content is inside a file
+                return Template(pathlib.Path(self.__query).read_text())
+        except Exception as e:
+            self.log.error("getTemplate() -> Error when reading the SQL template " + str(e))
+            return ""
+
+    def setSubstDict(self) -> dict:
+        """ returns a dictionnary with all the values to substitute in the SQL query.
+            By default no values to substitute
+        Returns:
+            dict: dictionnary with values
+        """
+        return {}
+
+    def build(self) -> str:
+        """Build the SQL Query based on a string template (stored in a file)
+        Returns:
+            str: built SQL Query
+        """
+        try: 
+            # Get the query skeleton in the sql file
+            sqlTemplate = self.getTemplate()
+            # Create the Substitute dict
+            valuesToReplace = self.setSubstDict()
+            # replace the values in the template
+            return sqlTemplate.substitute(valuesToReplace)
+
+        except Exception as e:
+            self.log.error("build() -> Unable to build the Blue Prism Query -> " + str(e))
             return C.EMPTY
```

## pipelines/readers/builders/blueprismSQLBuilder.py

```diff
@@ -1,70 +1,73 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.builders.SQLBuilder import SQLBuilder
-
-NO_FILTER = "1=1"
-
-class blueprismSQLBuilder(SQLBuilder):
-    
-    @property
-    def deltaDate(self):
-        return self.__deltaDate
-    @deltaDate.setter   
-    def deltaDate(self, value):
-        self.__deltaDate = value
-    
-    def setSubstDict(self) -> dict:
-        """ returns a dictionnary with all the values to substitute in the SQL query
-        Returns:
-            dict: dictionnary with values
-        """
-        try: 
-            processname = self.config.getParameter(C.PARAM_BPPROCESSNAME)
-            stagetypes = self.config.getParameter(C.PARAM_BPSTAGETYPES, "0")
-            deltasql = NO_FILTER
-            novbo = NO_FILTER
-
-            # Build the filters on the VBO only
-            if (self.config.getParameter(C.PARAM_BPINCLUDEVBO, C.YES) != C.YES):
-                novbo = C.BPLOG_PROCESSNAME_COL + " IS NULL"
-
-            # Date Filtering and/or DELTA vs FULL
-            if (self.deltaDate != ""):
-                self.log.info("DELTA Load requested - from <" + str(self.deltaDate) + ">")
-                # DELTA LOAD (get date from file first)
-                deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + self.deltaDate + "'"
-            else:
-                self.log.info("FULL Load requested")
-                
-                # FULL LOAD / Add the delta extraction filters if required (-fromdate and/or -todate filled)
-                fromdate = self.config.getParameter(C.PARAM_FROMDATE)
-                todate = self.config.getParameter(C.PARAM_TODATE)
-                if ((fromdate != C.EMPTY) and (todate != C.EMPTY)):
-                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') BETWEEN '" + fromdate + "' AND '" + todate + "'"
-                elif (fromdate != C.EMPTY):
-                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + fromdate + "'"
-                elif (todate != C.EMPTY):
-                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') <= '" + todate + "'"
-
-            # BP Logs in unicode ? (default no)
-            if (self.config.getParameter(C.PARAM_BPUNICODE) == C.YES):
-                tablelog = C.BPLOG_LOG_UNICODE
-            else:
-                tablelog = C.BPLOG_LOG_NONUNICODE
-                
-            # Finalize the SQL Query by replacing the parameters
-            valuesToReplace = { 
-                                "processname" : processname, 
-                                "stagetypefilters" : stagetypes, 
-                                "onlybpprocess" : novbo, 
-                                "delta" : deltasql, 
-                                "tablelog" : tablelog
-                                }
-            return valuesToReplace
-
-        except Exception as e:
-            self.log.error("build() -> Unable to build the Blue Prism Query " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.readers.builders.SQLBuilder import SQLBuilder
+
+NO_FILTER = "1=1"
+
+class blueprismSQLBuilder(SQLBuilder):
+    def setConnectionParams(self, 
+                            processName=C.EMPTY, 
+                            bpStageTypes="0", 
+                            includeVBO=C.YES, 
+                            fromDate=C.EMPTY, 
+                            toDate=C.EMPTY, 
+                            unicode=C.YES,
+                            deltaDate=C.EMPTY):
+        self.__processName = processName    # self.config.getParameter(C.PARAM_BPPROCESSNAME)
+        self.__bpStageTypes = bpStageTypes  # self.config.getParameter(C.PARAM_BPSTAGETYPES, "0")
+        self.__includeVBO = includeVBO      # self.config.getParameter(C.PARAM_BPINCLUDEVBO, C.YES)
+        self.__fromDate = fromDate        # self.config.getParameter(C.PARAM_FROMDATE)
+        self.__toDate = toDate          # self.config.getParameter(C.PARAM_TODATE)
+        self.__unicode = unicode        # self.config.getParameter(C.PARAM_BPUNICODE)
+        self.__deltaDate = deltaDate
+
+    def setSubstDict(self) -> dict:
+        """ returns a dictionnary with all the values to substitute in the SQL query
+        Returns:
+            dict: dictionnary with values
+        """
+        try: 
+            deltasql = NO_FILTER
+            novbo = NO_FILTER
+
+            # Build the filters on the VBO only
+            if (self.__includeVBO != C.YES):
+                novbo = C.BPLOG_PROCESSNAME_COL + " IS NULL"
+
+            # Date Filtering and/or DELTA vs FULL
+            if (self.__deltaDate != C.EMPTY):
+                self.log.info("DELTA Load requested - from <" + str(self.__deltaDate) + ">")
+                # DELTA LOAD (get date from file first)
+                deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + self.__deltaDate + "'"
+            else:
+                self.log.info("FULL Load requested")
+                # FULL LOAD / Add the delta extraction filters if required (-fromdate and/or -todate filled)
+                if ((self.__fromDate != C.EMPTY) and (self.__toDate != C.EMPTY)):
+                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') BETWEEN '" + self.__fromDate + "' AND '" + self.__toDate + "'"
+                elif (self.__fromDate != C.EMPTY):
+                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + self.__fromDate + "'"
+                elif (self.__toDate != C.EMPTY):
+                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') <= '" + self.__toDate + "'"
+
+            # BP Logs in unicode ? (default no)
+            if (self.__unicode == C.YES):
+                tablelog = C.BPLOG_LOG_UNICODE
+            else:
+                tablelog = C.BPLOG_LOG_NONUNICODE
+                
+            # Finalize the SQL Query by replacing the parameters
+            valuesToReplace = { 
+                                "processname" : self.__processName, 
+                                "stagetypefilters" : self.__bpStageTypes, 
+                                "onlybpprocess" : novbo, 
+                                "delta" : deltasql, 
+                                "tablelog" : tablelog
+                                }
+            return valuesToReplace
+
+        except Exception as e:
+            self.log.error("build() -> Unable to build the Blue Prism Query " + str(e))
             return ""
```

## pipelines/repository/bppiPLRBluePrismApi.py

```diff
@@ -1,223 +1,48 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-import warnings
-from pipelines.bppi.repository.bppiRepository import bppiRepository
-import pandas as pd
-import requests 
-import urllib.parse
-
-BP_MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
-                           C.PARAM_BPPIURL, 
-                           C.PARAM_BPPROCESSNAME,
-                           C.PARAM_BPAPI_CLIENT_ID,
-                           C.PARAM_BPAPI_SECRET,
-                           C.PARAM_BPAPI_AUTH_URL]
-
-AUTH_TOKEN_SUFFIX_URL = "/connect/token"
-
-warnings.filterwarnings('ignore')
-
-""" Manages the Blue Prism API extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiPLRBluePrismApi
-"""
-class bppiPLRBluePrismApi(bppiRepository):
-    def __init__(self, config):
-        super().__init__(config)
-
-    @property
-    def mandatoryParameters(self) -> str:
-        return BP_MANDATORY_PARAM_LIST
-
-    def initialize(self) -> bool:
-        return super().initialize()
-
-    def transform(self, df) -> pd.DataFrame:
-        return super().transform(df)
-
-    def __buildAPIURL(self):
-        return self.config.getParameter(C.PARAM_BPAPI_API_URL, C.EMPTY) + C.PBAPI_VER
-
-    def __getSSLVerification(self):
-        return (self.config.getParameter(C.PARAM_BPAPI_SSL_VERIF, C.YES) == C.YES)
-
-    def __getPageSize(self):
-        return self.config.getParameter(C.PARAM_BPAPI_API_PAGESIZE, "10")
-
-    def __getAccessToken(self):
-        """ OAuth2 protocol usage with the Blue Prism API to get the access token
-        Returns:
-            str: Blue Prism API Access Token
-        """
-        try:
-            self.log.debug("BP API - Get the Blue Prism API access token")
-            # Blue Prism Hub/API, OAuth2 credentials
-            client_id = self.config.getParameter(C.PARAM_BPAPI_CLIENT_ID, C.EMPTY)
-            client_secret = self.config.getParameter(C.PARAM_BPAPI_SECRET, C.EMPTY)
-            token_url = self.config.getParameter(C.PARAM_BPAPI_AUTH_URL, C.EMPTY) + AUTH_TOKEN_SUFFIX_URL
-            # Obtain an access token using client credentials grant
-            token_params = {
-                "grant_type": "client_credentials",
-                "client_id": client_id,
-                "client_secret": client_secret,
-            }
-            token_response = requests.post(token_url, 
-                                           data=token_params, 
-                                           verify=self.__getSSLVerification())
-            token_data = token_response.json()
-            self.log.debug("BP API - Blue Prism Access Token has been returned successfully")
-            # The access token can be extracted from the response
-            return token_data["access_token"]
-        
-        except Exception as e:
-            self.log.error("bppiPLRBluePrismApi.__getAccessToken() -> Unable to get the Blue Prism API Access Token, " + str(e))
-            return None
-
-    def __getSessionIDList(self, access_token):
-        """ Get the list of Blue Prism Sessions,by using the access token for making authorized API requests
-        Args:
-            access_token (str): Blue Prism API Token access
-        Returns:
-            DataFrame: List of Session ID
-        """
-        try:
-            self.log.debug("BP API - Get the Blue Prism session list")
-            headers = {
-                "Authorization": "Bearer " + access_token,
-            }
-            api_endpoint = self.__buildAPIURL() + C.BPAPI_SESSIONS_LIST
-            params = { 'sessionParameters.processName.eq': self.config.getParameter(C.PARAM_BPPROCESSNAME) }
-            api_endpoint += "?" + urllib.parse.urlencode(params, quote_via=urllib.parse.quote)
-            api_response = requests.get(api_endpoint, 
-                                        headers=headers, 
-                                        verify=self.__getSSLVerification())
-            if (api_response.status_code == C.HTTP_API_OK):
-                df = pd.DataFrame.from_dict(api_response.json()["items"], orient='columns')
-                self.log.debug("BP API - {} sessions have been returned.".format(len(df)))
-                return df["sessionId"]
-            else:
-                self.log.error("bppiPLRBluePrismApi.__getSessionIDList() -> API Call error, {}".format((api_response.status_code)))
-                return pd.DataFrame()
-            
-        except Exception as e:
-            self.log.error("bppiPLRBluePrismApi.__getSessionIDList() -> Unable to get the Blue Prism session list, " + str(e))
-            return pd.DataFrame()
- 
-    def __getSessionDetails(self, access_token, sessionID):
-        """ Returns the global informations on a Blue Prism Session (header)
-        Args:
-            access_token (str): Blue Prism API Token access
-            sessionID (str): Blue Prism Session ID
-        Returns:
-            DataFrame: Session details
-        """
-        try:
-            self.log.debug("BP API - Get the Blue Prism session information (header)")
-            api_endpoint = (self.__buildAPIURL() + C.BPAPI_SESSION_HEAD).format(sessionID)
-            headers = {
-                "Authorization": "Bearer " + access_token,
-            }
-            api_response = requests.get(api_endpoint, 
-                                        headers=headers, 
-                                        verify=self.__getSSLVerification())
-            if (api_response.status_code == C.HTTP_API_OK):
-                return api_response.json()
-            else:
-                raise Exception("API Call error, {}".format((api_response.status_code)))
-            
-        except Exception as e:
-            self.log.error("bppiPLRBluePrismApi.__getSessionDetails() -> Unable to get the Blue Prism session global info, " + str(e))
-            return pd.DataFrame()
-        
-    def __getSessionLogs(self, access_token, sessionID):
-        """ Returns the all the sessions logs. The API works with pages (Max 1000 logs per page), so we've to loop into the returned pages.
-        Args:
-            access_token (str): Blue Prism API Token access
-            sessionID (str): Blue Prism Session ID
-        Returns:
-            DataFrame: Session logs
-        """
-        try:
-            self.log.debug("BP API - Get the Blue Prism session [{}] details".format(sessionID))
-            loop_on_page = True
-            all_logs = pd.DataFrame()
-            next_page_token = ""
-            iteration = 1
-            # The API returns logs per pages (Max 1000 logs per page)
-            while (loop_on_page):
-                self.log.debug("BP API - Get logs per page, iteration N{}".format(iteration))
-                # Build URL API Call
-                api_endpoint = (self.__buildAPIURL() + C.BPAPI_SESSION_LOGS).format(sessionID)
-                params = { 'sessionLogsParameters.itemsPerPage': self.__getPageSize() }
-                if (next_page_token != ""):
-                    params.update( { "sessionLogsParameters.pagingToken" : next_page_token })
-                api_endpoint += "?" + urllib.parse.urlencode(params, quote_via=urllib.parse.quote)
-
-                headers = {  "Authorization": "Bearer " + access_token }
-                api_response = requests.get(api_endpoint, 
-                                            headers=headers, 
-                                            verify=self.__getSSLVerification())
-                # Aggregate the logs (all pages)
-                if (api_response.status_code == C.HTTP_API_OK):
-                    df = pd.DataFrame.from_dict(api_response.json()["items"], orient='columns')
-                    all_logs = pd.concat([all_logs, df]) 
-                    next_page_token = api_response.json()["pagingToken"]
-                if (next_page_token == None or api_response.status_code != C.HTTP_API_OK):
-                    self.log.debug("__getSessionInfos() -> No more pages")
-                    loop_on_page = False
-                iteration += 1
-            self.log.debug("BP API - {} sessions details (steps/stages) have been returned.".format(len(all_logs)))
-            return all_logs
-        
-        except Exception as e:
-            self.log.error("bppiPLRBluePrismApi.__getSessionLogs() -> Unable to get the Blue Prism session [{}] details, {}".format(sessionID, str(e)))
-            return pd.DataFrame()
-
-    def __getSessionParameters(self, access_token, sessionID):
-        """ Returns the all the sessions parameters. 
-            *** In progress **
-        Args:
-            access_token (str): Blue Prism API Token access
-            sessionID (str): Blue Prism Session ID
-        Returns:
-            json: parameters
-        """
-        ssl_verification = self.config.getParameter(C.PARAM_BPAPI_SSL_VERIF, C.YES)
-        api_endpoint = (self.__buildAPIURL() + C.BPAPI_SESSION_PARAMS).format(sessionID)
-        headers = {
-            "Authorization": "Bearer " + access_token,
-        }
-        api_response = requests.get(api_endpoint, headers=headers, verify=ssl_verification)
-        return api_response.json()
-
-    def extract(self) -> pd.DataFrame: 
-        """Read the Excel file and build the dataframe
-        Returns:
-            pd.DataFrame: Dataframe with the source data
-        """
-        try:
-            access_token = self.__getAccessToken()
-            if (access_token != None):
-                sessionIDList = self.__getSessionIDList(access_token)
-                logs = pd.DataFrame()
-                # Aggregate the logs from all the sessions
-                for session in sessionIDList:
-                    self.log.debug("BP API - Collect logs from session {} ...".format(session))
-                    session_info = self.__getSessionDetails(access_token, session)
-                    session_logs = self.__getSessionLogs(access_token, session)
-                    # Add Session log data
-                    session_logs["ResourceName"] = session_info['resourceName']
-                    session_logs["status"] = session_info['status']
-                    session_logs["SessionID"] = session
-                    logs = pd.concat([logs, session_logs]) 
-                    self.log.debug("BP API - session {} logs collected successfully, Total: {} rows/logs".format(session, logs.shape[0]))
-            return logs
-        
-        except Exception as e:
-            self.log.error("Extract() Error -> " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.bppi.repository.bppiRepository import bppiRepository
+import pandas as pd
+from pipelines.readers.bpAPIReader import bpAPIReader
+
+BP_MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
+                           C.PARAM_BPPIURL, 
+                           C.PARAM_BPPROCESSNAME,
+                           C.PARAM_BPAPI_CLIENT_ID,
+                           C.PARAM_BPAPI_SECRET,
+                           C.PARAM_BPAPI_AUTH_URL]
+
+""" Manages the Blue Prism API extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiPLRBluePrismApi
+"""
+class bppiPLRBluePrismApi(bppiRepository):
+    @property
+    def mandatoryParameters(self) -> str:
+        return BP_MANDATORY_PARAM_LIST
+
+    def extract(self) -> pd.DataFrame: 
+        """Read the Excel file and build the dataframe
+        Returns:
+            pd.DataFrame: Dataframe with the source data
+        """
+        try:
+            api = bpAPIReader(self.log)
+            api.setConnectionParams(bpProcessName=self.config.getParameter(C.PARAM_BPPROCESSNAME),
+                                    clientID=self.config.getParameter(C.PARAM_BPAPI_CLIENT_ID, C.EMPTY),
+                                    pageSize=self.config.getParameter(C.PARAM_BPAPI_API_PAGESIZE, "10"),
+                                    sslCheck=self.config.getParameter(C.PARAM_BPAPI_SSL_VERIF, C.YES),
+                                    secret=self.config.getParameter(C.PARAM_BPAPI_SECRET, C.EMPTY),
+                                    urlApi=self.config.getParameter(C.PARAM_BPAPI_API_URL, C.EMPTY),
+                                    urlAuth=self.config.getParameter(C.PARAM_BPAPI_AUTH_URL, C.EMPTY))
+            if (not api.read()):
+                raise Exception("Error while accessing the Blue Prism API")
+            return api.content
+        
+        except Exception as e:
+            self.log.error("Extract() Error -> " + str(e))
             return super().extract()
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## pipelines/repository/bppiPLRBluePrismRepo.py

```diff
@@ -1,222 +1,222 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.repository.bppiPLRODBC import bppiPLRODBC
-import pandas as pd
-import xml.etree.ElementTree as ET
-import warnings
-import numpy as np
-from pipelines.readers.builders.blueprismSQLBuilder import blueprismSQLBuilder
-import datetime
-
-warnings.filterwarnings('ignore')
-CANCEL_SQL_FILTER = "1=1"
-BP_MANDATORY_PARAM_LIST = [C.PARAM_CONNECTIONSTRING, 
-                           C.PARAM_BPPITOKEN, 
-                           C.PARAM_BPPIURL, 
-                           C.PARAM_BPPROCESSNAME]
-
-""" Manages the Blue Prism Repository extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiPLRCSVFile
-                - pipelines.repository.bppiPLRODBC
-                    - pipelines.repository.bppiPLRBluePrismRepo
-"""
-class bppiPLRBluePrismRepo(bppiPLRODBC):
-    def __init__(self, config):
-        super().__init__(config)
-
-    @property
-    def mandatoryParameters(self) -> str:
-        return BP_MANDATORY_PARAM_LIST
-    @property
-    def query(self) -> str:
-        return self.__buildQuery()
-    
-    def initialize(self) -> bool:
-        return super().initialize()
-
-    def __getDeltaTag(self):
-        """ Get the last load date to use for the delta loading (when requested)
-        Returns:
-            _type_: date in straing format
-        """
-        if (self.config.getParameter(C.PARAM_BPDELTA, C.NO) == C.YES):
-            filedelta = self.config.getParameter(C.PARAM_BPDELTA_FILE, C.BP_DEFAULT_DELTAFILE)
-            try:
-                with open(filedelta, "r") as file:
-                    fromdate = file.read()
-                return fromdate
-            except:
-                self.log.error("__getDeltaLoadLastDate() -> Unable to read/get the tagged delta date")
-                return C.EMPTY
-        else:
-            return C.EMPTY
-
-    def __updDeltaTag(self):
-        """ Update the date for the next delta load
-        """
-        if (self.config.getParameter(C.PARAM_BPDELTA, C.NO) == C.YES):
-            try:
-                filedelta = self.config.getParameter(C.PARAM_BPDELTA_FILE, C.BP_DEFAULT_DELTAFILE)
-                with open(filedelta, "w") as file: # store in the delta file the latest delta load 
-                    file.write(datetime.datetime.now().strftime(C.BP_DELTADATE_FMT))
-            except:
-                self.log.error("__updDeltaLoadLastDate() -> Unable to write the tagged new delta date")
-
-    def __buildQuery(self) -> str:
-        """Build the SQL Query to get the BP logs against the BP repository
-            The BP Logs SQL qeury is stored in the bp.config file and can be customized with several args:
-                * {attrxml}: Name of the INPUT/OUTPUT attributes columns (XML format)
-                * {processname}: Process Name in Blue Prism
-                * {stagetypefilter}: list of stage to filter out
-                * {delta}: Delta loading condition on datetime (Between or < >)
-                * {tablelog}: Name of the Log table (unicode or not unicode)
-        Returns:
-            str: built SQL Query
-        """
-        try: 
-            # Get the last delta load if needed:
-            lastDeltaDate = self.__getDeltaTag()
-            # Build the Query
-            sqlBuilder = blueprismSQLBuilder(self.log, self.config)
-            sqlBuilder.deltaDate = lastDeltaDate
-            sql = sqlBuilder.build()
-            # Update the date for the next delta load
-            self.__updDeltaTag()
-            return sql
-        except Exception as e:
-            self.log.error("__buildQuery() -> Unable to build the Blue Prism Query " + str(e))
-            return C.EMPTY
-        
-    def __parseAttrs(self, logid, attribute, dfattributes) -> pd.DataFrame:
-        """ Parse the attributexml field and extract (only) the text data (not the collection)
-        Args:
-            logid (str): ID of the log line (for later merge)
-            attribute (str): attributexml value (XML format)
-            dfattributes (DataFrame): Dataframe with tne incremental parameters added into
-
-        Returns:
-            pd.DataFrame: _description_
-        """
-        try:
-            #    Blue Prism Log Format expected:
-            #    <parameters>
-            #        <inputs>
-            #            <input name="Nom" type="text" value="Benoit Cayla" />
-            #            ...
-            #        </inputs>
-            #        <outputs>
-            #            <output name="Contact Form" type="flag" value="True" />
-            #            ...
-            #        </outputs>
-            #    </parameters>
-            root = ET.fromstring(attribute)
-            if (root.tag == "parameters"):
-                for input in root.findall("./inputs/input"):
-                    if (input.attrib["type"] == "text"):    # only get the text input parameters
-                        df_new_row = pd.DataFrame.from_records({'logid': logid, 
-                                                                'Name' : input.attrib["name"], 
-                                                                'value' :input.attrib["value"], 
-                                                                'in_out' : 'I'}, index=[0])
-                        dfattributes = pd.concat([dfattributes, df_new_row])
-                for output in root.findall("./outputs/output"):
-                    if (output.attrib["type"] == "text"):    # only get the text output parameters
-                        df_new_row = pd.DataFrame.from_records({'logid': logid, 
-                                                                'Name' : output.attrib["name"], 
-                                                                'value' :output.attrib["value"], 
-                                                                'in_out' : 'O'}, index=[0])
-                        dfattributes = pd.concat([dfattributes, df_new_row]) 
-            return dfattributes
-        except Exception as e:
-            self.log.error("__parseAttrs() -> Unable to parse the BP Attribute " + str(e))
-            return dfattributes
-
-    def __getAttributesFromLogs(self, df) -> pd.DataFrame:
-        """Extract the logs (especially the parameters from the logs which are stored in XML format)
-            Note: if no parameters in the list, no import
-        Args:
-            df (Dataframe): Dataframe with the logs
-            config (bppiapi.appConfig): list of parameters from the INI file
-        Returns:
-            DataFrame: logs altered with parameters
-        """
-        try:
-            parameters = self.config.getParameter(C.PARAM_BPPARAMSATTR, C.EMPTY)
-            # Manage the IN/OUT parameters from the logs
-            if (len(parameters) > 0):
-                # Extract the input and output parameters
-                self.log.info("Extract the input and output parameters")
-                dfattributes = pd.DataFrame(columns= ["logid", "Name", "value", "in_out"])
-                for index, row in df.iterrows():
-                    if (row[C.BPLOG_ATTRIBUTE_COL] != None):
-                        dfattributes = self.__parseAttrs(row["logid"], row[C.BPLOG_ATTRIBUTE_COL], dfattributes)
-                self.log.debug("Number of attributes found: {}".format(str(dfattributes.shape[0])))
-                # Only keep the desired parameters
-                self.log.debug("Filter out the desired parameters")
-                # Build the filter with the parameters list
-                params = [ "\"" + x + "\"" for x in parameters.split(",") ]
-                paramQuery = "Name in (" + ",".join(params) + ")"
-                dfattributes = dfattributes.query(paramQuery)
-                self.log.debug("Number of attributes found: {}".format(str(dfattributes.shape[0])))
-                # Pivot the parameter values to create one new column per parameter
-                self.log.info("Build the final dataset with the desired parameters")
-                # add the IN or OUT parameter (the commented line below creates 2 differents parameters if the same param for IN and OUT)
-                dfattributes['FullName'] = dfattributes['Name']
-                dfattributesInCols = pd.pivot_table(dfattributes, values='value', index=['logid'], columns=['FullName'], aggfunc=np.sum, fill_value="")
-                dfattributesInCols.reset_index()
-                # Merge the Dataframes
-                dffinal = df.merge(dfattributesInCols, on="logid", how='left')
-                dffinal = dffinal.drop(C.BPLOG_ATTRIBUTE_COL, axis=1)
-                return dffinal
-            else:
-                self.log.info("No parameters required in the configuration file")
-                return df
-            
-        except Exception as e:
-            self.log.error("__getAttributesFromLogs() -> Unable to get attributes from the Blue Prism logs " + str(e))
-            return df
-        
-    def transform(self, df) -> pd.DataFrame:
-        """Alter the collected data (from the BP Repository) by managing the attributes (stored in a XML format)
-        Args:
-            df (pd.DataFrame): Data source
-        Returns:
-            pd.DataFrame: Altered dataset with the selected parameters as new columns
-        """
-
-        try:
-            # Filter out the df by selecting only the Start & End (main page / process) stages if requested
-            if (self.config.getParameter(C.PARAM_BPFILTERSTEND) == C.YES):
-                mainpage = self.config.getParameter(C.PARAM_BPMAINPROCESSPAGE, C.BP_MAINPAGE_DEFAULT) 
-                # Remove the logs with stagename = "End" outside the "Main Page"
-                oldCount = df.shape[0]
-                df = df[~((df[C.BPLOG_STAGENAME_COL] == C.BP_STAGE_END) & (df[C.BPLOG_PAGENAME_COL] != mainpage))]
-                self.log.warning("{} records have been removed (No <End> stage outside the Main Process Page)".format(oldCount - df.shape[0]))
-                # Remove the logs with stagename = "Start" outside the "Main Page"
-                oldCount = df.shape[0] 
-                df = df[~((df[C.BPLOG_STAGENAME_COL] == C.BP_STAGE_START) & (df[C.BPLOG_PAGENAME_COL] != mainpage))]
-                self.log.warning("{} records have been removed (No <Start> stage outside the Main Process Page)".format(oldCount - df.shape[0]))
-            
-            # Get the attributes from the BP logs
-            df = self.__getAttributesFromLogs(df)
-
-            # Create a new col OBJECT_TAB with the page name or the VBO action
-            df[C.COL_OBJECT_TAB] = df.apply(lambda row: row["pagename"] if row["pagename"] != None else row["actionname"], axis=1)
-            # Create the unique stage Identifier: STAGE_ID: STAGE_ID format: {VBO|PROC}/{Process or Object Name}/{Process Page or VBO Action}/{Stage name}
-            df[C.COL_STAGE_ID] = df[['OBJECT_TYPE', 'OBJECT_NAME', C.COL_OBJECT_TAB, 'stagename']].agg('/'.join, axis=1)
-            # Change the event to map by default if not filled out (surcharge the events.eventcolumn INI parameter)
-            if (self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.EMPTY) == C.EMPTY):
-                self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.COL_STAGE_ID)
-
-            # Filter and/or update the event names if needed/configured
-            df = super().transform(df)
-            return df
-        
-        except Exception as e:
-            self.log.error("transform() -> Unable to update the data " + str(e))
-            return super().transform(df)
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.repository.bppiPLRODBC import bppiPLRODBC
+import pandas as pd
+import xml.etree.ElementTree as ET
+import warnings
+import numpy as np
+from pipelines.readers.builders.blueprismSQLBuilder import blueprismSQLBuilder
+import datetime
+
+warnings.filterwarnings('ignore')
+CANCEL_SQL_FILTER = "1=1"
+BP_MANDATORY_PARAM_LIST = [C.PARAM_CONNECTIONSTRING, 
+                           C.PARAM_BPPITOKEN, 
+                           C.PARAM_BPPIURL, 
+                           C.PARAM_BPPROCESSNAME]
+
+""" Manages the Blue Prism Repository extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiPLRCSVFile
+                - pipelines.repository.bppiPLRODBC
+                    - pipelines.repository.bppiPLRBluePrismRepo
+"""
+class bppiPLRBluePrismRepo(bppiPLRODBC):
+    @property
+    def mandatoryParameters(self) -> str:
+        return BP_MANDATORY_PARAM_LIST
+
+    def __getDeltaTag(self):
+        """ Get the last load date to use for the delta loading (when requested)
+        Returns:
+            _type_: date in straing format
+        """
+        if (self.config.getParameter(C.PARAM_BPDELTA, C.NO) == C.YES):
+            filedelta = self.config.getParameter(C.PARAM_BPDELTA_FILE, C.BP_DEFAULT_DELTAFILE)
+            try:
+                with open(filedelta, "r") as file:
+                    fromdate = file.read()
+                return fromdate
+            except:
+                self.log.error("__getDeltaLoadLastDate() -> Unable to read/get the tagged delta date")
+                return C.EMPTY
+        else:
+            return C.EMPTY
+
+    def __updDeltaTag(self):
+        """ Update the date for the next delta load
+        """
+        if (self.config.getParameter(C.PARAM_BPDELTA, C.NO) == C.YES):
+            try:
+                filedelta = self.config.getParameter(C.PARAM_BPDELTA_FILE, C.BP_DEFAULT_DELTAFILE)
+                with open(filedelta, "w") as file: # store in the delta file the latest delta load 
+                    file.write(datetime.datetime.now().strftime(C.BP_DELTADATE_FMT))
+            except:
+                self.log.error("__updDeltaLoadLastDate() -> Unable to write the tagged new delta date")
+
+    @property
+    def query(self) -> str:
+        """Build the SQL Query to get the BP logs against the BP repository
+            The BP Logs SQL qeury is stored in the bp.config file and can be customized with several args:
+                * {attrxml}: Name of the INPUT/OUTPUT attributes columns (XML format)
+                * {processname}: Process Name in Blue Prism
+                * {stagetypefilter}: list of stage to filter out
+                * {delta}: Delta loading condition on datetime (Between or < >)
+                * {tablelog}: Name of the Log table (unicode or not unicode)
+        Returns:
+            str: built SQL Query
+        """
+        try: 
+            # Get the last delta load if needed:
+            lastDeltaDate = self.__getDeltaTag()
+            # Build the Query
+            sqlBuilder = blueprismSQLBuilder(log=self.log,
+                                            query=self.config.getParameter(C.PARAM_QUERY),
+                                            configtype=self.config.getParameter(C.CONFIG_SOURCE_NAME, C.CONFIG_SOURCE_INI))
+            sqlBuilder.setConnectionParams(bpStageTypes=self.config.getParameter(C.PARAM_BPSTAGETYPES, "0"),
+                                           processName=self.config.getParameter(C.PARAM_BPPROCESSNAME),
+                                           includeVBO=self.config.getParameter(C.PARAM_BPINCLUDEVBO, C.YES),
+                                           unicode=self.config.getParameter(C.PARAM_BPUNICODE),
+                                           fromDate=self.config.getParameter(C.PARAM_FROMDATE),
+                                           toDate=self.config.getParameter(C.PARAM_TODATE),
+                                           deltaDate=lastDeltaDate)
+            sql = sqlBuilder.build()
+            # Update the date for the next delta load
+            self.__updDeltaTag()
+            return sql
+        except Exception as e:
+            self.log.error("__buildQuery() -> Unable to build the Blue Prism Query " + str(e))
+            return C.EMPTY
+        
+    def __parseAttrs(self, logid, attribute, dfattributes) -> pd.DataFrame:
+        """ Parse the attributexml field and extract (only) the text data (not the collection)
+        Args:
+            logid (str): ID of the log line (for later merge)
+            attribute (str): attributexml value (XML format)
+            dfattributes (DataFrame): Dataframe with tne incremental parameters added into
+
+        Returns:
+            pd.DataFrame: _description_
+        """
+        try:
+            #    Blue Prism Log Format expected:
+            #    <parameters>
+            #        <inputs>
+            #            <input name="Nom" type="text" value="Benoit Cayla" />
+            #            ...
+            #        </inputs>
+            #        <outputs>
+            #            <output name="Contact Form" type="flag" value="True" />
+            #            ...
+            #        </outputs>
+            #    </parameters>
+            root = ET.fromstring(attribute)
+            if (root.tag == "parameters"):
+                for input in root.findall("./inputs/input"):
+                    if (input.attrib["type"] == "text"):    # only get the text input parameters
+                        df_new_row = pd.DataFrame.from_records({'logid': logid, 
+                                                                'Name' : input.attrib["name"], 
+                                                                'value' :input.attrib["value"], 
+                                                                'in_out' : 'I'}, index=[0])
+                        dfattributes = pd.concat([dfattributes, df_new_row])
+                for output in root.findall("./outputs/output"):
+                    if (output.attrib["type"] == "text"):    # only get the text output parameters
+                        df_new_row = pd.DataFrame.from_records({'logid': logid, 
+                                                                'Name' : output.attrib["name"], 
+                                                                'value' :output.attrib["value"], 
+                                                                'in_out' : 'O'}, index=[0])
+                        dfattributes = pd.concat([dfattributes, df_new_row]) 
+            return dfattributes
+        except Exception as e:
+            self.log.error("__parseAttrs() -> Unable to parse the BP Attribute " + str(e))
+            return dfattributes
+
+    def __getAttributesFromLogs(self, df) -> pd.DataFrame:
+        """Extract the logs (especially the parameters from the logs which are stored in XML format)
+            Note: if no parameters in the list, no import
+        Args:
+            df (Dataframe): Dataframe with the logs
+            config (bppiapi.appConfig): list of parameters from the INI file
+        Returns:
+            DataFrame: logs altered with parameters
+        """
+        try:
+            parameters = self.config.getParameter(C.PARAM_BPPARAMSATTR, C.EMPTY)
+            # Manage the IN/OUT parameters from the logs
+            if (len(parameters) > 0):
+                # Extract the input and output parameters
+                self.log.info("Extract the input and output parameters")
+                dfattributes = pd.DataFrame(columns= ["logid", "Name", "value", "in_out"])
+                for index, row in df.iterrows():
+                    if (row[C.BPLOG_ATTRIBUTE_COL] != None):
+                        dfattributes = self.__parseAttrs(row["logid"], row[C.BPLOG_ATTRIBUTE_COL], dfattributes)
+                self.log.debug("Number of attributes found: {}".format(str(dfattributes.shape[0])))
+                # Only keep the desired parameters
+                self.log.debug("Filter out the desired parameters")
+                # Build the filter with the parameters list
+                params = [ "\"" + x + "\"" for x in parameters.split(",") ]
+                paramQuery = "Name in (" + ",".join(params) + ")"
+                dfattributes = dfattributes.query(paramQuery)
+                self.log.debug("Number of attributes found: {}".format(str(dfattributes.shape[0])))
+                # Pivot the parameter values to create one new column per parameter
+                self.log.info("Build the final dataset with the desired parameters")
+                # add the IN or OUT parameter (the commented line below creates 2 differents parameters if the same param for IN and OUT)
+                dfattributes['FullName'] = dfattributes['Name']
+                dfattributesInCols = pd.pivot_table(dfattributes, values='value', index=['logid'], columns=['FullName'], aggfunc=np.sum, fill_value="")
+                dfattributesInCols.reset_index()
+                # Merge the Dataframes
+                dffinal = df.merge(dfattributesInCols, on="logid", how='left')
+                dffinal = dffinal.drop(C.BPLOG_ATTRIBUTE_COL, axis=1)
+                return dffinal
+            else:
+                self.log.info("No parameters required in the configuration file")
+                return df
+            
+        except Exception as e:
+            self.log.error("__getAttributesFromLogs() -> Unable to get attributes from the Blue Prism logs " + str(e))
+            return df
+    
+    def transform(self, df) -> pd.DataFrame:
+        """Alter the collected data (from the BP Repository) by managing the attributes (stored in a XML format)
+        Args:
+            df (pd.DataFrame): Data source
+        Returns:
+            pd.DataFrame: Altered dataset with the selected parameters as new columns
+        """
+
+        try:
+            # Filter out the df by selecting only the Start & End (main page / process) stages if requested
+            if (self.config.getParameter(C.PARAM_BPFILTERSTEND) == C.YES):
+                mainpage = self.config.getParameter(C.PARAM_BPMAINPROCESSPAGE, C.BP_MAINPAGE_DEFAULT) 
+                # Remove the logs with stagename = "End" outside the "Main Page"
+                oldCount = df.shape[0]
+                df = df[~((df[C.BPLOG_STAGENAME_COL] == C.BP_STAGE_END) & (df[C.BPLOG_PAGENAME_COL] != mainpage))]
+                self.log.warning("{} records have been removed (No <End> stage outside the Main Process Page)".format(oldCount - df.shape[0]))
+                # Remove the logs with stagename = "Start" outside the "Main Page"
+                oldCount = df.shape[0] 
+                df = df[~((df[C.BPLOG_STAGENAME_COL] == C.BP_STAGE_START) & (df[C.BPLOG_PAGENAME_COL] != mainpage))]
+                self.log.warning("{} records have been removed (No <Start> stage outside the Main Process Page)".format(oldCount - df.shape[0]))
+            
+            # Get the attributes from the BP logs
+            df = self.__getAttributesFromLogs(df)
+
+            # Create a new col OBJECT_TAB with the page name or the VBO action
+            df[C.COL_OBJECT_TAB] = df.apply(lambda row: row["pagename"] if row["pagename"] != None else row["actionname"], axis=1)
+            # Create the unique stage Identifier: STAGE_ID: STAGE_ID format: {VBO|PROC}/{Process or Object Name}/{Process Page or VBO Action}/{Stage name}
+            df[C.COL_STAGE_ID] = df[['OBJECT_TYPE', 'OBJECT_NAME', C.COL_OBJECT_TAB, 'stagename']].agg('/'.join, axis=1)
+            # Change the event to map by default if not filled out (surcharge the events.eventcolumn INI parameter)
+            if (self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.EMPTY) == C.EMPTY):
+                self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.COL_STAGE_ID)
+
+            # Filter and/or update the event names if needed/configured
+            df = super().transform(df)
+            return df
+        
+        except Exception as e:
+            self.log.error("transform() -> Unable to update the data " + str(e))
+            return super().transform(df)
```

## pipelines/repository/bppiPLRCSVFile.py

```diff
@@ -1,43 +1,40 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.bppi.repository.bppiRepository import bppiRepository
-import pandas as pd
-from pipelines.readers.csvFileReader import csvFileReader
-
-CSV_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME]
-
-""" Manages the CSV file extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiPLRCSVFile
-"""
-class bppiPLRCSVFile(bppiRepository):
-
-    def __init__(self, config):
-        super().__init__(config)
-
-    @property
-    def mandatoryParameters(self) -> str:
-        return CSV_MANDATORY_PARAM_LIST
-
-    def extract(self) -> pd.DataFrame: 
-        """Read the CSV file and build the dataframe
-        Returns:
-            pd.DataFrame: Dataframe with the source data
-        """
-        try:
-            csv = csvFileReader(self.log)
-            csv.filename = self.config.getParameter(C.PARAM_FILENAME)
-            csv.separator = self.config.getParameter(C.PARAM_CSV_SEPARATOR, C.DEFCSVSEP)
-            if (not csv.read()):
-                raise Exception("Error while reading the CSV file")
-            return csv.content
-        
-        except Exception as e:
-            self.log.error("extract() Error" + str(e))
-            return super().extract()
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.bppi.repository.bppiRepository import bppiRepository
+import pandas as pd
+from pipelines.readers.csvFileReader import csvFileReader
+
+CSV_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME]
+
+""" Manages the CSV file extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiPLRCSVFile
+"""
+class bppiPLRCSVFile(bppiRepository):
+
+    @property
+    def mandatoryParameters(self) -> str:
+        return CSV_MANDATORY_PARAM_LIST
+
+    def extract(self) -> pd.DataFrame: 
+        """Read the CSV file and build the dataframe
+        Returns:
+            pd.DataFrame: Dataframe with the source data
+        """
+        try:
+            csv = csvFileReader(self.log)
+            csv.filename = self.config.getParameter(C.PARAM_FILENAME)
+            csv.separator = self.config.getParameter(C.PARAM_CSV_SEPARATOR, C.DEFCSVSEP)
+            if (not csv.read()):
+                raise Exception("Error while reading the CSV file")
+            return csv.content
+        
+        except Exception as e:
+            self.log.error("extract() Error" + str(e))
+            return super().extract()
```

## pipelines/repository/bppiPLRChorusExtract.py

```diff
@@ -1,27 +1,24 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.repository.bppiPLRCSVFile import bppiPLRCSVFile
-import pandas as pd
-
-CHORUSFILE_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME, 
-                                    C.PARAM_BPPITOKEN, 
-                                    C.PARAM_BPPIURL]
-
-""" Manages the Chorus by file extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiPLRCSVFile
-                - pipelines.repository.bppiPLRChorusExtract
-"""
-class bppiPLRChorusExtract(bppiPLRCSVFile):
-
-    def __init__(self, config):
-        super().__init__(config)
-
-    @property
-    def mandatoryParameters(self) -> str:
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.repository.bppiPLRCSVFile import bppiPLRCSVFile
+import pandas as pd
+
+CHORUSFILE_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME, 
+                                    C.PARAM_BPPITOKEN, 
+                                    C.PARAM_BPPIURL]
+
+""" Manages the Chorus by file extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiPLRCSVFile
+                - pipelines.repository.bppiPLRChorusExtract
+"""
+class bppiPLRChorusExtract(bppiPLRCSVFile):
+
+    @property
+    def mandatoryParameters(self) -> str:
         return CHORUSFILE_MANDATORY_PARAM_LIST
```

## pipelines/repository/bppiPLRExcelFile.py

```diff
@@ -1,45 +1,42 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.bppi.repository.bppiRepository import bppiRepository
-import pandas as pd
-from pipelines.readers.excelFileReader import excelFileReader
-
-EXCEL_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME, 
-                              C.PARAM_BPPITOKEN, 
-                              C.PARAM_BPPIURL]
-
-""" Manages the Blue Prism Repository extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiPLRExcelFile
-"""
-class bppiPLRExcelFile(bppiRepository):
-
-    def __init__(self, config):
-        super().__init__(config)
-
-    @property
-    def mandatoryParameters(self) -> str:
-        return EXCEL_MANDATORY_PARAM_LIST
-
-    def extract(self) -> pd.DataFrame: 
-        """Read the Excel file and build the dataframe
-        Returns:
-            pd.DataFrame: Dataframe with the source data
-        """
-        try:
-            excel = excelFileReader(self.log)
-            excel.filename = self.config.getParameter(C.PARAM_FILENAME)
-            excel.sheet = self.config.getParameter(C.PARAM_EXCELSHEETNAME)
-            if (not excel.read()):
-                raise Exception("Error while reading the Excel file")
-            return excel.content
-        
-        except Exception as e:
-            self.log.error("extract() Error -> " + str(e))
-            return super().extract()
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.bppi.repository.bppiRepository import bppiRepository
+import pandas as pd
+from pipelines.readers.excelFileReader import excelFileReader
+
+EXCEL_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME, 
+                              C.PARAM_BPPITOKEN, 
+                              C.PARAM_BPPIURL]
+
+""" Manages the Blue Prism Repository extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiPLRExcelFile
+"""
+class bppiPLRExcelFile(bppiRepository):
+
+    @property
+    def mandatoryParameters(self) -> str:
+        return EXCEL_MANDATORY_PARAM_LIST
+
+    def extract(self) -> pd.DataFrame: 
+        """Read the Excel file and build the dataframe
+        Returns:
+            pd.DataFrame: Dataframe with the source data
+        """
+        try:
+            excel = excelFileReader(self.log)
+            excel.filename = self.config.getParameter(C.PARAM_FILENAME)
+            excel.sheet = self.config.getParameter(C.PARAM_EXCELSHEETNAME)
+            if (not excel.read()):
+                raise Exception("Error while reading the Excel file")
+            return excel.content
+        
+        except Exception as e:
+            self.log.error("extract() Error -> " + str(e))
+            return super().extract()
```

## pipelines/repository/bppiPLRODBC.py

```diff
@@ -1,49 +1,50 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.repository.bppiPLRCSVFile import bppiPLRCSVFile
-from pipelines.readers.builders.SQLBuilder import SQLBuilder
-import pandas as pd
-from pipelines.readers.odbcReader import odbcReader
-
-# Mandatory params to check
-ODBC_MANDATORY_PARAM_LIST = [C.PARAM_CONNECTIONSTRING, 
-                             C.PARAM_BPPITOKEN, 
-                             C.PARAM_BPPIURL, 
-                             C.PARAM_QUERY]
-
-""" Manages the Blue Prism Repository extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiPLRCSVFile
-                - pipelines.repository.bppiPLRODBC
-"""
-class bppiPLRODBC(bppiPLRCSVFile):
-    @property
-    def mandatoryParameters(self) -> str:
-        return ODBC_MANDATORY_PARAM_LIST
-
-    @property
-    def query(self) -> str:
-        return SQLBuilder(self.log, self.config).build()
-    
-    def extract(self) -> pd.DataFrame: 
-        """Read the DB by executing the query and build the dataframe
-        Returns:
-            pd.DataFrame: Dataframe with the source data
-        """
-        tableResult = pd.DataFrame()
-        try:
-            odbc = self.config.getParameter(C.PARAM_CONNECTIONSTRING)
-            reader = odbcReader(self.log)
-            reader.setConnectionParams(odbc, self.query)
-            if (not reader.read()):
-                raise Exception("Error while connecting/reading the ODBC Data Source")
-            return reader.content
-
-        except Exception as e:
-            self.log.error("extract() Error -> " + str(e))
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.repository.bppiPLRCSVFile import bppiPLRCSVFile
+from pipelines.readers.builders.SQLBuilder import SQLBuilder
+import pandas as pd
+from pipelines.readers.odbcReader import odbcReader
+
+# Mandatory params to check
+ODBC_MANDATORY_PARAM_LIST = [C.PARAM_CONNECTIONSTRING, 
+                             C.PARAM_BPPITOKEN, 
+                             C.PARAM_BPPIURL, 
+                             C.PARAM_QUERY]
+
+""" Manages the Blue Prism Repository extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiPLRCSVFile
+                - pipelines.repository.bppiPLRODBC
+"""
+class bppiPLRODBC(bppiPLRCSVFile):
+    @property
+    def mandatoryParameters(self) -> str:
+        return ODBC_MANDATORY_PARAM_LIST
+
+    @property
+    def query(self) -> str:
+        return SQLBuilder(self.log, 
+                          self.config.getParameter(C.PARAM_QUERY),
+                          self.config.getParameter(C.CONFIG_SOURCE_NAME, C.CONFIG_SOURCE_INI)).build()
+    
+    def extract(self) -> pd.DataFrame: 
+        """Read the DB by executing the query and build the dataframe
+        Returns:
+            pd.DataFrame: Dataframe with the source data
+        """
+        try:
+            odbc = self.config.getParameter(C.PARAM_CONNECTIONSTRING)
+            reader = odbcReader(self.log)
+            reader.setConnectionParams(odbc, self.query)
+            if (not reader.read()):
+                raise Exception("Error while connecting/reading the ODBC Data Source")
+            return reader.content
+
+        except Exception as e:
+            self.log.error("extract() Error -> " + str(e))
             return super().extract()
```

## pipelines/repository/bppiPLRSAPRfcTable.py

```diff
@@ -1,68 +1,59 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.bppi.repository.bppiRepository import bppiRepository
-import pandas as pd
-from pipelines.readers.sapRFCTableReader import sapRFCTableReader
-
-"""
-    SE37 check in SAP
-    RFC_READ_TABLE (function module)
-
-"""
-SAP_MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
-                            C.PARAM_BPPIURL,
-                            C.PARAM_SAP_ASHOST,
-                            C.PARAM_SAP_CLIENT,
-                            C.PARAM_SAP_SYSNR,
-                            C.PARAM_SAP_USER, 
-                            C.PARAM_SAP_PASSWD,
-                            C.PARAM_SAP_RFC_TABLE]
-
-""" Manages the Blue Prism Repository extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiRepository
-                - pipelines.repository.bppiPLRSAPRfcTable
-"""
-class bppiPLRSAPRfcTable(bppiRepository):
-    @property
-    def mandatoryParameters(self) -> str:
-        return SAP_MANDATORY_PARAM_LIST
-
-    def extract(self) -> pd.DataFrame: 
-        """Read the SAP Table file and build the dataframe
-        Returns:
-            pd.DataFrame: Dataframe with the source data
-        """
-        try:
-            ASHOST = self.config.getParameter(C.PARAM_SAP_ASHOST, C.EMPTY) 
-            CLIENT = self.config.getParameter(C.PARAM_SAP_CLIENT, C.EMPTY) 
-            SYSNR = self.config.getParameter(C.PARAM_SAP_SYSNR, C.EMPTY)
-            USER = self.config.getParameter(C.PARAM_SAP_USER, C.EMPTY) 
-            PASSWD = self.config.getParameter(C.PARAM_SAP_PASSWD, C.EMPTY)
-            SAPROUTER = self.config.getParameter(C.PARAM_SAP_ROUTER, C.EMPTY)
-            field_names = self.config.getParameter(C.PARAM_SAP_RFC_FIELDS, C.EMPTY).split(',')
-            table_name = self.config.getParameter(C.PARAM_SAP_RFC_TABLE)
-            row_limit = int(self.config.getParameter(C.PARAM_SAP_RFC_ROWCOUNT, "0"))
-            sap = sapRFCTableReader(self.log)
-            sap.setConnectionParams(ahost=ASHOST, 
-                                    client=CLIENT, 
-                                    sysnr=SYSNR, 
-                                    user=USER, 
-                                    pwd=PASSWD, 
-                                    router=SAPROUTER)
-            sap.setImportParameters(rfcfields=field_names,
-                                    rfctable=table_name,
-                                    rowcount=row_limit)
-            if (not sap.read()):
-                raise Exception("Error while reading the XES file")
-            return sap.content
-
-        except Exception as e:
-            self.log.error("extract() Error -> " + str(e))
-            return super().extract()
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.bppi.repository.bppiRepository import bppiRepository
+import pandas as pd
+from pipelines.readers.sapRFCTableReader import sapRFCTableReader
+
+"""
+    SE37 check in SAP
+    RFC_READ_TABLE (function module)
+
+"""
+SAP_MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
+                            C.PARAM_BPPIURL,
+                            C.PARAM_SAP_ASHOST,
+                            C.PARAM_SAP_CLIENT,
+                            C.PARAM_SAP_SYSNR,
+                            C.PARAM_SAP_USER, 
+                            C.PARAM_SAP_PASSWD,
+                            C.PARAM_SAP_RFC_TABLE]
+
+""" Manages the Blue Prism Repository extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiRepository
+                - pipelines.repository.bppiPLRSAPRfcTable
+"""
+class bppiPLRSAPRfcTable(bppiRepository):
+    @property
+    def mandatoryParameters(self) -> str:
+        return SAP_MANDATORY_PARAM_LIST
+
+    def extract(self) -> pd.DataFrame: 
+        """Read the SAP Table file and build the dataframe
+        Returns:
+            pd.DataFrame: Dataframe with the source data
+        """
+        try:
+            sap = sapRFCTableReader(self.log)
+            sap.setConnectionParams(ahost=self.config.getParameter(C.PARAM_SAP_ASHOST, C.EMPTY), 
+                                    client=self.config.getParameter(C.PARAM_SAP_CLIENT, C.EMPTY), 
+                                    sysnr=self.config.getParameter(C.PARAM_SAP_SYSNR, C.EMPTY), 
+                                    user=self.config.getParameter(C.PARAM_SAP_USER, C.EMPTY), 
+                                    pwd=self.config.getParameter(C.PARAM_SAP_PASSWD, C.EMPTY), 
+                                    router=self.config.getParameter(C.PARAM_SAP_ROUTER, C.EMPTY))
+            sap.setImportParameters(rfcfields=self.config.getParameter(C.PARAM_SAP_RFC_FIELDS, C.EMPTY).split(','),
+                                    rfctable=self.config.getParameter(C.PARAM_SAP_RFC_TABLE),
+                                    rowcount=int(self.config.getParameter(C.PARAM_SAP_RFC_ROWCOUNT, "0")))
+            if (not sap.read()):
+                raise Exception("Error while reading the XES file")
+            return sap.content
+
+        except Exception as e:
+            self.log.error("extract() Error -> " + str(e))
+            return super().extract()
```

## pipelines/repository/bppiPLRXESFile.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import utils.constants as C
-from pipelines.bppi.repository.bppiRepository import bppiRepository
-import pandas as pd
-from pipelines.readers.xesFileReader import xesFileReader
-
-XES_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME]
-
-""" Manages the Blue Prism Repository extraction interface
-    Class hierarchy:
-    - bppiapi.bppiPipeline
-        - bppiapi.repository.bppiRepository
-            - pipelines.repository.bppiPLRXESFile
-"""
-class bppiPLRXESFile(bppiRepository):
-
-    @property
-    def mandatoryParameters(self) -> str:
-        return XES_MANDATORY_PARAM_LIST
-
-    def extract(self) -> pd.DataFrame: 
-        """Read the XES file and build the dataframe
-        Returns:
-            pd.DataFrame: Dataframe with the source data
-        """
-        try:
-            filename = self.config.getParameter(C.PARAM_FILENAME)
-            xes = xesFileReader(self.log)
-            xes.filename = filename
-            if (not xes.read()):
-                raise Exception("Error while reading the XES file")
-            return xes.content
-
-        except Exception as e:
-            self.log.error("bppiPLRXESFile.extract() Error: " + str(e))
-            return super().extract()
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import utils.constants as C
+from pipelines.bppi.repository.bppiRepository import bppiRepository
+import pandas as pd
+from pipelines.readers.xesFileReader import xesFileReader
+
+XES_MANDATORY_PARAM_LIST = [C.PARAM_FILENAME]
+
+""" Manages the Blue Prism Repository extraction interface
+    Class hierarchy:
+    - bppiapi.bppiPipeline
+        - bppiapi.repository.bppiRepository
+            - pipelines.repository.bppiPLRXESFile
+"""
+class bppiPLRXESFile(bppiRepository):
+
+    @property
+    def mandatoryParameters(self) -> str:
+        return XES_MANDATORY_PARAM_LIST
+
+    def extract(self) -> pd.DataFrame: 
+        """Read the XES file and build the dataframe
+        Returns:
+            pd.DataFrame: Dataframe with the source data
+        """
+        try:
+            filename = self.config.getParameter(C.PARAM_FILENAME)
+            xes = xesFileReader(self.log)
+            xes.filename = filename
+            if (not xes.read()):
+                raise Exception("Error while reading the XES file")
+            return xes.content
+
+        except Exception as e:
+            self.log.error("bppiPLRXESFile.extract() Error: " + str(e))
+            return super().extract()
```

## utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-class cursorByField(object):
-    """ This class aims to use the cursor by using the fields directly
-    Args:
-        cursor (_type_): SQL cursor
-        row (_type_): resultset
-    """
-    def __init__(self, cursor, row):
-        for (attr, val) in zip((d[0] for d in cursor.description), row) :
-            setattr(self, attr, val)
-
-    def get(self, attribute):
-        try:
-            return getattr(self, attribute)
-        except:
+__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+class cursorByField(object):
+    """ This class aims to use the cursor by using the fields directly
+    Args:
+        cursor (_type_): SQL cursor
+        row (_type_): resultset
+    """
+    def __init__(self, cursor, row):
+        for (attr, val) in zip((d[0] for d in cursor.description), row) :
+            setattr(self, attr, val)
+
+    def get(self, attribute):
+        try:
+            return getattr(self, attribute)
+        except:
             return None
```

## utils/constants.py

```diff
@@ -1,136 +1,122 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import logging
-
-ENCODING = "utf-8"
-YES = "yes"
-NO = "no"
-EMPTY = ""
-DEFCSVSEP = ","
-PIPELINE_FOLDER = 'pipelines.repository.'                # Pipeline datasource classes
-
-# Configuration type
-CONFIG_SOURCE_NAME = "configsource"
-CONFIG_SOURCE_SQ3 = "sqlite3"
-CONFIG_SOURCE_INI = "ini"   
-
-# Parameter Names (INI/Command line)
-PARAM_SRCTYPE = "sourcetype"                            # Data source type {csv|excel|odbc|blueprism}
-PARAM_SRCTYPE_VALCSV = "csv"                            # sourcetype = csv
-PARAM_SRCTYPE_VALXES = "xes"                            # sourcetype = xes
-PARAM_SRCTYPE_VALODBC = "odbc"                          # sourcetype = odbc
-PARAM_SRCTYPE_VALBP = "bprepo"                          # sourcetype = blueprism Repository
-PARAM_SRCTYPE_VALBPAPI = "bpapi"                        # sourcetype = blueprism api
-PARAM_SRCTYPE_VALXLS = "excel"                          # sourcetype = excel
-PARAM_SRCTYPE_VALSAPTABLE = "saptable"                  # sourcetype = SAP RFC Table
-PARAM_SRCTYPE_CHORUSFILE = "chfile"                     # sourcetype = SS&C Chorus Extraction file
-PARAM_SRCTYPE_SUPPORTED = [PARAM_SRCTYPE_VALCSV,
-                           PARAM_SRCTYPE_VALODBC,
-                           PARAM_SRCTYPE_VALXES,
-                           PARAM_SRCTYPE_VALXLS,
-                           PARAM_SRCTYPE_VALBP,
-                           PARAM_SRCTYPE_VALSAPTABLE,
-                           PARAM_SRCTYPE_VALBPAPI,
-                           PARAM_SRCTYPE_CHORUSFILE]
-PARAM_FILENAME = "filename"                             # {csv|xes} Source file dataset
-PARAM_CSV_SEPARATOR ="sep"                              # {csv} CSV fields separator (by default comma)
-PARAM_CONFIGFILE = "configfile"                         # {odbc|bprepo} Config / INI file
-PARAM_EXCELSHEETNAME = "sheet"                          # {excel} Excel spreadsheet name
-PARAM_SQ_ID = "id"                                      # When using SQLite config / ID of the config
-# Parameters which can be in the INI file
-PARAM_BPPITOKEN = "bppi.token"                          # {csv|xes|excel|odbc|bprepo} BPPI Token
-PARAM_BPPIURL = "bppi.url"                              # {csv|xes|excel|odbc|bprepo} BPPI URL
-PARAM_CONNECTIONSTRING = "database.connectionstring"    # {ODBC/Blue Prism} ODBC Connection String
-PARAM_QUERY = "database.query"                          # {ODBC|bprepo} Query to gather data
-PARAM_FROMDATE = "fromdate"                             # {bprepo}From Date (delta extraction)
-PARAM_TODATE = "todate"                                 # {bprepo}To Date (delta extraction)
-PARAM_BPPROCESSNAME = "blueprism.processname"           # {bprepo} Process Name  (to gather the logs from)
-PARAM_BPSTAGETYPES = "blueprism.stagetypefilters"       # {bprepo} filter out these stages (list of stages type separated by comma)
-PARAM_BPINCLUDEVBO = "blueprism.includevbo"             # {bprepo} yes/no : Extract the VBO logs
-PARAM_BPUNICODE = "blueprism.unicode"                   # {bprepo} yes/no : Blue Prism logs in unicode or not
-PARAM_BPPITABLE = "bppi.table"                          # {odbc|bprepo} Name of the table in the BPPI repository
-PARAM_BPPITODOACTIVED = "bppi.todos"                    # {csv|xes|excel|odbc|bprepo} Execute the to do (yes/no)
-PARAM_BPPITODOS = "bppi.todolist"                       # {odbc|bprepo} List of BPPI TO DOs to execute after loading
-PARAM_LOGFILENAME = "other.logfilename"                 # {csv|xes|excel|odbc|bprepo} Filename of the Log file
-PARAM_LOGFOLDER = "other.logfolder"                     # {csv|xes|excel|odbc|bprepo} Folder to store the Logs
-PARAM_LOGLEVEL = "other.loglevel"                       # {csv|xes|excel|odbc|bprepo} Log level (DEBUG|INFO|WARNING|ERROR)
-PARAM_LOGFORMAT = "other.logformat"                     # {csv|xes|excel|odbc|bprepo} Log format (Cf. Python logger doc)
-PARAM_BPPARAMSATTR = "blueprism.parameters"             # {bprepo} List (separated by a comma) of the BP parameters/attributes to gather (will be added in new columns)
-PARAM_EVENTMAP = "events.eventmap"                           # {odbc|bprepo} yes/no : manage event mapping
-PARAM_EVENTMAPTABLE = "events.eventmaptable"                 # {odbc|bprepo} Map the events with the dataset. This param contains the name of the csv file which stores the event map (col 1: source event name, col 2: new event name)
-PARAM_EVENTMAPNAME = "events.eventcolumn"                    # {odbc|bprepo} Name of the event column name in the original source
-PARAM_BPFILTERSTEND = "blueprism.startendfilter"        # {bprepo} yes/no: filtr out all Start & End stages except the Main Page ones
-PARAM_BPMAINPROCESSPAGE = "blueprism.mainprocesspage"   # {bprepo} BP Process Main Page name
-PARAM_BPDELTA = "blueprism.delta"                       # {bprepo} delta load activated (yes/no), if no full load
-PARAM_BPDELTA_FILE = "blueprism.deltafile"              # {bprepo} file where the latest date load is saved (for delta load only)
-PARAM_SAP_ASHOST = "sap.ashost"                         # {saptable} AP Host name or IP
-PARAM_SAP_CLIENT = "sap.client"                         # {saptable} SAP Client
-PARAM_SAP_SYSNR = "sap.sysnr"                           # {saptable} SAP System Number
-PARAM_SAP_USER = "sap.user"                             # {saptable} SAP User
-PARAM_SAP_PASSWD = "sap.passwd"                         # {saptable} SAP Password
-PARAM_SAP_ROUTER = "sap.saprouter"                      # {saptable} SAP Router (if any)
-PARAM_SAP_RFC_TABLE = "sap.rfctable"                    # {saptable} RFC Table to request
-PARAM_SAP_RFC_FIELDS = "sap.rfcfields"                  # {saptable} List of fields to gather (separated by a comma)
-PARAM_SAP_RFC_ROWCOUNT = "sap.rowlimit"                 # {saptable} Row Count limit (Nb Max of rows retreived from SAP)
-PARAM_BPAPI_SSL_VERIF = "blueprismapi.ssl_verification" # {bpapi} Verification SSL ?
-PARAM_BPAPI_CLIENT_ID = "blueprismapi.client_id"        # {bpapi} BP API Client ID
-PARAM_BPAPI_SECRET = "blueprismapi.client_secret"       # {bpapi} BP API Client Secret 
-PARAM_BPAPI_AUTH_URL = "blueprismapi.auth_url"          # {bpapi} BP Authentication Server
-PARAM_BPAPI_API_URL = "blueprismapi.api_url"            # {bpapi} BP API Server
-PARAM_BPAPI_API_PAGESIZE = "blueprismapi.api_page_size" # {bpapi} API Page size
-
-# GLOBAL HTTP REQUEST
-HTTP_API_OK = 200
-
-# BLUE PRISM API
-PBAPI_VER = "/api/v7"
-BPAPI_SESSIONS_LIST = "/sessions"
-BPAPI_SESSION_HEAD = "/sessions/{}"
-BPAPI_SESSION_LOGS = "/sessions/{}/logs"
-BPAPI_SESSION_PARAMS = "/sessions/{}/parameters"
-
-# BPPI API
-API_1_0 = "/api/ext/1.0/"
-API_REPOSITORY_CONFIG = "repository/repository-import-configuration"
-API_SERVER_UPLOAD_INFOS = "repository/{}/file/upload-url"
-API_SERVER_LOAD_2_REPO = "repository/{}/load"
-API_PROCESSING_STATUS = "processing"
-API_EXECUTE_TODO = "repository/{}/execute-todo-list"
-API_DEF_WAIT_DURATION_SEC = 2
-API_DEF_NB_ITERATION_MAX = 60
-API_STATUS_IN_PROGRESS = "IN_PROGRESS"
-API_STATUS_ERROR = "ERROR"
-API_BLOC_SIZE_LIMIT = 10000 # Same limitation as the current API call via java
-
-# Logger configuration
-TRACE_DEFAULT_LEVEL = logging.DEBUG
-TRACE_DEFAULT_FORMAT = "%(asctime)s|%(name)s|%(levelname)s|%(message)s"
-TRACE_FILENAME = "bppiapiwrapper.log"
-TRACE_MAXBYTES = 1000000
-
-# Dump file suffix
-TEMP_SQLDUMP = "-temp-sqlserver-dump.csv"
-
-# Blue Prism stuff
-BPLOG_STAGETYPE_COL = "stagetype"                   # Name of the stagetype column in the BP Repo
-BPLOG_STAGENAME_COL = "stagename"                   # Name of the stagename column in the BP Repo
-BPLOG_PAGENAME_COL = "pagename"                     # Name of the pagename column in the BP Repo
-BPLOG_PROCESSNAME_COL = "processname"               # Name of the process name column in the BP Repo
-BPLOG_STARTDATETIME_COL = "startdatetime"           # Name of the Start Date & time column in the BP Repo
-BPLOG_ATTRIBUTE_COL = "attributexml"                # Name of the attributexml column in the BP Repo
-BPLOG_LOG_UNICODE = "BPASessionLog_Unicode"         # BP Log table name for unicode
-BPLOG_LOG_NONUNICODE = "BPASessionLog_NonUnicode"   # BP Log table name for non unicode
-BPLOG_INI4SQL = "bplogs.sql"                        # File which contains the BP SQL Query
-BP_STAGE_START = "Start"                            # Name of the BP Start stage
-BP_STAGE_END = "End"                                # Name of the BP End stage
-BP_MAINPAGE_DEFAULT = "Main Page"                   # Name of the BP Main Page (process)
-BP_DEFAULT_DELTAFILE = "bpdelta.tag"                # Default filename for the delta tag
-BP_DELTADATE_FMT = "%Y-%m-%d %H:%M:%S"              # Delta date format %Y-%m-%d %H:%M:%S
-COL_STAGE_ID = "STAGE_ID"
-COL_OBJECT_TAB = "OBJECT_TAB"
-
-# SQLite configuration SPecifics
-SQLITE_GETCONFIG = "SELECT * FROM VIEW_GET_FULLCONFIG_BLUEPRISM_REPO WHERE ID={}"
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import logging
+
+ENCODING = "utf-8"
+YES = "yes"
+NO = "no"
+EMPTY = ""
+DEFCSVSEP = ","
+
+# Pipeline datasource default classes storage
+PIPELINE_FOLDER = 'pipelines.repository.' 
+
+# Configuration type
+CONFIG_SOURCE_NAME = "configsource"
+CONFIG_SOURCE_SQ3 = "sqlite3"
+CONFIG_SOURCE_INI = "ini"   
+
+# Parameter Names (INI/Command line)
+PARAM_PIPELINE_PATH = "pipeline.path"                   # default path where the pipelines are stored (optional)
+PARAM_PIPELINE_CLASSNAME = "pipeline.classname"         # pipeline class name / must derive from the pipeline class
+PARAM_FILENAME = "source.filename"                      # {csv|xes} Source file dataset
+PARAM_CSV_SEPARATOR ="source.separator"                 # {csv} CSV fields separator (by default comma)
+PARAM_CONFIGFILE = "configfile"                         # {odbc|bprepo} Config / INI file
+PARAM_EXCELSHEETNAME = "source.sheet"                   # {excel} Excel spreadsheet name
+PARAM_FROMDATE = "source.fromdate"                      # {bprepo}From Date (delta extraction)
+PARAM_TODATE = "source.todate"                          # {bprepo}To Date (delta extraction)
+PARAM_BPPITOKEN = "bppi.token"                          # {csv|xes|excel|odbc|bprepo} BPPI Token
+PARAM_BPPIURL = "bppi.url"                              # {csv|xes|excel|odbc|bprepo} BPPI URL
+PARAM_CONNECTIONSTRING = "database.connectionstring"    # {ODBC/Blue Prism} ODBC Connection String
+PARAM_QUERY = "database.query"                          # {ODBC|bprepo} Query to gather data
+PARAM_BPPROCESSNAME = "blueprism.processname"           # {bprepo} Process Name  (to gather the logs from)
+PARAM_BPSTAGETYPES = "blueprism.stagetypefilters"       # {bprepo} filter out these stages (list of stages type separated by comma)
+PARAM_BPINCLUDEVBO = "blueprism.includevbo"             # {bprepo} yes/no : Extract the VBO logs
+PARAM_BPUNICODE = "blueprism.unicode"                   # {bprepo} yes/no : Blue Prism logs in unicode or not
+PARAM_BPPITABLE = "bppi.table"                          # {odbc|bprepo} Name of the table in the BPPI repository
+PARAM_BPPITODOACTIVED = "bppi.todos"                    # {csv|xes|excel|odbc|bprepo} Execute the to do (yes/no)
+PARAM_BPPITODOS = "bppi.todolist"                       # {odbc|bprepo} List of BPPI TO DOs to execute after loading
+PARAM_LOGFILENAME = "other.logfilename"                 # {csv|xes|excel|odbc|bprepo} Filename of the Log file
+PARAM_LOGFOLDER = "other.logfolder"                     # {csv|xes|excel|odbc|bprepo} Folder to store the Logs
+PARAM_LOGLEVEL = "other.loglevel"                       # {csv|xes|excel|odbc|bprepo} Log level (DEBUG|INFO|WARNING|ERROR)
+PARAM_LOGFORMAT = "other.logformat"                     # {csv|xes|excel|odbc|bprepo} Log format (Cf. Python logger doc)
+PARAM_BPPARAMSATTR = "blueprism.parameters"             # {bprepo} List (separated by a comma) of the BP parameters/attributes to gather (will be added in new columns)
+PARAM_EVENTMAP = "events.eventmap"                           # {odbc|bprepo} yes/no : manage event mapping
+PARAM_EVENTMAPTABLE = "events.eventmaptable"                 # {odbc|bprepo} Map the events with the dataset. This param contains the name of the csv file which stores the event map (col 1: source event name, col 2: new event name)
+PARAM_EVENTMAPNAME = "events.eventcolumn"                    # {odbc|bprepo} Name of the event column name in the original source
+PARAM_BPFILTERSTEND = "blueprism.startendfilter"        # {bprepo} yes/no: filtr out all Start & End stages except the Main Page ones
+PARAM_BPMAINPROCESSPAGE = "blueprism.mainprocesspage"   # {bprepo} BP Process Main Page name
+PARAM_BPDELTA = "blueprism.delta"                       # {bprepo} delta load activated (yes/no), if no full load
+PARAM_BPDELTA_FILE = "blueprism.deltafile"              # {bprepo} file where the latest date load is saved (for delta load only)
+PARAM_SAP_ASHOST = "sap.ashost"                         # {saptable} AP Host name or IP
+PARAM_SAP_CLIENT = "sap.client"                         # {saptable} SAP Client
+PARAM_SAP_SYSNR = "sap.sysnr"                           # {saptable} SAP System Number
+PARAM_SAP_USER = "sap.user"                             # {saptable} SAP User
+PARAM_SAP_PASSWD = "sap.passwd"                         # {saptable} SAP Password
+PARAM_SAP_ROUTER = "sap.saprouter"                      # {saptable} SAP Router (if any)
+PARAM_SAP_RFC_TABLE = "sap.rfctable"                    # {saptable} RFC Table to request
+PARAM_SAP_RFC_FIELDS = "sap.rfcfields"                  # {saptable} List of fields to gather (separated by a comma)
+PARAM_SAP_RFC_ROWCOUNT = "sap.rowlimit"                 # {saptable} Row Count limit (Nb Max of rows retreived from SAP)
+PARAM_BPAPI_SSL_VERIF = "blueprismapi.ssl_verification" # {bpapi} Verification SSL ?
+PARAM_BPAPI_CLIENT_ID = "blueprismapi.client_id"        # {bpapi} BP API Client ID
+PARAM_BPAPI_SECRET = "blueprismapi.client_secret"       # {bpapi} BP API Client Secret 
+PARAM_BPAPI_AUTH_URL = "blueprismapi.auth_url"          # {bpapi} BP Authentication Server
+PARAM_BPAPI_API_URL = "blueprismapi.api_url"            # {bpapi} BP API Server
+PARAM_BPAPI_API_PAGESIZE = "blueprismapi.api_page_size" # {bpapi} API Page size
+
+# GLOBAL HTTP REQUEST
+HTTP_API_OK = 200
+
+# BLUE PRISM API
+PBAPI_VER = "/api/v7"
+BPAPI_SESSIONS_LIST = "/sessions"
+BPAPI_SESSION_HEAD = "/sessions/{}"
+BPAPI_SESSION_LOGS = "/sessions/{}/logs"
+BPAPI_SESSION_PARAMS = "/sessions/{}/parameters"
+
+# BPPI API
+API_1_0 = "/api/ext/1.0/"
+API_REPOSITORY_CONFIG = "repository/repository-import-configuration"
+API_SERVER_UPLOAD_INFOS = "repository/{}/file/upload-url"
+API_SERVER_LOAD_2_REPO = "repository/{}/load"
+API_PROCESSING_STATUS = "processing"
+API_EXECUTE_TODO = "repository/{}/execute-todo-list"
+API_DEF_WAIT_DURATION_SEC = 2
+API_DEF_NB_ITERATION_MAX = 60
+API_STATUS_IN_PROGRESS = "IN_PROGRESS"
+API_STATUS_ERROR = "ERROR"
+API_BLOC_SIZE_LIMIT = 10000 # Same limitation as the current API call via java
+
+# Logger configuration
+TRACE_DEFAULT_LEVEL = logging.DEBUG
+TRACE_DEFAULT_FORMAT = "%(asctime)s|%(name)s|%(levelname)s|%(message)s"
+TRACE_FILENAME = "bppiapiwrapper.log"
+TRACE_MAXBYTES = 1000000
+
+# Dump file suffix
+TEMP_SQLDUMP = "-temp-sqlserver-dump.csv"
+
+# Blue Prism stuff
+BPLOG_STAGETYPE_COL = "stagetype"                   # Name of the stagetype column in the BP Repo
+BPLOG_STAGENAME_COL = "stagename"                   # Name of the stagename column in the BP Repo
+BPLOG_PAGENAME_COL = "pagename"                     # Name of the pagename column in the BP Repo
+BPLOG_PROCESSNAME_COL = "processname"               # Name of the process name column in the BP Repo
+BPLOG_STARTDATETIME_COL = "startdatetime"           # Name of the Start Date & time column in the BP Repo
+BPLOG_ATTRIBUTE_COL = "attributexml"                # Name of the attributexml column in the BP Repo
+BPLOG_LOG_UNICODE = "BPASessionLog_Unicode"         # BP Log table name for unicode
+BPLOG_LOG_NONUNICODE = "BPASessionLog_NonUnicode"   # BP Log table name for non unicode
+BPLOG_INI4SQL = "bplogs.sql"                        # File which contains the BP SQL Query
+BP_STAGE_START = "Start"                            # Name of the BP Start stage
+BP_STAGE_END = "End"                                # Name of the BP End stage
+BP_MAINPAGE_DEFAULT = "Main Page"                   # Name of the BP Main Page (process)
+BP_DEFAULT_DELTAFILE = "bpdelta.tag"                # Default filename for the delta tag
+BP_DELTADATE_FMT = "%Y-%m-%d %H:%M:%S"              # Delta date format %Y-%m-%d %H:%M:%S
+COL_STAGE_ID = "STAGE_ID"
+COL_OBJECT_TAB = "OBJECT_TAB"
+
+# SQLite configuration SPecifics
+PARAM_SQ_ID = "id"                                  # When using SQLite config / ID of the config
+SQLITE_GETCONFIG = "SELECT * FROM VIEW_GET_FULLCONFIG_BLUEPRISM_REPO WHERE ID={}"
```

## utils/log.py

 * *Ordering differences only*

```diff
@@ -1,56 +1,56 @@
-__author__ = "Benoit CAYLA"
-__email__ = "benoit@datacorner.fr"
-__license__ = "MIT"
-
-import logging
-from logging.handlers import RotatingFileHandler
-import utils.constants as C
-
-class log:
-    def __init__(self, loggerName, logfilename, level, format):
-        self.__logger = logging.getLogger(loggerName)
-        logHandler = logging.handlers.RotatingFileHandler(logfilename, 
-                                                          mode="a", 
-                                                          maxBytes= C.TRACE_MAXBYTES, 
-                                                          backupCount=1 , 
-                                                          encoding=C.ENCODING)
-        logHandler.setFormatter(logging.Formatter(format))
-        if (level == "INFO"):
-            loglevel = logging.INFO
-        elif (level == "DEBUG"):
-            loglevel = logging.DEBUG
-        elif (level == "WARNING"):
-            loglevel = logging.WARNING
-        else:
-            loglevel = logging.ERROR
-        self.__logger.setLevel(loglevel)
-        self.__logger.addHandler(logHandler)
-
-    def display(self, message):
-        print(message)
-    
-    def buildMessage(self, _msg):
-        final_message = ""
-        for msg in _msg:
-            final_message += str(msg)
-        return final_message
-    
-    def info(self, *message):
-        final_message = self.buildMessage(message)
-        self.display("Info> " + final_message)
-        self.__logger.info(final_message)
-
-    def error(self, *message):
-        final_message = self.buildMessage(message)
-        self.display("**ERROR**> " + final_message)
-        self.__logger.error(final_message)
-
-    def debug(self, *message):
-        final_message = self.buildMessage(message)
-        self.display("Debug> " + final_message)
-        self.__logger.debug(final_message)
-
-    def warning(self, *message):
-        final_message = self.buildMessage(message)
-        self.display("*WARNING*> " + final_message)
+__author__ = "Benoit CAYLA"
+__email__ = "benoit@datacorner.fr"
+__license__ = "MIT"
+
+import logging
+from logging.handlers import RotatingFileHandler
+import utils.constants as C
+
+class log:
+    def __init__(self, loggerName, logfilename, level, format):
+        self.__logger = logging.getLogger(loggerName)
+        logHandler = logging.handlers.RotatingFileHandler(logfilename, 
+                                                          mode="a", 
+                                                          maxBytes= C.TRACE_MAXBYTES, 
+                                                          backupCount=1 , 
+                                                          encoding=C.ENCODING)
+        logHandler.setFormatter(logging.Formatter(format))
+        if (level == "INFO"):
+            loglevel = logging.INFO
+        elif (level == "DEBUG"):
+            loglevel = logging.DEBUG
+        elif (level == "WARNING"):
+            loglevel = logging.WARNING
+        else:
+            loglevel = logging.ERROR
+        self.__logger.setLevel(loglevel)
+        self.__logger.addHandler(logHandler)
+
+    def display(self, message):
+        print(message)
+    
+    def buildMessage(self, _msg):
+        final_message = ""
+        for msg in _msg:
+            final_message += str(msg)
+        return final_message
+    
+    def info(self, *message):
+        final_message = self.buildMessage(message)
+        self.display("Info> " + final_message)
+        self.__logger.info(final_message)
+
+    def error(self, *message):
+        final_message = self.buildMessage(message)
+        self.display("**ERROR**> " + final_message)
+        self.__logger.error(final_message)
+
+    def debug(self, *message):
+        final_message = self.buildMessage(message)
+        self.display("Debug> " + final_message)
+        self.__logger.debug(final_message)
+
+    def warning(self, *message):
+        final_message = self.buildMessage(message)
+        self.display("*WARNING*> " + final_message)
         self.__logger.warning(final_message)
```

## Comparing `pyBPPIBridge-0.4.10.dist-info/LICENSE` & `pyBPPIBridge-0.5.0.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-MIT License
-
-Copyright (c) 2023 Benot Cayla
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
+MIT License
+
+Copyright (c) 2023 Benot Cayla
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
```

## Comparing `pyBPPIBridge-0.4.10.dist-info/RECORD` & `pyBPPIBridge-0.5.0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,49 +1,49 @@
-bppibridge.py,sha256=MaR9Uk93THfC1txZdimiF_xQ3PTzUVewhFsLqY7jvAs,147
-bppibridgesq.py,sha256=rkp-hMxRIM2HIl3nYYViiOaLxGFB1wErNLx-HTKo-TM,401
-bppibridge/__init__.py,sha256=VV3IQ60-PTAS0yCiGVLAnr6b6rYY5LwztmlZsBtBrlA,439
+bppibridge.py,sha256=LaGna33A_pkR0yYOfZcLcUJz4l2uoetNR4IwFK7zdG4,154
+bppibridgesq.py,sha256=N4TmQy-KktD62ECtQYvfOyTtr77zGqrQgurwUwOX_hQ,413
+bppibridge/__init__.py,sha256=JoPBB6E_bWNytR3vlupEdLWLpnUSYQMVlihzUb7wliI,569
 config/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-config/appConfig.py,sha256=SIphnZLrLiw_wfEHsKAgUh_68IjX6W4KsWe_DtuKZKQ,4005
-config/cmdLineConfig.py,sha256=r4QDHcBHFcNkmdNyhjkDf6oH0Swa7fOaLxHjAKZdlQM,5639
+config/appConfig.py,sha256=Zhn9sOOnQEnp9UseWDs43JQrdFH6o-0IKuUYtdV7v2c,4112
+config/cmdLineConfig.py,sha256=zxQPpQLTnz7rcPDMhPcVSQI2z7fFVAu-45xXzVZs6oI,3059
 pipelines/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/pipeline.py,sha256=nRkyWNitboCRfdw9trmXCYToZ3TzcBjy-Fl6iOoBtFA,2417
-pipelines/pipelineFactory.py,sha256=qY3w2IdjQHr2wD-YrzkBDMgWDQfCZsVqR7_aUdWkhbk,4736
+pipelines/pipeline.py,sha256=yjrpk4rW8z3stts4aU4ToM6UMIKHzyEWX6Wb_7feGzA,3437
+pipelines/pipelineFactory.py,sha256=x9z6fte7aUvHz5KTEf73Lyao3XTFehFVImw-Qagqp6I,4644
 pipelines/bppi/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/bppi/bppiPipeline.py,sha256=r0-JWVyselTJoFn2oEW0idd-rPK_ULtkn7B2HpFaEcM,8041
-pipelines/bppi/uploadConfig.py,sha256=TGoIqgobEN-3DxKjY_kguoXalglx2hu9LasSCwDJU_0,1334
+pipelines/bppi/bppiPipeline.py,sha256=UCig65NnTW0MXkxQQ26Y5SMPUe0LWRNyMAeVaYISPlo,5058
+pipelines/bppi/uploadConfig.py,sha256=l1ffewmeUbnCjLg9xQqpCRQu_yQzGft-4usQRIk4h38,1384
 pipelines/bppi/project/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/bppi/project/bppiApiProjectWrapper.py,sha256=vRdT6d-hVNaTrM8K16l6lmOiVy34B2vMhT7TQunHV0s,930
-pipelines/bppi/project/bppiProject.py,sha256=3lGoyyUDxr1NxdeIQksNZcGl7etW-g_Ry0KuuSDabs4,508
+pipelines/bppi/project/bppiApiProjectWrapper.py,sha256=zgy-KDUXzNbxhtIJFmqh7dtHVQpMgDB1-K77NQeRBQc,969
+pipelines/bppi/project/bppiProject.py,sha256=sbDJfW5eL_A6ZSgvo7GCtMLV-ZMqjQ5DzSmz0jp97BA,523
 pipelines/bppi/repository/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/bppi/repository/bppiApiRepositoryWrapper.py,sha256=nxLipP5FWR3Awy14LFOEZO3m0m4rVM48i3d-IUIe7e4,8770
-pipelines/bppi/repository/bppiRepository.py,sha256=1e2KtkWS-n4nEaWLWkY-qMkMwWQY0oro5ED8Tn5oVuo,7064
-pipelines/bppi/repository/repConfig.py,sha256=rViiAPPbgw9X_M9Unb3mhL5oZkfY7bHDkC79c6DjKws,2126
+pipelines/bppi/repository/bppiApiRepositoryWrapper.py,sha256=aGxEjkEvdlSyIu8ecKofZ8M2h_iW5qYmD08O9YbzQl8,8963
+pipelines/bppi/repository/bppiRepository.py,sha256=UiqysVk1gLYg6ejv5PiKdvDFEJsZifabmCCMGfZ5gcE,9166
+pipelines/bppi/repository/repConfig.py,sha256=LppAvoRBr2WBML_XHHRnkXHX1-PA_zklJOUX5mDDNXA,2195
 pipelines/project/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/readers/Reader.py,sha256=KRdBWyMMaREexlQYLhyUNasbk8SNE7rXXJzRphz6XPc,650
+pipelines/readers/Reader.py,sha256=d9CnilYPZJ4pLiuAMHDCTHkmYzI7-IKQR0aFoPvpjKQ,679
 pipelines/readers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/readers/bpAPIReader.py,sha256=zhblZ8NWKh19PD0GLu9X9tsi49mIwmFCOmGZa_AuSug,519
-pipelines/readers/csvFileReader.py,sha256=FvLtoDszUzUtIfCEO2jX3DrmFG3X1SemgLTiquHNAew,1016
-pipelines/readers/excelFileReader.py,sha256=y-aULARc_1bjG9DasPFRKGBA7R_hPTJGV3jEF35PEyU,1068
-pipelines/readers/odbcReader.py,sha256=D70Wy7lMl_o3f5ICOTTPDZ5KzC47rc-npDO_SMSTT6I,1261
-pipelines/readers/sapRFCTableReader.py,sha256=lrWWq_FeBBUMCmgoo7h9y1EOrPhpw9BWKaIqh15Mj4g,3812
-pipelines/readers/xesFileReader.py,sha256=4z2rmmYjeqY7KEaNiNMlOGxu1UWUKhn_rxcINN3OT5E,3881
-pipelines/readers/builders/SQLBuilder.py,sha256=ChwWporNrUp8bTPUn0f8chF5BD3m-y2cCfv1lv0AgQM,2147
+pipelines/readers/bpAPIReader.py,sha256=QRw4EnKyhOwnZIwHRspk1_RrRvDTOGc_QeOyB9WWOag,9613
+pipelines/readers/csvFileReader.py,sha256=MRJRPLNLo68fho2BE4D2za8PuMZTnfGTQODuB3YvYX0,1052
+pipelines/readers/excelFileReader.py,sha256=ucW11dW8tawoB7lQkCG2RvVSRXLpf2TYApo55pxPXM8,1105
+pipelines/readers/odbcReader.py,sha256=OSEwcuGt6w3IiV--NMuax-XBbVZ6X_y4GZkMKjZRw0U,1422
+pipelines/readers/sapRFCTableReader.py,sha256=UeFFuyVfSWKzPdo0d0px1OVJKKkS6o204Jrh7e4K5D8,3913
+pipelines/readers/xesFileReader.py,sha256=V7ux7OH4giFPW9Lfxdb2UrLhpyJWk28p3Hjimet_TOc,3989
+pipelines/readers/builders/SQLBuilder.py,sha256=8oV51y4wNOqzp-qqNEIcCoNaLYfI41nsZEQAmQfuCzA,2209
 pipelines/readers/builders/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/readers/builders/blueprismSQLBuilder.py,sha256=EQe_rlQn6gUAg_ekyVGQV5sLoTjM7hZd-bs3AC0QJYM,3105
+pipelines/readers/builders/blueprismSQLBuilder.py,sha256=pe-WfydH8AqWvmluDnOv33b5uuV4mz2xq2-aZ289vPk,3676
 pipelines/repository/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/repository/bppiPLRBluePrismApi.py,sha256=MqvaBT76DgccY7SFbFF5ArgyqIQ998Uqy4EfYJqSDcw,10473
-pipelines/repository/bppiPLRBluePrismRepo.py,sha256=ukyczPZ2ltAYbfe0GAW0VywMEsIK1iEBFKe4t5AQKrg,11539
-pipelines/repository/bppiPLRCSVFile.py,sha256=weYurRrMuj8M9wpIg3jIYOv3vD0D4DkHJlaVHLVQS08,1365
-pipelines/repository/bppiPLRChorusExtract.py,sha256=lp8oYvYmcEv_FjZoBOSYXdkybUPBGohVwCYngYzjJUk,831
-pipelines/repository/bppiPLRExcelFile.py,sha256=4H1QhcZRsbMsIartosOvFtIwU1MOap7j5sCUhdMA-8k,1492
-pipelines/repository/bppiPLRODBC.py,sha256=0lhIhWboco1p2UZZrNAtV2PIEEViOsqHznxPy68nYBc,1726
-pipelines/repository/bppiPLRSAPRfcTable.py,sha256=ZJp8KhjrITbAMHhToi9jYiXYHP2_4XsQPjREyhg1bzc,2811
-pipelines/repository/bppiPLRXESFile.py,sha256=NA3VyivLdw23lhJOKDRV7xyJ_PO7v1nU7AXR5K_PDRk,1264
-utils/__init__.py,sha256=zOZNRT68rQWcknZvGj-d-YnZ-cwknxZ0m6i1HcQy9qU,537
-utils/constants.py,sha256=-2MUHT2m-_GZ9m2sjrNM2Ty7fjyN_qQJSmM2hlMhMb4,8863
-utils/log.py,sha256=sTS3HpXFDDf3CgIlKMfgMu6J6ZUwDjesQmDS5SvOnpA,2016
-pyBPPIBridge-0.4.10.dist-info/LICENSE,sha256=v-nn7Jb3mApcRpoJqKRKESesoX-Ch-Du3XYTwn1eUW0,1070
-pyBPPIBridge-0.4.10.dist-info/METADATA,sha256=l_1hwqPyWyCnFqRK334WSNNKQlJW-TQ2kvMs5PEL7vI,3753
-pyBPPIBridge-0.4.10.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
-pyBPPIBridge-0.4.10.dist-info/entry_points.txt,sha256=pBbss7e8g9wopGxcuuy7UkA9WeAxvcVkP-TxlNaPYag,47
-pyBPPIBridge-0.4.10.dist-info/top_level.txt,sha256=MQN5L-3gd5Xf4J_8VYMNSt6s2gKGU6l4AmnrHyIWbWA,47
-pyBPPIBridge-0.4.10.dist-info/RECORD,,
+pipelines/repository/bppiPLRBluePrismApi.py,sha256=JHSV8aLcNkbV1FEr3J2fCaz8qpeCVJgnevEeW0D15Y4,2164
+pipelines/repository/bppiPLRBluePrismRepo.py,sha256=LoDpjkgI6-k1KZcENo9zzVRf9EA6gApBOPgpni-b-Kw,12377
+pipelines/repository/bppiPLRCSVFile.py,sha256=Fr2-tCNHb7ukovoypNVKDwUFtVwymdCFB2xVKnLIsPE,1338
+pipelines/repository/bppiPLRChorusExtract.py,sha256=z2tQHkoIh8W3OqABnIymV3ncVTFpzDJRL0AgfYBa9Ls,788
+pipelines/repository/bppiPLRExcelFile.py,sha256=eu3EuLrWWJ0Z3dMiuNUNLL01l2pK-4xfwzxb24leJqE,1467
+pipelines/repository/bppiPLRODBC.py,sha256=Suvwk18PPjyTrKCx7Zovs83i7ZrYivNIK7d2ZIlA8ws,1888
+pipelines/repository/bppiPLRSAPRfcTable.py,sha256=VIi2U_AaaBlKl2es15jtjHC84QA9MbJxBfrw5U7-M9E,2590
+pipelines/repository/bppiPLRXESFile.py,sha256=y9plTprExkzk7BvXCVqXRtE70_GZ2YK2JPwDji14qhQ,1303
+utils/__init__.py,sha256=toHVlAEo46DsKhmOS5GRAJiEoNYR43sZkihAxwySMag,555
+utils/constants.py,sha256=lTghCBwhv4P5RIku51_c9q5Cf3Obhwm1WN96XmsSwsY,8001
+utils/log.py,sha256=-FL9vph0YXwuuvz3fNv0lISiYjigBE99k703XnS9qGY,2071
+pyBPPIBridge-0.5.0.dist-info/LICENSE,sha256=BvMmeujpLDerNqzsA1gOET7mphtef9ebt_u8-bp4bNA,1091
+pyBPPIBridge-0.5.0.dist-info/METADATA,sha256=v6nOYtttTuensgt2Qonh_T3VSQCQpaGt-lOOCm5D71g,3868
+pyBPPIBridge-0.5.0.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+pyBPPIBridge-0.5.0.dist-info/entry_points.txt,sha256=pBbss7e8g9wopGxcuuy7UkA9WeAxvcVkP-TxlNaPYag,47
+pyBPPIBridge-0.5.0.dist-info/top_level.txt,sha256=MQN5L-3gd5Xf4J_8VYMNSt6s2gKGU6l4AmnrHyIWbWA,47
+pyBPPIBridge-0.5.0.dist-info/RECORD,,
```

