# Comparing `tmp/ThymeBoost-0.1.8-py3-none-any.whl.zip` & `tmp/ThymeBoost-0.1.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,29 @@
-Zip file size: 52355 bytes, number of entries: 52
--rw-rw-rw-  2.0 fat    30591 b- defN 22-Jan-29 22:59 ThymeBoost/ThymeBoost.py
+Zip file size: 52988 bytes, number of entries: 52
+-rw-rw-rw-  2.0 fat    31021 b- defN 22-Mar-14 04:16 ThymeBoost/ThymeBoost.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Sep-03 16:23 ThymeBoost/__init__.py
 -rw-rw-rw-  2.0 fat     2757 b- defN 21-Oct-17 23:48 ThymeBoost/cost_functions.py
 -rw-rw-rw-  2.0 fat     1775 b- defN 21-Nov-13 03:07 ThymeBoost/ensemble.py
--rw-rw-rw-  2.0 fat     8163 b- defN 21-Nov-13 03:22 ThymeBoost/optimizer.py
+-rw-rw-rw-  2.0 fat     8665 b- defN 22-Feb-02 00:11 ThymeBoost/optimizer.py
 -rw-rw-rw-  2.0 fat     2935 b- defN 22-Jan-22 17:03 ThymeBoost/param_iterator.py
--rw-rw-rw-  2.0 fat     4333 b- defN 22-Jan-29 17:32 ThymeBoost/predict_functions.py
+-rw-rw-rw-  2.0 fat     4366 b- defN 22-Jan-30 17:56 ThymeBoost/predict_functions.py
 -rw-rw-rw-  2.0 fat     3161 b- defN 21-Sep-21 01:44 ThymeBoost/split_proposals.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Oct-17 19:50 ThymeBoost/Datasets/__init__.py
 -rw-rw-rw-  2.0 fat      245 b- defN 21-Nov-15 14:06 ThymeBoost/Datasets/examples.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Sep-12 17:06 ThymeBoost/exogenous_models/__init__.py
--rw-rw-rw-  2.0 fat      847 b- defN 21-Dec-26 02:49 ThymeBoost/exogenous_models/decision_tree_exogenous.py
+-rw-rw-rw-  2.0 fat      912 b- defN 22-Mar-14 04:00 ThymeBoost/exogenous_models/decision_tree_exogenous.py
 -rw-rw-rw-  2.0 fat     1634 b- defN 21-Sep-11 17:00 ThymeBoost/exogenous_models/exogenous_base_class.py
--rw-rw-rw-  2.0 fat      636 b- defN 21-Sep-12 02:38 ThymeBoost/exogenous_models/glm_exogenous.py
--rw-rw-rw-  2.0 fat      638 b- defN 21-Sep-12 02:38 ThymeBoost/exogenous_models/ols_exogenous.py
+-rw-rw-rw-  2.0 fat      773 b- defN 22-Mar-14 04:00 ThymeBoost/exogenous_models/glm_exogenous.py
+-rw-rw-rw-  2.0 fat      795 b- defN 22-Mar-14 03:58 ThymeBoost/exogenous_models/ols_exogenous.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Oct-17 19:50 ThymeBoost/fit_components/__init__.py
--rw-rw-rw-  2.0 fat     1312 b- defN 21-Dec-26 02:45 ThymeBoost/fit_components/fit_exogenous.py
+-rw-rw-rw-  2.0 fat     1370 b- defN 22-Mar-14 04:10 ThymeBoost/fit_components/fit_exogenous.py
 -rw-rw-rw-  2.0 fat     2267 b- defN 22-Jan-29 17:11 ThymeBoost/fit_components/fit_seasonality.py
 -rw-rw-rw-  2.0 fat     8253 b- defN 22-Jan-29 22:16 ThymeBoost/fit_components/fit_trend.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Sep-12 17:06 ThymeBoost/fitter/__init__.py
--rw-rw-rw-  2.0 fat     6283 b- defN 22-Jan-29 18:09 ThymeBoost/fitter/booster.py
+-rw-rw-rw-  2.0 fat     6980 b- defN 22-Jan-30 17:16 ThymeBoost/fitter/booster.py
 -rw-rw-rw-  2.0 fat     5703 b- defN 22-Jan-29 18:09 ThymeBoost/fitter/decompose.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Aug-29 15:57 ThymeBoost/seasonality_models/__init__.py
 -rw-rw-rw-  2.0 fat     1909 b- defN 21-Sep-05 18:32 ThymeBoost/seasonality_models/classic_seasonality.py
 -rw-rw-rw-  2.0 fat     3362 b- defN 21-Nov-01 22:27 ThymeBoost/seasonality_models/fourier_seasonality.py
 -rw-rw-rw-  2.0 fat     1787 b- defN 22-Jan-29 18:10 ThymeBoost/seasonality_models/naive_seasonality.py
 -rw-rw-rw-  2.0 fat     1801 b- defN 21-Sep-11 16:43 ThymeBoost/seasonality_models/seasonality_base_class.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Aug-25 15:51 ThymeBoost/trend_models/__init__.py
@@ -38,17 +38,17 @@
 -rw-rw-rw-  2.0 fat     1441 b- defN 22-Jan-29 22:41 ThymeBoost/trend_models/moving_average_trend.py
 -rw-rw-rw-  2.0 fat     3467 b- defN 22-Jan-29 22:44 ThymeBoost/trend_models/ransac_trend.py
 -rw-rw-rw-  2.0 fat     2278 b- defN 22-Jan-29 22:08 ThymeBoost/trend_models/svr_trend.py
 -rw-rw-rw-  2.0 fat     1209 b- defN 22-Jan-22 01:18 ThymeBoost/trend_models/theta_trend.py
 -rw-rw-rw-  2.0 fat     2387 b- defN 22-Jan-22 01:08 ThymeBoost/trend_models/trend_base_class.py
 -rw-rw-rw-  2.0 fat      993 b- defN 22-Jan-29 22:39 ThymeBoost/trend_models/zero_trend.py
 -rw-rw-rw-  2.0 fat       27 b- defN 21-Oct-17 19:50 ThymeBoost/utils/__init__.py
--rw-rw-rw-  2.0 fat     6681 b- defN 22-Jan-02 16:40 ThymeBoost/utils/build_output.py
+-rw-rw-rw-  2.0 fat     6681 b- defN 22-Jan-30 23:53 ThymeBoost/utils/build_output.py
 -rw-rw-rw-  2.0 fat     1227 b- defN 21-Dec-26 19:22 ThymeBoost/utils/get_complexity.py
--rw-rw-rw-  2.0 fat     3944 b- defN 21-Sep-06 01:40 ThymeBoost/utils/plotting.py
+-rw-rw-rw-  2.0 fat     4925 b- defN 22-Feb-02 00:32 ThymeBoost/utils/plotting.py
 -rw-rw-rw-  2.0 fat      999 b- defN 21-Dec-14 13:39 ThymeBoost/utils/trend_dampen.py
--rw-rw-rw-  2.0 fat     1079 b- defN 22-Jan-29 22:59 ThymeBoost-0.1.8.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat    22183 b- defN 22-Jan-29 22:59 ThymeBoost-0.1.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 22-Jan-29 22:59 ThymeBoost-0.1.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 22-Jan-29 22:59 ThymeBoost-0.1.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4799 b- defN 22-Jan-29 22:59 ThymeBoost-0.1.8.dist-info/RECORD
-52 files, 159978 bytes uncompressed, 44551 bytes compressed:  72.2%
+-rw-rw-rw-  2.0 fat     1079 b- defN 22-Mar-14 04:16 ThymeBoost-0.1.9.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat    22190 b- defN 22-Mar-14 04:16 ThymeBoost-0.1.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 22-Mar-14 04:16 ThymeBoost-0.1.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 22-Mar-14 04:16 ThymeBoost-0.1.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4799 b- defN 22-Mar-14 04:16 ThymeBoost-0.1.9.dist-info/RECORD
+52 files, 163045 bytes uncompressed, 45184 bytes compressed:  72.3%
```

## zipnote {}

```diff
@@ -135,23 +135,23 @@
 
 Filename: ThymeBoost/utils/plotting.py
 Comment: 
 
 Filename: ThymeBoost/utils/trend_dampen.py
 Comment: 
 
-Filename: ThymeBoost-0.1.8.dist-info/LICENSE.txt
+Filename: ThymeBoost-0.1.9.dist-info/LICENSE.txt
 Comment: 
 
-Filename: ThymeBoost-0.1.8.dist-info/METADATA
+Filename: ThymeBoost-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: ThymeBoost-0.1.8.dist-info/WHEEL
+Filename: ThymeBoost-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: ThymeBoost-0.1.8.dist-info/top_level.txt
+Filename: ThymeBoost-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: ThymeBoost-0.1.8.dist-info/RECORD
+Filename: ThymeBoost-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ThymeBoost/ThymeBoost.py

```diff
@@ -87,15 +87,15 @@
     scale_type : str, optional
         FIXME
         The type of scaling to apply. Options are ['standard', 'log'] for classic
         standardization or taking the log. The default is None.
 
     """
     __framework__ = 'main'
-    version = '0.1.7'
+    version = '0.1.9'
     author = 'Tyler Blume'
 
     def __init__(self,
                  verbose=0,
                  n_split_proposals=10,
                  approximate_splits=True,
                  exclude_splits=None,
@@ -122,14 +122,16 @@
         self.regularization = regularization
         if n_rounds is None:
             n_rounds = -1
         self.n_rounds = n_rounds
         self.smoothed_trend = smoothed_trend
         self.trend_cap_target = None
         self.ensemble_boosters = None
+        self._online_learning_ignore_params = None
+        self.online_learning = None
         self.error_handle = error_handle
 
     def scale_input(self, time_series):
         """
         Simple scaler method to scale and unscale the time series.  
         Used if 'additive' is False.
 
@@ -164,15 +166,14 @@
             elif self.error_handle == 'warn':
                 if not (time_series > 0).all():
                     print('Series can not contain neg. values or 0 for multiplicative fit or log scaling')
                     self.scale_type = None
             else:
                 if not (time_series > 0).all():
                     self.scale_type = None
-
         elif self.scale_type is None:
             pass
         else:
             raise ValueError('Scaler not recognized!')
         return time_series
 
     def unscale_input(self, scaled_series):
@@ -359,16 +360,19 @@
         Returns
         -------
         predicted_output : pd.DataFrame
             The predicted output dataframe.
 
         """
         if future_exogenous is not None:
-            assert len(future_exogenous) == forecast_horizon, 'Given future exogenous not equal to forecast horizon'
-
+            assert len(future_exogenous) == forecast_horizon, 'Given future exogenous len not equal to forecast horizon'
+        if self.online_learning is not None:
+            self._online_learning_ignore_params = {'scale_type': self.scale_type}
+        if self._online_learning_ignore_params is not None:
+            self.scale_type = self._online_learning_ignore_params['scale_type']
         if self.ensemble_boosters is None:
             trend, seas, exo, predictions = predict_rounds(self.booster_obj,
                                                            forecast_horizon,
                                                            trend_penalty,
                                                            future_exogenous,
                                                            )
             fitted_output = copy.deepcopy(fitted_output)
@@ -599,14 +603,15 @@
         Returns
         -------
         full_output : TYPE
             DESCRIPTION.
 
         """
         original_booster = self.booster_obj
+        self.online_learning = True
         predictions = self.predict(output, len(new_input), uncertainty=False)
         update_series = output['y'] - output['yhat']
         update_series = update_series.append(new_input - predictions['predictions'])
         updated_output = self.fit(update_series, **self._params)
         predictions.columns = ['yhat',
                                'trend',
                                'seasonality',
@@ -614,15 +619,16 @@
                                'yhat_upper',
                                'yhat_lower'
                                 ]
         predictions['y'] = new_input
         predictions = predictions[output.columns]
         full_output = output.append(predictions)
         full_output = full_output + updated_output
-        self.booster_obj.i = original_booster.i + self.booster_obj.i
+        self.booster_obj = original_booster + self.booster_obj
+        self.online_learning = None
         return full_output
 
     def ensemble(self,
                  time_series,
                  verbose=0,
                  **kwargs):
         """
@@ -738,7 +744,8 @@
         Returns
         -------
         None.
 
         """
         plotting.plot_components(fitted, predicted, figsize)
 
+
```

## ThymeBoost/optimizer.py

```diff
@@ -95,17 +95,22 @@
     def fit(self):
         #This needs to be refactored
         self.parameters = self.get_search_space()
         self.set_optimization_metric()
         results = {}
         for num_steps in range(1, self.optimization_steps + 1):
             y_copy = self.y.copy(deep=True)
-            test_y = y_copy[-self.lag - num_steps + 1:]
-            train_y = y_copy[:-self.lag - num_steps + 1]
-            test_y = test_y[:self.lag]
+            if self.optimization_strategy == 'cv':
+                test_y = y_copy[-self.lag * num_steps + 1:]
+                train_y = y_copy[:-self.lag * num_steps + 1]
+                test_y = test_y[:self.lag]
+            else:
+                test_y = y_copy[-self.lag - num_steps + 1:]
+                train_y = y_copy[:-self.lag - num_steps + 1]
+                test_y = test_y[:self.lag]
             results[str(num_steps)] = {}
             if self.verbose:
                 param_iters = tqdm(self.parameters)
             else:
                 param_iters = self.parameters
             for settings in param_iters:
                 # try:
@@ -142,21 +147,22 @@
 
                         params = copy.deepcopy(run_settings)
                     predicted = predicted_output['predictions']
                     if self.test_set == 'all':
                         test_error = self.optimization_metric(actuals=test_y,
                                                               predicted=predicted)
                     elif self.test_set == 'last':
-                        test_error = self.optimization_metric(actuals=test_y[-1],
-                                                              predicted=predicted[-1])
+                        test_error = self.optimization_metric(actuals=test_y.iloc[-1],
+                                                              predicted=predicted.iloc[-1])
                     key = ','.join(map(str, run_settings.values()))
                     results[str(num_steps)][key] = {}
                     results[str(num_steps)][key]['error'] = test_error
                     params.update(ensemble_dict)
                     results[str(num_steps)][key]['params'] = params
+                    results[str(num_steps)][key]['predictions'] = predicted
                 # except Exception as e:
                 #     results[str(num_steps)][','.join(map(str, run_settings))] = np.inf
                 #     print(f'{e} Error running settings: {run_settings}')
                 #     traceback.print_exc()
         return results
 
     def optimize(self):
@@ -167,17 +173,22 @@
             for step in opt_results.keys():
                 summation += opt_results[step][key]['error']
             average_result[key] = summation / len(opt_results.keys())
         average_result = pd.Series(average_result)
         average_result = average_result.sort_values()
         best_setting = average_result.index[0]
         self.run_settings = opt_results['1'][best_setting]['params']
+        self.cv_predictions = []
+        for k, v in opt_results.items():
+            self.cv_predictions.append(opt_results[k][best_setting]['predictions'])
         ensemble, _ = Optimizer.combiner_check(self.run_settings, wrap_values=False)
         if ensemble:
             output = self.model_object.ensemble(self.y, **self.run_settings)
         else:
             output = self.model_object.fit(self.y, **self.run_settings)
         if self.verbose:
             print(f'Optimal model configuration: {self.run_settings}')
             print(f'Params ensembled: {ensemble}')
         return output
 
+
+
```

## ThymeBoost/predict_functions.py

```diff
@@ -1,11 +1,12 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 import pandas as pd
+import matplotlib.pyplot as plt
 from ThymeBoost.utils import trend_dampen
 
 
 def predict_trend(booster_obj, boosting_round, forecast_horizon, trend_penalty):
     """
     Predict the trend component using the booster
```

## ThymeBoost/exogenous_models/decision_tree_exogenous.py

```diff
@@ -1,10 +1,11 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
+import pandas as pd
 from sklearn.tree import DecisionTreeRegressor
 from ThymeBoost.exogenous_models.exogenous_base_class import ExogenousBaseModel
 
 
 class DecisionTree(ExogenousBaseModel):
     model = 'decision_tree'
 
@@ -17,9 +18,10 @@
         exo_model = DecisionTreeRegressor(max_depth=tree_depth)
         self.model_obj = exo_model.fit(X, y)
         self.fitted = self.model_obj.predict(X)
         #exo_impact = (exo_model.params, fitted_model.cov_params())
         return self.fitted
 
     def predict(self, future_exogenous):
-        future_exogenous = np.array(future_exogenous).reshape((-1, 1))
+        if isinstance(future_exogenous, pd.DataFrame):
+            future_exogenous = future_exogenous.to_numpy()
         return self.model_obj.predict(future_exogenous)
```

## ThymeBoost/exogenous_models/glm_exogenous.py

```diff
@@ -1,10 +1,11 @@
 # -*- coding: utf-8 -*-
 
 import statsmodels.api as sm
+import pandas as pd
 from ThymeBoost.exogenous_models.exogenous_base_class import ExogenousBaseModel
 
 class GLM(ExogenousBaseModel):
     model = 'glm'
 
     def __init__(self):
         self.model_obj = None
@@ -14,8 +15,10 @@
         exo_model = sm.GLM(y, X)
         self.model_obj = exo_model.fit()
         self.fitted = self.model_obj.predict(X)
         #exo_impact = (exo_model.params, fitted_model.cov_params())
         return self.fitted
 
     def predict(self, future_exogenous):
+        if isinstance(future_exogenous, pd.DataFrame):
+            future_exogenous = future_exogenous.to_numpy()
         return self.model_obj.predict(future_exogenous)
```

## ThymeBoost/exogenous_models/ols_exogenous.py

```diff
@@ -1,9 +1,11 @@
 # -*- coding: utf-8 -*-
 import statsmodels.api as sm
+import numpy as np
+import pandas as pd
 from ThymeBoost.exogenous_models.exogenous_base_class import ExogenousBaseModel
 
 
 class OLS(ExogenousBaseModel):
     model = 'ols'
 
     def __init__(self):
@@ -14,8 +16,10 @@
         exo_model = sm.OLS(y, X)
         self.model_obj = exo_model.fit()
         self.fitted = self.model_obj.predict(X)
         #exo_impact = (exo_model.params, fitted_model.cov_params())
         return self.fitted
 
     def predict(self, future_exogenous):
+        if isinstance(future_exogenous, pd.DataFrame):
+            future_exogenous = future_exogenous.to_numpy()
         return self.model_obj.predict(future_exogenous)
```

## ThymeBoost/fit_components/fit_exogenous.py

```diff
@@ -1,9 +1,10 @@
 # -*- coding: utf-8 -*-
 import numpy as np
+import pandas as pd
 from ThymeBoost.exogenous_models import (ols_exogenous,
                                          decision_tree_exogenous,
                                          glm_exogenous)
 
 
 class FitExogenous:
     def __init__(self,
@@ -24,10 +25,11 @@
             fit_obj = decision_tree_exogenous.DecisionTree
         else:
             raise NotImplementedError('That Exo estimation is not availale yet, add it to the road map!')
         return fit_obj
 
     def fit_exogenous_component(self, time_residual, exogenous):
         self.model_obj = self.set_estimator(self.exo_estimator)()
-        exogenous = np.array(exogenous).reshape((-1, 1))
+        if isinstance(exogenous, pd.DataFrame):
+            exogenous = exogenous.to_numpy()
         exo_fitted = self.model_obj.fit(time_residual, exogenous, **self.kwargs)
         return self.exogenous_lr*np.array(exo_fitted)
```

## ThymeBoost/fitter/booster.py

```diff
@@ -39,14 +39,29 @@
         self.cost_penalty = cost_penalty
         self.normalize_seasonality = normalize_seasonality
         self.regularization = regularization
         self.n_rounds = n_rounds
         self.smoothed_trend = smoothed_trend
         self.additive = additive
 
+    def __add__(self, booster_obj):
+        self.i += booster_obj.i
+        self.trend_objs += booster_obj.trend_objs
+        self.seasonal_objs += booster_obj.seasonal_objs
+        self.exo_objs += booster_obj.exo_objs
+        self.trend_pred_params += booster_obj.trend_pred_params
+        self.seasonal_pred_params += booster_obj.seasonal_pred_params
+        self.exo_pred_params += booster_obj.exo_pred_params
+        self.trends += booster_obj.trends
+        self.seasonalities += booster_obj.seasonalities
+        self.errors += booster_obj.errors
+        self.fitted_exogenous += booster_obj.fitted_exogenous
+        self.additive = booster_obj.additive
+        return self
+
     def initialize_booster_values(self):
         self.split = None
         self.i = -1
         self.trend_objs = []
         self.seasonal_objs = []
         self.exo_objs = []
         self.trend_pred_params = []
```

## ThymeBoost/utils/plotting.py

```diff
@@ -1,11 +1,11 @@
 # -*- coding: utf-8 -*-
 
 import matplotlib.pyplot as plt
-
+from itertools import cycle
 
 def plot_components(fitted_df, predicted_df, figsize):
     """Simple plot of components for convenience"""
     if predicted_df is not None:
         rename_dict = {'predictions': 'yhat',
                        'predicted_trend': 'trend',
                        'predicted_seasonality': 'seasonality',
@@ -69,14 +69,36 @@
                         color='orange')
     ax.set_title('ThymeBoost Results')
     if 'outliers' in fitted_df.columns:
         outlier_df = fitted_df[fitted_df['outliers'] == True]
         ax.scatter(outlier_df.index, outlier_df['y'], marker='x', color='red')
     plt.show()
 
+def plot_optimization(fitted_df, opt_predictions, opt_type, figsize):
+    """Simple plot of optimization results for convenience"""
+    step_colors = ['tab:blue',
+                   'tab:orange',
+                   'tab:green',
+                   'tab:red',
+                   'tab:purple',
+                   'tab:cyan']
+    step_colors = cycle(step_colors)
+    min_opt_idx = opt_predictions[-1].index[0]
+    fitted_y = fitted_df['yhat'].loc[:min_opt_idx]
+    fig, ax = plt.subplots(figsize=figsize)
+    ax.plot(fitted_df['y'], color='black')
+    ax.plot(fitted_y, color='orange')
+    for idx, i in enumerate(opt_predictions):
+        section_color = next(step_colors)
+        ax.plot(i, color=section_color, linestyle='dashed')
+        if opt_type in ['holdout', 'cv']:
+            ax.axvspan(i.index[0], i.index[-1], alpha=0.2, color=section_color, linestyle='dashed')
+    ax.set_title('ThymeBoost Optimization')
+    plt.show()
+
 # def plot_rounds(self, figsize = (6,4)):
 #     if self.exogenous is not None:
 #         fig, ax = plt.subplots(3, figsize = figsize)
 #         for iteration in range(len(self.fitted_exogenous)-1):
 #             ax[2].plot(
 #                     np.sum(self.fitted_exogenous[:iteration], axis=0),
 #                     label=iteration
```

## Comparing `ThymeBoost-0.1.8.dist-info/LICENSE.txt` & `ThymeBoost-0.1.9.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `ThymeBoost-0.1.8.dist-info/METADATA` & `ThymeBoost-0.1.9.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ThymeBoost
-Version: 0.1.8
+Version: 0.1.9
 Summary: Gradient boosted time series decomposition for forecasting.
 Home-page: https://github.com/tblume1992/ThymeBoost
 Author: Tyler Blume
 Author-email: tblume@mail.USF.edu
 License: UNKNOWN
 Keywords: forecasting,time series,seasonality,trend
 Platform: UNKNOWN
@@ -19,15 +19,15 @@
 Requires-Dist: scikit-learn
 Requires-Dist: scipy
 Requires-Dist: more-itertools
 Requires-Dist: matplotlib
 Requires-Dist: tqdm
 Requires-Dist: pmdarima
 
-# ThymeBoost
+# ThymeBoost v0.1.8
 
 [![Documentation Status](https://readthedocs.org/projects/thymeboost/badge/?version=latest)](https://thymeboost.readthedocs.io/en/latest/?badge=latest) [![Downloads](https://pepy.tech/badge/thymeboost)](https://pepy.tech/project/thymeboost)
 
 ThymeBoost combines time series decomposition with gradient boosting to provide a flexible mix-and-match time series framework for forecasting. At the most granular level are the trend/level (going forward this is just referred to as 'trend') models, seasonal models, and endogenous models. These are used to approximate the respective components at each 'boosting round' and sequential rounds are fit on residuals in usual boosting fashion.
 
 Documentation is under construction at : https://thymeboost.readthedocs.io/en/latest/
```

## Comparing `ThymeBoost-0.1.8.dist-info/RECORD` & `ThymeBoost-0.1.9.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-ThymeBoost/ThymeBoost.py,sha256=ggMPjiQSbEbK0wA530VZaNXuDmnj8fjhx_uHpnUiu5s,30591
+ThymeBoost/ThymeBoost.py,sha256=1tKm4K9YKKNPhrKSkOsRYXwf26poLYhFNXmJeQdC3PI,31021
 ThymeBoost/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 ThymeBoost/cost_functions.py,sha256=m13l8teWDWn038ap0HebYjw90Gd96W0z_vlx-LYbFk0,2757
 ThymeBoost/ensemble.py,sha256=z0WQqFY1AYSQrDeJabbex4APelJ96Ur1OSHI8FnR2mk,1775
-ThymeBoost/optimizer.py,sha256=NbTAe8MWbMfaSNOKCws2Nxq4FT39yjiwXKGhWx8rjMg,8163
+ThymeBoost/optimizer.py,sha256=hQenoJBTQn85q--juUQlYrwnhntOIne8x_9Aw4m2K48,8665
 ThymeBoost/param_iterator.py,sha256=9VrwNPoX5hCM5EvJrQFEg2FtJhiW55aVbFtE-684DI8,2935
-ThymeBoost/predict_functions.py,sha256=s7ZhQ7XTjvlgDGoZP4kg0r8h4O3wvQQt4_yDCVwL9bc,4333
+ThymeBoost/predict_functions.py,sha256=tzHsIaI36diUT8kFyzvzR21EbXQYb3xggWgP7_D6cSE,4366
 ThymeBoost/split_proposals.py,sha256=2YrNhGXUaOnleAyl86tjbYQti-gsOuk5yqacHfVCA8g,3161
 ThymeBoost/Datasets/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 ThymeBoost/Datasets/examples.py,sha256=7voG1WRYR_AQkLXJMQSP_QzbZSUKelCIGI7sJoOVcrc,245
 ThymeBoost/exogenous_models/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
-ThymeBoost/exogenous_models/decision_tree_exogenous.py,sha256=lLWE7yxvBGoTL2BCdQeAFz0Ivoiqn3hxezPvkL93fa8,847
+ThymeBoost/exogenous_models/decision_tree_exogenous.py,sha256=dglnSDvxRQuDvBTp54BCsDelArRf_NeijXyeuybKfeY,912
 ThymeBoost/exogenous_models/exogenous_base_class.py,sha256=MvnuN-l5XieXVUkxokdmuGbQyrQZCBQVFiATAuZiaLA,1634
-ThymeBoost/exogenous_models/glm_exogenous.py,sha256=l11ZyeARAA3mawj8Jw3BbfadU4N_oHwmvwNFRx9iLGM,636
-ThymeBoost/exogenous_models/ols_exogenous.py,sha256=mKIUl__UOnW4P8cGJor1U1gjhbSew1ifqiaGNBpFWm4,638
+ThymeBoost/exogenous_models/glm_exogenous.py,sha256=0McNczxuVa8CZePV88x6XVJ9zuIrXPx4WX5jzc1IXOw,773
+ThymeBoost/exogenous_models/ols_exogenous.py,sha256=2Qe-UkNA9oAm2vDCIdCLcD_z8pcBKy1Nie8nn1S1wQ0,795
 ThymeBoost/fit_components/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
-ThymeBoost/fit_components/fit_exogenous.py,sha256=5nay3y9lcZ3KRgUrjUiSFDvRjPqQNM26Sh__pmNMWgI,1312
+ThymeBoost/fit_components/fit_exogenous.py,sha256=4l-CGIs0Jdh0TGw20vCEJt0C8Jj_bG6Oj6SgrS1L-8I,1370
 ThymeBoost/fit_components/fit_seasonality.py,sha256=x_mG5q0Pp7T77Ue1FGS699xgqlZhP7TQ63mgWsCgH2Q,2267
 ThymeBoost/fit_components/fit_trend.py,sha256=ETAKRjpJcpPG6bMPj7Png92OuqO35OOtGUjUucNkWJk,8253
 ThymeBoost/fitter/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
-ThymeBoost/fitter/booster.py,sha256=IrrYGL1CI_bo5-jVzao2jN5dbEG7ESpsPEf4gzbnjsw,6283
+ThymeBoost/fitter/booster.py,sha256=0x-IZZdBWsJzr_lhFcVHvIbG62Nr9wCvhsdLIcbtBRw,6980
 ThymeBoost/fitter/decompose.py,sha256=nHzoycIox1i69FM2fzoETnGfN1bcOxHdIiLKF_hYJB0,5703
 ThymeBoost/seasonality_models/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 ThymeBoost/seasonality_models/classic_seasonality.py,sha256=z3AMwLgCWayTcqusarBDA9rt3wueeTjjDzGteL4FVuU,1909
 ThymeBoost/seasonality_models/fourier_seasonality.py,sha256=RdJtxCiqy9tJSqRtFtXkUUlQDSkkbEzoVUjkysa09No,3362
 ThymeBoost/seasonality_models/naive_seasonality.py,sha256=iNfas_wrbYW31JnV6i2-s3g_h3wNy0BKkAjFqBdteXQ,1787
 ThymeBoost/seasonality_models/seasonality_base_class.py,sha256=0vYctNRoR-Q0P1P4S_84GbvVuaGeWCF_jeL-Iiin-Ho,1801
 ThymeBoost/trend_models/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
@@ -39,14 +39,14 @@
 ThymeBoost/trend_models/svr_trend.py,sha256=0w3TxVnN5AvumA1iyhyC_ojc64TriTaAa5ILAPPv3P4,2278
 ThymeBoost/trend_models/theta_trend.py,sha256=ampFsiJDLsJTE_5BEnEwkrtEfBypB2vO4YtQMX8IsRY,1209
 ThymeBoost/trend_models/trend_base_class.py,sha256=dR-a_unO1efqV3bMGYOzMNWcE1tS8Kw4czmqDUJ2yGQ,2387
 ThymeBoost/trend_models/zero_trend.py,sha256=15wX36VYS4hApeQUR7WyC4XwJB6JsUmwHotF6HNOZPo,993
 ThymeBoost/utils/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 ThymeBoost/utils/build_output.py,sha256=aybrCv7yH6g05g2k8Db0pPjIZIeK77CspVSgdEJTxec,6681
 ThymeBoost/utils/get_complexity.py,sha256=Xu2anF35H2ms6ruQKCEqFqFoqcmPKv2-blm5xHQSYCc,1227
-ThymeBoost/utils/plotting.py,sha256=5zfZcKsng3gBIEPKpbv4kLLY1xci3F247wSEQgeApA8,3944
+ThymeBoost/utils/plotting.py,sha256=oyhUCLoAWJboVJ_SOtubox6yD2lcL_DxFRJNW7PgXzM,4925
 ThymeBoost/utils/trend_dampen.py,sha256=0O6PhHQ-MAdEtsK4urcJyiRFAscWlDGtL9etnEypWgY,999
-ThymeBoost-0.1.8.dist-info/LICENSE.txt,sha256=rP8IPbkJympkS9v8KaBoTy1gznzycyC0-UilArC7SWU,1079
-ThymeBoost-0.1.8.dist-info/METADATA,sha256=V82kYSe04m3IEBA0YbaF9BY2rIRg7VKW9lOy0TuZkTs,22183
-ThymeBoost-0.1.8.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
-ThymeBoost-0.1.8.dist-info/top_level.txt,sha256=oVpDNDMoVW_6lNURRAjEGXOqqNArp636DukiC83XfkU,11
-ThymeBoost-0.1.8.dist-info/RECORD,,
+ThymeBoost-0.1.9.dist-info/LICENSE.txt,sha256=rP8IPbkJympkS9v8KaBoTy1gznzycyC0-UilArC7SWU,1079
+ThymeBoost-0.1.9.dist-info/METADATA,sha256=iETZP0XEjzD9rfPK3F93sJr3FbbkeNLH0Ftg0qH4C2M,22190
+ThymeBoost-0.1.9.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
+ThymeBoost-0.1.9.dist-info/top_level.txt,sha256=oVpDNDMoVW_6lNURRAjEGXOqqNArp636DukiC83XfkU,11
+ThymeBoost-0.1.9.dist-info/RECORD,,
```

