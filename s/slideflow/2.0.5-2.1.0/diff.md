# Comparing `tmp/slideflow-2.0.5-py3-none-any.whl.zip` & `tmp/slideflow-2.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,27 +1,27 @@
-Zip file size: 1849591 bytes, number of entries: 362
+Zip file size: 1888414 bytes, number of entries: 371
 -rw-rw-r--  2.0 unx     1411 b- defN 23-Apr-09 06:29 slideflow/__init__.py
 -rw-rw-r--  2.0 unx     1662 b- defN 23-Apr-09 06:29 slideflow/_backend.py
--rw-rw-r--  2.0 unx      497 b- defN 23-May-25 18:41 slideflow/_version.py
--rw-rw-r--  2.0 unx   161693 b- defN 23-May-25 18:12 slideflow/dataset.py
--rw-rw-r--  2.0 unx     3922 b- defN 23-May-25 18:12 slideflow/errors.py
--rw-rw-r--  2.0 unx    40025 b- defN 23-Apr-23 03:48 slideflow/heatmap.py
--rw-rw-r--  2.0 unx    26008 b- defN 23-May-25 18:12 slideflow/mosaic.py
--rw-rw-r--  2.0 unx   171513 b- defN 23-May-25 18:12 slideflow/project.py
--rw-rw-r--  2.0 unx    33540 b- defN 23-May-25 17:18 slideflow/project_utils.py
+-rw-rw-r--  2.0 unx      497 b- defN 23-Aug-02 19:56 slideflow/_version.py
+-rw-rw-r--  2.0 unx   168259 b- defN 23-Aug-02 19:14 slideflow/dataset.py
+-rw-rw-r--  2.0 unx     4029 b- defN 23-Aug-02 17:19 slideflow/errors.py
+-rw-rw-r--  2.0 unx    40271 b- defN 23-Aug-02 17:19 slideflow/heatmap.py
+-rw-rw-r--  2.0 unx    26054 b- defN 23-Aug-02 19:48 slideflow/mosaic.py
+-rw-rw-r--  2.0 unx   174290 b- defN 23-Aug-02 19:14 slideflow/project.py
+-rw-rw-r--  2.0 unx    33540 b- defN 23-Jun-08 13:54 slideflow/project_utils.py
 -rw-rw-r--  2.0 unx      978 b- defN 22-Jul-18 12:12 slideflow/sample_actions.py
 -rw-rw-r--  2.0 unx     1195 b- defN 23-Apr-09 06:29 slideflow/biscuit/__init__.py
 -rw-rw-r--  2.0 unx     4281 b- defN 23-Apr-09 06:29 slideflow/biscuit/delong.py
 -rw-rw-r--  2.0 unx      318 b- defN 23-Apr-09 06:29 slideflow/biscuit/errors.py
--rw-rw-r--  2.0 unx    46859 b- defN 23-May-25 17:18 slideflow/biscuit/experiment.py
+-rw-rw-r--  2.0 unx    46859 b- defN 23-Jun-08 13:54 slideflow/biscuit/experiment.py
 -rw-rw-r--  2.0 unx     1260 b- defN 23-Apr-09 06:29 slideflow/biscuit/hp.py
 -rw-rw-r--  2.0 unx    21373 b- defN 23-Apr-09 06:29 slideflow/biscuit/threshold.py
 -rw-rw-r--  2.0 unx    17143 b- defN 23-Apr-09 06:29 slideflow/biscuit/utils.py
--rw-rw-r--  2.0 unx    26315 b- defN 23-May-25 18:12 slideflow/cellseg/__init__.py
--rw-rw-r--  2.0 unx     5454 b- defN 23-May-25 18:12 slideflow/cellseg/seg_utils.py
+-rw-rw-r--  2.0 unx    26398 b- defN 23-Aug-02 17:19 slideflow/cellseg/__init__.py
+-rw-rw-r--  2.0 unx     5444 b- defN 23-Aug-02 17:19 slideflow/cellseg/seg_utils.py
 -rw-rw-r--  2.0 unx      444 b- defN 23-Apr-09 06:29 slideflow/clam/__init__.py
 -rw-rw-r--  2.0 unx      116 b- defN 22-Oct-06 15:16 slideflow/experimental/__init__.py
 -rw-rw-r--  2.0 unx    17224 b- defN 23-Feb-01 21:03 slideflow/experimental/embedding_search.py
 -rw-rw-r--  2.0 unx      273 b- defN 23-Feb-01 21:03 slideflow/gan/__init__.py
 -rw-rw-r--  2.0 unx    24993 b- defN 23-Apr-09 06:29 slideflow/gan/interpolate.py
 -rw-rw-r--  2.0 unx      949 b- defN 23-Feb-01 21:00 slideflow/gan/utils.py
 -rw-rw-r--  2.0 unx       45 b- defN 22-Oct-22 03:43 slideflow/gan/stylegan2/__init__.py
@@ -85,15 +85,15 @@
 -rw-rw-r--  2.0 unx     6901 b- defN 22-Sep-27 20:15 slideflow/gan/stylegan3/train.py
 -rw-rw-r--  2.0 unx     6095 b- defN 22-Sep-27 20:16 slideflow/gan/stylegan3/visualizer.py
 -rw-rw-r--  2.0 unx       77 b- defN 22-Nov-09 23:18 slideflow/gan/stylegan3/stylegan3/__init__.py
 -rw-rw-r--  2.0 unx     6128 b- defN 22-Oct-22 00:54 slideflow/gan/stylegan3/stylegan3/generate.py
 -rw-rw-r--  2.0 unx    17302 b- defN 22-Nov-05 01:09 slideflow/gan/stylegan3/stylegan3/legacy.py
 -rw-rw-r--  2.0 unx    15354 b- defN 23-Apr-05 13:33 slideflow/gan/stylegan3/stylegan3/train.py
 -rw-rw-r--  2.0 unx     1845 b- defN 22-Oct-22 03:51 slideflow/gan/stylegan3/stylegan3/utils.py
--rw-rw-r--  2.0 unx    13379 b- defN 23-Apr-08 14:23 slideflow/gan/stylegan3/stylegan3/visualizer.py
+-rw-rw-r--  2.0 unx    13387 b- defN 23-Jul-11 20:52 slideflow/gan/stylegan3/stylegan3/visualizer.py
 -rw-rw-r--  2.0 unx      488 b- defN 22-Aug-03 00:08 slideflow/gan/stylegan3/stylegan3/dnnlib/__init__.py
 -rw-rw-r--  2.0 unx    17870 b- defN 22-Dec-06 01:04 slideflow/gan/stylegan3/stylegan3/dnnlib/util.py
 -rw-rw-r--  2.0 unx     3153 b- defN 22-Oct-22 02:38 slideflow/gan/stylegan3/stylegan3/embedding/__init__.py
 -rw-rw-r--  2.0 unx      448 b- defN 22-Aug-03 00:08 slideflow/gan/stylegan3/stylegan3/gui_utils/__init__.py
 -rw-rw-r--  2.0 unx    16361 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/gui_utils/gl_utils.py
 -rw-rw-r--  2.0 unx     7848 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/gui_utils/glfw_window.py
 -rw-rw-r--  2.0 unx     7648 b- defN 22-Oct-28 14:24 slideflow/gan/stylegan3/stylegan3/gui_utils/imgui_utils.py
@@ -144,145 +144,153 @@
 -rw-rw-r--  2.0 unx      448 b- defN 22-Aug-03 00:08 slideflow/gan/stylegan3/stylegan3/viz/__init__.py
 -rw-rw-r--  2.0 unx     3694 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/capture_widget.py
 -rw-rw-r--  2.0 unx     5884 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/equivariance_widget.py
 -rw-rw-r--  2.0 unx     4414 b- defN 23-Apr-03 13:00 slideflow/gan/stylegan3/stylegan3/viz/latent_widget.py
 -rw-rw-r--  2.0 unx     9520 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/layer_widget.py
 -rw-rw-r--  2.0 unx     3566 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/performance_widget.py
 -rw-rw-r--  2.0 unx     9460 b- defN 23-Apr-05 13:33 slideflow/gan/stylegan3/stylegan3/viz/pickle_widget.py
--rw-rw-r--  2.0 unx    24417 b- defN 22-Oct-30 14:59 slideflow/gan/stylegan3/stylegan3/viz/renderer.py
+-rw-rw-r--  2.0 unx    25341 b- defN 23-Jul-11 20:52 slideflow/gan/stylegan3/stylegan3/viz/renderer.py
 -rw-rw-r--  2.0 unx     5573 b- defN 23-Apr-03 13:00 slideflow/gan/stylegan3/stylegan3/viz/stylemix_widget.py
 -rw-rw-r--  2.0 unx     5825 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/thumb_widget.py
 -rw-rw-r--  2.0 unx     3845 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/trunc_noise_widget.py
 -rw-rw-r--  2.0 unx    15662 b- defN 23-Apr-09 06:29 slideflow/grad/__init__.py
 -rw-rw-r--  2.0 unx     7376 b- defN 23-Feb-01 21:03 slideflow/grad/plot_utils.py
--rw-rw-r--  2.0 unx    11715 b- defN 23-May-25 18:12 slideflow/io/__init__.py
+-rw-rw-r--  2.0 unx    12623 b- defN 23-Aug-02 17:19 slideflow/io/__init__.py
 -rw-rw-r--  2.0 unx    10541 b- defN 22-Jul-18 12:12 slideflow/io/gaussian.py
 -rw-rw-r--  2.0 unx     9918 b- defN 23-Apr-09 06:29 slideflow/io/io_utils.py
--rw-rw-r--  2.0 unx    34511 b- defN 23-May-25 18:12 slideflow/io/tensorflow.py
--rw-rw-r--  2.0 unx    45085 b- defN 23-May-25 18:12 slideflow/io/torch.py
+-rw-rw-r--  2.0 unx    39451 b- defN 23-Aug-02 17:19 slideflow/io/tensorflow.py
+-rw-rw-r--  2.0 unx    53442 b- defN 23-Aug-02 17:19 slideflow/io/torch.py
 -rw-rw-r--  2.0 unx       68 b- defN 23-Mar-13 02:04 slideflow/io/preservedsite/__init__.py
 -rw-rw-r--  2.0 unx     8419 b- defN 23-Mar-13 02:04 slideflow/io/preservedsite/crossfolds.py
--rw-rw-r--  2.0 unx      284 b- defN 23-Apr-09 06:29 slideflow/mil/__init__.py
--rw-rw-r--  2.0 unx    14819 b- defN 23-Apr-09 06:29 slideflow/mil/_params.py
--rw-rw-r--  2.0 unx     5160 b- defN 23-Apr-14 13:28 slideflow/mil/data.py
--rw-rw-r--  2.0 unx    15507 b- defN 23-May-17 21:15 slideflow/mil/eval.py
+-rw-rw-r--  2.0 unx      300 b- defN 23-Aug-02 17:19 slideflow/mil/__init__.py
+-rw-rw-r--  2.0 unx    14819 b- defN 23-Jun-09 15:24 slideflow/mil/_params.py
+-rw-rw-r--  2.0 unx     5292 b- defN 23-Aug-02 17:19 slideflow/mil/data.py
+-rw-rw-r--  2.0 unx    19380 b- defN 23-Aug-02 17:19 slideflow/mil/eval.py
+-rw-rw-r--  2.0 unx     5121 b- defN 23-Aug-02 17:19 slideflow/mil/utils.py
 -rw-rw-r--  2.0 unx     3890 b- defN 23-Apr-09 06:29 slideflow/mil/clam/__init__.py
 -rw-rw-r--  2.0 unx     4334 b- defN 23-Apr-09 06:29 slideflow/mil/clam/create_attention.py
 -rw-rw-r--  2.0 unx     1131 b- defN 23-Apr-09 06:29 slideflow/mil/clam/datasets/__init__.py
 -rw-rw-r--  2.0 unx    14990 b- defN 23-Apr-09 06:29 slideflow/mil/clam/datasets/dataset_generic.py
 -rw-rw-r--  2.0 unx     5914 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/__init__.py
 -rw-rw-r--  2.0 unx    19782 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/core_utils.py
 -rw-rw-r--  2.0 unx     4150 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/eval_utils.py
 -rw-rw-r--  2.0 unx     1287 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/file_utils.py
--rw-rw-r--  2.0 unx     3497 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/loss_utils.py
--rw-rw-r--  2.0 unx      185 b- defN 23-Apr-09 06:29 slideflow/mil/models/__init__.py
+-rw-rw-r--  2.0 unx     3497 b- defN 23-Jun-09 15:24 slideflow/mil/clam/utils/loss_utils.py
+-rw-rw-r--  2.0 unx      185 b- defN 23-Jun-09 15:24 slideflow/mil/models/__init__.py
 -rw-rw-r--  2.0 unx      378 b- defN 23-Apr-09 06:29 slideflow/mil/models/_utils.py
 -rw-rw-r--  2.0 unx     3239 b- defN 23-Apr-21 21:43 slideflow/mil/models/att_mil.py
 -rw-rw-r--  2.0 unx    12816 b- defN 23-Apr-09 06:29 slideflow/mil/models/clam.py
 -rw-rw-r--  2.0 unx     3865 b- defN 23-Apr-18 22:07 slideflow/mil/models/mil_fc.py
 -rw-rw-r--  2.0 unx     4137 b- defN 23-Apr-09 06:29 slideflow/mil/models/transmil.py
--rw-rw-r--  2.0 unx    15008 b- defN 23-May-17 02:53 slideflow/mil/train/__init__.py
--rw-rw-r--  2.0 unx     8214 b- defN 23-May-25 18:12 slideflow/mil/train/_fastai.py
+-rw-rw-r--  2.0 unx    18243 b- defN 23-Aug-02 17:19 slideflow/mil/train/__init__.py
+-rw-rw-r--  2.0 unx     9714 b- defN 23-Aug-02 17:19 slideflow/mil/train/_fastai.py
 -rw-rw-r--  2.0 unx     7795 b- defN 23-Apr-09 06:29 slideflow/mil/train/_legacy.py
--rw-rw-r--  2.0 unx     7485 b- defN 23-Apr-09 06:29 slideflow/model/__init__.py
+-rw-rw-r--  2.0 unx     7504 b- defN 23-Aug-02 17:19 slideflow/model/__init__.py
 -rw-rw-r--  2.0 unx     1879 b- defN 23-Mar-13 02:04 slideflow/model/adv_utils.py
--rw-rw-r--  2.0 unx    23588 b- defN 23-Apr-19 02:19 slideflow/model/base.py
--rw-rw-r--  2.0 unx    55541 b- defN 23-May-25 18:12 slideflow/model/features.py
--rw-rw-r--  2.0 unx   110776 b- defN 23-May-25 18:12 slideflow/model/tensorflow.py
+-rw-rw-r--  2.0 unx    22346 b- defN 23-Aug-02 19:14 slideflow/model/base.py
+-rw-rw-r--  2.0 unx    67879 b- defN 23-Aug-02 19:48 slideflow/model/features.py
+-rw-rw-r--  2.0 unx   107500 b- defN 23-Aug-02 19:37 slideflow/model/tensorflow.py
 -rw-rw-r--  2.0 unx    22547 b- defN 23-Apr-03 05:27 slideflow/model/tensorflow_utils.py
--rw-rw-r--  2.0 unx   103271 b- defN 23-May-25 18:12 slideflow/model/torch.py
--rw-rw-r--  2.0 unx    17005 b- defN 23-May-25 18:12 slideflow/model/torch_utils.py
--rw-rw-r--  2.0 unx      394 b- defN 23-Apr-09 06:29 slideflow/model/extractors/__init__.py
--rw-rw-r--  2.0 unx     3708 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory.py
--rw-rw-r--  2.0 unx     4544 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory_tensorflow.py
--rw-rw-r--  2.0 unx     5711 b- defN 23-May-25 18:12 slideflow/model/extractors/_factory_torch.py
--rw-rw-r--  2.0 unx     1570 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_registry.py
--rw-rw-r--  2.0 unx     2646 b- defN 23-May-25 18:12 slideflow/model/extractors/_slide.py
--rw-rw-r--  2.0 unx    25681 b- defN 23-May-25 18:12 slideflow/model/extractors/ctranspath.py
--rw-rw-r--  2.0 unx    12045 b- defN 23-May-25 18:12 slideflow/model/extractors/retccl.py
--rw-rw-r--  2.0 unx    26156 b- defN 23-May-25 18:12 slideflow/norm/__init__.py
+-rw-rw-r--  2.0 unx   102386 b- defN 23-Aug-02 17:19 slideflow/model/torch.py
+-rw-rw-r--  2.0 unx    18288 b- defN 23-Aug-02 17:19 slideflow/model/torch_utils.py
+-rw-rw-r--  2.0 unx      575 b- defN 23-Aug-02 17:19 slideflow/model/extractors/__init__.py
+-rw-rw-r--  2.0 unx    10549 b- defN 23-Aug-02 19:23 slideflow/model/extractors/_factory.py
+-rw-rw-r--  2.0 unx     6025 b- defN 23-Aug-02 17:19 slideflow/model/extractors/_factory_tensorflow.py
+-rw-rw-r--  2.0 unx     6797 b- defN 23-Aug-02 17:19 slideflow/model/extractors/_factory_torch.py
+-rw-rw-r--  2.0 unx     1570 b- defN 23-Jul-18 15:26 slideflow/model/extractors/_registry.py
+-rw-rw-r--  2.0 unx     2751 b- defN 23-Jun-30 13:12 slideflow/model/extractors/_slide.py
+-rw-rw-r--  2.0 unx    26421 b- defN 23-Aug-02 17:19 slideflow/model/extractors/ctranspath.py
+-rw-rw-r--  2.0 unx    12778 b- defN 23-Aug-02 17:19 slideflow/model/extractors/retccl.py
+-rw-rw-r--  2.0 unx     5189 b- defN 23-Aug-02 17:19 slideflow/model/extractors/simclr.py
+-rw-rw-r--  2.0 unx      461 b- defN 23-Aug-02 17:19 slideflow/model/extractors/_slide/__init__.py
+-rw-rw-r--  2.0 unx     6785 b- defN 23-Aug-02 17:19 slideflow/model/extractors/_slide/_tf.py
+-rw-rw-r--  2.0 unx     3893 b- defN 23-Aug-02 17:19 slideflow/model/extractors/_slide/_torch.py
+-rw-rw-r--  2.0 unx     1352 b- defN 23-Aug-02 17:19 slideflow/model/extractors/_slide/_utils.py
+-rw-rw-r--  2.0 unx    26247 b- defN 23-Aug-02 19:12 slideflow/norm/__init__.py
 -rw-rw-r--  2.0 unx     1605 b- defN 23-Apr-09 06:29 slideflow/norm/augment.py
 -rw-rw-r--  2.0 unx    13438 b- defN 23-Apr-09 06:29 slideflow/norm/macenko.py
 -rw-rw-r--  2.0 unx   177672 b- defN 22-Jul-15 00:02 slideflow/norm/norm_tile.jpg
 -rw-rw-r--  2.0 unx    16964 b- defN 23-Apr-09 06:29 slideflow/norm/reinhard.py
 -rw-rw-r--  2.0 unx    15314 b- defN 23-Apr-09 06:29 slideflow/norm/utils.py
 -rw-rw-r--  2.0 unx     7836 b- defN 23-Apr-09 06:29 slideflow/norm/vahadane.py
 -rw-rw-r--  2.0 unx    11998 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/__init__.py
 -rw-rw-r--  2.0 unx     5879 b- defN 22-Jul-15 00:02 slideflow/norm/tensorflow/color.py
--rw-rw-r--  2.0 unx    22672 b- defN 23-May-25 18:12 slideflow/norm/tensorflow/macenko.py
+-rw-rw-r--  2.0 unx    22671 b- defN 23-Aug-02 17:19 slideflow/norm/tensorflow/macenko.py
 -rw-rw-r--  2.0 unx    26169 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/reinhard.py
 -rw-rw-r--  2.0 unx     1685 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/utils.py
 -rw-rw-r--  2.0 unx    12095 b- defN 23-Apr-19 02:24 slideflow/norm/torch/__init__.py
 -rw-rw-r--  2.0 unx     7826 b- defN 22-Aug-06 13:13 slideflow/norm/torch/color.py
--rw-rw-r--  2.0 unx    15306 b- defN 23-May-25 18:12 slideflow/norm/torch/macenko.py
+-rw-rw-r--  2.0 unx    15060 b- defN 23-Aug-02 17:19 slideflow/norm/torch/macenko.py
 -rw-rw-r--  2.0 unx    24997 b- defN 23-Apr-19 00:55 slideflow/norm/torch/reinhard.py
 -rw-rw-r--  2.0 unx     1673 b- defN 23-Apr-09 06:29 slideflow/norm/torch/utils.py
--rw-rw-r--  2.0 unx      458 b- defN 23-May-25 18:12 slideflow/simclr/__init__.py
+-rw-rw-r--  2.0 unx      458 b- defN 23-Jul-21 17:58 slideflow/simclr/__init__.py
 -rw-rw-r--  2.0 unx        6 b- defN 23-Feb-07 02:07 slideflow/simclr/simclr/__init__.py
--rw-rw-r--  2.0 unx    21046 b- defN 23-May-17 16:22 slideflow/simclr/simclr/tf2/__init__.py
--rw-rw-r--  2.0 unx    10701 b- defN 23-May-17 16:22 slideflow/simclr/simclr/tf2/data.py
--rw-rw-r--  2.0 unx    18550 b- defN 23-May-17 16:22 slideflow/simclr/simclr/tf2/data_util.py
+-rw-rw-r--  2.0 unx    21050 b- defN 23-Jul-11 21:23 slideflow/simclr/simclr/tf2/__init__.py
+-rw-rw-r--  2.0 unx    11777 b- defN 23-Jul-20 14:10 slideflow/simclr/simclr/tf2/data.py
+-rw-rw-r--  2.0 unx    19016 b- defN 23-Jul-20 15:30 slideflow/simclr/simclr/tf2/data_util.py
 -rw-rw-r--  2.0 unx     6505 b- defN 23-Apr-17 18:03 slideflow/simclr/simclr/tf2/lars_optimizer.py
 -rw-rw-r--  2.0 unx     2997 b- defN 23-Jan-23 19:16 slideflow/simclr/simclr/tf2/metrics.py
--rw-rw-r--  2.0 unx    12256 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/model.py
+-rw-rw-r--  2.0 unx    12256 b- defN 23-Aug-01 14:25 slideflow/simclr/simclr/tf2/model.py
 -rw-rw-r--  2.0 unx     4983 b- defN 23-Jan-23 19:16 slideflow/simclr/simclr/tf2/objective.py
--rw-rw-r--  2.0 unx    28397 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/resnet.py
+-rw-rw-r--  2.0 unx    28265 b- defN 23-Aug-01 14:25 slideflow/simclr/simclr/tf2/resnet.py
 -rw-rw-r--  2.0 unx     6524 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/run.py
--rw-rw-r--  2.0 unx     9517 b- defN 23-May-17 16:22 slideflow/simclr/simclr/tf2/utils.py
--rw-rw-r--  2.0 unx   115204 b- defN 23-May-25 18:12 slideflow/slide/__init__.py
+-rw-rw-r--  2.0 unx     9737 b- defN 23-Jul-20 15:23 slideflow/simclr/simclr/tf2/utils.py
+-rw-rw-r--  2.0 unx   117079 b- defN 23-Aug-02 19:15 slideflow/slide/__init__.py
 -rw-rw-r--  2.0 unx    19076 b- defN 23-Apr-21 17:48 slideflow/slide/report.py
 -rw-rw-r--  2.0 unx    30934 b- defN 23-Apr-09 06:29 slideflow/slide/slideflow-logo-name-small.jpg
 -rw-rw-r--  2.0 unx     5516 b- defN 23-Apr-10 16:45 slideflow/slide/utils.py
 -rw-rw-r--  2.0 unx     1035 b- defN 23-May-17 15:54 slideflow/slide/backends/__init__.py
--rw-rw-r--  2.0 unx    14301 b- defN 23-May-17 15:54 slideflow/slide/backends/cucim.py
--rw-rw-r--  2.0 unx    22863 b- defN 23-May-25 18:12 slideflow/slide/backends/vips.py
--rw-rw-r--  2.0 unx      117 b- defN 23-May-25 18:12 slideflow/slide/qc/__init__.py
--rw-rw-r--  2.0 unx     8151 b- defN 23-Apr-27 15:40 slideflow/slide/qc/deepfocus.py
+-rw-rw-r--  2.0 unx    14351 b- defN 23-Aug-02 17:19 slideflow/slide/backends/cucim.py
+-rw-rw-r--  2.0 unx    31738 b- defN 23-Aug-02 17:19 slideflow/slide/backends/vips.py
+-rw-rw-r--  2.0 unx      186 b- defN 23-Aug-02 17:19 slideflow/slide/qc/__init__.py
+-rw-rw-r--  2.0 unx     8151 b- defN 23-Aug-02 17:19 slideflow/slide/qc/deepfocus.py
 -rw-rw-r--  2.0 unx     1010 b- defN 22-Nov-29 16:52 slideflow/slide/qc/deepfocus_qc.py
--rw-rw-r--  2.0 unx     4304 b- defN 23-May-25 18:12 slideflow/slide/qc/gaussian.py
--rw-rw-r--  2.0 unx     5341 b- defN 23-Apr-27 15:40 slideflow/slide/qc/gaussian_v2.py
--rw-rw-r--  2.0 unx     5232 b- defN 23-May-25 18:12 slideflow/slide/qc/otsu.py
+-rw-rw-r--  2.0 unx     4484 b- defN 23-Aug-02 17:19 slideflow/slide/qc/gaussian.py
+-rw-rw-r--  2.0 unx     5341 b- defN 23-Aug-02 17:19 slideflow/slide/qc/gaussian_v2.py
+-rw-rw-r--  2.0 unx     5989 b- defN 23-Aug-02 17:19 slideflow/slide/qc/otsu.py
 -rw-rw-r--  2.0 unx     3028 b- defN 23-Mar-13 02:04 slideflow/slide/qc/saver.py
--rw-rw-r--  2.0 unx     4676 b- defN 23-May-25 18:12 slideflow/slide/qc/strided_dl.py
--rw-rw-r--  2.0 unx    13029 b- defN 23-Apr-27 15:40 slideflow/slide/qc/strided_qc.py
+-rw-rw-r--  2.0 unx     5327 b- defN 23-Aug-02 17:19 slideflow/slide/qc/strided_dl.py
+-rw-rw-r--  2.0 unx    13029 b- defN 23-Aug-02 17:19 slideflow/slide/qc/strided_qc.py
 -rw-rw-r--  2.0 unx      390 b- defN 23-Feb-01 21:02 slideflow/stats/__init__.py
 -rw-rw-r--  2.0 unx     4293 b- defN 23-Apr-09 06:29 slideflow/stats/delong.py
--rw-rw-r--  2.0 unx    35785 b- defN 23-Apr-09 06:29 slideflow/stats/metrics.py
+-rw-rw-r--  2.0 unx    35694 b- defN 23-Aug-02 17:19 slideflow/stats/metrics.py
 -rw-rw-r--  2.0 unx     5757 b- defN 23-Apr-09 06:29 slideflow/stats/plot.py
--rw-rw-r--  2.0 unx    43101 b- defN 23-May-25 18:12 slideflow/stats/slidemap.py
--rw-rw-r--  2.0 unx     3261 b- defN 23-May-25 18:12 slideflow/stats/stats_utils.py
--rw-rw-r--  2.0 unx    77576 b- defN 23-May-25 18:12 slideflow/studio/__init__.py
+-rw-rw-r--  2.0 unx    43878 b- defN 23-Aug-02 19:48 slideflow/stats/slidemap.py
+-rw-rw-r--  2.0 unx     3553 b- defN 23-Aug-02 17:19 slideflow/stats/stats_utils.py
+-rw-rw-r--  2.0 unx    80731 b- defN 23-Aug-02 17:19 slideflow/studio/__init__.py
 -rw-rw-r--  2.0 unx     2187 b- defN 23-Apr-09 06:29 slideflow/studio/__main__.py
--rw-rw-r--  2.0 unx    19662 b- defN 23-May-25 18:12 slideflow/studio/_renderer.py
+-rw-rw-r--  2.0 unx    19645 b- defN 23-Aug-02 17:19 slideflow/studio/_renderer.py
 -rw-rw-r--  2.0 unx     3686 b- defN 23-Apr-09 06:29 slideflow/studio/utils.py
 -rw-rw-r--  2.0 unx        8 b- defN 23-Apr-09 06:29 slideflow/studio/gui/__init__.py
--rw-rw-r--  2.0 unx    14387 b- defN 23-Apr-09 06:29 slideflow/studio/gui/_glfw.py
--rw-rw-r--  2.0 unx     4116 b- defN 23-Apr-09 06:29 slideflow/studio/gui/annotator.py
--rw-rw-r--  2.0 unx    11035 b- defN 23-Apr-09 06:29 slideflow/studio/gui/gl_utils.py
+-rw-rw-r--  2.0 unx    14387 b- defN 23-Jul-26 17:09 slideflow/studio/gui/_glfw.py
+-rw-rw-r--  2.0 unx     4608 b- defN 23-Aug-02 17:19 slideflow/studio/gui/annotator.py
+-rw-rw-r--  2.0 unx    12307 b- defN 23-Aug-02 17:19 slideflow/studio/gui/gl_utils.py
 -rw-rw-r--  2.0 unx    10029 b- defN 23-Apr-09 06:29 slideflow/studio/gui/imgui_utils.py
 -rw-rw-r--  2.0 unx    29775 b- defN 23-Apr-09 06:29 slideflow/studio/gui/logo_dark_outline.png
 -rw-rw-r--  2.0 unx   179066 b- defN 23-Apr-09 06:29 slideflow/studio/gui/splash.png
 -rw-rw-r--  2.0 unx     5291 b- defN 23-Apr-09 06:29 slideflow/studio/gui/text_utils.py
 -rw-rw-r--  2.0 unx     2241 b- defN 23-Apr-09 06:29 slideflow/studio/gui/theme.py
 -rw-rw-r--  2.0 unx     2333 b- defN 23-Apr-09 06:29 slideflow/studio/gui/toast.py
--rw-rw-r--  2.0 unx     7885 b- defN 23-Apr-09 06:29 slideflow/studio/gui/window.py
+-rw-rw-r--  2.0 unx     7908 b- defN 23-Aug-02 17:19 slideflow/studio/gui/window.py
 -rw-rw-r--  2.0 unx     2425 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_lightning.png
 -rw-rw-r--  2.0 unx     2127 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_lightning_highlighted.png
 -rw-rw-r--  2.0 unx     2278 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_plus.png
 -rw-rw-r--  2.0 unx     2017 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_plus_highlighted.png
 -rw-rw-r--  2.0 unx     1994 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_extensions.png
 -rw-rw-r--  2.0 unx     1823 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_extensions_highlighted.png
 -rw-rw-r--  2.0 unx     1297 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_floppy.png
 -rw-rw-r--  2.0 unx     1291 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_floppy_highlighted.png
 -rw-rw-r--  2.0 unx     1463 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_folder.png
 -rw-rw-r--  2.0 unx     1440 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_folder_highlighted.png
 -rw-rw-r--  2.0 unx     2776 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_gear.png
 -rw-rw-r--  2.0 unx     2139 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_gear_highlighted.png
 -rw-rw-r--  2.0 unx     1331 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_heatmap.png
 -rw-rw-r--  2.0 unx     1271 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_heatmap_highlighted.png
+-rw-rw-r--  2.0 unx     2648 b- defN 23-Aug-02 17:19 slideflow/studio/gui/buttons/button_mil.png
+-rw-rw-r--  2.0 unx     2389 b- defN 23-Aug-02 17:19 slideflow/studio/gui/buttons/button_mil_highlighted.png
 -rw-rw-r--  2.0 unx     2403 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model.png
 -rw-rw-r--  2.0 unx     2109 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model_highlighted.png
 -rw-rw-r--  2.0 unx     3842 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model_loaded.png
 -rw-rw-r--  2.0 unx     3622 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model_loaded_highlighted.png
 -rw-rw-r--  2.0 unx     1662 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_mosaic.png
 -rw-rw-r--  2.0 unx     1609 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_mosaic_highlighted.png
 -rw-rw-r--  2.0 unx     1164 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_pencil.png
@@ -309,56 +317,57 @@
 -rw-rw-r--  2.0 unx     7568 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/error.png
 -rw-rw-r--  2.0 unx     7286 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/info.png
 -rw-rw-r--  2.0 unx   125763 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/logo.png
 -rw-rw-r--  2.0 unx     7181 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/success.png
 -rw-rw-r--  2.0 unx     6817 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/warn.png
 -rw-rw-r--  2.0 unx      107 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/__init__.py
 -rw-rw-r--  2.0 unx     7874 b- defN 23-Apr-21 17:43 slideflow/studio/gui/viewer/_mosaic.py
--rw-rw-r--  2.0 unx    26041 b- defN 23-May-25 18:12 slideflow/studio/gui/viewer/_slide.py
--rw-rw-r--  2.0 unx    14452 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/_viewer.py
+-rw-rw-r--  2.0 unx    27584 b- defN 23-Aug-02 17:19 slideflow/studio/gui/viewer/_slide.py
+-rw-rw-r--  2.0 unx    14572 b- defN 23-Aug-02 17:19 slideflow/studio/gui/viewer/_viewer.py
 -rw-rw-r--  2.0 unx      439 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/__init__.py
 -rw-rw-r--  2.0 unx      735 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/_utils.py
 -rw-rw-r--  2.0 unx     4362 b- defN 23-May-17 15:54 slideflow/studio/widgets/capture.py
--rw-rw-r--  2.0 unx     5412 b- defN 23-May-25 18:12 slideflow/studio/widgets/extensions.py
--rw-rw-r--  2.0 unx    16890 b- defN 23-Apr-14 13:28 slideflow/studio/widgets/heatmap.py
+-rw-rw-r--  2.0 unx     6388 b- defN 23-Aug-02 17:19 slideflow/studio/widgets/extensions.py
+-rw-rw-r--  2.0 unx    16624 b- defN 23-Aug-02 17:19 slideflow/studio/widgets/heatmap.py
 -rw-rw-r--  2.0 unx     3842 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/layer_umap.py
+-rw-rw-r--  2.0 unx    16225 b- defN 23-Aug-02 17:19 slideflow/studio/widgets/mil.py
 -rw-rw-r--  2.0 unx    24474 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/model.py
--rw-rw-r--  2.0 unx    14307 b- defN 23-May-25 18:12 slideflow/studio/widgets/mosaic.py
+-rw-rw-r--  2.0 unx    14330 b- defN 23-Aug-02 17:19 slideflow/studio/widgets/mosaic.py
 -rw-rw-r--  2.0 unx     2597 b- defN 23-Mar-19 15:05 slideflow/studio/widgets/mosaic_experimental.py
 -rw-rw-r--  2.0 unx     4662 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/performance.py
 -rw-rw-r--  2.0 unx     6437 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/picam.py
 -rw-rw-r--  2.0 unx     7726 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/project.py
 -rw-rw-r--  2.0 unx     5983 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/seed_map.py
 -rw-rw-r--  2.0 unx    19875 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/segment.py
 -rw-rw-r--  2.0 unx     2071 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/settings.py
--rw-rw-r--  2.0 unx    31018 b- defN 23-Apr-27 17:21 slideflow/studio/widgets/slide.py
+-rw-rw-r--  2.0 unx    47938 b- defN 23-Aug-02 17:19 slideflow/studio/widgets/slide.py
 -rw-rw-r--  2.0 unx    16402 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/stylegan.py
 -rw-rw-r--  2.0 unx    32357 b- defN 23-Apr-24 21:32 slideflow/test/__init__.py
 -rw-rw-r--  2.0 unx    12446 b- defN 23-Feb-01 21:02 slideflow/test/dataset_test.py
 -rw-rw-r--  2.0 unx     9560 b- defN 23-Mar-26 19:33 slideflow/test/functional.py
 -rw-rw-r--  2.0 unx     8052 b- defN 23-Apr-09 06:29 slideflow/test/model_test.py
 -rw-rw-r--  2.0 unx    12098 b- defN 23-Mar-01 23:20 slideflow/test/norm_test.py
 -rw-rw-r--  2.0 unx     2909 b- defN 23-Apr-09 06:29 slideflow/test/slide_test.py
 -rw-rw-r--  2.0 unx    11481 b- defN 23-Mar-13 02:04 slideflow/test/stats_test.py
 -rw-rw-r--  2.0 unx    11752 b- defN 23-Apr-09 06:29 slideflow/test/utils.py
 -rw-rw-r--  2.0 unx      891 b- defN 23-Mar-13 02:04 slideflow/tfrecord/__init__.py
--rw-rw-r--  2.0 unx     2905 b- defN 22-Dec-02 04:52 slideflow/tfrecord/iterator_utils.py
+-rw-rw-r--  2.0 unx     2866 b- defN 23-Jun-08 13:54 slideflow/tfrecord/iterator_utils.py
 -rw-rw-r--  2.0 unx    15505 b- defN 23-Apr-21 04:57 slideflow/tfrecord/reader.py
 -rw-rw-r--  2.0 unx     5637 b- defN 22-Jul-18 12:12 slideflow/tfrecord/writer.py
 -rw-rw-r--  2.0 unx      179 b- defN 22-Jul-18 12:12 slideflow/tfrecord/tools/__init__.py
 -rw-rw-r--  2.0 unx      310 b- defN 22-Jul-18 12:12 slideflow/tfrecord/torch/__init__.py
 -rw-rw-r--  2.0 unx     7857 b- defN 22-Jul-18 12:12 slideflow/tfrecord/torch/dataset.py
--rw-rw-r--  2.0 unx    41943 b- defN 23-May-25 18:12 slideflow/util/__init__.py
+-rw-rw-r--  2.0 unx    44423 b- defN 23-Aug-02 17:19 slideflow/util/__init__.py
 -rw-rw-r--  2.0 unx      738 b- defN 22-Jul-15 00:02 slideflow/util/colors.py
 -rw-rw-r--  2.0 unx    17912 b- defN 22-Jul-18 12:12 slideflow/util/example_pb2.py
 -rw-rw-r--  2.0 unx     4468 b- defN 23-Feb-01 21:02 slideflow/util/log_utils.py
 -rw-rw-r--  2.0 unx     4381 b- defN 22-Jul-18 12:12 slideflow/util/neptune_utils.py
 -rw-rw-r--  2.0 unx    20061 b- defN 23-Apr-09 06:29 slideflow/util/smac_utils.py
--rw-rw-r--  2.0 unx     8064 b- defN 23-May-25 18:12 slideflow/util/tfrecord2idx.py
--rwxrwxr-x  2.0 unx    14085 b- defN 23-May-25 18:41 slideflow-2.0.5.data/scripts/slideflow-studio
--rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-10 13:18 slideflow-2.0.5.data/scripts/slideflow-studio.py
--rw-rw-r--  2.0 unx    35149 b- defN 23-May-25 18:41 slideflow-2.0.5.dist-info/LICENSE
--rw-rw-r--  2.0 unx    13038 b- defN 23-May-25 18:41 slideflow-2.0.5.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-25 18:41 slideflow-2.0.5.dist-info/WHEEL
--rw-rw-r--  2.0 unx       10 b- defN 23-May-25 18:41 slideflow-2.0.5.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    35917 b- defN 23-May-25 18:41 slideflow-2.0.5.dist-info/RECORD
-362 files, 4812105 bytes uncompressed, 1791535 bytes compressed:  62.8%
+-rw-rw-r--  2.0 unx    10308 b- defN 23-Aug-02 17:19 slideflow/util/tfrecord2idx.py
+-rwxrwxr-x  2.0 unx    14085 b- defN 23-Aug-02 19:56 slideflow-2.1.0.data/scripts/slideflow-studio
+-rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-10 13:18 slideflow-2.1.0.data/scripts/slideflow-studio.py
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Aug-02 19:56 slideflow-2.1.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    13125 b- defN 23-Aug-02 19:56 slideflow-2.1.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Aug-02 19:56 slideflow-2.1.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       10 b- defN 23-Aug-02 19:56 slideflow-2.1.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    36790 b- defN 23-Aug-02 19:56 slideflow-2.1.0.dist-info/RECORD
+371 files, 4954079 bytes uncompressed, 1828958 bytes compressed:  63.1%
```

## zipnote {}

```diff
@@ -492,14 +492,17 @@
 
 Filename: slideflow/mil/data.py
 Comment: 
 
 Filename: slideflow/mil/eval.py
 Comment: 
 
+Filename: slideflow/mil/utils.py
+Comment: 
+
 Filename: slideflow/mil/clam/__init__.py
 Comment: 
 
 Filename: slideflow/mil/clam/create_attention.py
 Comment: 
 
 Filename: slideflow/mil/clam/datasets/__init__.py
@@ -594,14 +597,29 @@
 
 Filename: slideflow/model/extractors/ctranspath.py
 Comment: 
 
 Filename: slideflow/model/extractors/retccl.py
 Comment: 
 
+Filename: slideflow/model/extractors/simclr.py
+Comment: 
+
+Filename: slideflow/model/extractors/_slide/__init__.py
+Comment: 
+
+Filename: slideflow/model/extractors/_slide/_tf.py
+Comment: 
+
+Filename: slideflow/model/extractors/_slide/_torch.py
+Comment: 
+
+Filename: slideflow/model/extractors/_slide/_utils.py
+Comment: 
+
 Filename: slideflow/norm/__init__.py
 Comment: 
 
 Filename: slideflow/norm/augment.py
 Comment: 
 
 Filename: slideflow/norm/macenko.py
@@ -834,14 +852,20 @@
 
 Filename: slideflow/studio/gui/buttons/button_heatmap.png
 Comment: 
 
 Filename: slideflow/studio/gui/buttons/button_heatmap_highlighted.png
 Comment: 
 
+Filename: slideflow/studio/gui/buttons/button_mil.png
+Comment: 
+
+Filename: slideflow/studio/gui/buttons/button_mil_highlighted.png
+Comment: 
+
 Filename: slideflow/studio/gui/buttons/button_model.png
 Comment: 
 
 Filename: slideflow/studio/gui/buttons/button_model_highlighted.png
 Comment: 
 
 Filename: slideflow/studio/gui/buttons/button_model_loaded.png
@@ -960,14 +984,17 @@
 
 Filename: slideflow/studio/widgets/heatmap.py
 Comment: 
 
 Filename: slideflow/studio/widgets/layer_umap.py
 Comment: 
 
+Filename: slideflow/studio/widgets/mil.py
+Comment: 
+
 Filename: slideflow/studio/widgets/model.py
 Comment: 
 
 Filename: slideflow/studio/widgets/mosaic.py
 Comment: 
 
 Filename: slideflow/studio/widgets/mosaic_experimental.py
@@ -1059,29 +1086,29 @@
 
 Filename: slideflow/util/smac_utils.py
 Comment: 
 
 Filename: slideflow/util/tfrecord2idx.py
 Comment: 
 
-Filename: slideflow-2.0.5.data/scripts/slideflow-studio
+Filename: slideflow-2.1.0.data/scripts/slideflow-studio
 Comment: 
 
-Filename: slideflow-2.0.5.data/scripts/slideflow-studio.py
+Filename: slideflow-2.1.0.data/scripts/slideflow-studio.py
 Comment: 
 
-Filename: slideflow-2.0.5.dist-info/LICENSE
+Filename: slideflow-2.1.0.dist-info/LICENSE
 Comment: 
 
-Filename: slideflow-2.0.5.dist-info/METADATA
+Filename: slideflow-2.1.0.dist-info/METADATA
 Comment: 
 
-Filename: slideflow-2.0.5.dist-info/WHEEL
+Filename: slideflow-2.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: slideflow-2.0.5.dist-info/top_level.txt
+Filename: slideflow-2.1.0.dist-info/top_level.txt
 Comment: 
 
-Filename: slideflow-2.0.5.dist-info/RECORD
+Filename: slideflow-2.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## slideflow/_version.py

```diff
@@ -4,18 +4,18 @@
 # unpacked source archive. Distribution tarballs contain a pre-generated copy
 # of this file.
 
 import json
 
 version_json = '''
 {
- "date": "2023-05-25T13:38:53-0500",
+ "date": "2023-08-02T14:52:07-0500",
  "dirty": false,
  "error": null,
- "full-revisionid": "ae6ad0e8937207efe60d23a400e88bf12f5db719",
- "version": "2.0.5"
+ "full-revisionid": "0c158549cb4b8f0a25f113b914367d999948e1e7",
+ "version": "2.1.0"
 }
 '''  # END VERSION_JSON
 
 
 def get_versions():
     return json.loads(version_json)
```

## slideflow/dataset.py

```diff
@@ -230,30 +230,56 @@
     except errors.TileCorruptionError:
         log.error(f'{path} corrupt; skipping')
     except (KeyboardInterrupt, SystemExit) as e:
         print('Exiting...')
         raise e
 
 
+def _buffer_slide(path: str, dest: str) -> str:
+    """Buffer a slide to a path."""
+    buffered = join(dest, basename(path))
+    shutil.copy(path, buffered)
+
+    # If this is an MRXS file, copy the associated folder.
+    if path.lower().endswith('mrxs'):
+        folder_path = join(dirname(path), path_to_name(path))
+        if exists(folder_path):
+            shutil.copytree(folder_path, join(dest, path_to_name(path)))
+        else:
+            log.debug("Could not find associated MRXS folder for slide buffer")
+
+    return buffered
+
+
+def _debuffer_slide(path: str) -> None:
+    """De-buffer a slide."""
+    os.remove(path)
+    # If this is an MRXS file, remove the associated folder.
+    if path.lower().endswith('mrxs'):
+        folder_path = join(dirname(path), path_to_name(path))
+        if exists(folder_path):
+            shutil.rmtree(folder_path)
+        else:
+            log.debug("Could not find MRXS folder for slide debuffer")
+
+
 def _fill_queue(
     slide_list: Sequence[str],
     q: Queue,
     q_size: int,
     buffer: Optional[str] = None
 ) -> None:
     """Fill a queue with slide paths, using an optional buffer."""
     for path in slide_list:
         warned = False
         if buffer:
             while True:
                 if q.qsize() < q_size:
                     try:
-                        buffered = join(buffer, basename(path))
-                        shutil.copy(path, buffered)
-                        q.put(buffered)
+                        q.put(_buffer_slide(path, buffer))
                         break
                     except OSError:
                         if not warned:
                             slide = _shortname(path_to_name(path))
                             log.debug(f'OSError for {slide}: buffer full?')
                             log.debug(f'Queue size: {q.qsize()}')
                             warned = True
@@ -677,15 +703,15 @@
 
         Verifies all tfrecords share the same image format.
 
         Returns:
             str: Image format of tfrecords (PNG or JPG), or None if no
             tfrecords have been extracted.
         """
-        return self.verify_img_format()
+        return self.verify_img_format(progress=False)
 
     def _tfrecords_set(self, source: str):
         if source not in self.sources:
             raise ValueError(f"Unrecognized dataset source {source}")
         config = self.sources[source]
         return 'tfrecords' in config and config['tfrecords']
 
@@ -921,30 +947,36 @@
         Args:
             force (bool): Force re-build existing indices.
 
         Returns:
             None
         """
         if force:
-            missing_index = self.tfrecords()
+            index_to_update = self.tfrecords()
         else:
-            missing_index = [
-                tfr for tfr in self.tfrecords()
-                if not tfrecord2idx.find_index(tfr)
-            ]
-            if not missing_index:
+            index_to_update = []
+            for tfr in self.tfrecords():
+                index = tfrecord2idx.find_index(tfr)
+                if not index:
+                    index_to_update.append(tfr)
+                elif (not tfrecord2idx.index_has_locations(index)
+                      and sf.io.tfrecord_has_locations(tfr)):
+                    os.remove(index)
+                    index_to_update.append(tfr)
+            if not index_to_update:
                 return
+
         index_fn = partial(_create_index, force=force)
         pool = mp.Pool(
-            os.cpu_count(),
+            sf.util.num_cpu(),
             initializer=sf.util.set_ignore_sigint
         )
-        for _ in track(pool.imap_unordered(index_fn, missing_index),
-                       description='Creating index files...',
-                       total=len(self.tfrecords()),
+        for _ in track(pool.imap_unordered(index_fn, index_to_update),
+                       description=f'Updating index files...',
+                       total=len(index_to_update),
                        transient=True):
             pass
         pool.close()
 
     def cell_segmentation(
         self,
         diam_um: float,
@@ -1086,15 +1118,15 @@
                     join(mask_dest, f'{wsi.name}-masks.zip'),
                     flows=save_flow,
                     centroids=save_centroid)
                 pb.advance(slide_task)
                 pb.remove_task(segment_task)
 
                 if buffer:
-                    os.remove(slide_path)
+                    _debuffer_slide(slide_path)
                 q.task_done()
         if buffer:
             thread.join()
 
     def check_duplicates(
         self,
         dataset: Optional["Dataset"] = None,
@@ -1434,16 +1466,17 @@
                 detection quality control - discarding tiles with detected
                 out-of-focus regions or artifact - and/or otsu's method.
                 Increases tile extraction time. Defaults to None.
             report (bool): Save a PDF report of tile extraction.
                 Defaults to True.
             normalizer (str, optional): Normalization strategy.
                 Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
             whitespace_fraction (float, optional): Range 0-1. Discard tiles
                 with this fraction of whitespace. If 1, will not perform
                 whitespace filtering. Defaults to 1.
             whitespace_threshold (int, optional): Range 0-255. Defaults to 230.
                 Threshold above which a pixel (RGB average) is whitespace.
             grayspace_fraction (float, optional): Range 0-1. Defaults to 0.6.
@@ -1494,14 +1527,18 @@
         if source:
             sources = sf.util.as_list(source)  # type: List[str]
         else:
             sources = list(self.sources.keys())
         all_reports = []
         self.verify_annotations_slides()
 
+        # Log the active slide reading backend
+        col = 'green' if sf.slide_backend() == 'cucim' else 'cyan'
+        log.info(f"Slide reading backend: [{col}]{sf.slide_backend()}[/]")
+
         # Set up kwargs for tile extraction generator and quality control
         qc_kwargs = {k[3:]: v for k, v in kwargs.items() if k[:3] == 'qc_'}
         kwargs = {k: v for k, v in kwargs.items() if k[:3] != 'qc_'}
         sf.slide.log_extraction_params(**kwargs)
 
         for source in sources:
             log.info(f'Working on dataset source [bold]{source}[/]...')
@@ -1571,26 +1608,27 @@
                 ctx = mp.get_context(ptype)
                 manager = ctx.Manager()
                 reports = manager.dict()
                 kwargs['report'] = report
 
                 # Use a single shared multiprocessing pool
                 if 'num_threads' not in kwargs:
-                    num_threads = os.cpu_count()
-                    if sf.slide_backend() == 'libvips':
-                        num_threads = min(num_threads, 32)
+                    num_threads = sf.util.num_cpu()
                     if num_threads is None:
                         num_threads = 8
+                    if sf.slide_backend() == 'libvips':
+                        num_threads = min(num_threads, 32)
                 else:
                     num_threads = kwargs['num_threads']
                 if num_threads != 1:
                     pool = kwargs['pool'] = ctx.Pool(
                         num_threads,
                         initializer=sf.util.set_ignore_sigint
                     )
+                    qc_kwargs['pool'] = pool
                 else:
                     pool = None
                     ptype = None
                 log.info(f'Using {num_threads} processes (pool={ptype})')
 
                 # Set up the multiprocessing progress bar
                 pb = TileExtractionProgress()
@@ -1638,15 +1676,15 @@
                         while True:
                             path = q.get()
                             if path is None:
                                 q.task_done()
                                 break
                             _tile_extractor(path, **extraction_kwargs)
                             pb.advance(slide_task)
-                            os.remove(path)
+                            _debuffer_slide(path)
                             q.task_done()
                         thread.join()
                     else:
                         for slide in slide_list:
                             wsi = _prepare_slide(
                                 slide,
                                 report_dir=tfrecord_dir,
@@ -1877,15 +1915,15 @@
         tfr_idx = sf.util.tfrecord2idx.find_index(tfr)
         if not tfr_idx:
             _create_index(tfr)
         elif tfr_idx.endswith('index'):
             log.info(f"Updating index for {tfr}...")
             os.remove(tfr_idx)
             _create_index(tfr)
-        return sf.io.get_locations_from_tfrecord(tfr, as_dict=False)
+        return sf.io.get_locations_from_tfrecord(tfr)
 
     def harmonize_labels(
         self,
         *args: "Dataset",
         header: Optional[str] = None
     ) -> Dict[str, int]:
         """Harmonize labels with another dataset.
@@ -2319,15 +2357,15 @@
             log.warning(f"Bags missing for {len(slides) - len(bags)} slides.")
         return bags
 
     def read_tfrecord_by_location(
         self,
         slide: str,
         loc: Tuple[int, int],
-        decode: bool = True
+        decode: Optional[bool] = None
     ) -> Any:
         """Read a record from a TFRecord, indexed by location.
 
         Finds the associated TFRecord for a slide, and returns the record
         inside which corresponds to a given tile location.
 
         Args:
@@ -2345,14 +2383,22 @@
 
         """
         tfr = self.find_tfrecord(slide=slide)
         if tfr is None:
             raise errors.TFRecordsError(
                 f"Could not find associated TFRecord for slide '{slide}'"
             )
+        if decode is None:
+            decode = True
+        else:
+            warnings.warn(
+                "The 'decode' argument to `Dataset.read_tfrecord_by_location` "
+                "is deprecated and will be removed in a future version. In the "
+                "future, all records will be decoded."
+            )
         return sf.io.get_tfrecord_by_location(tfr, loc, decode=decode)
 
     def remove_filter(self, **kwargs: Any) -> "Dataset":
         """Remove a specific filter from the active filters.
 
         Keyword Args:
             filters (list of str): Filter keys. Will remove filters with
@@ -2365,15 +2411,17 @@
 
         """
         for kwarg in kwargs:
             if kwarg not in ('filters', 'filter_blank'):
                 raise ValueError(f'Unknown filtering argument {kwarg}')
         ret = copy.deepcopy(self)
         if 'filters' in kwargs:
-            if not isinstance(kwargs['filters'], list):
+            if isinstance(kwargs['filters'], str):
+                kwargs['filters'] = [kwargs['filters']]
+            elif not isinstance(kwargs['filters'], list):
                 raise TypeError("'filters' must be a list.")
             for f in kwargs['filters']:
                 if f not in ret._filters:
                     raise errors.DatasetFilterError(
                         f"Filter {f} not found in dataset (active filters:"
                         f"{','.join(list(ret._filters.keys()))})"
                     )
@@ -2481,15 +2529,15 @@
         paths = self.slide_paths(source=source)
         pb = Progress(transient=True)
         read_task = pb.add_task('Reading slides...', total=len(paths))
         if not low_memory:
             otsu_task = pb.add_task("Otsu thresholding...", total=len(paths))
         pb.start()
         pool = mp.Pool(
-            16 if os.cpu_count is None else os.cpu_count(),
+            sf.util.num_cpu(default=16),
             initializer=sf.util.set_ignore_sigint
         )
         wsi_list = []
         to_remove = []
         counts = []
         for path in paths:
             try:
@@ -3112,52 +3160,71 @@
                 map slide names to outcome labels. If function, function must
                 accept an image (tensor) and slide name (str), and return a
                 dict {'image_raw': image (tensor)} and label (int or float).
                 If not provided, all labels will be None.
             batch_size (int): Batch size.
 
         Keyword Args:
-            onehot (bool, optional): Onehot encode labels. Defaults to False.
-            incl_slidenames (bool, optional): Include slidenames as third
-                returned variable. Defaults to False.
-            infinite (bool, optional): Infinitely repeat data.
+            augment (str or bool): Image augmentations to perform. Augmentations include:
+
+                * ``'x'``: Random horizontal flip
+                * ``'y'``: Random vertical flip
+                * ``'r'``: Random 90-degree rotation
+                * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+                * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+                * ``'n'``: Random :ref:`stain_augmentation` (requires stain normalizer)
+
+                Combine letters to define augmentations, such as ``'xyrjn'``.
+                A value of True will use ``'xyrjb'``.
+            deterministic (bool, optional): When num_parallel_calls is specified,
+                if this boolean is specified, it controls the order in which the
+                transformation produces elements. If set to False, the
+                transformation is allowed to yield elements out of order to trade
+                determinism for performance. Defaults to False.
+            drop_last (bool, optional): Drop the last non-full batch.
+                Defaults to False.
+            from_wsi (bool): Generate predictions from tiles dynamically
+                extracted from whole-slide images, rather than TFRecords.
+                Defaults to False (use TFRecords).
+            incl_loc (str, optional): 'coord', 'grid', or None. Return (x,y)
+                origin coordinates ('coord') for each tile along with tile
+                images, or the (x,y) grid coordinates for each tile.
+                Defaults to 'coord'.
+            incl_slidenames (bool, optional): Include slidenames as third returned
+                variable. Defaults to False.
+            infinite (bool, optional): Create an finite dataset. WARNING: If
+                infinite is False && balancing is used, some tiles will be skipped.
                 Defaults to True.
-            rank (int, optional): Worker ID to identify which worker this
-                represents. Used to interleave results among workers without
-                duplications. Defaults to 0 (first worker).
-            num_replicas (int, optional): Number of GPUs or unique instances
-                which will have their own DataLoader. Used to interleave
-                results among workers without duplications. Defaults to 1.
+            img_size (int): Image width in pixels.
             normalizer (:class:`slideflow.norm.StainNormalizer`, optional):
-                Normalizer to use on images.
-            seed (int, optional): Use the following seed when randomly
-                interleaving. Necessary for synchronized multiprocessing
-                distributed reading.
-            chunk_size (int, optional): Chunk size for image decoding.
-                Defaults to 16.
-            preload_factor (int, optional): Number of batches to preload.
-                Defaults to 1.
-            augment (str, optional): Image augmentations to perform. String
-                    containing characters designating augmentations.
-                    'x' indicates random x-flipping, 'y' y-flipping,
-                    'r' rotating, and 'j' JPEG compression/decompression at
-                    random quality levels. Passing either 'xyrj' or True will
-                    use all augmentations.
+                Normalizer to use on images. Defaults to None.
+            num_parallel_reads (int, optional): Number of parallel reads for each
+                TFRecordDataset. Defaults to 4.
+            num_shards (int, optional): Shard the tfrecord datasets, used for
+                multiprocessing datasets. Defaults to None.
+            pool (multiprocessing.Pool): Shared multiprocessing pool. Useful
+                if ``from_wsi=True``, for sharing a unified processing pool between
+                dataloaders. Defaults to None.
+            rois (list(str), optional): List of ROI paths. Only used if
+                from_wsi=True.  Defaults to None.
+            roi_method (str, optional): Method for extracting ROIs. Only used if
+                from_wsi=True. Defaults to 'auto'.
+            shard_idx (int, optional): Index of the tfrecord shard to use.
+                Defaults to None.
             standardize (bool, optional): Standardize images to (0,1).
                 Defaults to True.
-            num_workers (int, optional): Number of DataLoader workers.
-                Defaults to 2.
-            deterministic (bool, optional): When num_parallel_calls is
-                specified, if this boolean is specified (True or False), it
-                controls the order in which the transformation produces
-                elements. If set to False, the transformation is allowed to
-                yield elements out of order to trade determinism for
-                performance. Defaults to False.
-            drop_last (bool, optional): Drop the last non-full batch.
-                Defaults to False.
+            tile_um (int, optional): Size of tiles to extract from WSI, in
+                microns. Only used if from_wsi=True. Defaults to None.
+            tfrecord_parser (Callable, optional): Custom parser for TFRecords.
+                Defaults to None.
+            transform (Callable, optional): Arbitrary transform function.
+                Performs transformation after augmentations but before
+                standardization. Defaults to None.
+            **decode_kwargs (dict): Keyword arguments to pass to
+                :func:`slideflow.io.tensorflow.decode_image`.
 
         Returns:
             tf.data.Dataset
 
         """
         from slideflow.io.tensorflow import interleave
 
@@ -3178,15 +3245,15 @@
             clip = None
         else:
             tfrecords = self.tfrecords()
             prob_weights = self.prob_weights
             clip = self._clip
             if not tfrecords:
                 raise errors.TFRecordsNotFoundError
-            self.verify_img_format()
+            self.verify_img_format(progress=False)
 
         return interleave(paths=tfrecords,
                           labels=labels,
                           img_size=self.tile_px,
                           batch_size=batch_size,
                           prob_weights=prob_weights,
                           clip=clip,
@@ -3438,15 +3505,15 @@
             sf.io.write_tfrecords_multi(tiles_dir, tfrecord_dir)
             self.update_manifest()
             if delete_tiles:
                 shutil.rmtree(tiles_dir)
 
     def tfrecords_have_locations(self) -> bool:
         """Check if TFRecords have associated tile location information."""
-        for tfr in tqdm(self.tfrecords(), leave=False, desc="Working..."):
+        for tfr in self.tfrecords():
             try:
                 tfr_has_loc = sf.io.tfrecord_has_locations(tfr)
             except errors.TFRecordsError:
                 # Encountered when the TFRecord is empty.
                 continue
             if not tfr_has_loc:
                 log.info(f"{tfr}: Tile location information missing.")
@@ -3571,47 +3638,74 @@
                 with manually assigned labels, pass the first result of
                 dataset.labels(...). If None, returns slide instead of label.
             batch_size (int): Batch size.
             rebuild_index (bool): Re-build index files even if already present.
                 Defaults to True.
 
         Keyword Args:
-            onehot (bool, optional): Onehot encode labels. Defaults to False.
-            incl_slidenames (bool, optional): Include slidenames as third
-                returned variable. Defaults to False.
-            infinite (bool, optional): Infinitely repeat data.
-                Defaults to True.
-            rank (int, optional): Worker ID to identify which worker this
-                represents. Used to interleave results among workers without
-                duplications. Defaults to 0 (first worker).
-            num_replicas (int, optional): Number of GPUs or unique instances
-                which will have their own DataLoader. Used to interleave
-                results among workers without duplications. Defaults to 1.
-            normalizer (:class:`slideflow.norm.StainNormalizer`, optional):
-                Normalizer to use on images. Defaults to None.
-            seed (int, optional): Use the following seed when randomly
-                interleaving. Necessary for synchronized multiprocessing.
+            augment (str or bool): Image augmentations to perform. Augmentations include:
+
+                * ``'x'``: Random horizontal flip
+                * ``'y'``: Random vertical flip
+                * ``'r'``: Random 90-degree rotation
+                * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+                * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+                * ``'n'``: Random :ref:`stain_augmentation` (requires stain normalizer)
+
+                Combine letters to define augmentations, such as ``'xyrjn'``.
+                A value of True will use ``'xyrjb'``.
             chunk_size (int, optional): Chunk size for image decoding.
-                Defaults to 16.
-            preload_factor (int, optional): Number of batches to preload.
                 Defaults to 1.
-            augment (str, optional): Image augmentations to perform. Str
-                containing characters designating augmentations. 'x' indicates
-                random x-flipping, 'y' y-flipping, 'r' rotating, 'j' JPEG
-                compression/decompression at random quality levels, and 'b'
-                random gaussian blur. Passing either 'xyrjb' or True will use
-                all augmentations. Defaults to 'xyrjb'.
-            standardize (bool, optional): Standardize images to (0,1).
-                Defaults to True.
-            num_workers (int, optional): Number of DataLoader workers.
-                Defaults to 2.
-            pin_memory (bool, optional): Pin memory to GPU.
-                Defaults to True.
             drop_last (bool, optional): Drop the last non-full batch.
                 Defaults to False.
+            from_wsi (bool): Generate predictions from tiles dynamically
+                extracted from whole-slide images, rather than TFRecords.
+                Defaults to False (use TFRecords).
+            incl_loc (bool, optional): Include loc_x and loc_y as additional
+                returned variables. Defaults to False.
+            incl_slidenames (bool, optional): Include slidenames as third returned
+                variable. Defaults to False.
+            infinite (bool, optional): Infinitely repeat data. Defaults to True.
+            max_size (bool, optional): Unused argument present for legacy
+                compatibility; will be removed.
+            model_type (str, optional): Used to generate random labels
+                (for StyleGAN2). Not required. Defaults to 'categorical'.
+            num_replicas (int, optional): Number of GPUs or unique instances which
+                will have their own DataLoader. Used to interleave results among
+                workers without duplications. Defaults to 1.
+            num_workers (int, optional): Number of DataLoader workers.
+                Defaults to 2.
+            normalizer (:class:`slideflow.norm.StainNormalizer`, optional):
+                Normalizer. Defaults to None.
+            onehot (bool, optional): Onehot encode labels. Defaults to False.
+            persistent_workers (bool, optional): Sets the DataLoader
+                persistent_workers flag. Defaults toNone (4 if not using a SPAMS
+                normalizer, 1 if using SPAMS).
+            pin_memory (bool, optional): Pin memory to GPU. Defaults to True.
+            pool (multiprocessing.Pool): Shared multiprocessing pool. Useful
+                if from_wsi=True, for sharing a unified processing pool between
+                dataloaders. Defaults to None.
+            prefetch_factor (int, optional): Number of batches to prefetch in each
+                SlideflowIterator. Defaults to 1.
+            rank (int, optional): Worker ID to identify this worker.
+                Used to interleave results.
+                among workers without duplications. Defaults to 0 (first worker).
+            rois (list(str), optional): List of ROI paths. Only used if
+                from_wsi=True.  Defaults to None.
+            roi_method (str, optional): Method for extracting ROIs. Only used if
+                from_wsi=True. Defaults to 'auto'.
+            standardize (bool, optional): Standardize images to mean 0 and
+                variance of 1. Defaults to True.
+            tile_um (int, optional): Size of tiles to extract from WSI, in
+                microns. Only used if from_wsi=True. Defaults to None.
+            transform (Callable, optional): Arbitrary torchvision transform
+                function. Performs transformation after augmentations but
+                before standardization. Defaults to None.
+            tfrecord_parser (Callable, optional): Custom parser for TFRecords.
+                Defaults to None.
 
         """
         from slideflow.io.torch import interleave_dataloader
 
         if isinstance(labels, str):
             labels = self.labels(labels)[0]
         if self.tile_px is None:
@@ -3627,15 +3721,15 @@
             indices = None
             clip = None
         else:
             self.build_index(rebuild_index)
             tfrecords = self.tfrecords()
             if not tfrecords:
                 raise errors.TFRecordsNotFoundError
-            self.verify_img_format()
+            self.verify_img_format(progress=False)
             _idx_dict = self.load_indices()
             indices = [_idx_dict[path_to_name(tfr)] for tfr in tfrecords]
             clip = self._clip
 
         if self.prob_weights:
             prob_weights = [self.prob_weights[tfr] for tfr in tfrecords]
         else:
@@ -3815,28 +3909,31 @@
              | self.annotations.slide.isna())
         ])
         if n_missing == 1:
             log.warn("1 patient does not have a slide assigned.")
         if n_missing > 1:
             log.warn(f"{n_missing} patients do not have a slide assigned.")
 
-    def verify_img_format(self) -> Optional[str]:
+    def verify_img_format(self, *, progress: bool = True) -> Optional[str]:
         """Verify that all tfrecords have the same image format (PNG/JPG).
 
         Returns:
             str: image format (png or jpeg)
         """
         tfrecords = self.tfrecords()
         if len(tfrecords):
             img_formats = []
-            pb = track(
-                tfrecords,
-                description="Verifying tfrecord formats...",
-                transient=True
-            )
+            if progress:
+                pb = track(
+                    tfrecords,
+                    description="Verifying tfrecord formats...",
+                    transient=True
+                )
+            else:
+                pb = tfrecords
             for tfr in pb:
                 fmt = sf.io.detect_tfrecord_format(tfr)[-1]
                 if fmt is not None:
                     img_formats += [fmt]
             if len(set(img_formats)) > 1:
                 log_msg = "Mismatched TFRecord image formats:\n"
                 for tfr, fmt in zip(tfrecords, img_formats):
@@ -3847,7 +3944,48 @@
                 )
             if len(img_formats):
                 return img_formats[0]
             else:
                 return None
         else:
             return None
+
+    def verify_slide_names(self, allow_errors: bool = False) -> bool:
+        """Verify that slide names inside TFRecords match the file names.
+
+        Args:
+            allow_errors (bool): Do not raise an error if there is a mismatch.
+                Defaults to False.
+
+        Returns:
+            bool: If all slide names inside TFRecords match the TFRecord
+                file names.
+
+        Raises:
+            sf.errors.MismatchedSlideNamesError: If any slide names inside
+                TFRecords do not match the TFRecord file names,
+                and allow_errors=False.
+
+        """
+        tfrecords = self.tfrecords()
+        if len(tfrecords):
+            pb = track(
+                tfrecords,
+                description="Verifying tfrecord slide names...",
+                transient=True
+            )
+            for tfr in pb:
+                first_record = sf.io.get_tfrecord_by_index(tfr, 0)
+                if first_record['slide'] == sf.util.path_to_name(tfr):
+                    continue
+                elif allow_errors:
+                    return False
+                else:
+                    raise errors.MismatchedSlideNamesError(
+                        "Mismatched slide name in TFRecord {}: expected slide "
+                        "name {} based on filename, but found {}. ".format(
+                            tfr,
+                            sf.util.path_to_name(tfr),
+                            first_record['slide']
+                        )
+                )
+        return True
```

## slideflow/errors.py

```diff
@@ -55,14 +55,18 @@
         )
 
 
 class MismatchedImageFormatsError(DatasetError):
     pass
 
 
+class MismatchedSlideNamesError(DatasetError):
+    pass
+
+
 # --- Mosaic & Heatmap Errors -------------------------------------------------
 class HeatmapError(Exception):
     pass
 
 
 class MosaicError(Exception):
     pass
@@ -101,14 +105,18 @@
 
 
 # --- TFRecords errors --------------------------------------------------------
 class TFRecordsError(Exception):
     pass
 
 
+class TFRecordsIndexError(Exception):
+    pass
+
+
 class EmptyTFRecordsError(Exception):
     pass
 
 
 class InvalidTFRecordIndex(Exception):
     pass
```

## slideflow/heatmap.py

```diff
@@ -28,15 +28,22 @@
 
 
 Inset = namedtuple("Inset", "x y zoom loc mark1 mark2 axes")
 
 # -----------------------------------------------------------------------------
 
 class Heatmap:
-    """Generates a heatmap of predictions across a whole-slide image."""
+    """Generate a heatmap of predictions across a whole-slide image.
+
+    This interface is designed to be used with tile-based models, and
+    does not support multiple-instance learning models. Attention heatmaps
+    of multiple-instance learning models can be generated using
+    :func:`slideflow.mil.predict_slide`.
+
+    """
 
     def __init__(
         self,
         slide: Union[str, WSI],
         model: str,
         stride_div: Optional[int] = None,
         batch_size: int = 32,
@@ -45,15 +52,15 @@
         img_format: str = 'auto',
         generate: bool = True,
         generator_kwargs: Optional[Dict[str, Any]] = None,
         device: Optional["torch.device"] = None,
         load_method: Optional[str] = None,
         **wsi_kwargs
     ) -> None:
-        """Initialize a heatmap from a path to a slide or a :class:``slideflow.WSI``.
+        """Initialize a heatmap from a path to a slide or a :class:`slideflow.WSI`.
 
         Examples
             Create a heatmap from a path to a slide.
 
                 .. code-block:: python
 
                     model_path = 'path/to/saved_model'
@@ -992,33 +999,33 @@
             self.generate(**generator_kwargs)
         elif generator_kwargs:
             log.warn("Heatmap generate=False, ignoring generator_kwargs ("
                      f"{generator_kwargs})")
 
     def view(self):
         raise NotImplementedError
-    
+
 # -----------------------------------------------------------------------------
 
 def calculate_heatmap_extent(
-        wsi: "sf.WSI", 
-        thumbnail: "Image", 
+        wsi: "sf.WSI",
+        thumbnail: "Image",
         grid: np.ndarray
 ) -> Tuple[float, float, float, float]:
     """Calculate implot extent for a heatmap grid."""
     full_extract = int(wsi.tile_um / wsi.mpp)
     wsi_stride = int(full_extract / wsi.stride_div)
     _overlay_wsi_dim = (wsi_stride * (grid.shape[1]),
                         wsi_stride * (grid.shape[0]))
     _overlay_offset_wsi_dim = (
-        full_extract/2 - wsi_stride/2, 
+        full_extract/2 - wsi_stride/2,
         full_extract/2 - wsi_stride/2
     )
     thumb_ratio = (
-        wsi.dimensions[0] / thumbnail.size[0], 
+        wsi.dimensions[0] / thumbnail.size[0],
         wsi.dimensions[1] / thumbnail.size[1]
     )
     return (
         _overlay_offset_wsi_dim[0] / thumb_ratio[0],
         _overlay_wsi_dim[0] / thumb_ratio[0],
         _overlay_wsi_dim[1] / thumb_ratio[1],
         _overlay_offset_wsi_dim[1] / thumb_ratio[1]
```

## slideflow/mosaic.py

```diff
@@ -59,14 +59,16 @@
         try:
             if isinstance(image, np.ndarray):
                 return normalizer.rgb_to_rgb(image)
             elif img_format in ('jpg', 'jpeg'):
                 return normalizer.jpeg_to_rgb(image)
             elif img_format == 'png':
                 return normalizer.png_to_rgb(image)
+            else:
+                return normalizer.transform(image)
         except Exception as e:
             log.error("Error encountered during image normalization, "
                         f"displaying image tile non-normalized. {e}")
     if isinstance(image, np.ndarray):
         return image
     else:
         image_arr = np.fromstring(image, np.uint8)
@@ -177,18 +179,19 @@
                 to center of grid space. If 'centroid', for each grid, will
                 calculate which tile is nearest to centroid tile_meta.
                 Defaults to 'nearest'.
             tile_meta (dict, optional): Tile metadata, used for tile_select.
                 Dictionary should have slide names as keys, mapped to list of
                 metadata (length of list = number of tiles in slide).
                 Defaults to None.
-            normalizer ((str or `slideflow.norm.StainNormalizer`), optional):
+            normalizer ((str or :class:`slideflow.norm.StainNormalizer`), optional):
                 Normalization strategy to use on image tiles. Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, normalizer will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
         """
         self.tile_point_distances = []  # type: List[Dict]
         self.slide_map = None
         self.tfrecords = tfrecords
         self.grid_images = {}
         self.grid_coords = []   # type: np.ndarray
@@ -402,15 +405,14 @@
         max_y = y_points.max()
         min_y = y_points.min()
         log.debug(f'Loaded {len(self.points)} points.')
 
         self.tile_size = (max_x - min_x) / self.num_tiles_x
         self.num_tiles_y = int((max_y - min_y) / self.tile_size)
 
-        log.info("Building grid...")
         self.grid_idx = np.reshape(np.dstack(np.indices((self.num_tiles_x, self.num_tiles_y))), (self.num_tiles_x * self.num_tiles_y, 2))
         _grid_offset = np.array([(self.tile_size/2) + min_x, (self.tile_size/2) + min_y])
         self.grid_coords = (self.grid_idx * self.tile_size) + _grid_offset
 
         points_added = 0
         x_bins = np.arange(min_x, max_x, ((max_x - min_x) / self.num_tiles_x))[1:]
         y_bins = np.arange(min_y, max_y, ((max_y - min_y) / self.num_tiles_y))[1:]
@@ -444,25 +446,24 @@
                     raise errors.MosaicError(
                         'Mosaic centroid option requires tile_meta.'
                     )
                 else:
                     centroid_index = get_centroid_index(_points.meta.values)
                     self.points.loc[_points.index[centroid_index], 'selected'] = True
 
-        log.info('Selecting tile images...')
         start = time.time()
 
         if tile_select == 'first':
             grid_group = self.points.groupby(['grid_x', 'grid_y'])
             first_indices = grid_group.nth(0).points_index.values
             self.points.loc[first_indices, 'selected'] = True
         elif tile_select in ('nearest', 'centroid'):
             self.points['selected'] = False
             dist_fn = partial(select_nearest_points)
-            pool = DPool(os.cpu_count())
+            pool = DPool(sf.util.num_cpu())
             for i, _ in track(enumerate(pool.imap_unordered(dist_fn, range(len(self.grid_idx))), 1), total=len(self.grid_idx)):
                 pass
             pool.close()
             pool.join()
         else:
             raise ValueError(
                 f'Unrecognized value for tile_select: "{tile_select}"'
@@ -524,17 +525,14 @@
         """
         if (focus is not None or focus_slide is not None) and self.tfrecords is None:
             raise ValueError("Unable to plot with focus; slides/tfrecords not configured.")
 
         log.debug("Initializing figure...")
         self._initialize_figure(figsize=figsize, background=background)
 
-        # Next, prepare mosaic grid by placing tile outlines
-        log.info('Placing tile outlines...')
-
         # Reset alpha and display size
         if focus_slide:
             self.points['alpha'] = 1.
         self.points['display_size'] = self.tile_size
 
         if focus_slide:
             for idx in self.grid_idx:
@@ -562,15 +560,15 @@
                     log.error(f"TFRecord {tfr} not found in slide_map")
                     image = None
             else:
                 image = self.images[idx]
             to_map.append((idx, point.grid_x * self.tile_size, point.grid_y * self.tile_size, point.display_size, point.alpha, image))
 
         if pool is None:
-            pool = DPool(os.cpu_count())
+            pool = DPool(sf.util.num_cpu())
             should_close_pool = True
         for i, (point_idx, image, extent, alpha) in track(enumerate(pool.imap(partial(process_tile_image, decode_kwargs=self.decode_kwargs), to_map)), total=len(selected_points)):
             if point_idx is not None:
                 self._record_point(point_idx)
                 self._plot_tile_image(image, extent, alpha)
                 point = self.points.loc[point_idx]
                 self.grid_images[(point.grid_x, point.grid_y)] = image
@@ -636,9 +634,14 @@
 
         """
         from slideflow.studio.widgets import MosaicWidget
         from slideflow.studio import Studio
 
         studio = Studio(widgets=[MosaicWidget])
         mosaic = studio.get_widget('MosaicWidget')
-        mosaic.load(self.slide_map, tfrecords=self.tfrecords, slides=slides)
+        mosaic.load(
+            self.slide_map,
+            tfrecords=self.tfrecords,
+            slides=slides,
+            normalizer=self.normalizer
+        )
         studio.run()
```

## slideflow/project.py

```diff
@@ -36,15 +36,15 @@
 from .project_utils import (  # noqa: F401
     auto_dataset, auto_dataset_allow_none, get_validation_settings,
     get_first_nested_directory, get_matching_directory, BreastER, ThyroidBRS,
     LungAdenoSquam
 )
 
 if TYPE_CHECKING:
-    from slideflow.model import DatasetFeatures, Trainer
+    from slideflow.model import DatasetFeatures, Trainer, BaseFeatureExtractor
     from slideflow.slide import SlideReport
     from slideflow import simclr, mil
     from ConfigSpace import ConfigurationSpace, Configuration
     from smac.facade.smac_bb_facade import SMAC4BB  # noqa: F401
 
 
 class Project:
@@ -1277,15 +1277,17 @@
         *,
         k: int = 0,
         eval_tag: Optional[str] = None,
         filters: Optional[Dict] = None,
         filter_blank: Optional[Union[str, List[str]]] = None,
         attention_heatmaps: bool = True
     ) -> None:
-        """Evaluate CLAM model on activations and export attention heatmaps.
+        """Deprecated function.
+
+        Evaluate CLAM model on activations and export attention heatmaps.
 
         Args:
             exp_name (str): Name of experiment to evaluate (subfolder in clam/)
             pt_files (str): Path to pt_files containing tile-level features.
             outcomes (str or list): Annotation column that specifies labels.
             tile_px (int): Tile width in pixels.
             tile_um (int or str): Tile width in microns (int) or magnification
@@ -1305,14 +1307,18 @@
             attention_heatmaps (bool, optional): Save attention heatmaps of
                 validation dataset. Defaults to True.
 
         Returns:
             None
 
         """
+        warnings.warn("Project.evaluate_clam() is deprecated. Please use "
+                      "Project.evaluate_mil()",
+                      DeprecationWarning)
+
         import slideflow.clam as clam
         from slideflow.clam import export_attention
         from slideflow.clam import Generic_MIL_Dataset
 
         # Detect source CLAM experiment which we are evaluating.
         # First, assume it lives in this project's clam folder
         if exists(join(self.root, 'clam', exp_name, 'experiment.json')):
@@ -1613,16 +1619,17 @@
                 detection quality control - discarding tiles with detected
                 out-of-focus regions or artifact - and/or otsu's method.
                 Increases tile extraction time. Defaults to None.
             report (bool): Save a PDF report of tile extraction.
                 Defaults to True.
             normalizer (str, optional): Normalization strategy.
                 Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
             whitespace_fraction (float, optional): Range 0-1. Discard tiles
                 with this fraction of whitespace. If 1, will not perform
                 whitespace filtering. Defaults to 1.
             whitespace_threshold (int, optional): Range 0-255. Defaults to 230.
                 Threshold above which a pixel (RGB average) is whitespace.
             grayspace_fraction (float, optional): Range 0-1. Defaults to 0.6.
@@ -1957,31 +1964,41 @@
         df = sf.DatasetFeatures(model=model,
                                 dataset=dataset,
                                 annotations=labels,
                                 **kwargs)
         return df
 
     @auto_dataset_allow_none
-    def generate_features_for_clam(
+    def generate_features_for_clam(self, *args, **kwargs):
+        warnings.warn(
+            "Project.generate_features_for_clam() is deprecated. "
+            "Please use .generate_feature_bags()",
+            DeprecationWarning
+        )
+        self.generate_feature_bags(*args, **kwargs)
+
+    @auto_dataset_allow_none
+    def generate_feature_bags(
         self,
-        model: str,
+        model: Union[str, "BaseFeatureExtractor"],
+        dataset: Optional[Dataset] = None,
         outdir: str = 'auto',
         *,
-        dataset: Optional[Dataset] = None,
         filters: Optional[Dict] = None,
         filter_blank: Optional[Union[str, List[str]]] = None,
         min_tiles: int = 16,
         max_tiles: int = 0,
-        layers: Union[str, List[str]] = 'postconv',
         force_regenerate: bool = False,
-        batch_size: int = 32
+        batch_size: int = 32,
+        slide_batch_size: int = 16,
+        **kwargs: Any
     ) -> str:
-        """Generate tile-level features for slides for use with CLAM.
+        """Generate tile-level features for slides for use with MIL models.
 
-        By default, CLAM features are saved in the ``pt_files`` folder
+        By default, features are exported to the ``pt_files`` folder
         within the project root directory.
 
         Args:
             model (str): Path to model from which to generate activations.
                 May provide either this or "pt_files"
             outdir (str, optional): Save exported activations in .pt format.
                 Defaults to 'auto' (project directory).
@@ -1998,28 +2015,34 @@
                 blank values in these patient annotation columns.
                 Defaults to None.
             min_tiles (int, optional): Only include slides with this minimum
                 number of tiles. Defaults to 16.
             max_tiles (int, optional): Only include maximum of this many tiles
                 per slide. Defaults to 0 (all tiles).
             layers (list): Which model layer(s) generate activations.
-                Defaults to 'postconv'.
+                If ``model`` is a saved model, this defaults to 'postconv'.
+                Defaults to None.
             force_regenerate (bool): Forcibly regenerate activations
                 for all slides even if .pt file exists. Defaults to False.
             min_tiles (int, optional): Minimum tiles per slide. Skip slides
                 not meeting this threshold. Defaults to 16.
             batch_size (int): Batch size during feature calculation.
                 Defaults to 32.
+            slide_batch_size (int): Interleave feature calculation across
+                this many slides. Higher values may improve performance
+                but require more memory. Defaults to 16.
+            **kwargs: Additional keyword arguments are passed to
+                :class:`slideflow.DatasetFeatures`.
 
         Returns:
             Path to directory containing exported .pt files
 
         """
         # Check if the model exists and has a valid parameters file
-        if exists(model):
+        if isinstance(model, str) and exists(model):
             config = sf.util.get_model_config(model)
 
             if dataset is None:
                 log.debug(f"Auto-building dataset from provided model {model}")
                 dataset = self.dataset(
                     tile_px=config['tile_px'],
                     tile_um=config['tile_um'],
@@ -2041,32 +2064,46 @@
                 'an imagenet-pretrained model, or otherwise not a '
                 'saved Slideflow model.'
             )
 
         # Ensure min_tiles is applied to the dataset.
         dataset = dataset.filter(min_tiles=min_tiles)
 
-        # If the model does not exist, check if it is an architecture name
+        # Check if the model is an architecture name
         # (for using an Imagenet pretrained model)
-        if sf.model.is_extractor(model):
-            log.info(f"Building feature extractor {model}.")
+        if isinstance(model, str) and sf.model.is_extractor(model):
+            log.info(f"Building feature extractor: [green]{model}[/]")
+            layer_kw = dict(layers=kwargs['layers']) if 'layers' in kwargs else dict()
             model = sf.model.build_feature_extractor(
-                model, tile_px=dataset.tile_px
+                model, tile_px=dataset.tile_px, **layer_kw
             )
 
             # Set the pt_files directory if not provided
             if outdir.lower() == 'auto':
                 outdir = join(self.root, 'pt_files', model.tag)
 
-        elif not exists(model):
+        elif isinstance(model, str) and not exists(model):
             raise ValueError(
                 f"'{model}' is neither a path to a saved model nor the name "
                 "of a valid feature extractor (use sf.model.list_extractors() "
                 "for a list of all available feature extractors).")
 
+        elif not isinstance(model, str):
+            from slideflow.model.base import BaseFeatureExtractor
+            if not isinstance(model, BaseFeatureExtractor):
+                raise ValueError(
+                    f"'{model}' is neither a path to a saved model nor the name "
+                    "of a valid feature extractor (use sf.model.list_extractors() "
+                    "for a list of all available feature extractors).")
+
+            log.info(f"Using feature extractor: [green]{model.tag}[/]")
+            # Set the pt_files directory if not provided
+            if outdir.lower() == 'auto':
+                outdir = join(self.root, 'pt_files', model.tag)
+
         # Create the pt_files directory
         if not exists(outdir):
             os.makedirs(outdir)
 
         # Detect already generated pt files
         done = [
             path_to_name(f) for f in os.listdir(outdir)
@@ -2077,30 +2114,43 @@
             all_slides = dataset.slides()
             slides_to_generate = [s for s in all_slides if s not in done]
             if len(slides_to_generate) != len(all_slides):
                 to_skip = len(all_slides) - len(slides_to_generate)
                 skip_p = f'{to_skip}/{len(all_slides)}'
                 log.info(f"Skipping {skip_p} finished slides.")
             if not slides_to_generate:
-                log.warn("No slides to generate CLAM features.")
+                log.warn("No slides for which to generate features.")
                 return outdir
             dataset = dataset.filter(filters={'slide': slides_to_generate})
             filtered_slides_to_generate = dataset.slides()
             log.info(f'Skipping {len(done)} files already done.')
             log.info(f'Working on {len(filtered_slides_to_generate)} slides')
 
-        # Set up activations interface
-        df = sf.DatasetFeatures(
-            model=model,
-            dataset=dataset,
-            layers=layers,
-            include_preds=False,
-            batch_size=batch_size
-        )
-        df.to_torch(outdir)
+        # Set up activations interface.
+        # Calculate features one slide at a time to reduce memory consumption.
+        for slide_batch in tqdm(sf.util.batch(dataset.slides(), slide_batch_size),
+                                total=len(dataset.slides()) // slide_batch_size):
+            try:
+                _dataset = dataset.remove_filter(filters='slide')
+            except errors.DatasetFilterError:
+                _dataset = dataset
+            _dataset = _dataset.filter(filters={'slide': slide_batch})
+            df = sf.DatasetFeatures(
+                model=model,
+                dataset=_dataset,
+                include_preds=False,
+                include_uncertainty=False,
+                batch_size=batch_size,
+                verbose=False,
+                progress=False,
+                pool_sort=False,
+                **kwargs
+            )
+            df.to_torch(outdir, verbose=False)
+
         return outdir
 
     @auto_dataset
     def generate_heatmaps(
         self,
         model: str,
         *,
@@ -2463,15 +2513,15 @@
 
         Args:
             header_x (str): Annotations file header with X-axis coords.
             header_y (str): Annotations file header with Y-axis coords.
 
         Keyword Args:
             dataset (:class:`slideflow.Dataset`): Dataset object.
-            model (str, optional): Path to Tensorflow model to use when
+            model (str, optional): Path to model to use when
                 generating layer activations.
             Defaults to None.
                 If not provided, mosaic will not be calculated or saved.
                 If provided, saved in project mosaic directory.
             outcomes (list(str)): Column name(s) in annotations file from which
                 to read category labels.
             max_tiles (int, optional): Limits the number of tiles taken from
@@ -3328,16 +3378,16 @@
                 Defaults to None (same as training).
             checkpoint (str, optional): Path to cp.ckpt from which to load
                 weights. Defaults to None.
             pretrain (str, optional): Either 'imagenet' or path to Tensorflow
                 model from which to load weights. Defaults to 'imagenet'.
             multi_gpu (bool): Train using multiple GPUs when available.
                 Defaults to False.
-            resume_training (str, optional): Path to Tensorflow model to
-                continue training. Defaults to None.
+            resume_training (str, optional): Path to model to continue training.
+                Only valid in Tensorflow backend. Defaults to None.
             starting_epoch (int): Start training at the specified epoch.
                 Defaults to 0.
             steps_per_epoch_override (int): If provided, will manually set the
                 number of steps in an epoch. Default epoch length is the number
                 of total tiles.
             save_predictions (bool or str, optional): Save tile, slide, and
                 patient-level predictions at each evaluation. May be 'csv',
@@ -3346,15 +3396,15 @@
             save_model (bool, optional): Save models when evaluating at
                 specified epochs. Defaults to True.
             validate_on_batch (int): Perform validation every N batches.
                 Defaults to 0 (only at epoch end).
             validation_batch_size (int): Validation dataset batch size.
                 Defaults to 32.
             use_tensorboard (bool): Add tensorboard callback for realtime
-                training monitoring. Defaults to False.
+                training monitoring. Defaults to True.
             validation_steps (int): Number of steps of validation to perform
                 each time doing a mid-epoch validation check. Defaults to 200.
 
         Returns:
             Dict with model names mapped to train_acc, val_loss, and val_acc
 
         """
@@ -3618,14 +3668,16 @@
         simclr_args: "simclr.SimCLR_Args",
         train_dataset: Dataset,
         val_dataset: Optional[Dataset] = None,
         *,
         exp_label: Optional[str] = None,
         outcomes: Optional[Union[str, List[str]]] = None,
         dataset_kwargs: Optional[Dict[str, Any]] = None,
+        normalizer: Optional[Union[str, "sf.norm.StainNormalizer"]] = None,
+        normalizer_source: Optional[str] = None,
         **kwargs
     ) -> None:
         """Train SimCLR model.
 
         Models are saved in ``simclr`` folder in the project root directory.
 
         See :ref:`simclr_ssl` for more information.
@@ -3655,26 +3707,28 @@
             exp_label = 'simclr'
         if not exists(join(self.root, 'simclr')):
             os.makedirs(join(self.root, 'simclr'))
         outdir = sf.util.create_new_model_dir(
             join(self.root, 'simclr'), exp_label
         )
 
-        # get base SimCLR args/settings if not provided
+        # Get base SimCLR args/settings if not provided
         if not simclr_args:
             simclr_args = simclr.get_args()
         assert isinstance(simclr_args, simclr.SimCLR_Args)
 
         # Create dataset builder, which SimCLR will use to create
         # the input pipeline for training
         builder = simclr.DatasetBuilder(
             train_dts=train_dataset,
             val_dts=val_dataset,
             labels=outcomes,
-            dataset_kwargs=dataset_kwargs
+            dataset_kwargs=dataset_kwargs,
+            normalizer=normalizer,
+            normalizer_source=normalizer_source
         )
         simclr.run_simclr(simclr_args, builder, model_dir=outdir, **kwargs)
 
     def train_clam(self, *args, splits: str = 'splits.json', **kwargs):
         """Deprecated function.
 
         Train a CLAM model from layer activations
```

## slideflow/cellseg/__init__.py

```diff
@@ -19,14 +19,15 @@
 from functools import partial
 from PIL import Image, ImageDraw
 from cellpose.utils import outlines_list
 from cellpose.models import Cellpose
 from cellpose import transforms, plot, dynamics
 from slideflow.slide.utils import draw_roi
 from slideflow.util import batch_generator, log
+from slideflow.model import torch_utils
 
 from . import seg_utils
 
 if TYPE_CHECKING:
     from rich.progress import Progress, TaskID
     import shapely.geometry
 
@@ -470,15 +471,16 @@
     pb_tasks: Optional[List["TaskID"]] = None,
     show_progress: bool = True,
     save_flow: bool = True,
     cp_thresh: float = 0.0,
     flow_threshold: float = 0.4,
     interp: bool = True,
     tile: bool = True,
-    verbose: bool = True
+    verbose: bool = True,
+    device: Optional[str] = None,
 ) -> Segmentation:
     """Segment cells in a whole-slide image, returning masks and centroids.
 
     Args:
         slide (str, :class:`slideflow.WSI`): Whole-slide image. May be a path
             (str) or WSI object (`slideflow.WSI`).
 
@@ -548,15 +550,15 @@
         target_size = int(window_size / downscale)
     if slide.stride_div != 1:
         log.warn("Whole-slide cell segmentation not configured for strides "
                  f"other than 1 (got: {slide.stride_div}).")
 
     # Set up model and parameters. --------------------------------------------
     start_time = time.time()
-    device = torch.device('cuda:0')
+    device = torch_utils.get_device(device)
     model = Cellpose(gpu=True, device=device)
     cp = model.cp
     cp.batch_size = batch_size
     cp.net.load_model(cp.pretrained_model[0], cpu=(not cp.gpu))  # Modify to accept different models
     cp.net.eval()
     rescale = 1  # No rescaling, as we are manually setting diameter = diam_mean
     mask_dim = (slide.stride * (slide.shape[0]-1) + slide.tile_px,
```

## slideflow/cellseg/seg_utils.py

```diff
@@ -41,15 +41,15 @@
 
 
 def fast_outlines_list(masks, num_threads=None):
     """Get outlines of masks as a list to loop over for plotting. Accelerated
     by multithreading for large images.
     """
     if num_threads is None:
-        num_threads = os.cpu_count()
+        num_threads = sf.util.num_cpu()
 
     def get_mask_outline(mask_id):
         mn = (masks == mask_id)
         if mn.sum() > 0:
             contours = cv2.findContours(
                 mn.astype(np.uint8),
                 mode=cv2.RETR_EXTERNAL,
@@ -76,15 +76,15 @@
     return np.array([np.mean(np.unravel_index(row.data, shape), 1).astype(np.int32)
                      if row.getnnz()
                      else (0, 0)
                      for row in sparse_mask])
 
 
 def get_sparse_centroid(mask, sparse_mask):
-    n_proc = os.cpu_count() if os.cpu_count() else 8
+    n_proc = sf.util.num_cpu(default=8)
     with mp.Pool(n_proc) as pool:
         return np.concatenate(
             pool.map(
                 partial(get_sparse_chunk_centroid, shape=mask.shape),
                 [sparse_mask[i:j] for (i, j) in sparse_split_indices(sparse_mask.shape[0], n_proc)]
             ))
```

## slideflow/gan/stylegan3/stylegan3/visualizer.py

```diff
@@ -20,15 +20,15 @@
 from .viz import layer_widget
 from .viz import equivariance_widget
 from .gui_utils import imgui_window
 from .gui_utils import imgui_utils
 from .gui_utils import gl_utils
 from .gui_utils import text_utils
 
-from slideflow.studio.widgets import ModelWidget
+from slideflow.workbench.model_widget import ModelWidget
 
 try:
     from . import dnnlib
 except ImportError:
     # Error occurs when running script in StyleGAN3 directory
     import dnnlib
```

## slideflow/gan/stylegan3/stylegan3/viz/renderer.py

```diff
@@ -43,14 +43,30 @@
 class CaptureSuccess(Exception):
     def __init__(self, out):
         super().__init__()
         self.out = out
 
 #----------------------------------------------------------------------------
 
+def get_device(device=None):
+    if device is None and torch.cuda.is_available():
+        return torch.device('cuda')
+    elif (device is None 
+          and hasattr(torch.backends, 'mps') 
+          and torch.backends.mps.is_available()):
+        return torch.device('mps')
+    elif device is None:
+        return torch.device('cpu')
+    elif isinstance(device, str):
+        return torch.device(device)
+    else:
+        return device
+
+#----------------------------------------------------------------------------
+
 def _sinc(x):
     y = (x * np.pi).abs()
     z = torch.sin(y) / y.clamp(1e-30, float('inf'))
     return torch.where(y < 1e-30, torch.ones_like(x), z)
 
 def _lanczos_window(x, a):
     x = x.abs() / a
@@ -143,38 +159,47 @@
     return z, m
 
 #----------------------------------------------------------------------------
 
 class Renderer:
     def __init__(self, visualizer=None, gan_px=0, gan_um=0):
         self._visualizer        = visualizer
-        self._device            = torch.device('cuda')
+        self._device            = get_device()
         self._pkl_data          = dict()    # {pkl: dict | CapturedException, ...}
         self._networks          = dict()    # {cache_key: torch.nn.Module, ...}
         self._pinned_bufs       = dict()    # {(shape, dtype): torch.Tensor, ...}
         self._cmaps             = dict()    # {name: torch.Tensor, ...}
         self._is_timing         = False
-        self._start_event       = torch.cuda.Event(enable_timing=True)
-        self._end_event         = torch.cuda.Event(enable_timing=True)
         self._net_layers        = dict()    # {cache_key: [dnnlib.EasyDict, ...], ...}
         self._uq_thread         = None
         self._stop_uq_thread    = False
         self._stop_pred_thread  = False
         self.gan_px             = gan_px
         self.gan_um             = gan_um
 
+        # Only record stream if the device is CUDA,
+        # as the MPS device from MacOS does not yet support htis.
+        if self._device.type == 'cuda':
+            self._start_event   = torch.cuda.Event(enable_timing=True)
+            self._end_event     = torch.cuda.Event(enable_timing=True)
+        else:
+            self._start_event   = None
+            self._end_event     = None
+
     def render(self, **args):
-        self._is_timing = True
-        self._start_event.record(torch.cuda.current_stream(self._device))
+        if self._start_event is not None:
+            self._is_timing = True
+            self._start_event.record(torch.cuda.current_stream(self._device))
         res = dnnlib.EasyDict()
         try:
             self._render_impl(res, **args)
         except:
             res.error = CapturedException()
-        self._end_event.record(torch.cuda.current_stream(self._device))
+        if self._end_event is not None:
+            self._end_event.record(torch.cuda.current_stream(self._device))
         if 'image' in res:
             if not isinstance(res.image, np.ndarray):
                 res.image = self.to_cpu(res.image).numpy()
         if 'stats' in res:
             res.stats = self.to_cpu(res.stats).numpy()
         if 'error' in res:
             res.error = str(res.error)
@@ -244,15 +269,17 @@
         #        print(f'{name:<50s}{np.min(value):<16g}{np.max(value):g}')
         return net
 
     def _get_pinned_buf(self, ref):
         key = (tuple(ref.shape), ref.dtype)
         buf = self._pinned_bufs.get(key, None)
         if buf is None:
-            buf = torch.empty(ref.shape, dtype=ref.dtype).pin_memory()
+            buf = torch.empty(ref.shape, dtype=ref.dtype)
+            if self._device.type == 'cuda':
+                buf = buf.pin_memory()
             self._pinned_bufs[key] = buf
         return buf
 
     def to_device(self, buf):
         return self._get_pinned_buf(buf).copy_(buf).to(self._device)
 
     def to_cpu(self, buf):
```

## slideflow/io/__init__.py

```diff
@@ -8,32 +8,33 @@
 from os.path import exists, isdir, isfile, join
 from random import shuffle
 from typing import Any, Dict, Optional, Tuple, Union, List
 
 import slideflow as sf
 from slideflow import errors
 from slideflow.io.io_utils import detect_tfrecord_format, convert_dtype
-from slideflow.util import log
+from slideflow.util import log, tfrecord2idx
 from slideflow.util.tfrecord2idx import get_tfrecord_by_index, get_tfrecord_length
 from rich.progress import Progress
 
 # --- Backend-specific imports and configuration ------------------------------
 
 if sf.backend() == 'tensorflow':
-    from slideflow.io.tensorflow import get_tfrecord_parser  # noqa F401
-    from slideflow.io.tensorflow import read_and_return_record  # noqa F401
-    from slideflow.io.tensorflow import serialized_record
-
+    from slideflow.io.tensorflow import (
+        get_tfrecord_parser, read_and_return_record, serialized_record,
+        _decode_image
+    )
     from tensorflow.data import TFRecordDataset
     from tensorflow.io import TFRecordWriter
 
 elif sf.backend() == 'torch':
-    from slideflow.io.torch import \
-        get_tfrecord_parser  # type: ignore  # noqa F401
-    from slideflow.io.torch import read_and_return_record, serialized_record
+    from slideflow.io.torch import (
+        get_tfrecord_parser, read_and_return_record, serialized_record,
+        _decode_image
+    )
     from slideflow.tfrecord import TFRecordWriter
     from slideflow.tfrecord.torch.dataset import TFRecordDataset
 
 else:
     raise errors.UnrecognizedBackendError
 
 # -----------------------------------------------------------------------------
@@ -133,32 +134,49 @@
         location = tuple(location)
     if (not isinstance(location, tuple)
        or len(location) != 2
        or not isinstance(location[0], (int, np.integer))
        or not isinstance(location[1], (int, np.integer))):
         raise IndexError(f"index must be a tuple of two ints. Got: {location}")
 
-    dataset = TFRecordDataset(tfrecord)
-    parser = get_tfrecord_parser(
-        tfrecord,
-        ('slide', 'image_raw', 'loc_x', 'loc_y'),
-        decode_images=decode
-    )
-    for i, record in enumerate(dataset):
-        slide, image_raw, loc_x, loc_y = parser(record)
-        if (loc_x, loc_y) == location:
-            if decode:
-                return slide, image_raw
-            else:
-                return record
+    # Use index files, if available.
+    index = tfrecord2idx.find_index(tfrecord)
+    if decode and index and tfrecord2idx.index_has_locations(index):
+        locations = tfrecord2idx.get_locations_from_index(index)
+        try:
+            idx = locations.index(location)
+        except ValueError:
+            log.error(
+                f"Unable to find record with location {location} in {tfrecord}"
+            )
+            return False, False
+        record = tfrecord2idx.get_tfrecord_by_index(tfrecord, idx)
+        slide = record['slide']
+        image = sf.io._decode_image(record['image_raw'])
+        return slide, image
 
-    log.error(
-        f"Unable to find record with location {location} in {tfrecord}"
-    )
-    return False, False
+    else:
+        parser = get_tfrecord_parser(
+            tfrecord,
+            ('slide', 'image_raw', 'loc_x', 'loc_y'),
+            decode_images=decode
+        )
+        dataset = TFRecordDataset(tfrecord)
+        for i, record in enumerate(dataset):
+            slide, image, loc_x, loc_y = parser(record)
+            if (loc_x, loc_y) == location:
+                if decode:
+                    return slide, image
+                else:
+                    return record
+
+        log.error(
+            f"Unable to find record with location {location} in {tfrecord}"
+        )
+        return False, False
 
 
 def write_tfrecords_multi(input_directory: str, output_directory: str) -> None:
     '''Scans a folder for subfolders, assumes subfolders are slide names.
     Assembles all image tiles within subfolders and labels using the provided
     annotation_dict, assuming the subfolder is the slide name. Collects all
     image tiles and exports into multiple tfrecord files, one for each slide.
@@ -306,36 +324,38 @@
             os.makedirs(dest_folder)
         tile_filename = f"tile{i}.{img_type}"
         image_string = open(join(dest_folder, tile_filename), 'wb')
         image_string.write(image_raw)
         image_string.close()
 
 
-def get_locations_from_tfrecord(
-    filename: str,
-    as_dict: bool = True
-) -> Union[
-    Dict[int, Tuple[int, int]],
-    List[Tuple[int, int]],
-]:
-    '''Returns dictionary mapping indices to tile locations (X, Y)'''
-    out_dict = {}
+def get_locations_from_tfrecord(filename: str) -> List[Tuple[int, int]]:
+    """Return list of tile locations (X, Y) for all items in the TFRecord."""
+
+    # Use the TFRecord index file, if one exists and it has info stored.
+    index = tfrecord2idx.find_index(filename)
+    if index and tfrecord2idx.index_has_locations(index):
+        return tfrecord2idx.get_locations_from_index(index)
+
+    # Otherwise, read the TFRecord manually.
     out_list = []
     for i in range(sf.io.get_tfrecord_length(filename)):
         record = sf.io.get_tfrecord_by_index(filename, i)
         loc_x = record['loc_x']
         loc_y = record['loc_y']
-        if as_dict:
-            out_dict.update({i: (loc_x, loc_y)})
-        else:
-            out_list.append((loc_x, loc_y))
-    return out_dict if as_dict else out_list
+        out_list.append((loc_x, loc_y))
+    return out_list
 
 
 def tfrecord_has_locations(
     filename: str,
     check_x: int = True,
     check_y: bool = False
 ) -> bool:
     """Check if a given TFRecord has location information stored."""
+    index = tfrecord2idx.find_index(filename)
+    if index and tfrecord2idx.index_has_locations(index):
+        if check_y:
+            return np.load(index)['locations'].shape[1] == 2
+        return True
     record = sf.io.get_tfrecord_by_index(filename, 0)
     return (((not check_x) or 'loc_x' in record ) and ((not check_y) or 'loc_y' in record ))
```

## slideflow/io/tensorflow.py

```diff
@@ -136,18 +136,41 @@
 
 @tf.function
 def process_image(
     record: Union[tf.Tensor, Dict[str, tf.Tensor]],
     *args: Any,
     standardize: bool = False,
     augment: Union[bool, str] = False,
+    transform: Optional[Callable] = None,
     size: Optional[int] = None
 ) -> Tuple[Union[Dict, tf.Tensor], ...]:
-    """Applies augmentations and/or standardization to an image Tensor."""
+    """Applies augmentations and/or standardization to an image Tensor.
 
+    Args:
+        record (Union[tf.Tensor, Dict[str, tf.Tensor]]): Image Tensor.
+
+    Keyword Args:
+        standardize (bool, optional): Standardize images. Defaults to False.
+        augment (str or bool): Image augmentations to perform. Augmentations include:
+
+            * ``'x'``: Random horizontal flip
+            * ``'y'``: Random vertical flip
+            * ``'r'``: Random 90-degree rotation
+            * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+            * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+
+            Combine letters to define augmentations, such as ``'xyrj'``.
+            A value of True will use ``'xyrjb'``.
+            Note: this function does not support stain augmentation.
+        transform (Callable, optional): Arbitrary transform function.
+            Performs transformation after augmentations but before standardization.
+            Defaults to None.
+        size (int, optional): Set the image shape. Defaults to None.
+
+    """
     if isinstance(record, dict):
         image = record['tile_image']
     else:
         image = record
     if size is not None:
         image.set_shape([size, size, 3])
     if augment is True or (isinstance(augment, str) and 'j' in augment):
@@ -200,15 +223,17 @@
                     false_fn=lambda: gaussian.auto_gaussian(image, sigma=1.0),
                 ),
                 false_fn=lambda: gaussian.auto_gaussian(image, sigma=0.5),
             ),
             false_fn=lambda: image
         )
     if augment is True or (isinstance(augment, str) and 'i' in augment):
-        ...
+        raise NotImplementedError("Random pixel interpolation not implemented.")
+    if transform is not None:
+        image = transform(image)
     if standardize:
         image = tf.image.per_image_standardization(image)
 
     if isinstance(record, dict):
         to_return = {k: v for k, v in record.items() if k != 'tile_image'}
         to_return['tile_image'] = image
         return tuple([to_return] + list(args))
@@ -263,14 +288,21 @@
         image = tf.image.resize(image, (resize_target, resize_target), method=resize_method, antialias=resize_aa)
         image.set_shape([resize_target, resize_target, 3])
     elif size:
         image.set_shape([size, size, 3])
     return image
 
 
+def _decode_image(img_string: bytes, *, img_type: Optional[str] = None):
+    if img_type is None:
+        import imghdr
+        img_type = imghdr.what('', img_string)
+    return decode_image(img_string, img_type)
+
+
 def get_tfrecord_parser(
     tfrecord_path: str,
     features_to_return: Optional[Iterable[str]] = None,
     to_numpy: bool = False,
     decode_images: bool = True,
     img_size: Optional[int] = None,
     error_if_invalid: bool = True,
@@ -294,20 +326,25 @@
             Defaults to True.
         standardize (bool, optional): Standardize images into the range (0,1).
             Defaults to False.
         img_size (int): Width of images in pixels. Will call tf.set_shape(...)
             if provided. Defaults to False.
         normalizer (:class:`slideflow.norm.StainNormalizer`): Stain normalizer
             to use on images. Defaults to None.
-        augment (str): Image augmentations to perform. String containing
-            characters designating augmentations. 'x' indicates random
-            x-flipping, 'y' y-flipping, 'r' rotating, 'j' JPEG
-            compression/decompression at random quality levels, and 'b'
-            random gaussian blur. Passing either 'xyrjb' or True will use all
-            augmentations.
+        augment (str or bool): Image augmentations to perform. Augmentations include:
+
+            * ``'x'``: Random horizontal flip
+            * ``'y'``: Random vertical flip
+            * ``'r'``: Random 90-degree rotation
+            * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+            * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+            * ``'n'``: Random :ref:`stain_augmentation` (requires stain normalizer)
+
+            Combine letters to define augmentations, such as ``'xyrjn'``.
+            A value of True will use ``'xyrjb'``.
         error_if_invalid (bool, optional): Raise an error if a tfrecord cannot
             be read. Defaults to True.
     """
     features, img_type = detect_tfrecord_format(tfrecord_path)
     if features is None:
         log.debug(f"Unable to read tfrecord at {tfrecord_path} - is it empty?")
         return None
@@ -348,17 +385,24 @@
         else:
             return [process_feature(f) for f in features_to_return]
 
     return parser
 
 
 def parser_from_labels(labels: Labels) -> Callable:
-    '''Returns a label parsing function used for parsing slides into single
+    """Create a label parsing function used for parsing slides into single
     or multi-outcome labels.
-    '''
+
+    Args:
+        labels (dict): Dictionary mapping slide names to outcome labels.
+
+    Returns:
+        Callable: Label parsing function.
+
+    """
     outcome_labels = np.array(list(labels.values()))
     slides = list(labels.keys())
     if len(outcome_labels.shape) == 1:
         outcome_labels = np.expand_dims(outcome_labels, axis=1)
     with tf.device('/cpu'):
         annotations_tables = []
         for oi in range(outcome_labels.shape[1]):
@@ -381,89 +425,118 @@
 
     return label_parser
 
 
 def interleave(
     paths: List[str],
     *,
-    img_size: int,
+    augment: bool = False,
     batch_size: Optional[int],
-    prob_weights: Optional[Dict[str, float]] = None,
     clip: Optional[Dict[str, int]] = None,
-    labels: Optional[Labels] = None,
-    incl_slidenames: bool = False,
+    deterministic: bool = False,
+    drop_last: bool = False,
+    from_wsi: bool = False,
     incl_loc: Optional[str] = None,
+    incl_slidenames: bool = False,
     infinite: bool = True,
-    augment: bool = False,
-    standardize: bool = True,
+    img_size: int,
+    labels: Optional[Labels] = None,
     normalizer: Optional["StainNormalizer"] = None,
-    num_shards: Optional[int] = None,
-    shard_idx: Optional[int] = None,
     num_parallel_reads: int = 4,
-    deterministic: bool = False,
-    drop_last: bool = False,
-    from_wsi: bool = False,
-    tile_um: Optional[int] = None,
+    num_shards: Optional[int] = None,
+    pool: Optional["mp.pool.Pool"] = None,
+    prob_weights: Optional[Dict[str, float]] = None,
     rois: Optional[List[str]] = None,
     roi_method: str = 'auto',
-    pool: Optional["mp.pool.Pool"] = None,
+    shard_idx: Optional[int] = None,
+    standardize: bool = True,
+    tile_um: Optional[int] = None,
     tfrecord_parser: Optional[Callable] = None,
+    transform: Optional[Callable] = None,
     **decode_kwargs: Any
 ) -> Iterable:
+    """Generate an interleaved dataset from a collection of tfrecord files.
 
-    """Generates an interleaved dataset from a collection of tfrecord files,
-    sampling from tfrecord files randomly according to balancing if provided.
-    Requires manifest for balancing. Assumes TFRecord files are named by slide.
+    The interleaved dataset samples from tfrecord files randomly according to
+    balancing, if provided. Requires manifest for balancing. Assumes TFRecord
+    files are named by slide.
 
     Args:
         paths (list(str)): List of paths to TFRecord files or whole-slide
             images.
-        img_size (int): Image width in pixels.
+
+    Keyword Args:
+        augment (str or bool): Image augmentations to perform. Augmentations include:
+
+            * ``'x'``: Random horizontal flip
+            * ``'y'``: Random vertical flip
+            * ``'r'``: Random 90-degree rotation
+            * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+            * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+            * ``'n'``: Random :ref:`stain_augmentation` (requires stain normalizer)
+
+            Combine letters to define augmentations, such as ``'xyrjn'``.
+            A value of True will use ``'xyrjb'``.
         batch_size (int): Batch size.
-        prob_weights (dict, optional): Dict mapping tfrecords to probability of
-            including in batch. Defaults to None.
         clip (dict, optional): Dict mapping tfrecords to number of tiles to
             take per tfrecord. Defaults to None.
-        labels (dict or str, optional): Dict or function. If dict, must map
-            slide names to outcome labels. If function, function must accept an
-            image (tensor) and slide name (str), and return a dict
-            {'image_raw': image (tensor)} and label (int or float). If not
-            provided,  all labels will be None.
+        deterministic (bool, optional): When num_parallel_calls is specified,
+            if this boolean is specified, it controls the order in which the
+            transformation produces elements. If set to False, the
+            transformation is allowed to yield elements out of order to trade
+            determinism for performance. Defaults to False.
+        drop_last (bool, optional): Drop the last non-full batch.
+            Defaults to False.
+        from_wsi (bool): Generate predictions from tiles dynamically
+            extracted from whole-slide images, rather than TFRecords.
+            Defaults to False (use TFRecords).
+        incl_loc (str, optional): 'coord', 'grid', or None. Return (x,y)
+            origin coordinates ('coord') for each tile along with tile
+            images, or the (x,y) grid coordinates for each tile.
+            Defaults to 'coord'.
         incl_slidenames (bool, optional): Include slidenames as third returned
             variable. Defaults to False.
-        incl_loc (str, optional): 'coord', 'grid', or None. Return (x,y)
-                origin coordinates ('coord') for each tile along with tile
-                images, or the (x,y) grid coordinates for each tile.
-                Defaults to 'coord'.
         infinite (bool, optional): Create an finite dataset. WARNING: If
             infinite is False && balancing is used, some tiles will be skipped.
             Defaults to True.
-        augment (str): Image augmentations to perform. String containing
-            characters designating augmentations. 'x' indicates random
-            x-flipping, 'y' y-flipping, 'r' rotating, 'j' JPEG
-            compression/decompression at random quality levels, and 'b'
-            random gaussian blur. Passing either 'xyrjb' or True will use all
-            augmentations.
-        standardize (bool, optional): Standardize images to (0,1).
-            Defaults to True.
+        img_size (int): Image width in pixels.
+        labels (dict or str, optional): Dict or function. If dict, must map
+            slide names to outcome labels. If function, function must accept an
+            image (tensor) and slide name (str), and return a dict
+            {'image_raw': image (tensor)} and label (int or float). If not
+            provided,  all labels will be None.
         normalizer (:class:`slideflow.norm.StainNormalizer`, optional):
             Normalizer to use on images. Defaults to None.
+        num_parallel_reads (int, optional): Number of parallel reads for each
+            TFRecordDataset. Defaults to 4.
         num_shards (int, optional): Shard the tfrecord datasets, used for
             multiprocessing datasets. Defaults to None.
+        pool (multiprocessing.Pool): Shared multiprocessing pool. Useful
+            if ``from_wsi=True``, for sharing a unified processing pool between
+            dataloaders. Defaults to None.
+        prob_weights (dict, optional): Dict mapping tfrecords to probability of
+            including in batch. Defaults to None.
+        rois (list(str), optional): List of ROI paths. Only used if
+            from_wsi=True.  Defaults to None.
+        roi_method (str, optional): Method for extracting ROIs. Only used if
+            from_wsi=True. Defaults to 'auto'.
         shard_idx (int, optional): Index of the tfrecord shard to use.
             Defaults to None.
-        num_parallel_reads (int, optional): Number of parallel reads for each
-            TFRecordDataset. Defaults to 4.
-        deterministic (bool, optional): When num_parallel_calls is specified,
-            if this boolean is specified, it controls the order in which the
-            transformation produces elements. If set to False, the
-            transformation is allowed to yield elements out of order to trade
-            determinism for performance. Defaults to False.
-        drop_last (bool, optional): Drop the last non-full batch.
-            Defaults to False.
+        standardize (bool, optional): Standardize images to (0,1).
+            Defaults to True.
+        tile_um (int, optional): Size of tiles to extract from WSI, in
+            microns. Only used if from_wsi=True. Defaults to None.
+        tfrecord_parser (Callable, optional): Custom parser for TFRecords.
+            Defaults to None.
+        transform (Callable, optional): Arbitrary transform function.
+            Performs transformation after augmentations but before
+            standardization. Defaults to None.
+        **decode_kwargs (dict): Keyword arguments to pass to
+            :func:`slideflow.io.tensorflow.decode_image`.
+
     """
     if not len(paths):
         raise errors.TFRecordsNotFoundError
     _path_type = "slides" if from_wsi else "tfrecords"
     log.debug(
         f'Interleaving {len(paths)} {_path_type}: infinite={infinite}, '
         f'num_parallel_reads={num_parallel_reads}')
@@ -503,19 +576,19 @@
 
             def base_parser(record):
                 return tuple([record[f] for f in features_to_return])
 
             # Load slides and apply Otsu's thresholding
             if pool is None and sf.slide_backend() == 'cucim':
                 pool = mp.Pool(
-                    8 if os.cpu_count is None else os.cpu_count(),
+                    sf.util.num_cpu(default=8),
                     initializer=sf.util.set_ignore_sigint
                 )
             elif pool is None:
-                pool = mp.dummy.Pool(16 if os.cpu_count is None else os.cpu_count())
+                pool = mp.dummy.Pool(sf.util.num_cpu(default=16))
             wsi_list = []
             to_remove = []
             otsu_list = []
             for path in paths:
                 try:
                     wsi = sf.WSI(
                         path,
@@ -606,25 +679,22 @@
                 partial(normalizer.tf_to_tf, augment=(isinstance(augment, str)
                                                       and 'n' in augment)),
                 num_parallel_calls=tf.data.AUTOTUNE,
                 deterministic=deterministic,
             )
             if normalizer.vectorized:
                 dataset = dataset.unbatch()
-            if normalizer.method == 'macenko':
-                # Drop the images that causes an error, e.g. if eigen
-                # decomposition is unsuccessful.
-                dataset = dataset.apply(tf.data.experimental.ignore_errors())
 
         # ------- Standardize and augment images ------------------------------
         dataset = dataset.map(
             partial(
                 process_image,
                 standardize=standardize,
                 augment=augment,
+                transform=transform,
                 size=img_size
             ),
             num_parallel_calls=tf.data.AUTOTUNE,
             deterministic=deterministic
         )
         # ------- Batch and prefetch ------------------------------------------
         if batch_size:
@@ -687,15 +757,26 @@
 
 def tfrecord_example(
     slide: bytes,
     image_raw: bytes,
     loc_x: Optional[int] = 0,
     loc_y: Optional[int] = 0
 ) -> "Example":
-    '''Returns a Tensorflow Data example for TFRecord storage.'''
+    """Return a Tensorflow Data example for TFRecord storage.
+
+    Args:
+        slide (bytes): Slide name.
+        image_raw (bytes): Image bytes.
+        loc_x (Optional[int], optional): X coordinate of image. Defaults to 0.
+        loc_y (Optional[int], optional): Y coordinate of image. Defaults to 0.
+
+    Returns:
+        Example: Tensorflow Data example.
+
+    """
     feature = {
         'slide': _bytes_feature(slide),
         'image_raw': _bytes_feature(image_raw),
     }
     if loc_x is not None:
         feature.update({'loc_x': _int64_feature(loc_x)})
     if loc_y is not None:
@@ -705,21 +786,43 @@
 
 def serialized_record(
     slide: bytes,
     image_raw: bytes,
     loc_x: int = 0,
     loc_y: int = 0
 ) -> bytes:
-    '''Returns a serialized example for TFRecord storage, ready to be written
-    by a TFRecordWriter.'''
+    """Serialize a record for TFRecord storage.
+
+    The serialized record will be in a data format ready to be written
+    by a TFRecordWriter.
+
+    Args:
+        slide (bytes): Slide name.
+        image_raw (bytes): Image bytes.
+        loc_x (int, optional): X coordinate of image. Defaults to 0.
+        loc_y (int, optional): Y coordinate of image. Defaults to 0.
+
+    Returns:
+        bytes: Serialized record.
+
+    """
     return tfrecord_example(slide, image_raw, loc_x, loc_y).SerializeToString()
 
 
 def multi_image_example(slide: bytes, image_dict: Dict) -> "Example":
-    '''Returns a Tensorflow Data example for storage with multiple images.'''
+    """Returns a Tensorflow Data example for storage with multiple images.
+
+    Args:
+        slide (bytes): Slide name.
+        image_dict (Dict): Dictionary of image names and image bytes.
+
+    Returns:
+        Example: Tensorflow Data example.
+
+    """
     feature = {
         'slide': _bytes_feature(slide)
     }
     for image_label in image_dict:
         feature.update({
             image_label: _bytes_feature(image_dict[image_label])
         })
@@ -727,16 +830,24 @@
 
 
 def join_tfrecord(
     input_folder: str,
     output_file: str,
     assign_slide: str = None
 ) -> None:
-    '''Randomly samples from tfrecords in the input folder with shuffling,
-    and combines into a single tfrecord file.'''
+    """Randomly sample from tfrecords in the input folder with shuffling,
+    and combine into a single tfrecord file.
+
+    Args:
+        input_folder (str): Folder containing tfrecord files.
+        output_file (str): Output tfrecord file.
+        assign_slide (str, optional): Assign a slide name to all images.
+            Defaults to None.
+
+    """
     writer = tf.io.TFRecordWriter(output_file)
     tfrecord_files = glob(join(input_folder, "*.tfrecords"))
     datasets = []
     if assign_slide:
         slide = assign_slide.encode('utf-8')
     features, img_type = detect_tfrecord_format(tfrecord_files[0])
     parser = get_tfrecord_parser(
@@ -763,17 +874,22 @@
             continue
         writer.write(
             read_and_return_record(record, parser, slide)  # type: ignore
         )
 
 
 def split_tfrecord(tfrecord_file: str, output_folder: str) -> None:
-    '''Splits records from a single tfrecord file into individual tfrecord
-    files by slide.
-    '''
+    """Split records from a single tfrecord into individual tfrecord
+    files, stratified by slide.
+
+    Args:
+        tfrecord_file (str): Path to tfrecord file.
+        output_folder (str): Path to output folder.
+
+    """
     dataset = tf.data.TFRecordDataset(tfrecord_file)
     parser = get_tfrecord_parser(tfrecord_file, ['slide'], to_numpy=True)
     full_parser = get_tfrecord_parser(
         tfrecord_file,
         decode_images=False,
         to_numpy=True
     )
@@ -791,28 +907,37 @@
             read_and_return_record(record, full_parser)  # type: ignore
         )
     for slide in writers.keys():
         writers[slide].close()
 
 
 def print_tfrecord(target: str) -> None:
-    '''Prints the slide names (and locations, if present) for records
+    """Print the slide names (and locations, if present) for records
     in the given tfrecord file.
-    '''
+
+    Args:
+        target: Path to the tfrecord file or folder containing tfrecord files.
+
+    """
     if isfile(target):
         _print_record(target)
     else:
         tfrecord_files = glob(join(target, "*.tfrecords"))
         for tfr in tfrecord_files:
             _print_record(tfr)
 
 
 def checkpoint_to_tf_model(models_dir: str, model_name: str) -> None:
-    '''Converts a checkpoint file into a saved model.'''
+    """Convert a checkpoint file into a saved model.
 
+    Args:
+        models_dir: Directory containing the model.
+        model_name: Name of the model to convert.
+
+    """
     checkpoint = join(models_dir, model_name, "cp.ckpt")
     tf_model = join(models_dir, model_name, "untrained_model")
     updated_tf_model = join(models_dir, model_name, "checkpoint_model")
     model = tf.keras.models.load_model(tf_model)
     model.load_weights(checkpoint)
     try:
         model.save(updated_tf_model)
@@ -825,17 +950,28 @@
 def transform_tfrecord(
     origin: str,
     target: str,
     assign_slide: Optional[str] = None,
     hue_shift: Optional[float] = None,
     resize: Optional[float] = None,
 ) -> None:
-    '''Transforms images in a single tfrecord. Can perform hue shifting,
-    resizing, or re-assigning slide label.
-    '''
+    """Transform images in a single tfrecord.
+
+    Can perform hue shifting, resizing, or re-assigning slide label.
+
+    Args:
+        origin: Path to the original tfrecord file.
+        target: Path to the new tfrecord file.
+        assign_slide: If provided, will assign this slide name to all
+            records in the new tfrecord.
+        hue_shift: If provided, will shift the hue of all images by
+            this amount.
+        resize: If provided, will resize all images to this size.
+
+    """
     log.info(f"Transforming tiles in tfrecord [green]{origin}")
     log.info(f"Saving to new tfrecord at [green]{target}")
     if assign_slide:
         log.info(f"Assigning slide name [bold]{assign_slide}")
     if hue_shift:
         log.info(f"Shifting hue by [bold]{hue_shift}")
     if resize:
@@ -884,15 +1020,20 @@
             loc_y
         )
         writer.write(tf_example.SerializeToString())
     writer.close()
 
 
 def shuffle_tfrecord(target: str) -> None:
-    '''Shuffles records in a TFRecord, saving the original to a .old file.'''
+    """Shuffle records in a TFRecord, saving the original to a .old file.
+
+    Args:
+        target: Path to the tfrecord file.
+
+    """
 
     old_tfrecord = target+".old"
     shutil.move(target, old_tfrecord)
     dataset = tf.data.TFRecordDataset(old_tfrecord)
     writer = tf.io.TFRecordWriter(target)
     extracted_tfrecord = []
     for record in dataset:
@@ -900,14 +1041,18 @@
     shuffle(extracted_tfrecord)
     for record in extracted_tfrecord:
         writer.write(record)
     writer.close()
 
 
 def shuffle_tfrecords_by_dir(directory: str) -> None:
-    '''For each TFRecord in a directory, shuffles records in the TFRecord,
+    """For each TFRecord in a directory, shuffle records in the TFRecord,
     saving the original to a .old file.
-    '''
+
+    Args:
+        directory: Path to the directory containing tfrecord files.
+
+    """
     records = [tfr for tfr in listdir(directory) if tfr[-10:] == ".tfrecords"]
     for record in records:
         log.info(f'Working on {record}')
         shuffle_tfrecord(join(directory, record))
```

## slideflow/io/torch.py

```diff
@@ -58,15 +58,14 @@
         normalizer: Optional["StainNormalizer"] = None,
         clip: Optional[List[int]] = None,
         chunk_size: int = 1,
         use_labels: bool = True,
         model_type: str = 'categorical',
         onehot: bool = False,
         indices: Optional[np.ndarray] = None,
-        device: Optional[torch.device] = None,
         max_size: int = 0,
         from_wsi: bool = False,
         tile_um: Optional[int] = None,
         rois: Optional[List[str]] = None,
         roi_method: str = 'auto',
         pool: Optional[Any] = None,
         transform: Optional[Any] = None,
@@ -84,19 +83,25 @@
                 (returns image, label, slide). Defaults to False.
             incl_loc (bool, optional): Include location info. Returns samples
                 in the form (returns ..., loc_x, loc_y). Defaults to False.
             rank (int, optional): Which GPU replica this dataset is used for.
                 Assists with synchronization across GPUs. Defaults to 0.
             num_replicas (int, optional): Total number of GPU replicas.
                 Defaults to 1.
-            augment (str of bool, optional): Image augmentations to perform.
-                If string, 'x' performs horizontal flipping, 'y' performs
-                vertical flipping, 'r' performs rotation, 'j' performs random
-                JPEG compression (e.g. 'xyr', 'xyrj', 'xy'). If bool, True
-                performs all and False performs None. Defaults to True.
+            augment (str or bool): Image augmentations to perform. Augmentations include:
+
+                * ``'x'``: Random horizontal flip
+                * ``'y'``: Random vertical flip
+                * ``'r'``: Random 90-degree rotation
+                * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+                * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+                * ``'n'``: Random :ref:`stain_augmentation` (requires stain normalizer)
+
+                Combine letters to define augmentations, such as ``'xyrjn'``.
+                A value of True will use ``'xyrjb'``.
             standardize (bool, optional): Standardize images to mean 0 and
                 variance of 1. Defaults to True.
             num_tiles (int, optional): Dict mapping tfrecord names to number
                 of total tiles. Defaults to None.
             infinite (bool, optional): Inifitely loop through dataset.
                 Defaults to True.
             prob_weights (list(float), optional): Probability weights for
@@ -113,14 +118,31 @@
                 (for StyleGAN2). Not required. Defaults to 'categorical'.
             onehot (bool, optional): Onehot encode outcomes. Defaults to False.
             indices (numpy.ndarray, optional): Indices in form of array,
                 with np.loadtxt(index_path, dtype=np.int64) for each tfrecord.
                 Defaults to None.
             max_size (bool, optional): Unused argument present for legacy
                 compatibility; will be removed.
+            from_wsi (bool): Generate predictions from tiles dynamically
+                extracted from whole-slide images, rather than TFRecords.
+                Defaults to False (use TFRecords).
+            tile_um (int, optional): Size of tiles to extract from WSI, in
+                microns. Only used if from_wsi=True. Defaults to None.
+            rois (list(str), optional): List of ROI paths. Only used if
+                from_wsi=True.  Defaults to None.
+            roi_method (str, optional): Method for extracting ROIs. Only used if
+                from_wsi=True. Defaults to 'auto'.
+            pool (multiprocessing.Pool): Shared multiprocessing pool. Useful
+                if ``from_wsi=True``, for sharing a unified processing pool between
+                dataloaders. Defaults to None.
+            transform (Callable, optional): Arbitrary torchvision transform
+                function. Performs transformation after augmentations but
+                before standardization. Defaults to None.
+            tfrecord_parser (Callable, optional): Custom parser for TFRecords.
+                Defaults to None.
         """
         self.tfrecords = np.array(tfrecords).astype(np.string_)
         if prob_weights is not None:
             self.prob_weights = np.array(prob_weights)
         else:
             self.prob_weights = None  # type: ignore
         self.clip = clip
@@ -136,15 +158,14 @@
         self.chunk_size = chunk_size
         self.normalizer = normalizer
         self.onehot = onehot
         self.incl_slidenames = incl_slidenames
         self.incl_loc = incl_loc
         self.num_tiles = num_tiles
         self.model_type = model_type
-        self.device = device
         self.from_wsi = from_wsi
         self.tile_um = tile_um
         self.rois = rois
         self.roi_method = roi_method
         self.pool = pool
         self.transform = transform
         self.interleave_kwargs = interleave_kwargs
@@ -278,15 +299,14 @@
             clip=self.clip,
             infinite=self.infinite,
             normalizer=self.normalizer,
             num_replicas=self.num_replicas * num_workers,
             rank=self.rank + worker_id,
             chunk_size=self.chunk_size,
             indices=self.indices,
-            device=self.device,
             tile_px=self.img_size,
             from_wsi=self.from_wsi,
             tile_um=self.tile_um,
             rois=self.rois,
             roi_method=self.roi_method,
             pool=self.pool,
             transform=self.transform,
@@ -411,14 +431,15 @@
             to_return += [slide]
         return to_return
 
     def get_label(self, idx: Any) -> Any:
         """Returns a random label. Used for compatibility with StyleGAN2."""
         return np.random.rand(*self.label_shape)
 
+# -------------------------------------------------------------------------
 
 def _get_images_by_dir(directory: str) -> List[str]:
     files = [
         f for f in listdir(directory)
         if ((isfile(join(directory, f)))
             and (sf.util.path_to_ext(f) in ("jpg", "jpeg", "png")))
     ]
@@ -435,29 +456,32 @@
     splits: Optional[Dict[str, float]] = None,
     shard: Optional[Tuple[int, int]] = None,
     infinite: bool = True,
     **kwargs
 ) -> Iterable[Union[Dict[str, np.ndarray],
                     Tuple[Dict[str, np.ndarray],
                     Dict[str, List[np.ndarray]]]]]:
-    """Create an iterator by reading and merging multiple tfrecord datasets.
+    """Create an iterator by reading and merging multiple slide dataloaders.
 
     Args:
-        paths (list of str): List of tfrecord paths.
-        indices (dict): dict mapping tfrecord names to index paths.
-            Input index path pattern.
+        slides (list of str): List of slide paths.
         splits (dict):  Dictionary of (key, value) pairs, where the key is used
             to construct the data and index path(s) and the value determines
             the contribution of each split to the batch.
+        shard (tuple(int, int), optional): If provided, will only extract
+            tiles from the shard with index `shard[0]` out of `shard[1]`
+            shards. Defaults to None.
         infinite (bool, optional): Whether the returned iterator should be
             infinite or not. Defaults to True.
 
     Returns:
 
-        it (iterator): A repeating iterator that generates batches of data.
+        it (iterator): A repeating iterator that generates batches of data,
+        interleaving from the provided slides.
+
     """
     if splits is not None:
         splits_list = splits
     else:
         splits_list = np.array(  # type: ignore
             [0.5 for t in range(len(slides))]
         )
@@ -466,60 +490,218 @@
                            infinite=infinite,
                            **kwargs)
                for slide in slides]
     return RandomSampler(loaders, splits_list, shard=None)
 
 
 def is_cwh(img: torch.Tensor) -> torch.Tensor:
+    """Check if Tensor is in C x W x H format."""
     return (len(img.shape) == 3 and img.shape[0] == 3
             or (len(img.shape) == 4 and img.shape[1] == 3))
 
 
 def is_whc(img: torch.Tensor) -> torch.Tensor:
+    """Check if Tensor is in W x H x C format."""
     return img.shape[-1] == 3
 
 
 def cwh_to_whc(img: torch.Tensor) -> torch.Tensor:
+    """Convert torch tensor from C x W x H => W x H x C"""
     if len(img.shape) == 3:
         return img.permute(1, 2, 0)  # CWH -> WHC
     elif len(img.shape) == 4:
         return img.permute(0, 2, 3, 1)  # BCWH -> BWHC
     else:
         raise ValueError(
             "Invalid shape for channel conversion. Expected 3 or 4 dims, "
             f"got {len(img.shape)} (shape={img.shape})")
 
 
 def whc_to_cwh(img: torch.Tensor) -> torch.Tensor:
+    """Convert torch tensor from W x H x C => C x W x H"""
     if len(img.shape) == 3:
         return img.permute(2, 0, 1)  # WHC => CWH
     elif len(img.shape) == 4:
         return img.permute(0, 3, 1, 2)  # BWHC -> BCWH
     else:
         raise ValueError(
             "Invalid shape for channel conversion. Expected 3 or 4 dims, "
             f"got {len(img.shape)} (shape={img.shape})")
 
+# -------------------------------------------------------------------------
+
+class RandomCardinalRotation:
+    """Torchvision transform for random cardinal rotation."""
+    def __init__(self):
+        self.angles = [0, 90, 180, 270]
+
+    def __call__(self, x):
+        angle = random.choice(self.angles)
+        return transforms.functional.rotate(x, angle)
+
+
+class RandomGaussianBlur:
+    """Torchvision transform for random Gaussian blur."""
+    def __init__(self, sigma: List[float], weights: List[float]):
+        assert len(sigma) == len(weights)
+        self.sigma = sigma
+        self.weights = weights
+        self.blur_fn = {
+            s: (
+                transforms.GaussianBlur(self.calc_kernel(s), sigma=s)
+                if s else lambda x: x
+            ) for s in self.sigma
+        }
+
+    @staticmethod
+    def calc_kernel(sigma: float) -> int:
+        sigma = 0.5
+        opt_kernel = int((sigma * 4) + 1)
+        if opt_kernel % 2 == 0:
+            opt_kernel += 1
+        return opt_kernel
+
+    def __call__(self, x):
+        s = random.choices(self.sigma, weights=self.weights)[0]
+        return self.blur_fn[s](x)
 
-def auto_gaussian(image: torch.Tensor, sigma: float) -> torch.Tensor:
-    """Perform Gaussian blur on an image with a given sigma, automatically
-    calculating the appropriate Gaussian kernel size.
+
+def random_jpeg_compression(img: torch.Tensor):
+    """Perform random JPEG compression on an image.
 
     Args:
-        image (torch.Tensor): Image or batch of images.
-        sigma (float): Sigma.
+        img (torch.Tensor): Image tensor, shape C x W x H.
 
     Returns:
-        torch.Tensor: Image(s) with Gaussian blur applied.
+        torch.Tensor: Transformed image (C x W x H).
+
     """
-    opt_kernel = int((sigma * 4) + 1)
-    if opt_kernel % 2 == 0:
-        opt_kernel += 1
-    return torchvision.transforms.GaussianBlur(opt_kernel, sigma=sigma)(image)
+    q = (torch.rand(1)[0] * 50) + 50
+    img = torchvision.io.encode_jpeg(img, quality=q)
+    return torchvision.io.decode_image(img)
+
+
+def compose_color_distortion(s=1.0):
+    """Compose augmentation for random color distortion.
+
+    Args:
+        s (float): Strength of the distortion.
+
+    Returns:
+        Callable: PyTorch transform
+
+    """
+    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)
+    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)
+    rnd_gray = transforms.RandomGrayscale(p=0.2)
+    color_distort = transforms.Compose(
+        [whc_to_cwh, rnd_color_jitter, rnd_gray, cwh_to_whc])
+    return color_distort
+
+
+def compose_augmentations(
+    augment: Union[str, bool] = False,
+    *,
+    standardize: bool = False,
+    normalizer: Optional["StainNormalizer"] = None,
+    transform: Optional[Callable] = None,
+    whc: bool = False
+):
+    """Compose an augmentation pipeline for image processing.
+
+    Args:
+        augment (str or bool): Image augmentations to perform. Augmentations include:
+
+            * ``'x'``: Random horizontal flip
+            * ``'y'``: Random vertical flip
+            * ``'r'``: Random 90-degree rotation
+            * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+            * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+
+            Combine letters to define augmentations, such as ``'xyrj'``.
+            A value of True will use ``'xyrjb'``.
+            Note: this function does not support stain augmentation.
+
+    Keyword args:
+        standardize (bool, optional): Standardize images into the range (0,1)
+            using img / (255/2) - 1. Defaults to False.
+        normalizer (:class:`slideflow.norm.StainNormalizer`): Stain normalizer
+            to use on images. Defaults to None.
+        transform (Callable, optional): Arbitrary torchvision transform function.
+            Performs transformation after augmentations but before standardization.
+            Defaults to None.
+        whc (bool): Images are in W x H x C format. Defaults to False.
+    """
+
+    transformations = []
+
+    # Random JPEG compression.
+    if augment is True or (isinstance(augment, str) and 'j' in augment):
+        transformations.append(
+            lambda img: torch.where(
+                torch.rand(1)[0] < 0.5,
+                random_jpeg_compression(img),
+                img
+            )
+        )
 
+    # Random cardinal rotation.
+    if augment is True or (isinstance(augment, str) and 'r' in augment):
+        transformations.append(RandomCardinalRotation())
+
+    # Random x-flip.
+    if augment is True or (isinstance(augment, str) and 'x' in augment):
+        transformations.append(transforms.RandomHorizontalFlip(p=0.5))
+
+    # Random y-flip.
+    if augment is True or (isinstance(augment, str) and 'y' in augment):
+        transformations.append(transforms.RandomVerticalFlip(p=0.5))
+
+    # Stain normalization.
+    if normalizer is not None:
+        transformations.append(
+            lambda img: normalizer.torch_to_torch(  # type: ignore
+                img,
+                augment=(isinstance(augment, str) and 'n' in augment)
+            )
+        )
+
+    # Random color distortion.
+    if (isinstance(augment, str) and 'd' in augment):
+        transformations.append(compose_color_distortion())
+
+    # Random Gaussian blur.
+    if augment is True or (isinstance(augment, str) and 'b' in augment):
+        transformations.append(
+            RandomGaussianBlur(
+                sigma=[0, 0.5, 1.0, 1.5, 2.0],
+                weights=[0.9, 0.1, 0.05, 0.025, 0.0125]
+            )
+        )
+
+    # Arbitrary transformations via `augment` argument.
+    if callable(augment):
+        transformations.append(augment)
+
+    # Arbitrary transformations via `transform` argument.
+    if transform is not None:
+        transformations.append(transform)
+
+    # Image standardization.
+    # Note: not the same as tensorflow's per_image_standardization
+    # Convert back: image = (image + 1) * (255/2)
+    if standardize:
+        transformations.append(lambda img: img / (255/2) - 1)
+
+    if transformations and whc:
+        return transforms.Compose([whc_to_cwh] + transformations + [cwh_to_whc])
+    else:
+        return transforms.Compose(transformations)
+
+# -------------------------------------------------------------------------
 
 def read_and_return_record(
     record: bytes,
     parser: Callable,
     assign_slide: Optional[str] = None
 ) -> Dict:
     """Process raw TFRecord bytes into a format that can be written with
@@ -601,93 +783,52 @@
     if standardize:
         img = convert_dtype(img, torch.float32)
     return img
 
 
 def _decode_image(
     image: Union[bytes, str, torch.Tensor],
-    img_type: str,
-    standardize: bool = False,
-    normalizer: Optional["StainNormalizer"] = None,
-    augment: bool = False,
+    *,
+    img_type: Optional[str] = None,
     device: Optional[torch.device] = None,
     transform: Optional[Any] = None,
 ) -> torch.Tensor:
-    """Decodes image. Torch implementation; different than sf.io.tensorflow"""
+    """Decodes image string/bytes to Tensor (W x H x C).
+
+    Torch implementation; different than sf.io.tensorflow.
+
+    Args:
+        image (Union[bytes, str, torch.Tensor]): Image to decode.
+
+    Keyword args:
+        img_type (str, optional): Image type. Defaults to None.
+        device (torch.device, optional): Device to move image to.
+            Defaults to None.
+        transform (Callable, optional): Arbitrary torchvision transform function.
+            Performs transformation after augmentations but before standardization.
+            Defaults to None.
 
+    """
     if img_type != 'numpy':
         np_data = torch.from_numpy(np.fromstring(image, dtype=np.uint8))
         image = cwh_to_whc(torchvision.io.decode_image(np_data))
         # Alternative method using PIL decoding:
         # image = np.array(Image.open(BytesIO(img_string)))
+
     assert isinstance(image, torch.Tensor)
 
-    def random_jpeg_compression(img):
-        img = torchvision.io.encode_jpeg(
-            whc_to_cwh(img),
-            quality=(torch.rand(1)[0]*50 + 50)
-        )
-        return cwh_to_whc(torchvision.io.decode_image(img))
+    if device is not None:
+        image = image.to(device)
+
+    if transform is not None:
+        image = transform(image)
 
-    if augment is True or (isinstance(augment, str) and 'j' in augment):
-        image = torch.where(
-            torch.rand(1)[0] < 0.5,
-            random_jpeg_compression(image),
-            image
-        )
-    if augment is True or (isinstance(augment, str) and 'r' in augment):
-        # Rotate randomly 0, 90, 180, 270 degrees
-        image = torch.rot90(image, np.random.choice(range(5)))
-    if augment is True or (isinstance(augment, str) and 'x' in augment):
-        image = torch.where(
-            torch.rand(1)[0] < 0.5,
-            torch.fliplr(image),
-            image
-        )
-    if augment is True or (isinstance(augment, str) and 'y' in augment):
-        image = torch.where(
-            torch.rand(1)[0] < 0.5,
-            torch.flipud(image),
-            image
-        )
-    if augment is True or (isinstance(augment, str) and 'b' in augment):
-        # image = image.to(device)
-        image = whc_to_cwh(image)
-        image = torch.where(
-            (torch.rand(1)[0] < 0.1),  # .to(device),
-            torch.where(
-                (torch.rand(1)[0] < 0.5),  # .to(device),
-                torch.where(
-                    (torch.rand(1)[0] < 0.5),  # .to(device),
-                    torch.where(
-                        (torch.rand(1)[0] < 0.5),  # .to(device),
-                        auto_gaussian(image, sigma=2.0),
-                        auto_gaussian(image, sigma=1.5),
-                    ),
-                    auto_gaussian(image, sigma=1.0)
-                ),
-                auto_gaussian(image, sigma=0.5),
-            ),
-            image
-        )
-        image = cwh_to_whc(image)
-        # image = image.cpu()
-    if normalizer:
-        image = normalizer.torch_to_torch(
-            image,
-            augment=(isinstance(augment, str) and 'n' in augment)
-        )
-    if standardize:
-        # Note: not the same as tensorflow's per_image_standardization
-        # Convert back: image = (image + 1) * (255/2)
-        image = image / 127.5 - 1
-    if transform:
-        image = cwh_to_whc(transform(whc_to_cwh(image)))
     return image
 
+# -------------------------------------------------------------------------
 
 def worker_init_fn(worker_id) -> None:
     np.random.seed(np.random.get_state()[1][0])  # type: ignore
 
 
 def get_tfrecord_parser(
     tfrecord_path: str,
@@ -713,19 +854,25 @@
             If None, will return all features as a list.
         decode_images (bool, optional): Decode raw image strings into image
             arrays. Defaults to True.
         standardize (bool, optional): Standardize images into the range (0,1).
             Defaults to False.
         normalizer (:class:`slideflow.norm.StainNormalizer`): Stain normalizer
             to use on images. Defaults to None.
-        augment (str): Image augmentations to perform. String containing
-            characters designating augmentations. 'x' indicates random
-            x-flipping, 'y' y-flipping, 'r' rotating, and 'j' JPEG
-            compression/decompression at random quality levels. Passing either
-            'xyrj' or True will use all augmentations.
+        augment (str or bool): Image augmentations to perform. Augmentations include:
+
+            * ``'x'``: Random horizontal flip
+            * ``'y'``: Random vertical flip
+            * ``'r'``: Random 90-degree rotation
+            * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+            * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+
+            Combine letters to define augmentations, such as ``'xyrjn'``.
+            A value of True will use ``'xyrjb'``.
+            Note: this function does not support stain augmentation.
 
     Returns:
         A tuple containing
 
             func: Parsing function
 
             dict: Detected feature description for the tfrecord
@@ -740,32 +887,38 @@
         detected = ",".join(features)
         _ftrs = list(features_to_return.keys())  # type: ignore
         raise errors.TFRecordsError(
             f'Not all features {",".join(_ftrs)} '
             f'were found in the tfrecord {detected}'
         )
 
+    # Build the transformations / augmentations.
+    transform = compose_augmentations(
+        augment=augment,
+        standardize=standardize,
+        normalizer=normalizer,
+        whc=True
+    )
+
     def parser(record):
         """Each item in args is an array with one item, as the dareblopy reader
         returns items in batches and we have set our batch_size = 1 for
         interleaving.
         """
         features = {}
         if ('slide' in features_to_return):
             slide = bytes(record['slide']).decode('utf-8')
             features['slide'] = slide
         if ('image_raw' in features_to_return):
             img = bytes(record['image_raw'])
             if decode_images:
                 features['image_raw'] = _decode_image(
                     img,
-                    img_type,
-                    standardize,
-                    normalizer,
-                    augment
+                    img_type=img_type,
+                    transform=transform
                 )
             else:
                 features['image_raw'] = img
         if ('loc_x' in features_to_return):
             features['loc_x'] = record['loc_x'][0]
         if ('loc_y' in features_to_return):
             features['loc_y'] = record['loc_y'][0]
@@ -789,15 +942,14 @@
     standardize: bool = True,
     normalizer: Optional["StainNormalizer"] = None,
     num_threads: int = 4,
     chunk_size: int = 1,
     num_replicas: int = 1,
     rank: int = 0,
     indices: Optional[List[str]] = None,
-    device: Optional[torch.device] = None,
     from_wsi: bool = False,
     tile_px: Optional[int] = None,
     tile_um: Optional[int] = None,
     rois: Optional[List[str]] = None,
     roi_method: str = 'auto',
     pool: Optional[Any] = None,
     transform: Optional[Any] = None,
@@ -821,43 +973,62 @@
         incl_loc (bool, optional): Include loc_x and loc_y as additional
             returned variables. Defaults to False.
         clip (dict, optional): Dict mapping tfrecords to number of tiles to
             take per tfrecord. Defaults to None.
         infinite (bool, optional): Create an finite dataset. WARNING: If
             infinite is False && balancing is used, some tiles will be skipped.
             Defaults to True.
-        labels (dict, optional): Dict mapping slide names to outcome labels,
-            used for balancing. Defaults to None.
-        augment (str): Image augmentations to perform. String containing
-            characters designating augmentations. 'x' indicates random
-            x-flipping, 'y' y-flipping, 'r' rotating, and 'j' JPEG
-            compression/decompression at random quality levels. Passing either
-            'xyrj' or True will use all augmentations.
+        augment (str or bool): Image augmentations to perform. Augmentations include:
+
+            * ``'x'``: Random horizontal flip
+            * ``'y'``: Random vertical flip
+            * ``'r'``: Random 90-degree rotation
+            * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+            * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+            * ``'n'``: Random :ref:`stain_augmentation` (requires stain normalizer)
+
+            Combine letters to define augmentations, such as ``'xyrjn'``.
+            A value of True will use ``'xyrjb'``.
         standardize (bool, optional): Standardize images to (0,1).
             Defaults to True.
         normalizer (:class:`slideflow.norm.StainNormalizer`, optional):
             Normalizer to use on images. Defaults to None.
-        manifest (dict, optional): Dataset manifest containing number of tiles
-            per tfrecord.
         num_threads (int, optional): Number of threads to use decoding images.
             Defaults to 4.
         chunk_size (int, optional): Chunk size for image decoding.
             Defaults to 8.
         num_replicas (int, optional): Number of total workers reading the
             dataset with this interleave function, defined as number of
             gpus * number of torch DataLoader workers. Used to interleave
             results among workers without duplications. Defaults to 1.
         rank (int, optional): Worker ID to identify which worker this
             represents. Used to interleave results among workers without
             duplications. Defaults to 0 (first worker).
         indices (list(str)): Paths to TFRecord index files. If not provided,
             will generate. Defaults to None.
+        from_wsi (bool): Generate predictions from tiles dynamically
+            extracted from whole-slide images, rather than TFRecords.
+            Defaults to False (use TFRecords).
+        tile_px (int, optional): Size of tiles to extract from WSI, in pixels.
+            Only used if from_wsi=True. Defaults to None.
+        tile_um (int, optional): Size of tiles to extract from WSI, in
+            microns. Only used if from_wsi=True. Defaults to None.
+        rois (list(str), optional): List of ROI paths. Only used if
+            from_wsi=True.  Defaults to None.
+        roi_method (str, optional): Method for extracting ROIs. Only used if
+            from_wsi=True. Defaults to 'auto'.
         pool (multiprocessing.Pool): Shared multiprocessing pool. Useful
-            if from_wsi=True, for sharing a unified processing pool between
+            if ``from_wsi=True``, for sharing a unified processing pool between
             dataloaders. Defaults to None.
+        transform (Callable, optional): Arbitrary torchvision transform
+            function. Performs transformation after augmentations but
+            before standardization. Defaults to None.
+        tfrecord_parser (Callable, optional): Custom parser for TFRecords.
+            Defaults to None.
+
     """
     if not len(paths):
         raise errors.TFRecordsNotFoundError
     if rank == 0:
         _path_type = "slides" if from_wsi else "tfrecords"
         log.debug(
             f'Interleaving {len(paths)} {_path_type}: '
@@ -881,19 +1052,19 @@
         assert tile_um is not None and tile_px is not None
         if rank == 0:
             log.info(f"Reading {len(paths)} slides and thresholding...")
 
         # ---- Load slides and apply Otsu thresholding ------------------------
         if pool is None and sf.slide_backend() == 'cucim':
             pool = mp.Pool(
-                8 if os.cpu_count is None else os.cpu_count(),
+                sf.util.num_cpu(default=8),
                 initializer=sf.util.set_ignore_sigint
             )
         elif pool is None:
-            pool = mp.dummy.Pool(16 if os.cpu_count is None else os.cpu_count())
+            pool = mp.dummy.Pool(sf.util.num_cpu(default=16))
         wsi_list = []
         to_remove = []
         otsu_list = []
         for path in paths:
             if isinstance(path, bytes):
                 path= path.decode('utf-8')
             try:
@@ -982,25 +1153,30 @@
             prob_weights,
             shard=(rank, num_replicas),
             clip=[clip[(t if isinstance(t, str) else t.decode('utf-8'))] for t in paths] if clip else None,
             infinite=infinite
         )
         sampler_iter = iter(random_sampler)
 
+    # Compose augmentation transformations
+    transform_fn = compose_augmentations(
+        augment=augment,
+        standardize=standardize,
+        normalizer=normalizer,
+        transform=transform,
+        whc=True
+    )
+
     # Worker to decode images and process records
     def threading_worker(record):
         record = base_parser(record)
         record[0] = _decode_image(
             record[0],  # Image is the first returned variable
             img_type=img_type,
-            standardize=standardize,
-            normalizer=normalizer,
-            augment=augment,
-            transform=transform,
-            device=device
+            transform=transform_fn,
         )
         return record
 
     # Randomly interleaves datasets according to weights, reading parsed
     # records to a buffer and sending parsed results to a queue after
     # reaching a set buffer size
     class QueueRetriever:
@@ -1112,78 +1288,105 @@
 
     Args:
         tfrecords (list(str)): List of paths to TFRecord files.
         img_size (int): Tile size in pixels.
         batch_size (int): Batch size.
 
     Keyword Args:
-        prob_weights (dict, optional): Dict mapping tfrecords to probability
-            of including in batch. Defaults to None.
+        augment (str or bool): Image augmentations to perform. Augmentations include:
+
+            * ``'x'``: Random horizontal flip
+            * ``'y'``: Random vertical flip
+            * ``'r'``: Random 90-degree rotation
+            * ``'j'``: Random JPEG compression (50% chance to compress with quality between 50-100)
+            * ``'b'``: Random Gaussian blur (10% chance to blur with sigma between 0.5-2.0)
+            * ``'n'``: Random :ref:`stain_augmentation` (requires stain normalizer)
+
+            Combine letters to define augmentations, such as ``'xyrjn'``.
+            A value of True will use ``'xyrjb'``.
+        chunk_size (int, optional): Chunk size for image decoding.
+            Defaults to 1.
         clip (dict, optional): Dict mapping tfrecords to number of tiles to
             take per tfrecord. Defaults to None.
-        onehot (bool, optional): Onehot encode labels. Defaults to False.
-        incl_slidenames (bool, optional): Include slidenames as third returned
-            variable. Defaults to False.
+        drop_last (bool, optional): Drop the last non-full batch.
+            Defaults to False.
+        from_wsi (bool): Generate predictions from tiles dynamically
+            extracted from whole-slide images, rather than TFRecords.
+            Defaults to False (use TFRecords).
         incl_loc (bool, optional): Include loc_x and loc_y as additional
             returned variables. Defaults to False.
+        incl_slidenames (bool, optional): Include slidenames as third returned
+            variable. Defaults to False.
         infinite (bool, optional): Infinitely repeat data. Defaults to True.
-        rank (int, optional): Worker ID to identify this worker.
-            Used to interleave results.
-            among workers without duplications. Defaults to 0 (first worker).
+        indices (numpy.ndarray, optional): Indices in form of array,
+            with np.loadtxt(index_path, dtype=np.int64) for each tfrecord.
+            Defaults to None.
+        labels (dict, optional): Dict mapping slide names to outcome labels,
+            used for balancing. Defaults to None.
+        max_size (bool, optional): Unused argument present for legacy
+            compatibility; will be removed.
+        model_type (str, optional): Used to generate random labels
+            (for StyleGAN2). Not required. Defaults to 'categorical'.
         num_replicas (int, optional): Number of GPUs or unique instances which
             will have their own DataLoader. Used to interleave results among
             workers without duplications. Defaults to 1.
-        labels (dict, optional): Dict mapping slide names to outcome labels,
-            used for balancing. Defaults to None.
-        normalizer (:class:`slideflow.norm.StainNormalizer`, optional):
-            Normalizer to use on images. Defaults to None.
-        chunk_size (int, optional): Chunk size for image decoding.
-            Defaults to 1.
-        prefetch_factor (int, optional): Number of batches to prefetch in each
-            SlideflowIterator. Defaults to 1.
-        manifest (dict, optional): Dataset manifest containing number of tiles
-            per tfrecord.
-        balance (str, optional): Batch-level balancing. Options: category,
-            patient, and None. If infinite is not True, will drop tiles to
-            maintain proportions across the interleaved dataset.
-        augment (str, optional): Image augmentations to perform. String
-            containing characters designating augmentations. 'x' indicates
-            random x-flipping, 'y' y-flipping, 'r' rotating, and 'j' JPEG
-            compression/decompression at random quality levels. Passing either
-            'xyrj' or True will use all augmentations.
-        standardize (bool, optional): Standardize images to (0,1).
-            Defaults to True.
+        num_tiles (int, optional): Dict mapping tfrecord names to number
+            of total tiles. Defaults to None.
         num_workers (int, optional): Number of DataLoader workers.
             Defaults to 2.
+        normalizer (:class:`slideflow.norm.StainNormalizer`, optional):
+            Normalizer. Defaults to None.
+        onehot (bool, optional): Onehot encode labels. Defaults to False.
         persistent_workers (bool, optional): Sets the DataLoader
             persistent_workers flag. Defaults toNone (4 if not using a SPAMS
             normalizer, 1 if using SPAMS).
         pin_memory (bool, optional): Pin memory to GPU. Defaults to True.
-        drop_last (bool, optional): Drop the last non-full batch.
-            Defaults to False.
+        pool (multiprocessing.Pool): Shared multiprocessing pool. Useful
+            if ``from_wsi=True``, for sharing a unified processing pool between
+            dataloaders. Defaults to None.
+        prefetch_factor (int, optional): Number of batches to prefetch in each
+            SlideflowIterator. Defaults to 1.
+        prob_weights (dict, optional): Dict mapping tfrecords to probability
+            of including in batch. Defaults to None.
+        rank (int, optional): Worker ID to identify this worker.
+            Used to interleave results.
+            among workers without duplications. Defaults to 0 (first worker).
+        rois (list(str), optional): List of ROI paths. Only used if
+            from_wsi=True.  Defaults to None.
+        roi_method (str, optional): Method for extracting ROIs. Only used if
+            from_wsi=True. Defaults to 'auto'.
+        standardize (bool, optional): Standardize images to mean 0 and
+            variance of 1. Defaults to True.
+        tile_um (int, optional): Size of tiles to extract from WSI, in
+            microns. Only used if from_wsi=True. Defaults to None.
+        transform (Callable, optional): Arbitrary torchvision transform
+            function. Performs transformation after augmentations but
+            before standardization. Defaults to None.
+        tfrecord_parser (Callable, optional): Custom parser for TFRecords.
+            Defaults to None.
 
     Returns:
         torch.utils.data.DataLoader
 
     """
     if batch_size is None:
         replica_batch_size = None
     else:
         replica_batch_size = batch_size // num_replicas
     if from_wsi and num_workers:
         raise ValueError("Option `from_wsi=True` incompatible with "
                          "num_workers > 0")
 
-    if num_workers is None and os.cpu_count():
-        num_workers = os.cpu_count() // 4  # type: ignore
+    if num_workers is None and sf.util.num_cpu():
+        num_workers = max(sf.util.num_cpu() // 4, 1)  # type: ignore
     elif num_workers is None:
         num_workers = 8
     log.debug(f"Using num_workers={num_workers}")
-    if 'num_threads' not in kwargs and os.cpu_count():
-        kwargs['num_threads'] = int(math.ceil(os.cpu_count() / max(num_workers, 1)))
+    if 'num_threads' not in kwargs and sf.util.num_cpu():
+        kwargs['num_threads'] = int(math.ceil(sf.util.num_cpu() / max(num_workers, 1)))
         log.debug(f"Threads per worker={kwargs['num_threads']}")
 
     iterator = InterleaveIterator(
         tfrecords=tfrecords,
         img_size=img_size,
         use_labels=(labels is not None),
         num_replicas=num_replicas,
```

## slideflow/mil/__init__.py

```diff
@@ -1,7 +1,7 @@
 from .train import train_mil, train_clam, train_fastai, build_fastai_learner
-from .eval import eval_mil
+from .eval import eval_mil, predict_slide
 from .train._legacy import legacy_train_clam
 from ._params import (
     mil_config, _TrainerConfig, TrainerConfigFastAI, TrainerConfigCLAM,
     ModelConfigCLAM, ModelConfigFastAI
-)
+)
```

## slideflow/mil/data.py

```diff
@@ -17,32 +17,36 @@
     def _zip(bag, targets):
         features, lengths = bag
         if use_lens:
             return (features, lengths, targets.squeeze())
         else:
             return (features, targets.squeeze())
 
-    return MapDataset(
+    dataset = MapDataset(
         _zip,
         BagDataset(bags, bag_size=bag_size),
         EncodedDataset(encoder, targets),
     )
+    dataset.encoder = encoder
+    return dataset
 
 def build_clam_dataset(bags, targets, encoder, bag_size):
     assert len(bags) == len(targets)
 
     def _zip(bag, targets):
         features, lengths = bag
         return (features, targets.squeeze(), True), targets.squeeze()
 
-    return MapDataset(
+    dataset = MapDataset(
         _zip,
         BagDataset(bags, bag_size=bag_size),
         EncodedDataset(encoder, targets),
     )
+    dataset.encoder = encoder
+    return dataset
 
 # -----------------------------------------------------------------------------
 
 def _to_fixed_size_bag(
     bag: torch.Tensor, bag_size: int = 512
 ) -> Tuple[torch.Tensor, int]:
     # get up to bag_size elements
@@ -115,14 +119,15 @@
         elif datasets:
             self._len = min(len(ds) for ds in datasets)  # type: ignore
         else:
             self._len = 0
 
         self._datasets = datasets
         self.func = func
+        self.encoder = None
 
     def __len__(self) -> int:
         return self._len
 
     def __getitem__(self, index: int) -> Any:
         return self.func(*[ds[index] for ds in self._datasets])
```

## slideflow/mil/eval.py

```diff
@@ -1,22 +1,29 @@
 """Tools for evaluation MIL models."""
 
 import os
 import pandas as pd
 import slideflow as sf
 import numpy as np
 from rich.progress import Progress
-from os.path import join, exists, isdir, dirname
-from typing import Union, List, Optional, Callable, Tuple, Any
-from slideflow import Dataset, log, errors
+from os.path import join, exists, dirname
+from typing import Union, List, Optional, Callable, Tuple, Any, TYPE_CHECKING
+from slideflow import Dataset, log
 from slideflow.util import path_to_name
+from slideflow.model.extractors import rebuild_extractor
 from slideflow.stats.metrics import ClassifierMetrics
 from ._params import (
     _TrainerConfig, ModelConfigCLAM, TrainerConfigCLAM
 )
+from .utils import load_model_weights, _load_bag
+
+if TYPE_CHECKING:
+    import torch
+    from slideflow.norm import StainNormalizer
+    from slideflow.model.base import BaseFeatureExtractor
 
 # -----------------------------------------------------------------------------
 
 def eval_mil(
     weights: str,
     dataset: Dataset,
     outcomes: Union[str, List[str]],
@@ -60,29 +67,14 @@
             for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
             If 'two_slope', normalizes values less than 0 and greater than 0
             separately. Defaults to None.
 
     """
     import torch
 
-    if isinstance(config, TrainerConfigCLAM):
-        raise NotImplementedError
-
-    # Read configuration from saved model, if available
-    if config is None:
-        if not exists(join(weights, 'mil_params.json')):
-            raise errors.ModelError(
-                f"Could not find `mil_params.json` at {weights}. Check the "
-                "provided model/weights path, or provide a configuration "
-                "with 'config'."
-            )
-        else:
-            p = sf.util.load_json(join(weights, 'mil_params.json'))
-            config = sf.mil.mil_config(trainer=p['trainer'], **p['params'])
-
     # Prepare ground-truth labels
     labels, unique = dataset.labels(outcomes, format='id')
 
     # Prepare bags and targets
     slides = list(labels.keys())
     if isinstance(bags, str):
         bags = dataset.pt_files(bags)
@@ -94,38 +86,16 @@
 
     y_true = np.array([labels[s] for s in slides])
 
     # Detect feature size from bags
     n_features = torch.load(bags[0]).shape[-1]
     n_out = len(unique)
 
-    log.info(f"Building model {config.model_fn.__name__}")
-    if isinstance(config, TrainerConfigCLAM):
-        config_size = config.model_fn.sizes[config.model_config.model_size]
-        model = config.model_fn(size=[n_features] + config_size[1:])
-    elif isinstance(config.model_config, ModelConfigCLAM):
-        model = config.model_fn(size=[n_features, 256, 128])
-    else:
-        model = config.model_fn(n_features, n_out)
-    if isdir(weights):
-        if exists(join(weights, 'models', 'best_valid.pth')):
-            weights = join(weights, 'models', 'best_valid.pth')
-        elif exists(join(weights, 'results', 's_0_checkpoint.pt')):
-            weights = join(weights, 'results', 's_0_checkpoint.pt')
-        else:
-            raise errors.ModelError(
-                f"Could not find model weights at path {weights}"
-            )
-    log.info(f"Loading model weights from [green]{weights}[/]")
-    model.load_state_dict(torch.load(weights))
-
-    # Prepare device.
-    if hasattr(model, 'relocate'):
-        model.relocate()  # type: ignore
-    model.eval()
+    # Load model
+    model, config = load_model_weights(weights, config)
 
     # Inference.
     if (isinstance(config, TrainerConfigCLAM)
        or isinstance(config.model_config, ModelConfigCLAM)):
         y_pred, y_att = _predict_clam(model, bags, attention=True)
     else:
         y_pred, y_att = _predict_mil(
@@ -146,27 +116,24 @@
     for i in range(y_pred.shape[-1]):
         df_dict[f'y_pred{i}'] = y_pred[:, i]
     df = pd.DataFrame(df_dict)
     pred_out = join(model_dir, 'predictions.parquet')
     df.to_parquet(pred_out)
     log.info(f"Predictions saved to [green]{pred_out}[/]")
 
+    # Print categorical metrics, including per-category accuracy
+    outcome_name = outcomes if isinstance(outcomes, str) else '-'.join(outcomes)
+    metrics_df = df.rename(
+        columns={c: f"{outcome_name}-{c}" for c in df.columns if c != 'slide'}
+    )
+    sf.stats.metrics.categorical_metrics(metrics_df, level='slide')
+
     # Export attention
     if y_att:
-        att_path = join(model_dir, 'attention')
-        if not exists(att_path):
-            os.makedirs(att_path)
-        for slide, att in zip(slides, y_att):
-            if 'SF_ALLOW_ZIP' in os.environ and os.environ['SF_ALLOW_ZIP'] == '0':
-                out_path = join(att_path, f'{slide}_att.npy')
-                np.save(out_path, att)
-            else:
-                out_path = join(att_path, f'{slide}_att.npz')
-                np.savez(out_path, att)
-        log.info(f"Attention scores exported to [green]{out_path}[/]")
+        _export_attention(join(model_dir, 'attention'), y_att, slides)
 
     # Attention heatmaps
     if y_att and attention_heatmaps:
         generate_attention_heatmaps(
             outdir=join(model_dir, 'heatmaps'),
             dataset=dataset,
             bags=bags,
@@ -174,25 +141,143 @@
             **heatmap_kwargs
         )
 
     return df
 
 # -----------------------------------------------------------------------------
 
+def predict_slide(
+    model: str,
+    slide: Union[str, sf.WSI],
+    extractor: Optional["BaseFeatureExtractor"] = None,
+    *,
+    normalizer: Optional["StainNormalizer"] = None,
+    config: Optional[_TrainerConfig] = None,
+    attention: bool = False,
+) -> Tuple[np.ndarray, Optional[np.ndarray]]:
+    """Generate predictions (and attention) for a single slide.
+
+    Args:
+        model (str): Path to MIL model.
+        slide (str): Path to slide.
+        extractor (:class:`slideflow.mil.BaseFeatureExtractor`, optional):
+            Feature extractor. If not provided, will attempt to auto-detect
+            extractor from model.
+
+            .. note::
+                If the extractor has a stain normalizer, this will be used to
+                normalize the slide before extracting features.
+
+    Keyword Args:
+        normalizer (:class:`slideflow.stain.StainNormalizer`, optional):
+            Stain normalizer. If not provided, will attempt to use stain
+            normalizer from extractor.
+        config (:class:`slideflow.mil.TrainerConfigFastAI` or :class:`slideflow.mil.TrainerConfigCLAM`):
+            Configuration for building model. If None, will attempt to read
+            ``mil_params.json`` from the model directory and load saved
+            configuration. Defaults to None.
+        attention (bool): Whether to return attention scores. Defaults to
+            False.
+
+    Returns:
+        Tuple[np.ndarray, Optional[np.ndarray]]: Predictions and attention scores.
+        Attention scores are None if ``attention`` is False, otherwise
+        a masked 2D array with the same shape as the slide grid (arranged as a
+        heatmap, with unused tiles masked).
+
+    """
+    # Try to auto-determine the extractor
+    if extractor is None:
+        extractor, detected_normalizer = rebuild_extractor(model, allow_errors=True)
+        if extractor is None:
+            raise ValueError(
+                "Unable to auto-detect feature extractor used for model {}. "
+                "Please specify an extractor.".format(model)
+            )
+
+    # Determine stain normalization
+    if detected_normalizer is not None and normalizer is not None:
+        log.warning(
+            "Bags were generated with a stain normalizer, but a different stain "
+            "normalizer was provided to this function. Overriding with provided "
+            "stain normalizer."
+        )
+    elif detected_normalizer is not None:
+        normalizer = detected_normalizer
+
+    # Load model
+    model_fn, config = load_model_weights(model, config)
+    mil_params = sf.util.load_json(join(model, 'mil_params.json'))
+    if 'bags_extractor' not in mil_params:
+        raise ValueError(
+            "Unable to determine extractor used for model {}. "
+            "Please specify an extractor.".format(model)
+        )
+    bags_params = mil_params['bags_extractor']
+
+    # Load slide
+    if isinstance(slide, str):
+        if not all(k in bags_params for k in ('tile_px', 'tile_um')):
+            raise ValueError(
+                "Unable to determine tile size for slide {}. "
+                "Either slide must be a slideflow.WSI object, or tile_px and "
+                "tile_um must be specified in mil_params.json.".format(slide)
+            )
+        slide = sf.WSI(
+            slide,
+            tile_px=bags_params['tile_px'],
+            tile_um=bags_params['tile_um']
+        )
+
+    # Convert slide to bags
+    masked_bags = extractor(slide, normalizer=normalizer)
+    original_shape = masked_bags.shape
+    masked_bags = masked_bags.reshape((-1, masked_bags.shape[-1]))
+    mask = masked_bags.mask.any(axis=1)
+    valid_indices = np.where(~mask)
+    bags = masked_bags[valid_indices]
+    bags = np.expand_dims(bags, axis=0).astype(np.float32)
+
+    sf.log.info("Generated feature bags for {} tiles".format(bags.shape[1]))
+
+    # Generate predictions.
+    if (isinstance(config, TrainerConfigCLAM)
+       or isinstance(config.model_config, ModelConfigCLAM)):
+        y_pred, raw_att = _predict_clam(model_fn, bags, attention=attention)
+    else:
+        y_pred, raw_att = _predict_mil(
+            model_fn, bags, attention=attention, use_lens=config.model_config.use_lens
+        )
+
+    # Reshape attention to match original shape
+    if attention and raw_att is not None and len(raw_att):
+        y_att = raw_att[0]
+
+        # Create a fully masked array of shape (X, Y)
+        att_heatmap = np.ma.masked_all(masked_bags.shape[0], dtype=y_att.dtype)
+
+        # Unmask and fill the transformed data into the corresponding positions
+        att_heatmap[valid_indices] = y_att
+        y_att = np.reshape(att_heatmap, original_shape[0:2])
+    else:
+        y_att = None
+
+    return y_pred, y_att
+
 
 def predict_from_model(
     model: Callable,
     config: _TrainerConfig,
     dataset: "sf.Dataset",
     outcomes: Union[str, List[str]],
     bags: Union[str, np.ndarray, List[str]],
     *,
     attention: bool = False
 ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, List[np.ndarray]]]:
-    """Generate predictions from a model.
+    """Generate predictions for a dataset from a saved MIL model.
 
     Args:
         model (torch.nn.Module): Model from which to generate predictions.
         config (:class:`slideflow.mil.TrainerConfigFastAI` or :class:`slideflow.mil.TrainerConfigCLAM`):
             Configuration for the MIL model.
         dataset (sf.Dataset): Dataset from which to generation predictions.
         outcomes (str, list(str)): Outcomes.
@@ -245,15 +330,15 @@
 def generate_attention_heatmaps(
     outdir: str,
     dataset: "sf.Dataset",
     bags: Union[List[str], np.ndarray],
     attention: Union[np.ndarray, List[np.ndarray]],
     **kwargs
 ) -> None:
-    """Generate and save attention heatmaps.
+    """Generate and save attention heatmaps for a dataset.
 
     Args:
         outdir (str): Path at which to save heatmap images.
         dataset (sf.Dataset): Dataset.
         bags (str, list(str)): List of bag file paths.
             Each bag should contain PyTorch array of features from all tiles in
             a slide, with the shape ``(n_tiles, n_features)``.
@@ -307,14 +392,32 @@
                 outdir=outdir,
                 **kwargs
             )
     log.info(f"Attention heatmaps saved to [green]{outdir}[/]")
 
 # -----------------------------------------------------------------------------
 
+def _export_attention(
+    dest: str,
+    y_att: List[np.ndarray],
+    slides: List[str]
+) -> None:
+    """Export attention scores to a directory."""
+    if not exists(dest):
+        os.makedirs(dest)
+    for slide, att in zip(slides, y_att):
+        if 'SF_ALLOW_ZIP' in os.environ and os.environ['SF_ALLOW_ZIP'] == '0':
+            out_path = join(dest, f'{slide}_att.npy')
+            np.save(out_path, att)
+        else:
+            out_path = join(dest, f'{slide}_att.npz')
+            np.savez(out_path, att)
+    log.info(f"Attention scores exported to [green]{out_path}[/]")
+
+
 def _predict_clam(
     model: Callable,
     bags: Union[np.ndarray, List[str]],
     attention: bool = False,
     device: Optional[Any] = None
 ) -> Tuple[np.ndarray, List[np.ndarray]]:
 
@@ -339,15 +442,15 @@
         log.debug(f"Using {device}")
         device = torch.device(device)
 
     y_pred = []
     y_att  = []
     log.info("Generating predictions...")
     for bag in bags:
-        loaded = torch.load(bag).to(device)
+        loaded = _load_bag(bag).to(device)
         with torch.no_grad():
             if clam_kw:
                 logits, att, _ = model(loaded, **clam_kw)
             else:
                 logits, att = model(loaded, **clam_kw)
             if attention:
                 y_att.append(np.squeeze(att.cpu().numpy()))
@@ -387,26 +490,26 @@
             "Model '{}' does not have a method 'calculate_attention'. "
             "Unable to calculate or display attention heatmaps.".format(
                 model.__class__.__name__
             )
         )
         attention = False
     for bag in bags:
-        loaded = torch.load(bag).to(device)
-        loaded = torch.unsqueeze(loaded, dim=0)
+        loaded = torch.unsqueeze(_load_bag(bag).to(device), dim=0)
         with torch.no_grad():
             if use_lens:
                 lens = torch.from_numpy(np.array([loaded.shape[1]])).to(device)
                 model_args = (loaded, lens)
             else:
                 model_args = (loaded,)
             model_out = model(*model_args)
             if attention:
                 att = torch.squeeze(model.calculate_attention(*model_args))
                 if len(att.shape) == 2:
+                    log.warning("Pooling attention scores from 2D to 1D")
                     # Attention needs to be pooled
                     if attention_pooling == 'avg':
                         att = torch.mean(att, dim=-1)
                     elif attention_pooling == 'max':
                         att = torch.amax(att, dim=-1)
                     else:
                         raise ValueError(
```

## slideflow/mil/train/__init__.py

```diff
@@ -5,15 +5,15 @@
 import slideflow as sf
 from os.path import join, exists
 from typing import Union, List, Optional, TYPE_CHECKING
 from slideflow import Dataset, log
 from slideflow.util import path_to_name
 from os.path import join
 
-from ..eval import predict_from_model, generate_attention_heatmaps
+from ..eval import predict_from_model, generate_attention_heatmaps, _export_attention
 from .._params import (
     _TrainerConfig, TrainerConfigCLAM, TrainerConfigFastAI
 )
 
 if TYPE_CHECKING:
     from fastai.learner import Learner
 
@@ -75,24 +75,27 @@
             "Training without validation; metrics will be calculated on training data."
         )
         val_dataset = train_dataset
 
     # Set up experiment label
     if exp_label is None:
         try:
-            exp_label = config.model_config.model
+            exp_label = '{}-{}'.format(
+                config.model_config.model,
+                "-".join(outcomes if isinstance(outcomes, list) else [outcomes])
+            )
         except Exception:
             exp_label = 'no_label'
 
     # Set up output model directory
     if not exists(outdir):
         os.makedirs(outdir)
     outdir = sf.util.create_new_model_dir(outdir, exp_label)
-    sf.util.write_json(config.json_dump(), join(outdir, 'mil_params.json'))
 
+    # Execute training.
     return train_fn(
         config,
         train_dataset,
         val_dataset,
         outcomes,
         bags,
         outdir=outdir,
@@ -173,14 +176,22 @@
     # Set up bags.
     if isinstance(bags, str):
         train_bags = train_dataset.pt_files(bags)
         val_bags = val_dataset.pt_files(bags)
     else:
         train_bags = val_bags = bags
 
+    # Write slide/bag manifest
+    sf.util.log_manifest(
+        [b for b in train_bags],
+        [b for b in val_bags],
+        labels=labels,
+        filename=join(outdir, 'slide_manifest.csv')
+    )
+
     # Set up datasets.
     train_mil_dataset = CLAM_Dataset(
         train_bags,
         annotations=train_dataset.filtered_annotations,
         label_col=outcomes,
         label_dict=label_dict
     )
@@ -206,14 +217,17 @@
             f"match features ({num_features}). Updating model size to "
             f"{clam_args.model_size}. "
         )
 
     # Save clam settings
     sf.util.write_json(clam_args.to_dict(), join(outdir, 'experiment.json'))
 
+    # Save MIL settings
+    _log_mil_params(config, outcomes, unique_labels, bags, num_features, clam_args.n_classes, outdir)
+
     # Run CLAM
     datasets = (train_mil_dataset, val_mil_dataset, val_mil_dataset)
     model, results, test_auc, val_auc, test_acc, val_acc = clam.train(
         datasets, 0, clam_args
     )
 
     # Generate validation predictions
@@ -225,20 +239,37 @@
         bags=bags,
         attention=True
     )
     pred_out = join(outdir, 'results', 'predictions.parquet')
     df.to_parquet(pred_out)
     log.info(f"Predictions saved to [green]{pred_out}[/]")
 
+    # Print categorical metrics, including per-category accuracy
+    outcome_name = outcomes if isinstance(outcomes, str) else '-'.join(outcomes)
+    df.rename(
+        columns={c: f"{outcome_name}-{c}" for c in df.columns if c != 'slide'},
+        inplace=True
+    )
+    sf.stats.metrics.categorical_metrics(df, level='slide')
+
     # Attention heatmaps
     if isinstance(bags, str):
         val_bags = val_dataset.pt_files(bags)
     else:
         val_bags = np.array(bags)
 
+    # Export attention to numpy arrays
+    if attention:
+        _export_attention(
+            join(outdir, 'attention'),
+            attention,
+            [path_to_name(b) for b in val_bags]
+        )
+
+    # Save attention heatmaps
     if attention and attention_heatmaps:
         assert len(val_bags) == len(attention)
         generate_attention_heatmaps(
             outdir=join(outdir, 'heatmaps'),
             dataset=val_dataset,
             bags=val_bags,
             attention=attention,
@@ -251,14 +282,15 @@
     config: TrainerConfigFastAI,
     train_dataset: Dataset,
     val_dataset: Dataset,
     outcomes: Union[str, List[str]],
     bags: Union[str, np.ndarray, List[str]],
     *,
     outdir: str = 'mil',
+    return_shape: bool = False,
 ) -> "Learner":
     """Build a FastAI Learner for training an aMIL model.
 
     Args:
         train_dataset (:class:`slideflow.Dataset`): Training dataset.
         val_dataset (:class:`slideflow.Dataset`): Validation dataset.
         outcomes (str): Outcome column (annotation header) from which to
@@ -304,25 +336,36 @@
     val_slides = val_dataset.slides()
     val_idx = np.array([i for i, bag in enumerate(bags)
                             if path_to_name(bag) in val_slides])
 
     log.info("Training dataset: {} bags (from {} slides)".format(len(train_idx), len(train_slides)))
     log.info("Validation dataset: {} bags (from {} slides)".format(len(val_idx), len(val_slides)))
 
+    # Write slide/bag manifest
+    sf.util.log_manifest(
+        [bag for bag in bags if path_to_name(bag) in train_slides],
+        [bag for bag in bags if path_to_name(bag) in val_slides],
+        labels=labels,
+        filename=join(outdir, 'slide_manifest.csv')
+    )
+
     # Build FastAI Learner
-    learner = _fastai.build_learner(
+    learner, (n_in, n_out) = _fastai.build_learner(
         config,
         bags=bags,
         targets=targets,
         train_idx=train_idx,
         val_idx=val_idx,
         unique_categories=unique_categories,
         outdir=outdir,
     )
-    return learner
+    if return_shape:
+        return learner, (n_in, n_out)
+    else:
+        return learner
 
 
 def train_fastai(
     config: TrainerConfigFastAI,
     train_dataset: Dataset,
     val_dataset: Dataset,
     outcomes: Union[str, List[str]],
@@ -375,23 +418,36 @@
         val_bags = val_dataset.pt_files(bags)
         all_bags = np.concatenate((train_bags, val_bags))
     else:
         val_bags = np.array([b for b in bags if sf.util.path_to_name(b) in val_dataset.slides()])
         all_bags = np.array(bags)
 
     # Build learner.
-    learner = build_fastai_learner(
+    learner, (n_in, n_out) = build_fastai_learner(
         config,
         train_dataset,
         val_dataset,
         outcomes,
         bags=all_bags,
         outdir=outdir,
+        return_shape=True
     )
 
+    # Save MIL settings.
+    # Attempt to read the unique categories from the learner.
+    if not hasattr(learner.dls.train_ds, 'encoder'):
+        unique = None
+    else:
+        encoder = learner.dls.train_ds.encoder
+        if encoder is not None:
+            unique = encoder.categories_[0].tolist()
+        else:
+            unique = None
+    _log_mil_params(config, outcomes, unique, bags, n_in, n_out, outdir)
+
     # Train.
     _fastai.train(learner, config)
 
     # Generate validation predictions.
     df, attention = predict_from_model(
         learner.model,
         config,
@@ -400,18 +456,56 @@
         bags=val_bags,
         attention=True
     )
     pred_out = join(outdir, 'predictions.parquet')
     df.to_parquet(pred_out)
     log.info(f"Predictions saved to [green]{pred_out}[/]")
 
+    # Print categorical metrics, including per-category accuracy
+    outcome_name = outcomes if isinstance(outcomes, str) else '-'.join(outcomes)
+    df.rename(
+        columns={c: f"{outcome_name}-{c}" for c in df.columns if c != 'slide'},
+        inplace=True
+    )
+    sf.stats.metrics.categorical_metrics(df, level='slide')
+
+    # Export attention to numpy arrays
+    if attention:
+        _export_attention(
+            join(outdir, 'attention'),
+            attention,
+            [path_to_name(b) for b in val_bags]
+        )
+
     # Attention heatmaps.
     if attention and attention_heatmaps:
         generate_attention_heatmaps(
             outdir=join(outdir, 'heatmaps'),
             dataset=val_dataset,
             bags=val_bags,
             attention=attention,
             **heatmap_kwargs
         )
 
     return learner
+
+# ------------------------------------------------------------------------------
+
+def _log_mil_params(config, outcomes, unique, bags, n_in, n_out, outdir):
+    """Log MIL parameters to JSON."""
+    mil_params = config.json_dump()
+    mil_params['outcomes'] = outcomes
+    if unique is not None:
+        mil_params['outcome_labels'] = dict(zip(range(len(unique)), unique))
+    else:
+        mil_params['outcome_labels'] = None
+    mil_params['bags'] = bags
+    mil_params['input_shape'] = n_in
+    mil_params['output_shape'] = n_out
+    if isinstance(bags, str) and exists(join(bags, 'bags_config.json')):
+        mil_params['bags_extractor'] = sf.util.load_json(
+            join(bags, 'bags_config.json')
+        )
+    else:
+        mil_params['bags_extractor'] = None
+    sf.util.write_json(mil_params, join(outdir, 'mil_params.json'))
+    return mil_params
```

## slideflow/mil/train/_fastai.py

```diff
@@ -1,20 +1,23 @@
 import torch
 import pandas as pd
 import numpy as np
 import numpy.typing as npt
-from typing import List, Optional, Union
+from typing import List, Optional, Union, Tuple
 from torch import nn
 from sklearn.preprocessing import OneHotEncoder
+from sklearn import __version__ as sklearn_version
+from packaging import version
 from fastai.vision.all import (
     DataLoader, DataLoaders, Learner, RocAuc, SaveModelCallback, CSVLogger, FetchPredsCallback
 )
 
 from slideflow import log
 from slideflow.mil.data import build_clam_dataset, build_dataset
+from slideflow.model import torch_utils
 from .._params import TrainerConfigFastAI, ModelConfigCLAM
 
 # -----------------------------------------------------------------------------
 
 def train(learner, config, callbacks=None):
     """Train an attention-based multi-instance learning model with FastAI.
 
@@ -45,15 +48,15 @@
         else:
             lr = config.lr
         learner.fit(n_epoch=config.epochs, lr=lr, wd=config.wd, cbs=cbs)
     return learner
 
 # -----------------------------------------------------------------------------
 
-def build_learner(config, *args, **kwargs):
+def build_learner(config, *args, **kwargs) -> Tuple[Learner, Tuple[int, int]]:
     """Build a FastAI learner for training an MIL model.
 
     Args:
         config (``TrainerConfigFastAI``): Trainer and model configuration.
         bags (list(str)): Path to .pt files (bags) with features, one per patient.
         targets (np.ndarray): Category labels for each patient, in the same
             order as ``bags``.
@@ -63,15 +66,16 @@
             the validation set.
         unique_categories (np.ndarray(str)): Array of all unique categories
             in the targets. Used for one-hot encoding.
         outdir (str): Location in which to save training history and best model.
         device (torch.device or str): PyTorch device.
 
     Returns:
-        fastai.learner.Learner
+        fastai.learner.Learner, (int, int): FastAI learner and a tuple of the
+            number of input features and output classes.
 
     """
     if isinstance(config.model_config, ModelConfigCLAM):
         return _build_clam_learner(config, *args, **kwargs)
     else:
         return _build_fastai_learner(config, *args, **kwargs)
 
@@ -80,16 +84,16 @@
     config: TrainerConfigFastAI,
     bags: List[str],
     targets: npt.NDArray,
     train_idx: npt.NDArray[np.int_],
     val_idx: npt.NDArray[np.int_],
     unique_categories: npt.NDArray,
     outdir: Optional[str] = None,
-    device: Union[str, torch.device] = 'cuda',
-) -> Learner:
+    device: Optional[Union[str, torch.device]] = None,
+) -> Tuple[Learner, Tuple[int, int]]:
     """Build a FastAI learner for a CLAM model.
 
     Args:
         config (``TrainerConfigFastAI``): Trainer and model configuration.
         bags (list(str)): Path to .pt files (bags) with features, one per patient.
         targets (np.ndarray): Category labels for each patient, in the same
             order as ``bags``.
@@ -97,23 +101,32 @@
             the training set.
         val_idx (np.ndarray, int): Indices of bags/targets that constitutes
             the validation set.
         unique_categories (np.ndarray(str)): Array of all unique categories
             in the targets. Used for one-hot encoding.
         outdir (str): Location in which to save training history and best model.
         device (torch.device or str): PyTorch device.
+
+    Returns:
+        FastAI Learner, (number of input features, number of classes).
     """
     from ..clam.utils import loss_utils
 
     # Prepare device.
-    if isinstance(device, str):
-        device = torch.device('cuda')
+    device = torch_utils.get_device(device)
 
     # Prepare data.
-    encoder = OneHotEncoder(sparse=False).fit(unique_categories.reshape(-1, 1))
+    # Set oh_kw to a dictionary of keyword arguments for OneHotEncoder,
+    # using the argument sparse=False if the sklearn version is <1.2
+    # and sparse_output=False if the sklearn version is >=1.2.
+    if version.parse(sklearn_version) < version.parse("1.2"):
+        oh_kw = {"sparse": False}
+    else:
+        oh_kw = {"sparse_output": False}
+    encoder = OneHotEncoder(**oh_kw).fit(unique_categories.reshape(-1, 1))
 
     # Build dataloaders.
     train_dataset = build_clam_dataset(
         bags[train_idx],
         targets[train_idx],
         encoder=encoder,
         bag_size=config.bag_size
@@ -138,37 +151,45 @@
         shuffle=False,
         num_workers=8,
         persistent_workers=True,
         device=device
     )
 
     # Prepare model.
-    log.info(f"Training model {config.model_fn.__name__}, loss={config.loss_fn.__name__}")
     batch = train_dl.one_batch()
-    model = config.model_fn(size=[batch[0][0].shape[-1], 256, 128], n_classes=batch[-1].shape[-1])
+    n_features = batch[0][0].shape[-1]
+    n_classes = batch[-1].shape[-1]
+    config_size = config.model_fn.sizes[config.model_config.model_size]
+    model_size = [n_features] + config_size[1:]
+    log.info(f"Training model {config.model_fn.__name__} "
+             f"(size={model_size}, loss={config.loss_fn.__name__})")
+    model = config.model_fn(size=model_size, n_classes=n_classes)
+
     model.relocate()
 
     # Loss should weigh inversely to class occurences.
     loss_func = config.loss_fn()
 
     # Create learning and fit.
     dls = DataLoaders(train_dl, val_dl)
-    return Learner(dls, model, loss_func=loss_func, metrics=[loss_utils.RocAuc()], path=outdir)
+    learner = Learner(dls, model, loss_func=loss_func, metrics=[loss_utils.RocAuc()], path=outdir)
+
+    return learner, (n_features, n_classes)
 
 
 def _build_fastai_learner(
     config: TrainerConfigFastAI,
     bags: List[str],
     targets: npt.NDArray,
     train_idx: npt.NDArray[np.int_],
     val_idx: npt.NDArray[np.int_],
     unique_categories: npt.NDArray,
     outdir: Optional[str] = None,
-    device: Union[str, torch.device] = 'cuda',
-) -> Learner:
+    device: Optional[Union[str, torch.device]] = None,
+) -> Tuple[Learner, Tuple[int, int]]:
     """Build a FastAI learner for an MIL model.
 
     Args:
         config (``TrainerConfigFastAI``): Trainer and model configuration.
         bags (list(str)): Path to .pt files (bags) with features, one per patient.
         targets (np.ndarray): Category labels for each patient, in the same
             order as ``bags``.
@@ -176,21 +197,31 @@
             the training set.
         val_idx (np.ndarray, int): Indices of bags/targets that constitutes
             the validation set.
         unique_categories (np.ndarray(str)): Array of all unique categories
             in the targets. Used for one-hot encoding.
         outdir (str): Location in which to save training history and best model.
         device (torch.device or str): PyTorch device.
+
+    Returns:
+
+        FastAI Learner, (number of input features, number of classes).
     """
     # Prepare device.
-    if isinstance(device, str):
-        device = torch.device('cuda')
+    device = torch_utils.get_device(device)
 
     # Prepare data.
-    encoder = OneHotEncoder(sparse=False).fit(unique_categories.reshape(-1, 1))
+    # Set oh_kw to a dictionary of keyword arguments for OneHotEncoder,
+    # using the argument sparse=False if the sklearn version is <1.2
+    # and sparse_output=False if the sklearn version is >=1.2.
+    if version.parse(sklearn_version) < version.parse("1.2"):
+        oh_kw = {"sparse": False}
+    else:
+        oh_kw = {"sparse_output": False}
+    encoder = OneHotEncoder(**oh_kw).fit(unique_categories.reshape(-1, 1))
 
     # Build dataloaders.
     train_dataset = build_dataset(
         bags[train_idx],
         targets[train_idx],
         encoder=encoder,
         bag_size=config.bag_size,
@@ -217,25 +248,29 @@
         shuffle=False,
         num_workers=8,
         persistent_workers=True,
         device=device
     )
 
     # Prepare model.
-    log.info(f"Training model {config.model_fn.__name__}, loss={config.loss_fn.__name__}")
     batch = train_dl.one_batch()
-    model = config.model_fn(batch[0].shape[-1], batch[-1].shape[-1]).to(device)
+    n_in, n_out = batch[0].shape[-1], batch[-1].shape[-1]
+    log.info(f"Training model {config.model_fn.__name__} "
+             f"(in={n_in}, out={n_out}, loss={config.loss_fn.__name__})")
+    model = config.model_fn(n_in, n_out).to(device)
     if hasattr(model, 'relocate'):
         model.relocate()
 
     # Loss should weigh inversely to class occurences.
     counts = pd.value_counts(targets[train_idx])
     weight = counts.sum() / counts
     weight /= weight.sum()
     weight = torch.tensor(
         list(map(weight.get, encoder.categories_[0])), dtype=torch.float32
     ).to(device)
     loss_func = nn.CrossEntropyLoss(weight=weight)
 
     # Create learning and fit.
     dls = DataLoaders(train_dl, val_dl)
-    return Learner(dls, model, loss_func=loss_func, metrics=[RocAuc()], path=outdir)
+    learner = Learner(dls, model, loss_func=loss_func, metrics=[RocAuc()], path=outdir)
+
+    return learner, (n_in, n_out)
```

## slideflow/model/__init__.py

```diff
@@ -11,15 +11,15 @@
 from slideflow import errors
 from .base import BaseFeatureExtractor
 from .features import DatasetFeatures
 from .extractors import (
     list_extractors, list_torch_extractors, list_tensorflow_extractors,
     is_extractor, is_torch_extractor, is_tensorflow_extractor,
     build_feature_extractor, build_torch_feature_extractor,
-    build_tensorflow_feature_extractor
+    build_tensorflow_feature_extractor, rebuild_extractor
 )
 
 # --- Backend-specific imports ------------------------------------------------
 
 if sf.backend() == 'tensorflow':
     from slideflow.model.tensorflow import (CPHTrainer, Features, load, # noqa F401
                                             LinearTrainer, ModelParams,
```

## slideflow/model/base.py

```diff
@@ -5,15 +5,15 @@
 import os
 import warnings
 from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union
 
 import numpy as np
 import slideflow as sf
 from slideflow import errors
-from slideflow.util import log
+from slideflow.util import log, log_manifest  # noqa: F401
 
 if TYPE_CHECKING:
     from slideflow.norm import StainNormalizer
 
 
 class _ModelParams:
     """Build a set of hyperparameters."""
@@ -120,22 +120,23 @@
                       - Random y-flipping
                     * - r
                       - Random cardinal rotation
                     * - j
                       - Random JPEG compression (10% chance to JPEG compress with quality between 50-100%)
                     * - b
                       - Random Guassian blur (50% chance to blur with sigma between 0.5 - 2.0)
-                    * - s
-                      - Stain augmentation (must be using stain normalization)
+                    * - n
+                      - :ref:`stain_augmentation` (requires stain normalizer)
 
 
             normalizer (str, optional): Normalization strategy to use on image tiles. Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image. Defaults to None.
-                If None but using a normalizer, will use an internal tile for normalization.
-                Internal default tile can be found at slideflow.slide.norm_tile.jpg
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
+                Defaults to None.
             include_top (bool): Include post-convolution fully-connected layers from the core model. Defaults
                 to True. include_top=False is not currently compatible with the PyTorch backend.
             drop_images (bool): Drop images, using only other slide-level features as input.
                 Defaults to False.
         """
         if isinstance(tile_um, str):
             sf.util.assert_is_mag(tile_um)
@@ -492,63 +493,31 @@
 
     def is_torch(self):
         return self.backend == 'torch'
 
     def is_tensorflow(self):
         return self.backend == 'tensorflow'
 
+    def __call__(self, obj, **kwargs):
+        raise NotImplementedError
+
+    def dump_config(self):
+        """Dump the configuration of this feature extractor.
+
+        The configuration should be a dictionary of all parameters needed to
+        re-instantiate this feature extractor. The dictionary should have the
+        keys 'class' and 'kwargs', where 'class' is the name of the class, and
+        'kwargs' is a dictionary of keyword arguments.
+        """
+        raise NotImplementedError
+
 
 class HyperParameterError(Exception):
     pass
 
 
 class no_scope():
     def __enter__(self):
         return None
 
     def __exit__(self, exc_type, exc_value, traceback):
         return False
-
-
-def log_manifest(
-    train_tfrecords: Optional[List[str]] = None,
-    val_tfrecords: Optional[List[str]] = None,
-    *,
-    labels: Optional[Dict[str, Any]] = None,
-    filename: Optional[str] = None
-) -> str:
-    """Saves the training manifest in CSV format and returns as a string.
-
-    Args:
-        train_tfrecords (list(str)], optional): List of training TFRecords.
-            Defaults to None.
-        val_tfrecords (list(str)], optional): List of validation TFRecords.
-            Defaults to None.
-        labels (dict, optional): TFRecord outcome labels. Defaults to None.
-        filename (str, optional): Path to CSV file to save. Defaults to None.
-
-    Returns:
-        str: Saved manifest in str format.
-    """
-    out = ''
-    if filename:
-        save_file = open(os.path.join(filename), 'w')
-        writer = csv.writer(save_file)
-        writer.writerow(['slide', 'dataset', 'outcome_label'])
-    if train_tfrecords or val_tfrecords:
-        if train_tfrecords:
-            for tfrecord in train_tfrecords:
-                slide = sf.util.path_to_name(tfrecord)
-                outcome_label = labels[slide] if labels else 'NA'
-                out += ' '.join([slide, 'training', str(outcome_label)])
-                if filename:
-                    writer.writerow([slide, 'training', outcome_label])
-        if val_tfrecords:
-            for tfrecord in val_tfrecords:
-                slide = sf.util.path_to_name(tfrecord)
-                outcome_label = labels[slide] if labels else 'NA'
-                out += ' '.join([slide, 'validation', str(outcome_label)])
-                if filename:
-                    writer.writerow([slide, 'validation', outcome_label])
-    if filename:
-        save_file.close()
-    return out
```

## slideflow/model/features.py

```diff
@@ -56,45 +56,136 @@
         *,
         labels: Optional[Labels] = None,
         cache: Optional[str] = None,
         annotations: Optional[Labels] = None,
         **kwargs: Any
     ) -> None:
 
-        """Calculates features / layer activations from model, storing to
+        """Calculate features / layer activations from model, storing to
         internal parameters ``self.activations``, and ``self.predictions``,
         ``self.locations``, dictionaries mapping slides to arrays of activations,
         predictions, and locations for each tiles' constituent tiles.
 
         Args:
             model (str): Path to model from which to calculate activations.
             dataset (:class:`slideflow.Dataset`): Dataset from which to
                 generate activations.
             labels (dict, optional): Dict mapping slide names to outcome
                 categories.
             cache (str, optional): File for PKL cache.
-            annotations: Deprecated.
 
         Keyword Args:
-            layers (str): Model layer(s) from which to calculate activations.
-                Defaults to 'postconv'.
+            augment (bool, str, optional): Whether to use data augmentation
+                during feature extraction. If True, will use default
+                augmentation. If str, will use augmentation specified by the
+                string. Defaults to None.
             batch_size (int): Batch size for activations calculations.
                 Defaults to 32.
+            device (str, optional): Device to use for feature extraction.
+                Only used for PyTorch feature extractors. Defaults to None.
             include_preds (bool): Calculate and store predictions.
                 Defaults to True.
+            include_uncertainty (bool, optional): Whether to include model
+                uncertainty in the output. Only used if the feature generator
+                is a UQ-enabled model. Defaults to True.
+            layers (str, list(str)): Layers to extract features from. May be
+                the name of a single layer (str) or a list of layers (list).
+                Only used if model is a str. Defaults to 'postconv'.
+            normalizer ((str or :class:`slideflow.norm.StainNormalizer`), optional):
+                Stain normalization strategy to use on image tiles prior to
+                feature extraction. This argument is invalid if ``model`` is a
+                feature extractor built from a trained model, as stain
+                normalization will be specified by the model configuration.
+                Defaults to None.
+            normalizer_source (str, optional): Stain normalization preset
+                or path to a source image. Valid presets include 'v1', 'v2',
+                and 'v3'. If None, will use the default present ('v3').
+                This argument is invalid if ``model`` is a feature extractor
+                built from a trained model. Defaults to None.
+            num_workers (int, optional): Number of workers to use for feature
+                extraction. Only used for PyTorch feature extractors. Defaults
+                to None.
+            pool_sort (bool): Use multiprocessing pools to perform final
+                sorting. Defaults to True.
+            progress (bool): Show a progress bar during feature calculation.
+                Defaults to True.
+            verbose (bool): Show verbose logging output. Defaults to True.
+
+        Examples
+            Calculate features using a feature extractor.
+
+                .. code-block:: python
+
+                    import slideflow as sf
+                    from slideflow.model import build_feature_extractor
+
+                    # Create a feature extractor
+                    retccl = build_feature_extractor('retccl', tile_px=299)
+
+                    # Load a dataset
+                    P = sf.load_project(...)
+                    dataset = P.dataset(...)
+
+                    # Calculate features
+                    dts_ftrs = sf.DatasetFeatures(retccl, dataset)
+
+            Calculate features using a trained model (preferred).
+
+                .. code-block:: python
+
+                    from slideflow.model import build_feature_extractor
+
+                    # Create a feature extractor from the saved model.
+                    extractor = build_feature_extractor(
+                        '/path/to/trained_model.zip',
+                        layers=['postconv']
+                    )
+
+                    # Calculate features across the dataset
+                    dts_ftrs = sf.DatasetFeatures(extractor, dataset)
+
+            Calculate features using a trained model (legacy).
+
+                .. code-block:: python
+
+                    # This method is deprecated, and will be removed in a
+                    # future release. Please use the method above instead.
+                    dts_ftrs = sf.DatasetFeatures(
+                        '/path/to/trained_model.zip',
+                        dataset=dataset,
+                        layers=['postconv']
+                    )
+
+            Calculate features from a loaded model.
+
+                .. code-block:: python
+
+                    import tensorflow as tf
+                    import slideflow as sf
+
+                    # Load a model
+                    model = tf.keras.models.load_model('/path/to/model.h5')
+
+                    # Calculate features
+                    dts_ftrs = sf.DatasetFeatures(
+                        model,
+                        layers=['postconv'],
+                        dataset
+                    )
+
         """
         self.activations = defaultdict(list)  # type: Dict[str, Any]
         self.predictions = defaultdict(list)  # type: Dict[str, Any]
         self.uncertainty = defaultdict(list)  # type: Dict[str, Any]
         self.locations = defaultdict(list)  # type: Dict[str, Any]
         self.num_features = 0
         self.num_classes = 0
         self.model = model
         self.dataset = dataset
-        self.generator = None
+        self.feature_generator = None
         if dataset is not None:
             self.tile_px = dataset.tile_px
             self.manifest = dataset.manifest()
             self.tfrecords = np.array(dataset.tfrecords())
         else:
             # Used when creating via DatasetFeatures.from_df(),
             # otherwise dataset should not be None.
@@ -184,16 +275,31 @@
 
         # Show total number of features
         if self.num_features is None:
             self.num_features = self.activations[self.slides[0]].shape[-1]
         log.debug(f'Number of activation features: {self.num_features}')
 
     @classmethod
-    def from_df(cls, df: "pd.core.frame.DataFrame"):
-        """Load DataFrame of features, as exported by :meth:`DatasetFeatures.to_df()`"""
+    def from_df(cls, df: "pd.core.frame.DataFrame") -> "DatasetFeatures":
+        """Load DataFrame of features, as exported by :meth:`DatasetFeatures.to_df()`
+
+        Args:
+            df (:class:`pandas.DataFrame`): DataFrame of features, as exported by
+                :meth:`DatasetFeatures.to_df()`
+
+        Returns:
+            :class:`DatasetFeatures`: DatasetFeatures object
+
+        Examples
+            Recreate DatasetFeatures after export to a DataFrame.
+
+                >>> df = features.to_df()
+                >>> new_features = DatasetFeatures.from_df(df)
+
+        """
         obj = cls(None, None)  # type: ignore
         obj.slides = df.slide.unique().tolist()
         if 'activations' in df.columns:
             obj.activations = {
                 s: np.stack(df.loc[df.slide==s].activations.values)
                 for s in obj.slides
             }
@@ -205,38 +311,54 @@
             }
         if 'uncertainty' in df.columns:
             obj.uncertainty = {
                 s: np.stack(df.loc[df.slide==s].uncertainty.values)
                 for s in obj.slides
             }
         if 'predictions' in df.columns:
-            obj.uncertainty = {
+            obj.predictions = {
                 s: np.stack(df.loc[df.slide==s].predictions.values)
                 for s in obj.slides
             }
             obj.num_classes = next(df.iterrows())[1].predictions.shape[0]
         return obj
 
     @classmethod
     def concat(
         cls,
         args: Iterable["DatasetFeatures"],
-    ):
-        """Concatenates activations from multiple DatasetFeatures together.
+    ) -> "DatasetFeatures":
+        """Concatenate activations from multiple DatasetFeatures together.
 
         For example, if ``df1`` is a DatasetFeatures object with 2048 features
         and ``df2`` is a DatasetFeatures object with 1024 features,
         then ``sf.DatasetFeatures.concat([df1, df2])`` would return an object
         with 3072.
 
         Vectors from DatasetFeatures objects are concatenated in the given order.
         During concatenation, predictions and uncertainty are dropped.
 
         If there are any tiles that do not have calculated features in both
         dataframes, these will be dropped.
+
+        Args:
+            args (Iterable[:class:`DatasetFeatures`]): DatasetFeatures objects
+                to concatenate.
+
+        Returns:
+            :class:`DatasetFeatures`: DatasetFeatures object with concatenated
+            features.
+
+        Examples
+            Concatenate two DatasetFeatures objects.
+
+                >>> df1 = DatasetFeatures(model, dataset, layers='postconv')
+                >>> df2 = DatasetFeatures(model, dataset, layers='sepconv_3')
+                >>> df = DatasetFeatures.concat([df1, df2])
+
         """
         assert len(args) > 1
         dfs = []
         for f, ftrs in enumerate(args):
             log.debug(f"Creating dataframe {f} from features...")
             dfs.append(ftrs.to_df())
         if not all([len(df) == len(dfs[0]) for df in dfs]):
@@ -287,44 +409,58 @@
     @property
     def normalizer(self):
         if self.feature_generator is None:
             return None
         else:
             return self.feature_generator.normalizer
 
-    def _generate_features(self, cache: Optional[str] = None, **kwargs) -> None:
+    def _generate_features(
+        self,
+        cache: Optional[str] = None,
+        progress: bool = True,
+        verbose: bool = True,
+        pool_sort: bool = True,
+        **kwargs
+    ) -> None:
 
         """Calculates activations from a given model, saving to self.activations
 
         Args:
             model (str): Path to Tensorflow model from which to calculate final
                 layer activations.
             layers (str, optional): Layers from which to generate activations.
                 Defaults to 'postconv'.
             include_preds (bool, optional): Include logit predictions.
                 Defaults to True.
             include_uncertainty (bool, optional): Include uncertainty
                 estimation if UQ enabled. Defaults to True.
             batch_size (int, optional): Batch size to use during activations
                 calculations. Defaults to 32.
+            progress (bool): Show a progress bar during feature calculation.
+                Defaults to True.
+            verbose (bool): Show verbose logging output. Defaults to True.
+            pool_sort (bool): Use multiprocessing pools to perform final
+                sorting. Defaults to True.
             cache (str, optional): File in which to store PKL cache.
         """
 
         fg = self.feature_generator = _FeatureGenerator(
             self.model,
             self.dataset,
             **kwargs
         )
         self.num_features = fg.num_features
         self.num_classes = fg.num_classes
 
         # Calculate final layer activations for each tfrecord
         fla_start_time = time.time()
 
-        activations, predictions, locations, uncertainty = fg.generate()
+        activations, predictions, locations, uncertainty = fg.generate(
+            progress=progress, verbose=verbose
+        )
 
         self.activations = {s: np.stack(v) for s, v in activations.items()}
         self.predictions = {s: np.stack(v) for s, v in predictions.items()}
         self.locations = {s: np.stack(v) for s, v in locations.items()}
         self.uncertainty = {s: np.stack(v) for s, v in uncertainty.items()}
 
         # Sort using TFRecord location information,
@@ -333,19 +469,34 @@
             slides_to_sort = [
                 s for s in self.slides
                 if (self.activations[s].size
                     or not self.predictions[s].size
                     or not self.locations[s].size
                     or not self.uncertainty[s].size)
             ]
-            pool = mp.Pool(os.cpu_count())
-            for i, true_locs in enumerate(track(pool.imap(self.dataset.get_tfrecord_locations, slides_to_sort),
-                                                transient=False,
-                                                total=len(slides_to_sort),
-                                                description="Sorting...")):
+            if pool_sort and len(slides_to_sort) > 1:
+                pool = mp.Pool(sf.util.num_cpu())
+                imap_iterable = pool.imap(
+                    self.dataset.get_tfrecord_locations, slides_to_sort
+                )
+            else:
+                pool = None
+                imap_iterable = map(
+                    self.dataset.get_tfrecord_locations, slides_to_sort
+                )
+            if progress:
+                iterable = track(
+                    imap_iterable,
+                    transient=False,
+                    total=len(slides_to_sort),
+                    description="Sorting...")
+            else:
+                iterable = imap_iterable
+
+            for i, true_locs in enumerate(iterable):
                 slide = slides_to_sort[i]
                 # Get the order of locations stored in TFRecords,
                 # and the corresponding indices for sorting
                 cur_locs = self.locations[slide]
                 idx = [true_locs.index(tuple(cur_locs[i])) for i in range(cur_locs.shape[0])]
 
                 # Make sure that the TFRecord indices are continuous, otherwise
@@ -357,15 +508,16 @@
                 if slide in self.activations:
                     self.activations[slide] = self.activations[slide][sorted_idx]
                 if slide in self.predictions:
                     self.predictions[slide] = self.predictions[slide][sorted_idx]
                 if slide in self.uncertainty:
                     self.uncertainty[slide] = self.uncertainty[slide][sorted_idx]
                 self.locations[slide] = self.locations[slide][sorted_idx]
-            pool.close()
+            if pool is not None:
+                pool.close()
 
         fla_calc_time = time.time()
         log.debug(f'Calculation time: {fla_calc_time-fla_start_time:.0f} sec')
         log.debug(f'Number of activation features: {self.num_features}')
 
         if cache:
             self.save_cache(cache)
@@ -442,14 +594,32 @@
             snsbox.set_title(title)
             snsbox.set(xlabel='Category', ylabel='Average tile activation')
             plt.xticks(plt.xticks()[0], self.used_categories)
             boxplot_filename = join(outdir, f'boxplot_{title}.png')
             plt.gcf().canvas.start_event_loop(sys.float_info.min)
             plt.savefig(boxplot_filename, bbox_inches='tight')
 
+    def dump_config(self):
+        """Return a dictionary of the feature extraction configuration."""
+        if self.normalizer:
+            norm_dict = dict(
+                method=self.normalizer.method,
+                fit=self.normalizer.get_fit(as_list=True),
+            )
+        else:
+            norm_dict = None
+        config = dict(
+            extractor=self.feature_generator.generator.dump_config(),
+            normalizer=norm_dict,
+            num_features=self.num_features,
+            tile_px=self.dataset.tile_px,
+            tile_um=self.dataset.tile_um
+        )
+        return config
+
     def export_to_torch(self, *args, **kwargs):
         """Deprecated function; please use `.to_torch()`"""
         warnings.warn(
             "Deprecation warning: DatasetFeatures.export_to_torch() will"
             " be removed in a future version. Use .to_torch() instead.",
             DeprecationWarning
         )
@@ -526,47 +696,59 @@
                     else:
                         csvwriter.writerow([slide] + act)
         log.debug(f'Activations saved to [green]{filename}')
 
     def to_torch(
         self,
         outdir: str,
-        slides: Optional[List[str]] = None
+        slides: Optional[List[str]] = None,
+        verbose: bool = True
     ) -> None:
         """Export activations in torch format to .pt files in the directory.
 
         Used for training MIL models.
 
         Args:
             outdir (str): Path to directory in which to save .pt files.
+            verbose (bool): Verbose logging output. Defaults to True.
 
         """
         import torch
 
         if not exists(outdir):
             os.makedirs(outdir)
         slides = self.slides if not slides else slides
-        for slide in track(slides):
+        for slide in (slides if not verbose else track(slides)):
             if self.activations[slide] == []:
                 log.info(f'Skipping empty slide [green]{slide}')
                 continue
             slide_activations = torch.from_numpy(
                 self.activations[slide].astype(np.float32)
             )
             torch.save(slide_activations, join(outdir, f'{slide}.pt'))
             tfrecord2idx.save_index(
                 self.locations[slide],
                 join(outdir, f'{slide}.index')
             )
-        args = {
-            'model': self.model if isinstance(self.model, str) else '<NA>',
-            'num_features': self.num_features
-        }
-        sf.util.write_json(args, join(outdir, 'settings.json'))
-        log.info(f'Activations exported in Torch format to {outdir}')
+
+        # Log the feature extraction configuration
+        config = self.dump_config()
+        if exists(join(outdir, 'bags_config.json')):
+            old_config = sf.util.load_json(join(outdir, 'bags_config.json'))
+            if old_config != config:
+                log.warning(
+                    "Feature extraction configuration does not match the "
+                    "configuration used to generate the existing bags at "
+                    f"{outdir}. Current configuration will not be saved."
+                )
+        else:
+            sf.util.write_json(config, join(outdir, 'bags_config.json'))
+
+        log_fn = log.info if verbose else log.debug
+        log_fn(f'Activations exported in Torch format to {outdir}')
 
     def to_df(
         self
     ) -> pd.core.frame.DataFrame:
         """Export activations, predictions, uncertainty, and locations to
         a pandas DataFrame.
 
@@ -1052,21 +1234,68 @@
         model: Union[str, "BaseFeatureExtractor", "tf.keras.models.Model", "torch.nn.Module"],
         dataset: "sf.Dataset",
         *,
         layers: Union[str, List[str]] = 'postconv',
         include_preds: Optional[bool] = None,
         include_uncertainty: bool = True,
         batch_size: int = 32,
+        device: Optional[str] = None,
+        num_workers: Optional[int] = None,
+        augment: Optional[Union[bool, str]] = None,
         **kwargs
     ) -> None:
+        """Initializes FeatureGenerator.
+
+        Args:
+            model (str, BaseFeatureExtractor, tf.keras.models.Model, torch.nn.Module):
+                Model to use for feature extraction. If str, must be a path to
+                a saved model.
+            dataset (sf.Dataset): Dataset to use for feature extraction.
+
+        Keyword Args:
+            augment (bool, str, optional): Whether to use data augmentation
+                during feature extraction. If True, will use default
+                augmentation. If str, will use augmentation specified by the
+                string. Defaults to None.
+            batch_size (int, optional): Batch size to use for feature
+                extraction. Defaults to 32.
+            device (str, optional): Device to use for feature extraction.
+                Only used for PyTorch feature extractors. Defaults to None.
+            include_preds (bool, optional): Whether to include model
+                predictions. If None, will be set to True if
+                model has a num_classes attribute. Defaults to None.
+            include_uncertainty (bool, optional): Whether to include model
+                uncertainty in the output. Only used if the feature generator
+                is a UQ-enabled model. Defaults to True.
+            layers (str, list(str)): Layers to extract features from. May be
+                the name of a single layer (str) or a list of layers (list).
+                Only used if model is a str. Defaults to 'postconv'.
+            normalizer ((str or :class:`slideflow.norm.StainNormalizer`), optional):
+                Stain normalization strategy to use on image tiles prior to
+                feature extraction. This argument is invalid if ``model`` is a
+                feature extractor built from a trained model, as stain
+                normalization will be specified by the model configuration.
+                Defaults to None.
+            normalizer_source (str, optional): Stain normalization preset
+                or path to a source image. Valid presets include 'v1', 'v2',
+                and 'v3'. If None, will use the default present ('v3').
+                This argument is invalid if ``model`` is a feature extractor
+                built from a trained model. Defaults to None.
+            num_workers (int, optional): Number of workers to use for feature
+                extraction. Only used for PyTorch feature extractors. Defaults
+                to None.
+
+        """
         self.model = model
         self.dataset = dataset
         self.layers = sf.util.as_list(layers)
         self.batch_size = batch_size
         self.simclr_args = None
+        self.num_workers = num_workers
+        self.augment = augment
 
         # Check if location information is stored in TFRecords
         self.tfrecords_have_loc = self.dataset.tfrecords_have_locations()
         if not self.tfrecords_have_loc:
             log.warning(
                 "Some TFRecords do not have tile location information; "
                 "dataset iteration speed may be affected."
@@ -1075,36 +1304,43 @@
         if self.is_extractor() and include_preds is None:
             include_preds = self.model.num_classes > 0  # type: ignore
         elif include_preds is None:
             include_preds = True
         self.include_preds = include_preds
         self.include_uncertainty = include_uncertainty
 
+        # Determine UQ and stain normalization.
+        # If the `model` is a feature extractor, stain normalization
+        # will be determined via keyword arguments by self._prepare_generator()
         self._determine_uq_and_normalizer()
         self.generator = self._prepare_generator(**kwargs)
+
         self.num_features = self.generator.num_features
         self.num_classes = 0 if not include_preds else self.generator.num_classes
+        if self.is_torch():
+            from slideflow.model import torch_utils
+            self.device = torch_utils.get_device(device)
+        else:
+            self.device = None
         self._prepare_dataset_kwargs()
 
     def _calculate_feature_batch(self, batch_img):
         """Calculate features from a batch of images."""
 
         # If a PyTorch generator, wrap in no_grad() and perform on CUDA
         if self.is_torch():
             import torch
             with torch.no_grad():
-                batch_img = batch_img.to('cuda')
+                batch_img = batch_img.to(self.device)
                 if self.has_torch_gpu_normalizer():
                     batch_img = self.normalizer.preprocess(
                         batch_img,
                         standardize=self.standardize
                     )
                 return self.generator(batch_img)
-        elif self.is_simclr():
-            return self.generator(batch_img, training=False)
         else:
             return self.generator(batch_img)
 
     def _process_out(self, model_out, batch_slides, batch_loc):
         model_out = sf.util.as_list(model_out)
 
         # Process data if the output is Tensorflow (SimCLR or Tensorflow model)
@@ -1133,42 +1369,41 @@
                 for m in model_out
             ]
             if batch_loc[0] is not None:
                 loc = np.stack([batch_loc[0], batch_loc[1]], axis=1)
             else:
                 loc = None
 
-        # Final processing
-        if self.is_simclr():
-            model_out = model_out[0]
+        # Final processing.
+        # Order of return is features, predictions, uncertainty.
         if self.uq and self.include_uncertainty:
             uncertainty = model_out[-1]
             model_out = model_out[:-1]
         else:
             uncertainty = None
         if self.include_preds:
             predictions = model_out[-1]
             features = model_out[:-1]
         else:
             predictions = None
             features = model_out
 
-        # Concatenate features if we have features from >`` layer
-        if self.layers:
-            features = np.concatenate(features)
+        # Concatenate features if we have features from >1 layer
+        if isinstance(features, list):
+            features = np.concatenate(features, axis=1)
 
         return features, predictions, uncertainty, slides, loc
 
     def _prepare_dataset_kwargs(self):
         """Prepare keyword arguments for Dataset.tensorflow() or .torch()."""
 
         dts_kw = {
             'infinite': False,
             'batch_size': self.batch_size,
-            'augment': False,
+            'augment': self.augment,
             'incl_slidenames': True,
             'incl_loc': True,
         }
 
         # If this is a Feature Extractor, update the dataset kwargs
         # with any preprocessing instructions specified by the extractor
         if self.is_extractor():
@@ -1183,73 +1418,119 @@
         if self.has_torch_gpu_normalizer():
             log.info("Using GPU for stain normalization")
             dts_kw['standardize'] = False
         else:
             # Otherwise, let the dataset handle normalization/standardization.
             dts_kw['normalizer'] = self.normalizer
 
+        # This is not used by SimCLR feature extractors.
         self.dts_kw = dts_kw
 
     def _determine_uq_and_normalizer(self):
+        """Determines whether the model uses UQ and its stain normalizer."""
+
         # Load configuration if model is path to a saved model
         if isinstance(self.model, BaseFeatureExtractor):
             self.uq = self.model.num_uncertainty > 0
+            # If the feature extractor has a normalizer, use it.
+            # This will be overridden by keyword arguments if the
+            # feature extractor is not an instance of slideflow.model.Features.
             self.normalizer = self.model.normalizer
-        elif isinstance(self.model, str) and sf.util.is_simclr_model_path(self.model):
-            self.uq = False
-            self.normalizer = None
         elif isinstance(self.model, str):
             model_config = sf.util.get_model_config(self.model)
             hp = sf.ModelParams.from_dict(model_config['hp'])
             self.uq = hp.uq
             self.normalizer = hp.get_normalizer()
             if self.normalizer:
                 log.info(f'Using realtime {self.normalizer.method} normalization')
                 if 'norm_fit' in model_config:
                     self.normalizer.set_fit(**model_config['norm_fit'])
         else:
             self.normalizer = None
             self.uq = False
 
+    def _norm_from_kwargs(self, kwargs):
+        """Parse the stain normalizer from keyword arguments."""
+        if 'normalizer' in kwargs and kwargs['normalizer'] is not None:
+            norm = kwargs['normalizer']
+            del kwargs['normalizer']
+            if 'normalizer_source' in kwargs:
+                norm_src = kwargs['normalizer_source']
+                del kwargs['normalizer_source']
+            else:
+                norm_src = None
+            if isinstance(norm, str):
+                normalizer = sf.norm.autoselect(
+                    norm,
+                    source=norm_src,
+                    backend='tensorflow' if self.is_tf() else 'torch'
+                )
+            else:
+                normalizer = norm
+            log.info(f"Normalizing with {normalizer.method}")
+            return normalizer, kwargs
+        else:
+            if 'normalizer_source' in kwargs:
+                del kwargs['normalizer_source']
+            return None, kwargs
+
     def _prepare_generator(self, **kwargs) -> Callable:
         """Prepare the feature generator."""
 
         # Generator is a Feature Extractor
         if self.is_extractor():
+
+            # Handle the case where the extractor is built from a trained model
+            if self.is_tf():
+                from slideflow.model.tensorflow import Features as TFFeatures
+                is_tf_model_extractor = isinstance(self.model, TFFeatures)
+                is_torch_model_extractor = False
+            elif self.is_torch():
+                from slideflow.model.torch import Features as TorchFeatures
+                is_torch_model_extractor = isinstance(self.model, TorchFeatures)
+                is_tf_model_extractor = False
+            else:
+                is_tf_model_extractor = False
+                is_torch_model_extractor = False
+            if (is_tf_model_extractor or is_torch_model_extractor) and 'normalizer' in kwargs:
+                raise ValueError(
+                    "Cannot specify a normalizer when using a feature extractor "
+                    "created from a trained model. Stain normalization is auto-detected "
+                    "from the model configuration."
+                )
+            elif (is_tf_model_extractor or is_torch_model_extractor) and kwargs:
+                raise ValueError(
+                    f"Invalid keyword arguments: {', '.join(list(kwargs.keys()))}"
+                )
+            elif (is_tf_model_extractor or is_torch_model_extractor):
+                # Stain normalization has already been determined
+                # from the model configuration.
+                return self.model
+
+            # For all other feature extractors, stain normalization
+            # is determined from keyword arguments.
+            self.normalizer, kwargs = self._norm_from_kwargs(kwargs)
             if kwargs:
                 raise ValueError(
                     f"Invalid keyword arguments: {', '.join(list(kwargs.keys()))}"
                 )
             return self.model
 
-        # Generator is a model, and we're using UQ
-        elif self.uq and self.include_uncertainty:
+        # Generator is a path to a trained model, and we're using UQ
+        elif self.is_model_path() and (self.uq and self.include_uncertainty):
+            if self.include_preds is False:
+                raise ValueError(
+                    "include_preds must be True if include_uncertainty is True"
+                )
             return sf.model.UncertaintyInterface(
                 self.model,
                 layers=self.layers,
                 **kwargs
             )
 
-        # Generator is a SimCLR path
-        elif self.is_simclr():
-            from slideflow import simclr
-            self.simclr_args = simclr.load_model_args(self.model)
-            generator = simclr.load(self.model)
-            generator.num_features = self.simclr_args.proj_out_dim
-            generator.num_classes = self.simclr_args.num_classes
-            if 'normalizer' in kwargs:
-                self.normalizer = kwargs['normalizer']
-                del kwargs['normalizer']
-                log.info(f"Normalizing with {self.normalizer.method}")
-            if kwargs:
-                raise ValueError(
-                    f"Invalid keyword arguments: {', '.join(list(kwargs.keys()))}"
-                )
-            return generator
-
         # Generator is a path to a trained Slideflow model
         elif self.is_model_path():
             return sf.model.Features(
                 self.model,
                 layers=self.layers,
                 include_preds=self.include_preds,
                 **kwargs
@@ -1263,43 +1544,38 @@
                 include_preds=self.include_preds,
                 **kwargs
             )
 
         # Generator is a loaded torch.nn.Module
         elif self.is_torch():
             return sf.model.Features.from_model(
-                self.model.to('cuda'),
+                self.model.to(self.device),
                 tile_px=self.tile_px,
                 layers=self.layers,
                 include_preds=self.include_preds,
                 **kwargs
             )
 
-        # Unrecognized feature generator
+        # Unrecognized feature extractor
         else:
-            raise ValueError(f'Unrecognized model {self.model}')
+            raise ValueError(f'Unrecognized feature extractor {self.model}')
 
     def is_model_path(self):
         return isinstance(self.model, str) and (self.is_tf() or self.is_torch())
 
     def is_extractor(self):
         return isinstance(self.model, BaseFeatureExtractor)
 
-    def is_simclr(self):
-        return sf.util.is_simclr_model_path(self.model)
-
     def is_torch(self):
         if self.is_extractor():
             return self.model.is_torch()
         else:
             return sf.model.is_torch_model(self.model)
 
     def is_tf(self):
-        if self.is_simclr():
-            return True
         if self.is_extractor():
             return self.model.is_tensorflow()
         else:
             return sf.model.is_tensorflow_model(self.model)
 
     def has_torch_gpu_normalizer(self):
         return (
@@ -1307,48 +1583,39 @@
             and self.normalizer.__class__.__name__ == 'TorchStainNormalizer'
             and self.normalizer.device != 'cpu'
         )
 
     def build_dataset(self):
         """Build a dataloader."""
 
-        # Generator is a SimCLR model.
-        if self.is_simclr():
-            log.debug("Setting up Tensorflow/SimCLR dataset iterator")
-            from slideflow import simclr
-            builder = simclr.DatasetBuilder(
-                val_dts=self.dataset,
-                dataset_kwargs=dict(
-                    incl_slidenames=True,
-                    incl_loc=True,
-                    normalizer=self.normalizer
-                )
-            )
-            return builder.build_dataset(
-                self.batch_size,
-                is_training=False,
-                simclr_args=self.simclr_args
-            )
-
         # Generator is a Tensorflow model.
-        elif self.is_tf():
+        if self.is_tf():
             log.debug(
                 "Setting up Tensorflow dataset iterator (num_parallel_reads="
                 f"None, deterministic={not self.tfrecords_have_loc})"
             )
+            # Disable parallel reads if we're using tfrecords without location
+            # information, as we would need to read and receive data in order.
+            if not self.tfrecords_have_loc:
+                par_kw = dict(num_parallel_reads=None)
+            else:
+                par_kw = dict()
             return self.dataset.tensorflow(
                 None,
-                num_parallel_reads=None,
                 deterministic=(not self.tfrecords_have_loc),
+                **par_kw,
                 **self.dts_kw  # type: ignore
             )
 
         # Generator is a PyTorch model.
         elif self.is_torch():
-            n_workers = (4 if self.tfrecords_have_loc else 1)
+            if self.num_workers is None:
+                n_workers = (4 if self.tfrecords_have_loc else 1)
+            else:
+                n_workers = self.num_workers
             log.debug(
                 "Setting up PyTorch dataset iterator (num_workers="
                 f"{n_workers}, chunk_size=8)"
             )
             return self.dataset.torch(
                 None,
                 num_workers=n_workers,
@@ -1356,23 +1623,24 @@
                 **self.dts_kw  # type: ignore
             )
 
         # Unrecognized feature generator.
         else:
             raise ValueError(f"Unrecognized model type: {type(self.model)}")
 
-    def generate(self):
+    def generate(self, *, verbose: bool = True, progress: bool = True):
 
         # Get the dataloader for iterating through tfrecords
         dataset = self.build_dataset()
 
         # Rename tfrecord_array to tfrecords
-        log.info(f'Calculating activations for {len(self.dataset.tfrecords())} '
-                 f'tfrecords (layers={self.layers})')
-        log.info(f'Generating from [green]{self.model}')
+        log_fn = log.info if verbose else log.debug
+        log_fn(f'Calculating activations for {len(self.dataset.tfrecords())} '
+               'tfrecords')
+        log_fn(f'Generating from [green]{self.model}')
 
         # Interleave tfrecord datasets
         estimated_tiles = self.dataset.num_tiles
 
         activations = defaultdict(list)  # type: Dict[str, Any]
         predictions = defaultdict(list)  # type: Dict[str, Any]
         uncertainty = defaultdict(list)  # type: Dict[str, Any]
@@ -1399,23 +1667,27 @@
                         uncertainty[slide].append(unc[d])
                     if loc is not None:
                         locations[slide].append(loc[d])
 
         batch_proc_thread = threading.Thread(target=batch_worker, daemon=True)
         batch_proc_thread.start()
 
-        pb = Progress(*Progress.get_default_columns(),
-                      ImgBatchSpeedColumn(),
-                      transient=sf.getLoggingLevel()>20)
-        task = pb.add_task("Generating...", total=estimated_tiles)
-        pb.start()
+        if progress:
+            pb = Progress(*Progress.get_default_columns(),
+                        ImgBatchSpeedColumn(),
+                        transient=sf.getLoggingLevel()>20)
+            task = pb.add_task("Generating...", total=estimated_tiles)
+            pb.start()
+        else:
+            pb = None
         with sf.util.cleanup_progress(pb):
             for batch_img, _, batch_slides, batch_loc_x, batch_loc_y in dataset:
                 model_output = self._calculate_feature_batch(batch_img)
                 q.put((model_output, batch_slides, (batch_loc_x, batch_loc_y)))
-                pb.advance(task, self.batch_size)
+                if progress:
+                    pb.advance(task, self.batch_size)
         q.put((None, None, None))
         batch_proc_thread.join()
         if hasattr(dataset, 'close'):
             dataset.close()
 
         return activations, predictions, locations, uncertainty
```

## slideflow/model/tensorflow.py

```diff
@@ -1656,16 +1656,16 @@
                 of steps per epoch. Defaults to 0 (automatic).
             save_predictions (bool or str, optional): Save tile, slide, and
                 patient-level predictions at each evaluation. May be 'csv',
                 'feather', or 'parquet'. If False, will not save predictions.
                 Defaults to 'parquet'.
             save_model (bool, optional): Save models when evaluating at
                 specified epochs. Defaults to True.
-            resume_training (str, optional): Path to Tensorflow model to
-                continue training. Defaults to None.
+            resume_training (str, optional): Path to model to continue training.
+                Only valid in Tensorflow backend. Defaults to None.
             pretrain (str, optional): Either 'imagenet' or path to Tensorflow
                 model from which to load weights. Defaults to 'imagenet'.
             checkpoint (str, optional): Path to cp.ckpt from which to load
                 weights. Defaults to None.
             save_checkpoint (bool, optional): Save checkpoints at each epoch.
                 Defaults to True.
             multi_gpu (bool, optional): Enable multi-GPU training using
@@ -1714,15 +1714,15 @@
                 config = sf.util.load_json(config_path)
             config['norm_fit'] = self.normalizer.get_fit(as_list=True)
             sf.util.write_json(config, config_path)
 
         # Prepare multiprocessing pool if from_wsi=True
         if from_wsi:
             pool = mp.Pool(
-                8 if os.cpu_count is None else os.cpu_count(),
+                sf.util.num_cpu(default=8),
                 initializer=sf.util.set_ignore_sigint
             )
         else:
             pool = None
 
         # Save training / validation manifest
         if val_dts is None:
@@ -2171,14 +2171,16 @@
                 may improve compatibility across hardware & environments.
         """
         super().__init__('tensorflow', include_preds=include_preds)
         if layers and isinstance(layers, str):
             layers = [layers]
         self.layers = layers
         self.path = path
+        self._pooling = None
+        self._include_preds = None
         if path is not None:
             self._model = load(self.path, method=load_method)  # type: ignore
             config = sf.util.get_model_config(path)
             if 'img_format' in config:
                 self.img_format = config['img_format']
             self.hp = sf.ModelParams()
             self.hp.load_dict(config['hp'])
@@ -2212,29 +2214,37 @@
             model (:class:`tensorflow.keras.models.Model`): Loaded model.
             layers (list(str), optional): Layers from which to generate
                 activations.  The post-convolution activation layer is accessed
                 via 'postconv'. Defaults to 'postconv'.
             include_preds (bool, optional): Include predictions in output. Will be
                 returned last. Defaults to False.
             wsi_normalizer (:class:`slideflow.norm.StainNormalizer`): Stain
-                normalizer to use on whole-slide images. Is not used on
+                normalizer to use on whole-slide images. Not used on
                 individual tile datasets via __call__. Defaults to None.
         """
         obj = cls(None, layers, include_preds)
         if isinstance(model, tf.keras.models.Model):
             obj._model = model
         else:
             raise errors.ModelError(f"Model {model} is not a valid Tensorflow "
                                     "model.")
         obj._build(
             layers=layers, include_preds=include_preds, pooling=pooling  # type: ignore
         )
         obj.wsi_normalizer = wsi_normalizer
         return obj
 
+    def __repr__(self):
+        return ("{}(\n".format(self.__class__.__name__) +
+                "    path={!r},\n".format(self.path) +
+                "    layers={!r},\n".format(self.layers) +
+                "    include_preds={!r},\n".format(self._include_preds) +
+                "    pooling={!r},\n".format(self._pooling) +
+                ")")
+
     def __call__(
         self,
         inp: Union[tf.Tensor, "sf.WSI"],
         **kwargs
     ) -> Optional[Union[np.ndarray, tf.Tensor]]:
         """Process a given input and return features and/or predictions.
         Expects either a batch of images or a :class:`slideflow.WSI`.
@@ -2255,154 +2265,44 @@
         img_format: str = 'auto',
         batch_size: int = 32,
         dtype: type = np.float16,
         grid: Optional[np.ndarray] = None,
         shuffle: bool = False,
         show_progress: bool = True,
         callback: Optional[Callable] = None,
+        normalizer: Optional[Union[str, "sf.norm.StainNormalizer"]] = None,
+        normalizer_source: Optional[str] = None,
         **kwargs
     ) -> Optional[np.ndarray]:
         """Generate activations from slide => activation grid array."""
 
-        log.debug(f"Slide prediction (batch_size={batch_size}, "
-                  f"img_format={img_format})")
+        # Check image format
         if img_format == 'auto' and self.img_format is None:
             raise ValueError(
                 'Unable to auto-detect image format (png or jpg). Set the '
                 'format by passing img_format=... to the call function.'
             )
         elif img_format == 'auto':
             assert self.img_format is not None
             img_format = self.img_format
-        if img_format == 'png':  # PNG is lossless; this is equivalent but faster
-            log.debug("Using numpy image format instead of PNG")
-            img_format = 'numpy'
-        total_out = self.num_features + self.num_classes + self.num_uncertainty
-        if grid is None:
-            features_grid = np.ones((
-                    slide.grid.shape[1],
-                    slide.grid.shape[0],
-                    total_out),
-                dtype=dtype)
-            features_grid *= -99
-        else:
-            assert grid.shape == (slide.grid.shape[1], slide.grid.shape[0], total_out)
-            features_grid = grid
-        generator = slide.build_generator(
+
+        return sf.model.extractors.features_from_slide(
+            self,
+            slide,
             img_format=img_format,
+            batch_size=batch_size,
+            dtype=dtype,
+            grid=grid,
             shuffle=shuffle,
             show_progress=show_progress,
+            callback=callback,
+            normalizer=(normalizer if normalizer else self.wsi_normalizer),
+            normalizer_source=normalizer_source,
             **kwargs
         )
-        if not generator:
-            log.error(f"No tiles extracted from slide [green]{slide.name}")
-            return None
-
-        def tile_generator():
-            for image_dict in generator():
-                yield {
-                    'grid': image_dict['grid'],
-                    'image': image_dict['image']
-                }
-
-        @tf.function
-        def _parse(record):
-            image = record['image']
-            if img_format.lower() in ('jpg', 'jpeg'):
-                image = tf.image.decode_jpeg(image, channels=3)
-            image.set_shape([slide.tile_px, slide.tile_px, 3])
-            loc = record['grid']
-            return image, loc
-
-        @tf.function
-        def _standardize(image, loc):
-            parsed_image = tf.image.per_image_standardization(image)
-            return parsed_image, loc
-
-        # Generate dataset from the generator
-        with tf.name_scope('dataset_input'):
-            output_signature = {
-                'grid': tf.TensorSpec(shape=(2), dtype=tf.uint32)
-            }
-            if img_format.lower() in ('jpg', 'jpeg'):
-                output_signature.update({
-                    'image': tf.TensorSpec(shape=(), dtype=tf.string)
-                })
-            else:
-                output_signature.update({
-                    'image': tf.TensorSpec(shape=(slide.tile_px,
-                                                  slide.tile_px,
-                                                  3),
-                                           dtype=tf.uint8)
-                })
-            tile_dataset = tf.data.Dataset.from_generator(
-                tile_generator,
-                output_signature=output_signature
-            )
-            tile_dataset = tile_dataset.map(
-                _parse,
-                num_parallel_calls=tf.data.AUTOTUNE,
-                deterministic=True
-            )
-            if self.wsi_normalizer:
-                if self.wsi_normalizer.vectorized:
-                    log.debug("Using vectorized normalization")
-                    norm_batch_size = 32 if not batch_size else batch_size
-                    tile_dataset = tile_dataset.batch(norm_batch_size, drop_remainder=False)
-                else:
-                    log.debug("Using per-image normalization")
-                tile_dataset = tile_dataset.map(
-                    self.wsi_normalizer.tf_to_tf,
-                    num_parallel_calls=tf.data.AUTOTUNE,
-                    deterministic=True
-                )
-                if self.wsi_normalizer.vectorized:
-                    tile_dataset = tile_dataset.unbatch()
-                if self.wsi_normalizer.method == 'macenko':
-                    # Drop the images that causes an error, e.g. if eigen
-                    # decomposition is unsuccessful.
-                    tile_dataset = tile_dataset.apply(tf.data.experimental.ignore_errors())
-            tile_dataset = tile_dataset.map(
-                _standardize,
-                num_parallel_calls=tf.data.AUTOTUNE,
-                deterministic=True
-            )
-            tile_dataset = tile_dataset.batch(batch_size, drop_remainder=False)
-            tile_dataset = tile_dataset.prefetch(8)
-
-        for i, (batch_images, batch_loc) in enumerate(tile_dataset):
-            model_out = self._predict(batch_images)
-            if not isinstance(model_out, (list, tuple)):
-                model_out = [model_out]
-
-            # Flatten the output, relevant when
-            # there are multiple outcomes / classifier heads
-            _act_batch = []
-            for m in model_out:
-                if isinstance(m, list):
-                    _act_batch += [_m.numpy() for _m in m]
-                else:
-                    _act_batch.append(m.numpy())
-            _act_batch = np.concatenate(_act_batch, axis=-1)
-
-            _loc_batch = batch_loc.numpy()
-            grid_idx_updated = []
-            for i, act in enumerate(_act_batch):
-                xi = _loc_batch[i][0]
-                yi = _loc_batch[i][1]
-                if callback:
-                    grid_idx_updated.append((yi, xi))
-                features_grid[yi][xi] = act
-
-            # Trigger a callback signifying that the grid has been updated.
-            # Useful for progress tracking.
-            if callback:
-                callback(grid_idx_updated)
-
-        return features_grid
 
     @tf.function
     def _predict(self, inp: tf.Tensor) -> tf.Tensor:
         """Return activations for a single batch of images."""
         return self.model(inp, training=False)
 
     def _build(
@@ -2411,14 +2311,17 @@
         include_preds: bool = True,
         pooling: Optional[Any] = None
     ) -> None:
         """Builds the interface model that outputs feature activations at the
         designated layers and/or predictions. Intermediate layers are returned in
         the order of layers. predictions are returned last."""
 
+        self._pooling = pooling
+        self._include_preds = include_preds
+
         if isinstance(pooling, str):
             if pooling == 'avg':
                 pooling = tf.keras.layers.GlobalAveragePooling2D
             elif pooling == 'max':
                 pooling = tf.keras.layers.GlobalMaxPool2D
             else:
                 raise ValueError(f"Unrecognized pooling value {pooling}. "
@@ -2487,20 +2390,30 @@
         else:
             self.num_classes = 0
 
         if include_preds:
             log.debug(f'Number of classes: {self.num_classes}')
         log.debug(f'Number of activation features: {self.num_features}')
 
+    def dump_config(self):
+        return {
+            'class': 'slideflow.model.tensorflow.Features',
+            'kwargs': {
+                'path': self.path,
+                'layers': self.layers,
+                'include_preds': self._include_preds,
+                'pooling': self._pooling
+            }
+        }
 
 class UncertaintyInterface(Features):
     def __init__(
         self,
         path: Optional[str],
-        layers: Optional[Union[str, List[str]]] = None,
+        layers: Optional[Union[str, List[str]]] = 'postconv',
         load_method: str = 'weights',
         pooling: Optional[Any] = None
     ) -> None:
         super().__init__(
             path,
             layers=layers,
             include_preds=True,
@@ -2530,14 +2443,20 @@
                                     "model.")
         obj._build(
             layers=layers, include_preds=True, pooling=pooling  # type: ignore
         )
         obj.wsi_normalizer = wsi_normalizer
         return obj
 
+    def __repr__(self):
+        return ("{}(\n".format(self.__class__.__name__) +
+                "    path={!r},\n".format(self.path) +
+                "    layers={!r},\n".format(self.layers) +
+                "    pooling={!r},\n".format(self._pooling) +
+                ")")
 
     @tf.function
     def _predict(self, inp):
         """Return activations (mean), predictions (mean), and uncertainty
         (stdev) for a single batch of images."""
 
         out_drop = [[] for _ in range(self.num_outputs)]
@@ -2560,14 +2479,23 @@
                 tf.math.reduce_mean(out_drop[n], axis=0)
                 for n in range(self.num_outputs-1)
             ]
             return out + [predictions, uncertainty]
         else:
             return predictions, uncertainty
 
+    def dump_config(self):
+        return {
+            'class': 'slideflow.model.tensorflow.UncertaintyInterface',
+            'kwargs': {
+                'path': self.path,
+                'layers': self.layers,
+                'pooling': self._pooling
+            }
+        }
 
 def load(
     path: str,
     method: str = 'weights',
     custom_objects: Optional[Dict[str, Any]] = None,
     training: bool = False
 ) -> tf.keras.models.Model:
```

## slideflow/model/torch.py

```diff
@@ -19,14 +19,15 @@
                     Union, Callable)
 
 import slideflow as sf
 import slideflow.util.neptune_utils
 from slideflow import errors
 from slideflow.model import base as _base
 from slideflow.model import torch_utils
+from slideflow.model.torch_utils import autocast
 from slideflow.model.base import log_manifest, no_scope, BaseFeatureExtractor
 from slideflow.util import log, NormFit, ImgBatchSpeedColumn
 
 if TYPE_CHECKING:
     import pandas as pd
     from slideflow.norm import StainNormalizer
 
@@ -424,14 +425,15 @@
         allow_tf32: bool = False,
         config: Dict[str, Any] = None,
         use_neptune: bool = False,
         neptune_api: Optional[str] = None,
         neptune_workspace: Optional[str] = None,
         load_method: str = 'weights',
         custom_objects: Optional[Dict[str, Any]] = None,
+        device: Optional[str] = None,
     ):
         """Sets base configuration, preparing model inputs and outputs.
 
         Args:
             hp (:class:`slideflow.ModelParams`): ModelParams object.
             outdir (str): Destination for event logs and checkpoints.
             labels (dict): Dict mapping slide names to outcome labels (int or
@@ -469,15 +471,15 @@
         self.outdir = outdir
         self.labels = labels
         self.patients = dict()  # type: Dict[str, str]
         self.name = name
         self.model = None  # type: Optional[torch.nn.Module]
         self.inference_model = None  # type: Optional[torch.nn.Module]
         self.mixed_precision = mixed_precision
-        self.device = torch.device('cuda:0')
+        self.device = torch_utils.get_device(device)
         self.mid_train_val_dts: Optional[Iterable] = None
         self.loss_fn: torch.nn.modules.loss._Loss
         self.use_tensorboard: bool
         self.writer: SummaryWriter
         self._reset_training_params()
 
         if custom_objects is not None:
@@ -1047,17 +1049,16 @@
 
         for _ in range(self.validation_steps):
             val_img, val_label, slides, *_ = next(self.mid_train_val_dts)  # type:ignore
             val_img = val_img.to(self.device)
             val_img = val_img.to(memory_format=torch.channels_last)
 
             with torch.no_grad():
-                _mp = self.mixed_precision
-                _ns = no_scope()
-                with torch.cuda.amp.autocast() if _mp else _ns:  # type: ignore
+                _mp = (self.mixed_precision and self.device.type in ('cuda', 'cpu'))
+                with autocast(self.device.type, mixed_precision=_mp):  # type: ignore
 
                     # GPU normalization, if specified.
                     if self._has_gpu_normalizer():
                         val_img = self.normalizer.preprocess(val_img)
 
                     if self.num_slide_features:
                         _slide_in = [self.slide_input[s] for s in slides]
@@ -1122,15 +1123,15 @@
                 self.optimizer,
                 gamma=self.hp.learning_rate_decay
             )
             log.debug("Using exponentially decaying learning rate")
         else:
             self.scheduler = None  # type: ignore
         self.loss_fn = self.hp.get_loss()
-        if self.mixed_precision:
+        if self.mixed_precision and self.device.type == 'cuda':
             self.scaler = torch.cuda.amp.GradScaler()
 
     def _prepare_neptune_run(self, dataset: "sf.Dataset", label: str) -> None:
         if self.use_neptune:
             tags = [label]
             if 'k-fold' in self.config['validation_strategy']:
                 tags += [f'k-fold{self.config["k_fold_i"]}']
@@ -1203,15 +1204,14 @@
             num_replicas=1,
             labels=(self.labels if incl_labels else None),
             chunk_size=8,
             pin_memory=True,
             num_workers=4 if not from_wsi else 0,
             onehot=False,
             incl_slidenames=True,
-            device=self.device,
             from_wsi=from_wsi,
             **kwargs
         )
         # Use GPU stain normalization for PyTorch normalizers, if supported
         if self._has_gpu_normalizer():
             log.info("Using GPU for stain normalization")
             interleave_args.standardize = False
@@ -1265,17 +1265,16 @@
         assert self.model is not None
         images, labels, slides = next(self.dataloaders['train'])
         images = images.to(self.device, non_blocking=True)
         images = images.to(memory_format=torch.channels_last)
         labels = self._labels_to_device(labels, self.device)
         self.optimizer.zero_grad()
         with torch.set_grad_enabled(True):
-            _mp = self.mixed_precision
-            _ns = no_scope()
-            with torch.cuda.amp.autocast() if _mp else _ns:  # type: ignore
+            _mp = (self.mixed_precision and self.device.type in ('cuda', 'cpu'))
+            with autocast(self.device.type, mixed_precision=_mp):  # type: ignore
 
                 # GPU normalization, if specified.
                 if self._has_gpu_normalizer():
                     images = self.normalizer.preprocess(
                         images,
                         augment=(isinstance(self.hp.augment, str)
                                  and 'n' in self.hp.augment)
@@ -1287,15 +1286,15 @@
                     inp = (images, Tensor(_slide_in).to(self.device))
                 else:
                     inp = (images,)  # type: ignore
                 outputs = self.model(*inp)
                 loss = self._calculate_loss(outputs, labels, self.loss_fn)
 
             # Update weights
-            if self.mixed_precision:
+            if self.mixed_precision and self.device.type == 'cuda':
                 self.scaler.scale(loss).backward()
                 self.scaler.step(self.optimizer)
                 self.scaler.update()
             else:
                 loss.backward()
                 self.optimizer.step()
 
@@ -1456,26 +1455,25 @@
 
         # Fit normalizer
         self._fit_normalizer(norm_fit)
 
         # Load and initialize model
         if not self.model:
             raise errors.ModelNotLoadedError
-        device = torch.device('cuda:0')
-        self.model.to(device)
+        self.model.to(self.device)
         self.model.eval()
         self._log_manifest(None, dataset, labels=None)
 
         if from_wsi and sf.slide_backend() == 'libvips':
             pool = mp.Pool(
-                os.cpu_count() if os.cpu_count() else 8,
+                sf.util.num_cpu(default=8),
                 initializer=sf.util.set_ignore_sigint
             )
         elif from_wsi:
-            pool = mp.dummy.Pool(os.cpu_count() if os.cpu_count() else 8)
+            pool = mp.dummy.Pool(sf.util.num_cpu(default=8))
         else:
             pool = None
         if not batch_size:
             batch_size = self.hp.batch_size
 
         self._setup_dataloaders(
             train_dts=None,
@@ -1562,19 +1560,19 @@
             self.hp.uq = uq
         if batch_size:
             self.validation_batch_size = batch_size
         if not self.model:
             raise errors.ModelNotLoadedError
         if from_wsi and sf.slide_backend() == 'libvips':
             pool = mp.Pool(
-                os.cpu_count() if os.cpu_count() else 8,
+                sf.util.num_cpu(default=8),
                 initializer=sf.util.set_ignore_sigint
             )
         elif from_wsi:
-            pool = mp.dummy.Pool(os.cpu_count() if os.cpu_count() else 8)
+            pool = mp.dummy.Pool(sf.util.num_cpu(default=8))
         else:
             pool = None
 
         self._detect_patients(dataset)
         self._verify_img_format(dataset)
         self._fit_normalizer(norm_fit)
         self.model.to(self.device)
@@ -1717,19 +1715,19 @@
         self.ema_observations = ema_observations
         self.ema_smoothing = ema_smoothing
         self.use_tensorboard = use_tensorboard
         self.log_frequency = log_frequency
 
         if from_wsi and sf.slide_backend() == 'libvips':
             pool = mp.Pool(
-                os.cpu_count() if os.cpu_count() else 8,
+                sf.util.num_cpu(default=8),
                 initializer=sf.util.set_ignore_sigint
             )
         elif from_wsi:
-            pool = mp.dummy.Pool(os.cpu_count() if os.cpu_count() else 8)
+            pool = mp.dummy.Pool(sf.util.num_cpu(default=8))
         else:
             pool = None
 
         # Validate early stopping parameters
         self._validate_early_stop()
 
         # Fit normalizer to dataset, if applicable
@@ -1974,18 +1972,26 @@
         super().__init__('torch', include_preds=include_preds)
         if layers and isinstance(layers, str):
             layers = [layers]
         self.layers = layers
         self.path = path
         self.apply_softmax = apply_softmax
         self.mixed_precision = mixed_precision
+        self._model = None
+        self._pooling = None
+        self._include_preds = None
+
+        # Transformation for standardizing uint8 images to float32
+        self.transform = torchvision.transforms.Lambda(lambda x: x / 127.5 - 1)
+
         # Hook for storing layer activations during model inference
         self.activation = {}  # type: Dict[Any, Tensor]
-        self.device = device if device is not None else torch.device('cuda')
-        self._model = None
+
+        # Configure device
+        self.device = torch_utils.get_device(device)
 
         if path is not None:
             config = sf.util.get_model_config(path)
             if 'img_format' in config:
                 self.img_format = config['img_format']
             self.hp = ModelParams()  # type: Optional[ModelParams]
             self.hp.load_dict(config['hp'])
@@ -2087,124 +2093,71 @@
 
         """
         if isinstance(inp, sf.slide.WSI):
             return self._predict_slide(inp, **kwargs)
         else:
             return self._predict(inp, **kwargs)
 
+    def __repr__(self):
+        return ("{}(\n".format(self.__class__.__name__) +
+                "    path={!r},\n".format(self.path) +
+                "    layers={!r},\n".format(self.layers) +
+                "    include_preds={!r},\n".format(self.include_preds) +
+                "    apply_softmax={!r},\n".format(self.apply_softmax) +
+                "    pooling={!r},\n".format(self._pooling) +
+                ")")
+
     def _predict_slide(
         self,
         slide: "sf.WSI",
         *,
         img_format: str = 'auto',
         batch_size: int = 32,
         dtype: type = np.float16,
         grid: Optional[np.ndarray] = None,
         shuffle: bool = False,
         show_progress: bool = True,
         callback: Optional[Callable] = None,
+        normalizer: Optional[Union[str, "StainNormalizer"]] = None,
+        normalizer_source: Optional[str] = None,
         **kwargs
     ) -> Optional[np.ndarray]:
         """Generate activations from slide => activation grid array."""
 
-        log.debug(f"Slide prediction (batch_size={batch_size}, "
-                  f"img_format={img_format})")
+        # Check image format
         if img_format == 'auto' and self.img_format is None:
             raise ValueError(
                 'Unable to auto-detect image format (png or jpg). Set the '
                 'format by passing img_format=... to the call function.'
             )
         elif img_format == 'auto':
             assert self.img_format is not None
             img_format = self.img_format
-        if img_format == 'png':  # PNG is lossless; this is equivalent but faster
-            log.debug("Using numpy image format instead of PNG")
-            img_format = 'numpy'
-        total_out = self.num_features + self.num_classes + self.num_uncertainty
-        if grid is None:
-            features_grid = np.ones((
-                    slide.grid.shape[1],
-                    slide.grid.shape[0],
-                    total_out),
-                dtype=dtype)
-            features_grid *= -99
-        else:
-            assert grid.shape == (slide.grid.shape[1], slide.grid.shape[0], total_out)
-            features_grid = grid
-        generator = slide.build_generator(
+
+        return sf.model.extractors.features_from_slide(
+            self,
+            slide,
+            img_format=img_format,
+            batch_size=batch_size,
+            dtype=dtype,
+            grid=grid,
             shuffle=shuffle,
             show_progress=show_progress,
-            img_format=img_format,
-            **kwargs)
-        if not generator:
-            log.error(f"No tiles extracted from slide [green]{slide.name}")
-            return None
-
-        class SlideIterator(torch.utils.data.IterableDataset):
-            def __init__(self, parent, *args, **kwargs):
-                super(SlideIterator).__init__(*args, **kwargs)
-                self.parent = parent
-
-            def __iter__(self):
-                for image_dict in generator():
-                    img = image_dict['image']
-                    if img_format not in ('numpy', 'png'):
-                        np_data = torch.from_numpy(
-                            np.fromstring(img, dtype=np.uint8))
-                        img = torchvision.io.decode_image(np_data)
-                    else:
-                        img = torch.from_numpy(img).permute(2, 0, 1)
-
-                    if self.parent.wsi_normalizer:
-                        img = img.permute(1, 2, 0)  # CWH => WHC
-                        img = torch.from_numpy(
-                            self.parent.wsi_normalizer.rgb_to_rgb(img.numpy())
-                        )
-                        img = img.permute(2, 0, 1)  # WHC => CWH
-                    loc = np.array(image_dict['grid'])
-                    img = img / 127.5 - 1
-                    yield img, loc
-
-        tile_dataset = torch.utils.data.DataLoader(
-            SlideIterator(self),
-            batch_size=batch_size)
-
-        for i, (batch_images, batch_loc) in enumerate(tile_dataset):
-            model_out = sf.util.as_list(self._predict(batch_images))
-
-            # Flatten the output, relevant when
-            # there are multiple outcomes / classifier heads
-            _act_batch = []
-            for m in model_out:
-                if isinstance(m, (list, tuple)):
-                    _act_batch += [_m.cpu().detach().numpy() for _m in m]
-                else:
-                    _act_batch.append(m.cpu().detach().numpy())
-            _act_batch = np.concatenate(_act_batch, axis=-1)
-
-            grid_idx_updated = []
-            for i, act in enumerate(_act_batch):
-                xi = batch_loc[i][0]
-                yi = batch_loc[i][1]
-                if callback:
-                    grid_idx_updated.append([yi, xi])
-                features_grid[yi][xi] = act
-
-            # Trigger a callback signifying that the grid has been updated.
-            # Useful for progress tracking.
-            if callback:
-                callback(grid_idx_updated)
-
-        return features_grid
+            callback=callback,
+            normalizer=(normalizer if normalizer else self.wsi_normalizer),
+            normalizer_source=normalizer_source,
+            preprocess_fn=self.transform,
+            **kwargs
+        )
 
     def _predict(self, inp: Tensor, no_grad: bool = True) -> List[Tensor]:
         """Return activations for a single batch of images."""
         assert torch.is_floating_point(inp), "Input tensor must be float"
-        _mp = self.mixed_precision
-        with torch.cuda.amp.autocast() if _mp else no_scope():  # type: ignore
+        _mp = (self.mixed_precision and self.device.type in ('cuda', 'cpu'))
+        with autocast(self.device.type, mixed_precision=_mp):  # type: ignore
             with torch.no_grad() if no_grad else no_scope():
                 inp = inp.to(self.device).to(memory_format=torch.channels_last)
                 logits = self._model(inp)
                 if isinstance(logits, (tuple, list)) and self.apply_softmax:
                     logits = [softmax(l, dim=1) for l in logits]
                 elif self.apply_softmax:
                     logits = softmax(logits, dim=1)
@@ -2270,14 +2223,16 @@
 
         Args:
             pooling (Callable or str, optional): PyTorch pooling function to use
                 on feature layers. May be a string ('avg' or 'max') or a
                 callable PyTorch function.
         """
 
+        self._pooling = pooling
+
         if isinstance(pooling, str):
             if pooling == 'avg':
                 pooling = lambda x: torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))
             elif pooling == 'max':
                 pooling = lambda x: torch.nn.functional.adaptive_max_pool2d(x, (1, 1))
             else:
                 raise ValueError(f"Unrecognized pooling value {pooling}. "
@@ -2327,14 +2282,26 @@
             self.num_outputs = 0
         self.num_features = sum([f.shape[1] for f in self.activation.values()])
 
         if self.include_preds:
             log.debug(f'Number of classes: {self.num_classes}')
         log.debug(f'Number of activation features: {self.num_features}')
 
+    def dump_config(self):
+        return {
+            'class': 'slideflow.model.torch.Features',
+            'kwargs': {
+                'path': self.path,
+                'layers': self.layers,
+                'include_preds': self.include_preds,
+                'apply_softmax': self.apply_softmax,
+                'pooling': self._pooling
+            }
+        }
+
 
 class UncertaintyInterface(Features):
 
     def __init__(
         self,
         path: Optional[str],
         layers: Optional[Union[str, List[str]]] = 'postconv',
@@ -2369,26 +2336,34 @@
         if 'include_preds' in kwargs and not kwargs['include_preds']:
             raise ValueError("UncertaintyInterface requires include_preds=True")
         kwargs['include_preds'] = None
         obj = super().from_model(*args, **kwargs)
         torch_utils.enable_dropout(obj._model)
         return obj
 
+    def __repr__(self):
+        return ("{}(\n".format(self.__class__.__name__) +
+                "    path={!r},\n".format(self.path) +
+                "    layers={!r},\n".format(self.layers) +
+                "    apply_softmax={!r},\n".format(self.apply_softmax) +
+                "    pooling={!r},\n".format(self._pooling) +
+                ")")
+
     def _predict(self, inp: Tensor, no_grad: bool = True) -> List[Tensor]:
         """Return activations (mean), predictions (mean), and uncertainty
         (stdev) for a single batch of images."""
 
         assert torch.is_floating_point(inp), "Input tensor must be float"
-        _mp = self.mixed_precision
+        _mp = (self.mixed_precision and self.device.type in ('cuda', 'cpu'))
 
         out_pred_drop = [[] for _ in range(self.num_outputs)]
         if self.layers:
             out_act_drop = [[] for _ in range(len(self.layers))]
         for _ in range(30):
-            with torch.cuda.amp.autocast() if _mp else no_scope():  # type: ignore
+            with autocast(self.device.type, mixed_precision=_mp):  # type: ignore
                 with torch.no_grad() if no_grad else no_scope():
                     inp = inp.to(self.device)
                     inp = inp.to(memory_format=torch.channels_last)
                     logits = self._model(inp)
                     if isinstance(logits, (tuple, list)) and self.apply_softmax:
                         logits = [softmax(l, dim=1) for l in logits]
                     elif self.apply_softmax:
@@ -2427,14 +2402,25 @@
                 torch.mean(out_act_drop[n], dim=0)
                 for n in range(len(self.layers))
             ]
             return reduced_activations + [predictions, uncertainty]
         else:
             return predictions, uncertainty
 
+    def dump_config(self):
+        return {
+            'class': 'slideflow.model.torch.UncertaintyInterface',
+            'kwargs': {
+                'path': self.path,
+                'layers': self.layers,
+                'apply_softmax': self.apply_softmax,
+                'pooling': self._pooling
+            }
+        }
+
 # -----------------------------------------------------------------------------
 
 def load(path: str) -> torch.nn.Module:
     """Load a model trained with Slideflow.
 
     Args:
         path (str): Path to saved model. Must be a model trained in Slideflow.
```

## slideflow/model/torch_utils.py

```diff
@@ -3,19 +3,22 @@
 import types
 from types import SimpleNamespace
 from typing import Dict, Generator, Iterable, List, Tuple, Union, Optional
 
 import torch
 import numpy as np
 import slideflow as sf
+import contextlib
+from packaging import version
 from pandas.core.frame import DataFrame
 from scipy.special import softmax
 from slideflow.stats import df_from_pred
 from slideflow.errors import DatasetError
 from slideflow.util import log, ImgBatchSpeedColumn
+from slideflow.model.base import no_scope
 from rich.progress import Progress, TimeElapsedColumn, SpinnerColumn
 from functools import reduce
 
 # -----------------------------------------------------------------------------
 
 def cycle(iterable: Iterable) -> Generator:
     while True:
@@ -29,14 +32,33 @@
 
     Works even when there is a Sequential in the module.
     """
     names = access_string.split(sep='.')
     return reduce(getattr, names, module)
 
 
+@contextlib.contextmanager
+def autocast(device_type: Optional[str] = None, mixed_precision: bool = True):
+    """Autocast with mixed precision."""
+    if not mixed_precision:
+        with no_scope():
+            yield
+    elif version.parse(torch.__version__) >= version.parse("1.12"):
+        with torch.amp.autocast(device_type):
+            yield
+    elif device_type == 'cuda':
+        with torch.cuda.amp.autocast():
+            yield
+    elif device_type == 'cpu':
+        with torch.cpu.amp.autocast():
+            yield
+    else:
+        raise ValueError("Unrecognized device type: {}".format(device_type))
+
+
 def print_module_summary(
     module: torch.nn.Module,
     inputs: List[torch.Tensor],
     max_nesting: int = 3,
     skip_redundant: bool = True
 ) -> str:
     """Prints and returns summary of a torch module.
@@ -194,14 +216,15 @@
     torch_args: Optional[SimpleNamespace],
     uq: bool = False,
     uq_n: int = 30,
     steps: Optional[int] = None,
     pb_label: str = "Evaluating...",
     verbosity: str = 'full',
     predict_only: bool = False,
+    device: Optional[str] = None,
 ) -> Tuple[DataFrame, float, float]:
     """Evaluates a model from a dataset of (y_true, y_pred, tile_to_slide),
     returning predictions, accuracy, and loss.
 
     Args:
         model (str): Path to PyTorch model.
         dataset (tf.data.Dataset): PyTorch dataloader.
@@ -233,15 +256,16 @@
     if not predict_only and torch_args is None:
         raise ValueError("Argument `torch_args` must be supplied if evaluating.")
 
     y_true, y_pred, tile_to_slides, locations, y_std = [], [], [], [], []
     losses, total, num_outcomes, batch_size = 0, 0, 0, 0
     corrects, acc, loss = None, None, None
     model.eval()
-    device = torch.device('cuda:0')
+    device = get_device(device)
+    _mp = (device.type in ('cuda', 'cpu'))
 
     if verbosity != 'silent':
         pb = Progress(SpinnerColumn(), transient=True)
         pb.add_task(pb_label, total=None)
         pb.start()
     else:
         pb = None
@@ -280,15 +304,15 @@
                 img, yt, slide = batch
 
             if verbosity != 'silent':
                 pb.advance(task, img.shape[0])
 
             img = img.to(device, non_blocking=True)
             img = img.to(memory_format=torch.channels_last)
-            with torch.cuda.amp.autocast():
+            with autocast(device.type, mixed_precision=_mp):
                 with torch.no_grad():
                     # GPU normalization
                     if torch_args is not None and torch_args.normalizer:
                         img = torch_args.normalizer.preprocess(img)
 
                     # Slide-level features
                     if torch_args is not None and torch_args.num_slide_features:
@@ -300,30 +324,30 @@
                         inp = (img,)  # type: ignore
 
                     if uq:
                         res, yp_std, num_outcomes = get_uq_predictions(
                             inp, model, num_outcomes, uq_n
                         )
                         if isinstance(yp_std, list):
-                            yp_std = [y.cpu().numpy().copy() for y in yp_std]
+                            yp_std = [y.float().cpu().numpy().copy() for y in yp_std]
                         else:
-                            yp_std = yp_std.cpu().numpy().copy()
+                            yp_std = yp_std.float().cpu().numpy().copy()
                         y_std += [yp_std]  # type: ignore
                     else:
                         res = model(*inp)
 
                     if not predict_only:
                         assert torch_args is not None
                         corrects = torch_args.update_corrects(res, yt, corrects)
                         losses = torch_args.update_loss(res, yt, losses, img.size(0))
 
                     if isinstance(res, list):
-                        res = [r.cpu().numpy().copy() for r in res]
+                        res = [r.float().cpu().numpy().copy() for r in res]
                     else:
-                        res = res.cpu().numpy().copy()
+                        res = res.float().cpu().numpy().copy()
 
                     y_pred += [res]
 
             if not predict_only and type(yt) == dict:
                 y_true += [[yt[f'out-{o}'] for o in range(len(yt))]]
             elif not predict_only:
                 yt = yt.detach().numpy().copy()
@@ -436,14 +460,29 @@
         torch_args=torch_args,
         pb_label=pb_label,
         predict_only=True,
         **kwargs
     )
     return df
 
+
+def get_device(device: Optional[str] = None):
+    if device is None and torch.cuda.is_available():
+        return torch.device('cuda')
+    elif (device is None
+          and hasattr(torch.backends, 'mps')
+          and torch.backends.mps.is_available()):
+        return torch.device('mps')
+    elif device is None:
+        return torch.device('cpu')
+    elif isinstance(device, str):
+        return torch.device(device)
+    else:
+        return device
+
 # -----------------------------------------------------------------------------
 
 def xception(*args, **kwargs):
     import pretrainedmodels
     return pretrainedmodels.xception(*args, **kwargs)
```

## slideflow/model/extractors/__init__.py

```diff
@@ -1,7 +1,10 @@
 """Module for building pretrained feature extractors."""
 
 from ._registry import (list_extractors, list_torch_extractors,
                         list_tensorflow_extractors, is_extractor,
                         is_torch_extractor, is_tensorflow_extractor)
 from ._factory import (build_feature_extractor, build_torch_feature_extractor,
-                       build_tensorflow_feature_extractor)
+                       build_tensorflow_feature_extractor, rebuild_extractor)
+from ._factory_tensorflow import TensorflowImagenetLayerExtractor
+from ._factory_torch import TorchImagenetLayerExtractor
+from ._slide import features_from_slide
```

## slideflow/model/extractors/_factory.py

```diff
@@ -1,26 +1,45 @@
 """Factory for building feature extractors."""
 
+import importlib
 import slideflow as sf
+from os.path import join, exists
+from typing import Optional, Tuple, TYPE_CHECKING
 from slideflow import errors
+from slideflow.model import BaseFeatureExtractor
 
 from ._registry import (is_tensorflow_extractor, is_torch_extractor,
                         _tf_extractors, _torch_extractors)
 from ._factory_tensorflow import build_tensorflow_feature_extractor
 from ._factory_torch import build_torch_feature_extractor
 
+if TYPE_CHECKING:
+    from slideflow.norm import StainNormalizer
 
-def build_feature_extractor(name, **kwargs):
+# -----------------------------------------------------------------------------
+
+def build_feature_extractor(
+    name: str,
+    backend: Optional[str] = None,
+    **kwargs
+) -> BaseFeatureExtractor:
     """Build a feature extractor.
 
     The returned feature extractor is a callable object, which returns
-    features (often layer activations) for a batch of images.
-    Images are expected to be in (B, W, H, C) format and non-standardized
-    (scaled 0-255) with dtype uint8. The feature extractors perform
-    all needed preprocessing on the fly.
+    features (often layer activations) for either a batch of images or a
+    :class:`slideflow.WSI` object.
+
+    If generating features for a batch of images, images are expected to be in
+    (B, W, H, C) format and non-standardized (scaled 0-255) with dtype uint8.
+    The feature extractors perform all needed preprocessing on the fly.
+
+    If generating features for a slide, the slide is expected to be a
+    :class:`slideflow.WSI` object. The feature extractor will generate features
+    for each tile in the slide, returning a numpy array of shape (W, H, F),
+    where F is the number of features.
 
     Args:
         name (str): Name of the feature extractor to build. Available
             feature extractors are listed with
             :func:`slideflow.model.list_extractors()`.
 
     Keyword arguments:
@@ -82,22 +101,174 @@
 
                 # Calculate features for the entire dataset
                 features = sf.DatasetFeatures(
                     resnet,
                     dataset=dataset
                 )
 
+        Generate a map of features across a slide.
+
+            .. code-block:: python
+
+                import slideflow as sf
+                from slideflow.model import build_feature_extractor
+
+                # Load a slide
+                wsi = sf.WSI(...)
+
+                # Create a feature extractor
+                retccl = build_feature_extractor(
+                    'retccl',
+                    tile_px=299
+                )
+
+                # Create a feature map, a 2D array of shape
+                # (W, H, F), where F is the number of features.
+                features = retccl(wsi)
+
     """
+    # Build feature extractor according to manually specified backend
+    if backend is not None and backend not in ('tensorflow', 'torch'):
+        raise ValueError(f"Invalid backend: {backend}")
+
+    # Build a feature extractor from a finetuned model
+    if sf.util.is_tensorflow_model_path(name):
+        model_config = sf.util.get_model_config(name)
+        if model_config['hp']['uq']:
+            from slideflow.model.tensorflow import UncertaintyInterface
+            return UncertaintyInterface(name, **kwargs)
+        else:
+            from slideflow.model.tensorflow import Features
+            return Features(name, **kwargs)
+    elif sf.util.is_torch_model_path(name):
+        model_config = sf.util.get_model_config(name)
+        if model_config['hp']['uq']:
+            from slideflow.model.torch import UncertaintyInterface
+            return UncertaintyInterface(name, **kwargs)
+        else:
+            from slideflow.model.torch import Features  # noqa: F401
+            return Features(name, **kwargs)
+
+    # Build feature extractor with a specific backend
+    if backend == 'tensorflow':
+        if not is_tensorflow_extractor(name):
+            raise errors.InvalidFeatureExtractor(
+                f"Feature extractor {name} not available in Tensorflow backend")
+        return build_tensorflow_feature_extractor(name, **kwargs)
+    elif backend == 'torch':
+        if not is_torch_extractor(name):
+            raise errors.InvalidFeatureExtractor(
+                f"Feature extractor {name} not available in PyTorch backend")
+        return build_torch_feature_extractor(name, **kwargs)
+
+    # Auto-build feature extractor according to available backends
     if is_tensorflow_extractor(name) and is_torch_extractor(name):
         sf.log.info(
             f"Feature extractor {name} available in both Tensorflow and "
             f"PyTorch backends; using active backend {sf.backend()}")
         if sf.backend() == 'tensorflow':
             return build_tensorflow_feature_extractor(name, **kwargs)
         else:
             return build_torch_feature_extractor(name, **kwargs)
     if is_tensorflow_extractor(name):
         return build_tensorflow_feature_extractor(name, **kwargs)
     elif is_torch_extractor(name):
         return build_torch_feature_extractor(name, **kwargs)
     else:
         raise errors.InvalidFeatureExtractor(f"Unrecognized feature extractor: {name}")
+
+
+def rebuild_extractor(
+    bags_or_model: str,
+    allow_errors: bool = False
+) -> Tuple[Optional["BaseFeatureExtractor"], Optional["StainNormalizer"]]:
+    """Recreate the extractor used to generate features stored in bags.
+
+    Args:
+        bags_or_model (str): Either a path to directory containing feature bags,
+            or a path to a trained MIL model. If a path to a trained MIL model,
+            the extractor used to generate features will be recreated.
+        allow_errors (bool): If True, return None if the extractor
+            cannot be rebuilt. If False, raise an error. Defaults to False.
+
+    Returns:
+        Optional[BaseFeatureExtractor]: Extractor function, or None if ``allow_errors`` is
+            True and the extractor cannot be rebuilt.
+
+        Optional[StainNormalizer]: Stain normalizer used when generating
+            feature bags, or None if no stain normalization was used.
+
+    """
+    # Load bags configuration
+    is_bag_config = bags_or_model.endswith('bags_config.json')
+    is_bag_dir = exists(join(bags_or_model, 'bags_config.json'))
+    is_model_dir = exists(join(bags_or_model, 'mil_params.json'))
+    if not (is_bag_dir or is_model_dir or is_bag_config):
+        if allow_errors:
+            return None, None
+        else:
+            raise ValueError(
+                'Could not find bags or MIL model configuration at '
+                f'{bags_or_model}.'
+            )
+    if is_bag_config:
+        bags_config = sf.util.load_json(bags_or_model)
+    elif is_model_dir:
+        mil_config = sf.util.load_json(join(bags_or_model, 'mil_params.json'))
+        if 'bags_extractor' not in mil_config:
+            if allow_errors:
+                return None, None
+            else:
+                raise ValueError(
+                    'Could not rebuild extractor from configuration at '
+                    f'{bags_or_model}; missing "bags_extractor" key in '
+                    'mil_params.json.'
+                )
+        bags_config = mil_config['bags_extractor']
+    else:
+        bags_config = sf.util.load_json(join(bags_or_model, 'bags_config.json'))
+    if ('extractor' not in bags_config
+       or any(n not in bags_config['extractor'] for n in ['class', 'kwargs'])):
+        if allow_errors:
+            return None, None
+        else:
+            raise ValueError(
+                'Could not rebuild extractor from configuration at '
+                f'{bags_or_model}; missing "extractor" class or kwargs.'
+            )
+
+    # Rebuild extractor
+    extractor_name = bags_config['extractor']['class'].split('.')
+    extractor_class = extractor_name[-1]
+    extractor_kwargs = bags_config['extractor']['kwargs']
+    module = importlib.import_module('.'.join(extractor_name[:-1]))
+    try:
+        extractor = getattr(module, extractor_class)(**extractor_kwargs)
+    except Exception:
+        if allow_errors:
+            return None
+        else:
+            raise ValueError(
+                f'Could not rebuild extractor from configuration at {bags_or_model}.'
+            )
+
+    # Rebuild stain normalizer
+    if bags_config['normalizer'] is not None:
+        normalizer = sf.norm.autoselect(
+            bags_config['normalizer']['method'],
+            backend=extractor.backend
+        )
+        normalizer.set_fit(**bags_config['normalizer']['fit'])
+    else:
+        normalizer = None
+    if (hasattr(extractor, 'normalizer')
+       and extractor.normalizer is not None
+       and normalizer is not None):
+        sf.log.warning(
+            'Extractor already has a stain normalizer. Overwriting with '
+            'normalizer from bags configuration.'
+        )
+        extractor.normalizer = normalizer
+    elif hasattr(extractor, 'normalizer') and extractor.normalizer is not None:
+        normalizer = extractor.normalizer
+
+    return extractor, normalizer
```

## slideflow/model/extractors/_factory_tensorflow.py

```diff
@@ -1,12 +1,15 @@
 """Factory for building Tensorflow feature extractors."""
 
 import importlib
+import numpy as np
+import slideflow as sf
 from slideflow import errors
 
+from ._slide import features_from_slide
 from ._registry import _tf_extractors, is_tensorflow_extractor, register_tf
 from ..base import BaseFeatureExtractor
 
 
 def build_tensorflow_feature_extractor(name, **kwargs):
     if is_tensorflow_extractor(name):
         if name in _tf_extractors:
@@ -15,80 +18,85 @@
             return _tf_extractors[name+'_imagenet'](**kwargs)
     else:
         raise errors.InvalidFeatureExtractor(f"Unrecognized feature extractor: {name}")
 
 # -----------------------------------------------------------------------------
 
 @register_tf
+def simclr(ckpt, **kwargs):
+    from .simclr import SimCLR_Features
+    return SimCLR_Features(ckpt, **kwargs)
+
+@register_tf
 def xception_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('xception', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('xception', tile_px, **kwargs)
 
 @register_tf
 def vgg16_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('vgg16', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('vgg16', tile_px, **kwargs)
 
 @register_tf
 def vgg19_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('vgg19', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('vgg19', tile_px, **kwargs)
 
 @register_tf
 def resnet50_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet50', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet50', tile_px, **kwargs)
 
 @register_tf
 def resnet101_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet101', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet101', tile_px, **kwargs)
 
 @register_tf
 def resnet101_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet101', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet101', tile_px, **kwargs)
 
 @register_tf
 def resnet152_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet152', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet152', tile_px, **kwargs)
 
 @register_tf
 def resnet152_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet152', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet152', tile_px, **kwargs)
 
 @register_tf
 def resnet50_v2_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet50_v2', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet50_v2', tile_px, **kwargs)
 
 @register_tf
 def resnet101_v2_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet101_v2', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet101_v2', tile_px, **kwargs)
 
 @register_tf
 def resnet152_v2_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('resnet152_v2', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('resnet152_v2', tile_px, **kwargs)
 
 @register_tf
 def inception_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('inception', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('inception', tile_px, **kwargs)
 
 @register_tf
 def nasnet_large_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('nasnet_large', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('nasnet_large', tile_px, **kwargs)
 
 @register_tf
 def inception_resnet_v2_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('inception_resnet_v2', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('inception_resnet_v2', tile_px, **kwargs)
 
 @register_tf
 def mobilenet_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('mobilenet', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('mobilenet', tile_px, **kwargs)
 
 @register_tf
 def mobilenet_v2_imagenet(tile_px, **kwargs):
-    return _TensorflowImagenetLayerExtractor('mobilenet_v2', tile_px, **kwargs)
+    return TensorflowImagenetLayerExtractor('mobilenet_v2', tile_px, **kwargs)
 
 # -----------------------------------------------------------------------------
 
-class _TensorflowImagenetLayerExtractor(BaseFeatureExtractor):
+class TensorflowImagenetLayerExtractor(BaseFeatureExtractor):
     """Feature extractor that calculates layer activations for
     imagenet-pretrained Tensorflow models."""
 
     def __init__(self, model_name, tile_px, **kwargs):
         super().__init__(backend='tensorflow')
 
         from ..tensorflow import ModelParams, Features
@@ -98,33 +106,69 @@
         model = _hp.build_model(num_classes=1, pretrain='imagenet')
         submodule = importlib.import_module(f'tensorflow.keras.applications.{model_name}')
         self.model_name = model_name
         self.ftrs = Features.from_model(model, **kwargs)
         self.tag = model_name + "_" + '-'.join(self.ftrs.layers)
         self.num_features = self.ftrs.num_features
         self.num_classes = 0
+        self._tile_px = tile_px
 
         @tf.function
         def _transform(x):
             x = tf.cast(x, tf.float32)
             return submodule.preprocess_input(x)
 
         self.transform = _transform
         self.preprocess_kwargs = dict(standardize=False)
 
+    @property
+    def tile_px(self):
+        return self._tile_px
+
     def __repr__(self):
         return str(self)
 
     def __str__(self):
         return "<TensorflowImagenetLayerExtractor model={} layers={} n_features={}>".format(
             self.model_name,
             self.ftrs.layers,
             self.num_features,
         )
 
-    def __call__(self, batch_images):
+    def _predict(self, batch_images):
+        """Generate features for a batch of images."""
         import tensorflow as tf
-        assert batch_images.dtype == tf.uint8
-        batch_images = tf.cast(batch_images, tf.float32)
-        batch_images = self.transform(batch_images)
+        if batch_images.dtype == tf.uint8:
+            batch_images = tf.cast(batch_images, tf.float32)
+            batch_images = self.transform(batch_images)
         return self.ftrs._predict(batch_images)
 
+    def __call__(self, obj, **kwargs):
+        """Generate features for a batch of images or a WSI."""
+        if isinstance(obj, sf.WSI):
+            grid = features_from_slide(self, obj, preprocess_fn=self.transform, **kwargs)
+            return np.ma.masked_where(grid == -99, grid)
+        elif kwargs:
+            raise ValueError(
+                f"{self.__class__.__name__} does not accept keyword arguments "
+                "when extracting features from a batch of images."
+            )
+        else:
+            return self._predict(obj)
+
+    def dump_config(self):
+        """Return a dictionary of configuration parameters.
+
+        These configuration parameters can be used to reconstruct the
+        feature extractor, using ``slideflow.model.build_feature_extractor()``.
+
+        """
+        return {
+            'class': 'slideflow.model.extractors.TensorflowImagenetLayerExtractor',
+            'kwargs': {
+                'model_name': self.model_name,
+                'tile_px': self._tile_px,
+                'layers': self.ftrs.layers,
+                'pooling': self.ftrs._pooling,
+            }
+        }
+
```

## slideflow/model/extractors/_factory_torch.py

```diff
@@ -1,11 +1,14 @@
 """Factory for building PyTorch feature extractors."""
 
+import numpy as np
+import slideflow as sf
 from slideflow import errors
 
+from ._slide import features_from_slide
 from ._registry import _torch_extractors, is_torch_extractor, register_torch
 from ..base import BaseFeatureExtractor
 
 
 def build_torch_feature_extractor(name, **kwargs):
     if is_torch_extractor(name):
         if name in _torch_extractors:
@@ -25,117 +28,119 @@
 @register_torch
 def retccl(tile_px, **kwargs):
     from .retccl import RetCCLFeatures
     return RetCCLFeatures(center_crop=(tile_px != 256), **kwargs)
 
 @register_torch
 def resnet18_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('resnet18', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('resnet18', tile_px, **kwargs)
 
 @register_torch
 def resnet50_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('resnet50', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('resnet50', tile_px, **kwargs)
 
 @register_torch
 def alexnet_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('alexnet', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('alexnet', tile_px, **kwargs)
 
 @register_torch
 def squeezenet_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('squeezenet', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('squeezenet', tile_px, **kwargs)
 
 @register_torch
 def densenet_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('densenet', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('densenet', tile_px, **kwargs)
 
 @register_torch
 def inception_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('inception', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('inception', tile_px, **kwargs)
 
 @register_torch
 def googlenet_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('googlenet', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('googlenet', tile_px, **kwargs)
 
 @register_torch
 def shufflenet_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('shufflenet', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('shufflenet', tile_px, **kwargs)
 
 @register_torch
 def resnext50_32x4d_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('resnext50_32x4d', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('resnext50_32x4d', tile_px, **kwargs)
 
 @register_torch
 def vgg16_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('vgg16', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('vgg16', tile_px, **kwargs)
 
 @register_torch
 def mobilenet_v2_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('mobilenet_v2', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('mobilenet_v2', tile_px, **kwargs)
 
 @register_torch
 def mobilenet_v3_small_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('mobilenet_v3_small', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('mobilenet_v3_small', tile_px, **kwargs)
 
 @register_torch
 def mobilenet_v3_large_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('mobilenet_v3_large', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('mobilenet_v3_large', tile_px, **kwargs)
 
 @register_torch
 def wide_resnet50_2_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('wide_resnet50_2', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('wide_resnet50_2', tile_px, **kwargs)
 
 @register_torch
 def mnasnet_imagenet(tile_px, **kwargs):
-    return _TorchImagenetLayerExtractor('mnasnet', tile_px, **kwargs)
+    return TorchImagenetLayerExtractor('mnasnet', tile_px, **kwargs)
 
 @register_torch
 def xception_imagenet(tile_px, **kwargs):
     from torchvision import transforms
-    extractor = _TorchImagenetLayerExtractor('xception', tile_px, **kwargs)
+    extractor = TorchImagenetLayerExtractor('xception', tile_px, **kwargs)
     extractor.transform = transforms.Compose([
         transforms.Lambda(lambda x: x / 255.),
         transforms.Normalize(
             mean=[0.5, 0.5, 0.5],
             std=[0.5, 0.5, 0.5])
     ])
     return extractor
 
 @register_torch
 def nasnet_large_imagenet(tile_px, **kwargs):
     from torchvision import transforms
-    extractor = _TorchImagenetLayerExtractor('nasnet_large', tile_px, **kwargs)
+    extractor = TorchImagenetLayerExtractor('nasnet_large', tile_px, **kwargs)
     extractor.transform = transforms.Compose([
         transforms.Lambda(lambda x: x / 255.),
         transforms.Normalize(
             mean=[0.5, 0.5, 0.5],
             std=[0.5, 0.5, 0.5])
     ])
     return extractor
 
 # -----------------------------------------------------------------------------
 
-class _TorchImagenetLayerExtractor(BaseFeatureExtractor):
+class TorchImagenetLayerExtractor(BaseFeatureExtractor):
     """Feature extractor that calculates layer activations for
     imagenet-pretrained PyTorch models."""
 
     def __init__(self, model_name, tile_px, device=None, **kwargs):
         super().__init__(backend='torch')
 
-        import torch
         from ..torch import ModelParams, Features
+        from .. import torch_utils
         from torchvision import transforms
 
-        self.device = device if device is not None else torch.device('cuda')
+
+        self.device = torch_utils.get_device(device)
         _hp = ModelParams(tile_px=tile_px, model=model_name, include_top=False, hidden_layers=0)
         model = _hp.build_model(num_classes=1, pretrain='imagenet').to(self.device)
         self.model_name = model_name
         self.ftrs = Features.from_model(model, tile_px=tile_px, **kwargs)
         self.tag = model_name + "_" + '-'.join(self.ftrs.layers)
         self.num_features = self.ftrs.num_features
         self.num_classes = 0
+        self._tile_px = tile_px
 
         # Normalization for Imagenet pretrained models
         # as described here: https://pytorch.org/vision/0.11/models.html
         all_transforms = [
             transforms.Lambda(lambda x: x / 255.),
             transforms.Normalize(
                 mean=(0.485, 0.456, 0.406),
@@ -150,12 +155,39 @@
     def __str__(self):
         return "<TorchImagenetLayerExtractor model={} layers={} n_features={}>".format(
             self.model_name,
             self.ftrs.layers,
             self.num_features,
         )
 
-    def __call__(self, batch_images):
-        import torch
-        assert batch_images.dtype == torch.uint8
-        batch_images = self.transform(batch_images).to(self.device)
-        return self.ftrs._predict(batch_images)
+    def __call__(self, obj, **kwargs):
+        """Generate features for a batch of images or a WSI."""
+        if isinstance(obj, sf.WSI):
+            grid = features_from_slide(self, obj, **kwargs)
+            return np.ma.masked_where(grid == -99, grid)
+        elif kwargs:
+            raise ValueError(
+                f"{self.__class__.__name__} does not accept keyword arguments "
+                "when extracting features from a batch of images."
+            )
+        else:
+            import torch
+            assert obj.dtype == torch.uint8
+            obj = self.transform(obj).to(self.device)
+            return self.ftrs._predict(obj)
+
+    def dump_config(self):
+        """Return a dictionary of configuration parameters.
+
+        These configuration parameters can be used to reconstruct the
+        feature extractor, using ``slideflow.model.build_feature_extractor()``.
+
+        """
+        return {
+            'class': 'slideflow.model.extractors.TorchImagenetLayerExtractor',
+            'kwargs': {
+                'model_name': self.model_name,
+                'tile_px': self._tile_px,
+                'layers': self.ftrs.layers,
+                'pooling': self.ftrs._pooling,
+            }
+        }
```

## slideflow/model/extractors/_slide.py

```diff
@@ -11,21 +11,24 @@
     *,
     img_format: str = 'auto',
     batch_size: int = 32,
     dtype: type = np.float16,
     grid: Optional[np.ndarray] = None,
     shuffle: bool = False,
     show_progress: bool = True,
-    device: str = 'cuda',
+    device: Optional[str] = None,
     **kwargs
 ) -> Optional[np.ndarray]:
         """Generate features from tiles in a slide into an array."""
 
         import torch
 
+        from slideflow.model import torch_utils
+        device = torch_utils.get_device(device)
+
         log.debug(f"Slide prediction (batch_size={batch_size})")
         total_out = extractor.num_features + extractor.num_classes
         if grid is None:
             features_grid = np.ones((
                     slide.grid.shape[1],
                     slide.grid.shape[0],
                     total_out),
```

## slideflow/model/extractors/ctranspath.py

```diff
@@ -8,28 +8,29 @@
 """
 
 import torch
 import torch.nn as nn
 import math
 import torch.utils.checkpoint as checkpoint
 import slideflow as sf
+import numpy as np
 from typing import Optional
 from torchvision import transforms
 from huggingface_hub import hf_hub_download
 
 from timm.models.layers.helpers import to_2tuple
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.models.helpers import build_model_with_cfg
 from timm.models.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, lecun_normal_
 from timm.models.layers import _assert
 from timm.models.swin_transformer import window_partition, window_reverse
 from timm.models.vision_transformer import checkpoint_filter_fn
 
 from ..base import BaseFeatureExtractor
-from ._slide import features_from_slide_torch
+from ._slide import features_from_slide
 
 # -----------------------------------------------------------------------------
 
 def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):
     """ ViT weight initialization
     * When called without n, head_bias, jax_impl args it will behave exactly the same
       as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).
@@ -572,50 +573,64 @@
     https://doi.org/10.1016/j.media.2022.102559
 
     GitHub: https://github.com/Xiyue-Wang/TransPath
     """
 
     tag = 'ctranspath'
 
-    def __init__(self, device='cuda', center_crop=False):
+    def __init__(self, device=None, center_crop=False):
         super().__init__(backend='torch')
 
-        self.device = device
+        from slideflow.model import torch_utils
+        self.device = torch_utils.get_device(device)
         self.model = _build_ctranspath_model()
-        self.model.head = torch.nn.Identity().to(device)
+        self.model.head = torch.nn.Identity().to(self.device)
 
         checkpoint_path = hf_hub_download(
             repo_id='jamesdolezal/CTransPath',
             filename='ctranspath.pth'
         )
-        td = torch.load(checkpoint_path)
+        td = torch.load(checkpoint_path, map_location=self.device)
         self.model.load_state_dict(td['model'], strict=True)
-        self.model = self.model.to(device)
+        self.model = self.model.to(self.device)
         self.model.eval()
 
         # ---------------------------------------------------------------------
         self.num_features = 768
         self.num_classes = 0
         all_transforms = [transforms.CenterCrop(224)] if center_crop else []
         all_transforms += [
             transforms.Lambda(lambda x: x / 255.),
             transforms.Normalize(
                 mean=(0.485, 0.456, 0.406),
                 std=(0.229, 0.224, 0.225))
         ]
         self.transform = transforms.Compose(all_transforms)
         self.preprocess_kwargs = dict(standardize=False)
+        self._center_crop = center_crop
         # ---------------------------------------------------------------------
 
     def __call__(self, obj, **kwargs):
         """Generate features for a batch of images or a WSI."""
         if isinstance(obj, sf.WSI):
-            return features_from_slide_torch(
-                self,
-                obj,
-                device=self.device,
-                **kwargs
+            grid = features_from_slide(self, obj, **kwargs)
+            return np.ma.masked_where(grid == -99, grid)
+        elif kwargs:
+            raise ValueError(
+                f"{self.__class__.__name__} does not accept keyword arguments "
+                "when extracting features from a batch of images."
             )
         assert obj.dtype == torch.uint8
         obj = self.transform(obj)
         return self.model(obj)
 
+    def dump_config(self):
+        """Return a dictionary of configuration parameters.
+
+        These configuration parameters can be used to reconstruct the
+        feature extractor, using ``slideflow.model.build_feature_extractor()``.
+
+        """
+        return {
+            'class': 'slideflow.model.extractors.ctranspath.CTransPathFeatures',
+            'kwargs': {'center_crop': self._center_crop}
+        }
```

## slideflow/model/extractors/retccl.py

```diff
@@ -1,19 +1,20 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.nn as nn
 import slideflow as sf
+import numpy as np
 
 from torch.nn import Parameter
 from torchvision import transforms
 from huggingface_hub import hf_hub_download
 
 from ..base import BaseFeatureExtractor
-from ._slide import features_from_slide_torch
+from ._slide import features_from_slide
 
 # -----------------------------------------------------------------------------
 
 def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
     """3x3 convolution with padding"""
     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                      padding=dilation, groups=groups, bias=False, dilation=dilation)
@@ -272,56 +273,71 @@
     Feature dimensions: 2048
 
     GitHub: https://github.com/Xiyue-Wang/RetCCL
     """
 
     tag = 'retccl'
 
-    def __init__(self, device='cuda', center_crop=False):
+    def __init__(self, device=None, center_crop=False):
         super().__init__(backend='torch')
 
-        self.device = device
+        from slideflow.model import torch_utils
+        self.device = torch_utils.get_device(device)
         self.model = ResNet50(
             block=Bottleneck,
             layers=[3, 4, 6, 3],
             num_classes=128,
             mlp=False,
             two_branch=False,
             normlinear=True
         )
-        self.model.fc = torch.nn.Identity().to(device)
+        self.model.fc = torch.nn.Identity().to(self.device)
 
         checkpoint_path = hf_hub_download(
             repo_id='jamesdolezal/RetCCL',
             filename='retccl.pth'
         )
-        td = torch.load(checkpoint_path)
+        td = torch.load(checkpoint_path, map_location=self.device)
         self.model.load_state_dict(td, strict=True)
-        self.model = self.model.to(device)
+        self.model = self.model.to(self.device)
         self.model.eval()
 
         # ---------------------------------------------------------------------
         self.num_features = 2048
         self.num_classes = 0
         all_transforms = [transforms.CenterCrop(256)] if center_crop else []
         all_transforms += [
             transforms.Lambda(lambda x: x / 255.),
             transforms.Normalize(
                 mean=(0.485, 0.456, 0.406),
                 std=(0.229, 0.224, 0.225)),
         ]
         self.transform = transforms.Compose(all_transforms)
         self.preprocess_kwargs = dict(standardize=False)
+        self._center_crop = center_crop
         # ---------------------------------------------------------------------
 
     def __call__(self, obj, **kwargs):
         """Generate features for a batch of images or a WSI."""
         if isinstance(obj, sf.WSI):
-            return features_from_slide_torch(
-                self,
-                obj,
-                device=self.device,
-                **kwargs
+            grid = features_from_slide(self, obj, **kwargs)
+            return np.ma.masked_where(grid == -99, grid)
+        elif kwargs:
+            raise ValueError(
+                f"{self.__class__.__name__} does not accept keyword arguments "
+                "when extracting features from a batch of images."
             )
         assert obj.dtype == torch.uint8
         obj = self.transform(obj)
-        return self.model(obj)
+        return self.model(obj)
+
+    def dump_config(self):
+        """Return a dictionary of configuration parameters.
+
+        These configuration parameters can be used to reconstruct the
+        feature extractor, using ``slideflow.model.build_feature_extractor()``.
+
+        """
+        return {
+            'class': 'slideflow.model.extractors.retccl.RetCCLFeatures',
+            'kwargs': {'center_crop': self._center_crop}
+        }
```

## slideflow/norm/__init__.py

```diff
@@ -187,15 +187,15 @@
                 during fitting, if fitting to a dataset. Defaults to 'auto'.
         """
 
         # Fit to a dataset
         if isinstance(arg1, Dataset):
             # Set up thread pool
             if num_threads == 'auto':
-                num_threads = os.cpu_count()  # type: ignore
+                num_threads = sf.util.num_cpu(default=8)  # type: ignore
             log.debug(f"Setting up pool (size={num_threads}) for norm fitting")
             log.debug(f"Using normalizer batch size of {batch_size}")
             pool = mp.dummy.Pool(num_threads)  # type: ignore
 
             dataset = arg1
             if sf.backend() == 'tensorflow':
                 dts = dataset.tensorflow(
@@ -626,15 +626,15 @@
         from the context (whole-slide image) rather than the image being
         normalized. This may improve stain normalization for sections of
         a slide that are predominantly eosin (e.g. necrosis or low cellularity).
 
         When calculating max concentrations from the image context,
         white pixels (255) will be masked.
 
-         If a slide (``sf.WSI``) is used for context, any existing QC filters
+        If a slide (``sf.WSI``) is used for context, any existing QC filters
         and regions of interest will be used to mask out background as white
         pixels, and the masked thumbnail will be used for creating the
         normalizer context. If no QC has been applied to the slide and the
         slide does not have any Regions of Interest, then both otsu's
         thresholding and Gaussian blur filtering will be applied
         to the thumbnail for masking.
 
@@ -678,16 +678,17 @@
     are slower than the numpy implementations. Thus, with the PyTorch backend,
     all normalizers will be the default numpy implementations.
 
     Args:
         method (str): Normalization method. Options include 'macenko',
             'reinhard', 'reinhard_fast', 'reinhard_mask', 'reinhard_fast_mask',
             'vahadane', 'vahadane_spams', 'vahadane_sklearn', and 'augment'.
-        source (str, optional): Path to a source image. If provided, will
-            fit the normalizer to this image. Defaults to None.
+        source (str, optional): Stain normalization preset or path to a source
+            image. Valid presets include 'v1', 'v2', and 'v3'. If None, will
+            use the default present ('v3'). Defaults to None.
 
     Returns:
         StainNormalizer:    Initialized StainNormalizer.
     """
     if backend is None:
         backend = sf.backend()
     if backend == 'tensorflow':
```

## slideflow/norm/tensorflow/macenko.py

```diff
@@ -13,15 +13,15 @@
 @tf.function
 def is_self_adjoint(matrix):
     return tf.reduce_all(tf.math.equal(matrix, tf.linalg.adjoint(matrix)))
 
 @tf.function
 def normalize_c(C):
     return tf.stack([
-        tfp.stats.percentile(C[0, :], 99), 
+        tfp.stats.percentile(C[0, :], 99),
         tfp.stats.percentile(C[1, :], 99)]
     )
 
 @tf.function
 def _matrix_and_concentrations(
     img: tf.Tensor,
     Io: int = 255,
```

## slideflow/norm/torch/macenko.py

```diff
@@ -31,14 +31,15 @@
 # -----------------------------------------------------------------------------
 
 class MacenkoNormalizer:
 
     vectorized = False
     preferred_device = 'cpu'
     preset_tag = 'macenko'
+    standardize = True
 
     def __init__(
         self,
         Io: int = 255,
         alpha: float = 1,
         beta: float = 0.15,
     ) -> None:
@@ -174,16 +175,15 @@
             self._augment_params['matrix_stdev'] = torch.from_numpy(ut._as_numpy(matrix_stdev))
         if concentrations_stdev is not None:
             self._augment_params['concentrations_stdev'] = torch.from_numpy(ut._as_numpy(concentrations_stdev))
 
     def _matrix_and_concentrations(
         self,
         img: torch.Tensor,
-        mask: bool = False,
-        standardize: bool = True
+        mask: bool = False
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Gets the H&E stain matrix and concentrations for a given image.
 
         Args:
             img (torch.Tensor): Image (RGB uint8) with dimensions W, H, C.
             mask (bool): Mask white pixels (255) during calculation.
                 Defaults to False.
@@ -198,15 +198,15 @@
                 torch.Tensor: Concentrations of individual stains
         """
         img = img.reshape((-1, 3))
 
         if mask:
             ones = torch.all(img == 255, dim=1)
 
-        if standardize:
+        if self.standardize:
             img = standardize_brightness(img, mask=mask)
 
         # Calculate optical density.
         OD = -torch.log((img.to(torch.float32) + 1) / self.Io)
 
         # Remove transparent pixles.
         if mask:
@@ -411,15 +411,8 @@
 
 
 class MacenkoFastNormalizer(MacenkoNormalizer):
 
     """Macenko H&E stain normalizer, with brightness standardization disabled."""
 
     preset_tag = 'macenko_fast'
-
-    def _matrix_and_concentrations(
-        self,
-        img: torch.Tensor,
-        mask: bool = False,
-        standardize: bool = False
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        return super()._matrix_and_concentrations(img, mask, standardize=False)
+    standardize = False
```

## slideflow/simclr/simclr/tf2/__init__.py

```diff
@@ -15,14 +15,15 @@
 # ==============================================================================
 """The main training pipeline."""
 
 import json
 import math
 import os
 
+from tqdm import tqdm
 from slideflow import log as logging
 from . import data as data_lib
 from . import metrics
 from . import model as model_lib
 from . import objective as obj_lib
 from . import utils as utils_lib
 
@@ -403,15 +404,15 @@
       # Restore checkpoint if available.
       logging.debug("Attempting to restore from checkpoint")
       checkpoint_manager = try_restore_from_checkpoint(
           model, optimizer.iterations, optimizer, model_dir, checkpoint_path,
           keep_checkpoint_max=args.keep_checkpoint_max,
           zero_init_logits_layer=args.zero_init_logits_layer)
 
-    steps_per_loop = checkpoint_steps
+    steps_per_loop = min(checkpoint_steps, train_steps)
 
     def single_step(features, labels):
       with tf.GradientTape() as tape:
         # Log summaries on the last step of the training loop to match
         # logging frequency of other scalar summaries.
         #
         # Notes:
@@ -478,26 +479,27 @@
           logging.debug(var.name)
         grads = tape.gradient(loss, model.trainable_variables)
         optimizer.apply_gradients(zip(grads, model.trainable_variables))
 
     with strategy.scope():
 
       @tf.function
+      def train_single_step(iterator):
+        # Drop the "while" prefix created by tf.while_loop which otherwise
+        # gets prefixed to every variable name. This does not affect training
+        # but does affect the checkpoint conversion script.
+        # TODO(b/161712658): Remove this.
+        with tf.name_scope(''):
+          images, labels = next(iterator)
+          features, labels = images, {'labels': labels}
+          strategy.run(single_step, (features, labels))
+
       def train_multiple_steps(iterator):
-        # `tf.range` is needed so that this runs in a `tf.while_loop` and is
-        # not unrolled.
-        for _ in tf.range(steps_per_loop):
-          # Drop the "while" prefix created by tf.while_loop which otherwise
-          # gets prefixed to every variable name. This does not affect training
-          # but does affect the checkpoint conversion script.
-          # TODO(b/161712658): Remove this.
-          with tf.name_scope(''):
-            images, labels = next(iterator)
-            features, labels = images, {'labels': labels}
-            strategy.run(single_step, (features, labels))
+        for _ in tqdm(range(steps_per_loop)):
+          train_single_step(iterator)
 
       global_step = optimizer.iterations
       cur_step = global_step.numpy()
       iterator = iter(ds)
       logging.debug("Beginning training")
       while cur_step < train_steps:
         # Calls to tf.summary.xyz lookup the summary writer resource which is
```

## slideflow/simclr/simclr/tf2/data.py

```diff
@@ -12,26 +12,27 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific simclr governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Data pipeline."""
 
 import functools
+import slideflow as sf
 from slideflow import log as logging
 
 from . import data_util
 import tensorflow.compat.v2 as tf
 import tensorflow_datasets as tfds
 
 
 class DatasetBuilder:
 
     def __init__(self, train_dts=None, val_dts=None, test_dts=None, *, labels=None,
-                 val_kwargs=None, steps_per_epoch_override=None,
-                 dataset_kwargs=None):
+                 val_kwargs=None, steps_per_epoch_override=None, normalizer=None,
+                 normalizer_source=None, dataset_kwargs=None):
         """Build a training/validation dataset pipeline for SimCLR.
 
         Args:
             train_dts (sf.Dataset, optional): Training dataset.
             val_dts (sf.Dataset, optional): Optional validation dataset.
             test_dts (sf.Dataset, optional): Optional held-out test set.
 
@@ -90,27 +91,34 @@
             self.test_dts = test_dts
         if steps_per_epoch_override:
             train_tiles = steps_per_epoch_override
         elif self.train_dts:
             train_tiles = self.train_dts.num_tiles
         else:
             train_tiles = 0
+
+        if isinstance(normalizer, str):
+            self.normalizer = sf.norm.autoselect(normalizer,
+                                                 source=normalizer_source,
+                                                 backend='tensorflow')
+        else:
+            self.normalizer = normalizer
         self.num_classes = 0 if self.labels is None else len(set(list(self.labels.values())))
         self.dataset_kwargs = dict() if dataset_kwargs is None else dataset_kwargs
         self.info = data_util.EasyDict(
             features=data_util.EasyDict(
                 label=data_util.EasyDict(num_classes=self.num_classes)
             ),
             splits=data_util.EasyDict(
                 train=data_util.EasyDict(num_examples=train_tiles),
                 validation=data_util.EasyDict(num_examples=(0 if not self.val_dts else self.val_dts.num_tiles)),
                 test=data_util.EasyDict(num_examples=(0 if not self.test_dts else self.test_dts.num_tiles))
             ))
 
-    def as_dataset(self, split, read_config, shuffle_files, as_supervised):
+    def as_dataset(self, split, read_config, shuffle_files, as_supervised, **kwargs):
         logging.info(f"Dataset split requested: {split}")
         if split == 'train':
             dts = self.train_dts
         elif split == 'validation':
             dts = self.val_dts
         elif split == 'test':
             dts = self.test_dts
@@ -123,15 +131,16 @@
         return dts.tensorflow(
             labels=self.labels,
             num_shards=read_config.input_context.num_input_pipelines,
             shard_idx=read_config.input_context.input_pipeline_id,
             deterministic=True,
             standardize=False,
             infinite=(split == 'train'),
-            **self.dataset_kwargs
+            **self.dataset_kwargs,
+            **kwargs
         )
 
     def build_dataset(self, *args, **kwargs):
         """Builds a distributed dataset.
 
         Args:
             batch_size (int): Global batch size across devices.
@@ -163,19 +172,27 @@
 
   def _input_fn(input_context):
     """Inner input function."""
     batch_size = input_context.get_per_replica_batch_size(global_batch_size)
     logging.info('Global batch size: %d', global_batch_size)
     logging.info('Per-replica batch size: %d', batch_size)
     preprocess_fn_pretrain = get_preprocess_fn(
-      is_training, is_pretrain=True, image_size=simclr_args.image_size,
-      color_jitter_strength=simclr_args.color_jitter_strength)
+      is_training,
+      is_pretrain=True,
+      image_size=simclr_args.image_size,
+      color_jitter_strength=simclr_args.color_jitter_strength,
+      normalizer=(builder.normalizer if is_training else None),
+      normalizer_augment=simclr_args.stain_augment)
     preprocess_fn_finetune = get_preprocess_fn(
-      is_training, is_pretrain=False, image_size=simclr_args.image_size,
-      color_jitter_strength=simclr_args.color_jitter_strength)
+      is_training,
+      is_pretrain=False,
+      image_size=simclr_args.image_size,
+      color_jitter_strength=simclr_args.color_jitter_strength,
+      normalizer=(builder.normalizer if is_training else None),
+      normalizer_augment=simclr_args.stain_augment)
     num_classes = builder.info.features['label'].num_classes
 
     def map_fn(image, label, *args):
       """Produces multiple transformations of the same batch."""
       if is_training and simclr_args.train_mode == 'pretrain':
         xs = []
         for _ in range(2):  # Two transformations
@@ -184,25 +201,33 @@
       else:
         image = preprocess_fn_finetune(image)
       if num_classes:
         label = tf.one_hot(label, num_classes)
       return detuple(image, label, args)
 
     logging.info('num_input_pipelines: %d', input_context.num_input_pipelines)
+
+    # Perform stain normalization within sf.Dataset.tensorflow()
+    # If this is for inference.
+    if builder.normalizer and not is_training:
+      dts_kw = dict(normalizer=builder.normalizer)
+    else:
+      dts_kw = {}
     dataset = builder.as_dataset(
         split=simclr_args.train_split if is_training else simclr_args.eval_split,
         shuffle_files=is_training,
         as_supervised=True,
         # Passing the input_context to TFDS makes TFDS read different parts
         # of the dataset on different workers. We also adjust the interleave
         # parameters to achieve better performance.
         read_config=tfds.ReadConfig(
             interleave_cycle_length=32,
             interleave_block_length=1,
-            input_context=input_context))
+            input_context=input_context),
+        **dts_kw)
     if cache_dataset:
       dataset = dataset.cache()
     if is_training:
       options = tf.data.Options()
       options.experimental_deterministic = False
       options.experimental_slack = True
       dataset = dataset.with_options(options)
@@ -224,29 +249,33 @@
     strategy = tf.distribute.get_strategy()
   input_fn = build_input_fn(
     builder, batch_size, is_training, simclr_args, cache_dataset=cache_dataset
   )
   return strategy.distribute_datasets_from_function(input_fn)
 
 
-def get_preprocess_fn(is_training, is_pretrain, image_size, color_jitter_strength=1.0):
+def get_preprocess_fn(is_training, is_pretrain, image_size,
+                      color_jitter_strength=1.0, normalizer=None,
+                      normalizer_augment=True, center_crop=True):
   """Get function that accepts an image and returns a preprocessed image."""
   # Disable test cropping for small images (e.g. CIFAR)
-  if image_size <= 32:
+  if not center_crop or image_size <= 32:
     test_crop = False
   else:
     test_crop = True
   return functools.partial(
-      data_util.preprocess_image,
-      height=image_size,
-      width=image_size,
-      color_jitter_strength=color_jitter_strength,
-      is_training=is_training,
-      color_distort=is_pretrain,
-      test_crop=test_crop)
+    data_util.preprocess_image,
+    height=image_size,
+    width=image_size,
+    color_jitter_strength=color_jitter_strength,
+    is_training=is_training,
+    color_distort=is_pretrain,
+    test_crop=test_crop,
+    normalizer=normalizer,
+    normalizer_augment=normalizer_augment)
 
 # -----------------------------------------------------------------------------
 
 def detuple(image, label, args):
     """Detuple optional arguments for return.
 
     Adds support for returning args via wildcard in Python 3.7. The following:
```

## slideflow/simclr/simclr/tf2/data_util.py

```diff
@@ -460,15 +460,17 @@
     image,
     height,
     width,
     color_jitter_strength=1.0,
     color_distort=True,
     crop=True,
     flip=True,
-    impl='simclrv2'
+    impl='simclrv2',
+    normalizer=None,
+    normalizer_augment=True,
 ):
   """Preprocesses the given image for training.
 
   Args:
     image: `Tensor` representing an image of arbitrary size.
     height: Height of output image.
     width: Width of output image.
@@ -477,59 +479,71 @@
     flip: Whether or not to flip left and right of an image.
     impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's
         version of random brightness.
 
   Returns:
     A preprocessed image `Tensor`.
   """
+  if normalizer:
+    image = normalizer.transform(image, augment=normalizer_augment)
+  image = tf.image.convert_image_dtype(image, dtype=tf.float32)
   if crop:
     image = random_crop_with_resize(image, height, width)
   if flip:
     image = tf.image.random_flip_left_right(image)
   if color_distort:
     image = random_color_jitter(image, strength=color_jitter_strength,
                                 impl=impl)
   image = tf.reshape(image, [height, width, 3])
   image = tf.clip_by_value(image, 0., 1.)
   return image
 
 
-def preprocess_for_eval(image, height, width, crop=True):
+def preprocess_for_eval(image, height, width, crop=True, normalizer=None):
   """Preprocesses the given image for evaluation.
 
   Args:
     image: `Tensor` representing an image of arbitrary size.
     height: Height of output image.
     width: Width of output image.
     crop: Whether or not to (center) crop the test images.
 
   Returns:
     A preprocessed image `Tensor`.
   """
+  if normalizer:
+    image = normalizer.transform(image)
+  image = tf.image.convert_image_dtype(image, dtype=tf.float32)
   if crop:
     image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)
   image = tf.reshape(image, [height, width, 3])
   image = tf.clip_by_value(image, 0., 1.)
   return image
 
 
-def preprocess_image(image, height, width, color_jitter_strength=1.0, is_training=False,
-                     color_distort=True, test_crop=True):
+def preprocess_image(image, height, width, color_jitter_strength=1.0,
+                     is_training=False, color_distort=True, test_crop=True,
+                     normalizer=None, normalizer_augment=True):
   """Preprocesses the given image.
 
   Args:
     image: `Tensor` representing an image of arbitrary size.
     height: Height of output image.
     width: Width of output image.
     is_training: `bool` for whether the preprocessing is for training.
     color_distort: whether to apply the color distortion.
     test_crop: whether or not to extract a central crop of the images
         (as for standard ImageNet evaluation) during the evaluation.
 
   Returns:
     A preprocessed image `Tensor` of range [0, 1].
   """
-  image = tf.image.convert_image_dtype(image, dtype=tf.float32)
   if is_training:
-    return preprocess_for_train(image, height, width, color_jitter_strength, color_distort)
+    return preprocess_for_train(
+        image, height, width, color_jitter_strength, color_distort,
+        normalizer=normalizer, normalizer_augment=normalizer_augment
+    )
   else:
-    return preprocess_for_eval(image, height, width, test_crop)
+    return preprocess_for_eval(
+        image, height, width, test_crop,
+        normalizer=normalizer
+    )
```

## slideflow/simclr/simclr/tf2/resnet.py

```diff
@@ -95,18 +95,14 @@
   def call(self, net, training):
     keep_prob = self.keep_prob
     dropblock_size = self.dropblock_size
     data_format = self.data_format
     if not training or keep_prob is None:
       return net
 
-    tf.logging.info(
-        'Applying DropBlock: dropblock_size {}, net.shape {}'.format(
-            dropblock_size, net.shape))
-
     if data_format == 'channels_last':
       _, width, height, _ = net.get_shape().as_list()
     else:
       _, _, width, height = net.get_shape().as_list()
     if width != height:
       raise ValueError('Input tensor with width!=height is not supported.')
 
@@ -123,15 +119,15 @@
         tf.logical_and(h_i >= int(dropblock_size // 2),
                        h_i < width - (dropblock_size - 1) // 2))
 
     valid_block_center = tf.expand_dims(valid_block_center, 0)
     valid_block_center = tf.expand_dims(
         valid_block_center, -1 if data_format == 'channels_last' else 0)
 
-    randnoise = tf.random_uniform(net.shape, dtype=tf.float32)
+    randnoise = tf.random.uniform(net.shape, dtype=tf.float32)
     block_pattern = (
         1 - tf.cast(valid_block_center, dtype=tf.float32) + tf.cast(
             (1 - seed_drop_rate), dtype=tf.float32) + randnoise) >= 1
     block_pattern = tf.cast(block_pattern, dtype=tf.float32)
 
     if dropblock_size == width:
       block_pattern = tf.reduce_min(
```

## slideflow/simclr/simclr/tf2/utils.py

```diff
@@ -7,18 +7,18 @@
 from slideflow import log
 
 # -----------------------------------------------------------------------------
 
 class SimCLR_Args:
     def __init__(
         self,
-        learning_rate=0.3,
-        learning_rate_scaling='linear',
+        learning_rate=0.075,
+        learning_rate_scaling='sqrt',
         warmup_epochs=10,
-        weight_decay=1e-6,
+        weight_decay=1e-4,
         batch_norm_decay=0.9,
         train_batch_size=512,
         train_split='train',
         train_epochs=100,
         train_steps=0,
         eval_steps=0,
         eval_batch_size=256,
@@ -47,14 +47,15 @@
         resnet_depth=50,
         sk_ratio=0.,
         se_ratio=0.,
         image_size=224,
         color_jitter_strength=1.0,
         use_blur=True,
         num_classes=None,
+        stain_augment=True,
     ) -> None:
       """SimCLR arguments.
 
       A class containg all default - if not overwritten at initialization -
         SimCLR arguments.
 
       Keyword Args:
@@ -120,14 +121,20 @@
       for argname, argval in dict(locals()).items():
           setattr(self, argname, argval)
 
     def to_dict(self):
         return {k:v for k,v in vars(self).items()
                 if k not in ('model_kwargs', 'self')}
 
+    def __repr__(self):
+        return '{}(\n{}\n)'.format(
+            self.__class__.__name__,
+            ',\n'.join('  {}={!r}'.format(k, v) for k, v in self.to_dict().items())
+        )
+
     @property
     def model_kwargs(self):
         return {
             k: getattr(self, k)
             for k in ('num_classes', 'resnet_depth', 'width_multiplier',
                       'sk_ratio', 'se_ratio', 'image_size', 'batch_norm_decay',
                       'train_mode', 'use_blur', 'proj_out_dim', 'proj_head_mode',
```

## slideflow/slide/__init__.py

```diff
@@ -315,16 +315,16 @@
         self.filter_magnification = (self.filter_dimensions[0]
                                     / self.dimensions[0])
         self.filter_px = int(self.full_extract_px * self.filter_magnification)
 
         # Calculate shape and stride
         self.downsample_level = ds_level
         self.downsample_dimensions = self.slide.level_dimensions[ds_level]
-        self.stride = int(self.extract_px // stride_div)
-        self.full_stride = int(self.full_extract_px // stride_div)
+        self.stride = int(np.round(self.extract_px / stride_div))
+        self.full_stride = int(np.round(self.full_extract_px / stride_div))
 
     def __getstate__(self):
         state = self.__dict__.copy()
         # Remove the unpicklable entries.
         if '__slide' in state:
             state['__slide'] = None
         if '_BaseLoader__slide' in state:
@@ -437,15 +437,16 @@
     def qc(
         self,
         method: Union[str, Callable, List[Callable]],
         *,
         blur_radius: int = 3,
         blur_threshold: float = 0.02,
         filter_threshold: float = 0.6,
-        blur_mpp: Optional[float] = None
+        blur_mpp: Optional[float] = None,
+        pool: Optional["mp.Pool"] = None
     ) -> Optional[Image.Image]:
         """Applies quality control to a slide, performing filtering based on
         a whole-slide image thumbnail.
 
         'blur' method filters out blurry or out-of-focus slide sections.
         'otsu' method filters out background based on automatic saturation
         thresholding in the HSV colorspace.
@@ -482,28 +483,33 @@
             method.remove('both')       # type: ignore
             method.insert(idx, 'otsu')  # type: ignore
             # Blur should be performed before Otsu's thresholding
             method.insert(idx, 'blur')  # type: ignore
         if 'blur' in method:
             idx = method.index('blur')  # type: ignore
             method.remove('blur')       # type: ignore
-            method.insert(idx, sf.slide.qc.Gaussian(mpp=blur_mpp,
-                                                    sigma=blur_radius,
-                                                    threshold=blur_threshold))
+            method.insert(idx, sf.slide.qc.GaussianV2(mpp=blur_mpp,
+                                                      sigma=blur_radius,
+                                                      threshold=blur_threshold))
         if 'otsu' in method:
             idx = method.index('otsu')  # type: ignore
             method.remove('otsu')       # type: ignore
             method.insert(idx, sf.slide.qc.Otsu())
 
         starttime = time.time()
         img = None
         log.debug(f"Applying QC: {method}")
         for qc in method:
             if isinstance(method, str):
                 raise errors.QCError(f"Unknown QC method {method}")
+            if pool is not None:
+                try:
+                    qc.pool = pool
+                except Exception as e:
+                    log.debug(f"Unable to set pool for QC method {qc}")
             mask = qc(self)
             if mask is not None:
                 img = self.apply_qc_mask(mask, filter_threshold=filter_threshold)
         dur = f'(time: {time.time()-starttime:.2f}s)'
         log.debug(f'QC ({method}) complete for slide {self.shortname} {dur}')
         return img
 
@@ -710,16 +716,17 @@
                 Discard tiles with this fraction of grayspace. If 1, will not
                 perform grayspace filtering.
             grayspace_threshold (float, optional): Range 0-1. Defaults to 0.05.
                 Pixels in HSV format with saturation below this threshold are
                 considered grayspace.
             normalizer (str, optional): Normalization to use on image tiles.
                 Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
             full_core (bool, optional): Extract an entire detected core, rather
                 than subdividing into image tiles. Defaults to False.
             shuffle (bool): Shuffle images during extraction.
             num_threads (int): Number of threads to allocate to workers.
             yolo (bool, optional): Export yolo-formatted tile-level ROI
                 annotations (.txt) in the tile directory. Requires that
@@ -950,14 +957,15 @@
         roi_dir: Optional[str] = None,
         rois: Optional[List[str]] = None,
         roi_method: str = 'auto',
         roi_filter_method: Union[str, float] = 'center',
         randomize_origin: bool = False,
         pb: Optional[Progress] = None,
         verbose: bool = True,
+        use_edge_tiles: bool = False,
         **kwargs
     ) -> None:
         """Loads slide and ROI(s).
 
         Args:
             path (str): Path to slide.
             tile_px (int): Size of tiles to extract, in pixels.
@@ -1027,14 +1035,15 @@
         self.roi_scale = 10  # type: float
         self.roi_method = roi_method
         self.roi_filter_method = roi_filter_method
         self.randomize_origin = randomize_origin
         self.verbose = verbose
         self.segmentation = None
         self.grid = None
+        self.use_edge_tiles = use_edge_tiles
 
         if (not isinstance(roi_filter_method, (int, float))
            and roi_filter_method != 'center'):
             raise ValueError(
                 "Unrecognized value for argument 'roi_filter_method': {} ."
                 "Expected either float or 'center'.".format(roi_filter_method)
             )
@@ -1177,16 +1186,14 @@
             )
         )
         return image_dict['image']
 
     def _build_coord(self) -> None:
         '''Set up coordinate grid.'''
 
-        log.debug("Setting up coordinate grid.")
-
         # Calculate window sizes, strides, and coordinates for windows
         self.extracted_x_size = self.dimensions[0] - self.full_extract_px
         self.extracted_y_size = self.dimensions[1] - self.full_extract_px
 
         # Randomize origin, if desired
         if self.randomize_origin:
             start_x = random.randint(0, self.full_stride-1)
@@ -1194,22 +1201,23 @@
             log.info(f"Random origin: X: {start_x}, Y: {start_y}")
         else:
             start_x = start_y = 0
 
         # Coordinates must be in level 0 (full) format
         # for the read_region function
         self.coord = []  # type: Union[List, np.ndarray]
+        edge_buffer = 0 if self.use_edge_tiles else self.full_extract_px
         y_range = np.arange(
             start_y,
-            (self.dimensions[1]+1) - self.full_extract_px,
+            (self.dimensions[1]+1) - edge_buffer,
             self.full_stride
         )
         x_range = np.arange(
             start_x,
-            (self.dimensions[0]+1) - self.full_extract_px,
+            (self.dimensions[0]+1) - edge_buffer,
             self.full_stride
         )
         self.grid = np.ones((len(x_range), len(y_range)), dtype=bool)
 
         # ROI filtering
         roi_by_center = (self.roi_filter_method == 'center')
         if self.has_rois():
@@ -1262,29 +1270,30 @@
                 self.coord.append([x, y, xi, yi])
 
                 # ROI filtering
                 if self.has_rois() and roi_by_center:
                     point_in_roi = self.roi_mask[yi, xi]
                     # If the extraction method is 'inside',
                     # skip the tile if it's not in an ROI
-                    if (((self.roi_method == 'inside') and not point_in_roi)
+                    if (((self.roi_method in ('inside', 'auto')) and not point_in_roi)
                        or ((self.roi_method == 'outside') and point_in_roi)):
                         self.grid[xi, yi] = 0
 
         # If roi_filter_method is a float, then perform tile selection
         # based on what proportion of the tile is in an ROI,
         # rather than choosing a tile by centroid (roi_filter_method='center')
         if self.has_rois() and not roi_by_center:
             self.apply_qc_mask(
                 ~self.roi_mask,
                 filter_threshold=(1-self.roi_filter_method)
             )
 
         self.coord = np.array(self.coord)
         self.estimated_num_tiles = int(self.grid.sum())
+        log.debug(f"Set up coordinate grid, shape={self.grid.shape}")
 
     @property
     def shape(self):
         return self.grid.shape
 
     def apply_segmentation(self, segmentation):
         # Filter out masks outside of ROIs, if present.
@@ -1299,14 +1308,27 @@
         self.seg_coord = np.concatenate(
             (centroids, np.expand_dims(np.arange(centroids.shape[0]), axis=-1)),
             axis=-1)
         nonzero = self.seg_coord[:, 0] > 0
         self.seg_coord[:, 0:2][nonzero] -= int(self.full_extract_px/2)
         self.estimated_num_tiles = centroids.shape[0]
 
+    def area(self) -> float:
+        """Calculate area (mm^2) of slide that passes QC masking."""
+        dim_x, dim_y = self.dimensions[0], self.dimensions[1]
+        total_area_in_sq_microns = (dim_x * self.mpp) * (dim_y * self.mpp)
+        if self.qc_mask is not None:
+            s = self.qc_mask.shape
+            p = 1 - (self.qc_mask.sum() / (s[0] * s[1]))
+            area_in_sq_microns = p * total_area_in_sq_microns
+        else:
+            area_in_sq_microns = total_area_in_sq_microns
+        area_in_sq_mm = area_in_sq_microns * 1e-6
+        return area_in_sq_mm
+
     def get_tile_mask(self, index, sparse_mask):
 
         # Get the corresponding segmentation mask, reading from the sparse matrix
         seg = self.segmentation
         mask_idx = self.seg_coord[index][2] + 1  # sparse mask index starts at 1
         mask_y, mask_x = np.unravel_index(sparse_mask[mask_idx].data, seg.masks.shape)
 
@@ -1403,16 +1425,17 @@
                 Discard tiles with this fraction of grayspace. If 1, will not
                 perform grayspace filtering.
             grayspace_threshold (float, optional): Range 0-1. Defaults to 0.05.
                 Pixels in HSV format with saturation below this threshold are
                 considered grayspace.
             normalizer (str, optional): Normalization for image tiles.
                 Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
             full_core (bool, optional): Extract an entire detected core, rather
                 than subdividing into image tiles. Defaults to False.
             shuffle (bool): Shuffle images during extraction.
             num_threads (int): Number of threads to allocate to workers.
             yolo (bool, optional): Export yolo-formatted tile-level ROI
                 annotations (.txt) in the tile directory. Requires that
@@ -1481,15 +1504,15 @@
         self,
         *,
         shuffle: bool = True,
         whitespace_fraction: float = None,
         whitespace_threshold: float = None,
         grayspace_fraction: float = None,
         grayspace_threshold: float = None,
-        normalizer: str = None,
+        normalizer: Optional[Union[str, "slideflow.norm.StainNormalizer"]] = None,
         normalizer_source: str = None,
         context_normalize: bool = False,
         num_threads: Optional[int] = None,
         num_processes: Optional[int] = None,
         show_progress: bool = False,
         img_format: str = 'numpy',
         full_core: bool = False,
@@ -1517,16 +1540,17 @@
                 Discard tiles with this fraction of grayspace. If 1, will not
                 perform grayspace filtering.
             grayspace_threshold (float, optional): Range 0-1. Defaults to 0.05.
                 Pixels in HSV format with saturation below this threshold are
                 considered grayspace.
             normalizer (str, optional): Normalization strategy to use on image
                 tiles. Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
             context_normalize (bool): If normalizing, use context from
                 the rest of the slide when calculating stain matrix
                 concentrations. Defaults to False (normalize each image tile
                 as separate images).
             num_threads (int): If specified, will extract tiles with a
                 ThreadPool using the specified number of threads. Cannot
@@ -1556,14 +1580,17 @@
                 `WSI.apply_segmentation()`. Defaults to False.
             apply_masks (bool): Apply cell segmentation masks to tiles. Ignored
                 if cell segmentation has been applied to the slide.
                 Defaults to True.
             deterministic (bool): Return tile images in reproducible,
                 deterministic order. May slightly decrease iteration time.
                 Defaults to True.
+            shard (tuple(int, int), optional): If provided, will only extract
+                tiles from the shard with index `shard[0]` out of `shard[1]`
+                shards. Defaults to None.
 
         Returns:
             dict: Dict with keys 'image' (image data), 'yolo' (optional
             yolo-formatted annotations, (x_center, y_center,
             width, height)) and 'grid' ((x, y) slide or grid coordinates)
 
         """
@@ -1704,29 +1731,31 @@
                 non_roi_coord = sharded_coords[shard_idx]
 
             # Set up worker pool
             if pool is None:
                 if num_threads is None and num_processes is None:
                     # Libvips is extremely slow with ThreadPools.
                     # In the cuCIM backend, ThreadPools are used by default
-                    # to reduce memory utilization.
+                    #   to reduce memory utilization.
                     # In the Libvips backend, a multiprocessing pool is default
-                    # to significantly improve performance.
-                    n_cores = os.cpu_count() if os.cpu_count() else 8
+                    #   to significantly improve performance.
+                    n_cores = sf.util.num_cpu(default=8)
                     if sf.slide_backend() == 'libvips':
                         num_processes = max(int(n_cores/2), 1)
                     else:
                         num_threads = n_cores
                 if num_threads is not None and num_threads > 1:
                     log.debug(f"Building generator ThreadPool({num_threads})")
                     pool = mp.dummy.Pool(processes=num_threads)
                     should_close = True
                 elif num_processes is not None and num_processes > 1:
-                    log.debug(f"Building generator with Pool({num_processes})")
-                    ctx = mp.get_context('spawn')
+                    ptype = 'spawn' if sf.slide_backend() == 'libvips' else 'fork'
+                    log.debug(f"Building generator with Pool({num_processes}), "
+                              f"type={ptype}")
+                    ctx = mp.get_context(ptype)
                     pool = ctx.Pool(
                         processes=num_processes,
                         initializer=sf.util.set_ignore_sigint,
                     )
                     should_close = True
                 else:
                     log.debug(f"Building generator without multithreading")
@@ -1871,14 +1900,16 @@
     def load_roi_array(self, array: np.ndarray, process: bool = True):
         existing = [
             int(r.name[4:]) for r in self.rois
             if r.name.startswith('ROI_') and r.name[4:].isnumeric()
         ]
         roi_id = list(set(list(range(len(existing)+1))) - set(existing))[0]
         self.rois.append(ROI(f'ROI_{roi_id}', array))
+        if self.roi_method == 'auto':
+            self.roi_method = 'inside'
         if process:
             self.process_rois()
 
     def load_csv_roi(self, path: str, process: bool = True) -> int:
         '''Loads CSV ROI from a given path.'''
 
         # Clear any previously loaded ROIs.
@@ -1928,14 +1959,16 @@
             json_data = json.load(json_file)['shapes']
         for shape in json_data:
             area_reduced = np.multiply(shape['points'], scale)
             self.rois.append(ROI(f"Object{len(self.rois)}"))
             self.rois[-1].add_shape(area_reduced)
         if process:
             self.process_rois()
+        if self.roi_method == 'auto':
+            self.roi_method = 'inside'
         return len(self.rois)
 
     def masked_thumb(self, background: str = 'white', **kwargs) -> np.ndarray:
         """Return a masked thumbnail of a slide, using QC and/or ROI masks.
         Masked areas will be white.
         """
         if background not in ('white', 'black'):
@@ -1950,15 +1983,15 @@
             # Apply Otsu's threshold to background area
             # to prevent whitespace from interfering with normalization
             from slideflow.slide.qc import Otsu, Gaussian
             sf.log.debug(
                 "Applying Otsu's thresholding & Gaussian blur filter "
                 "to stain norm context"
             )
-            _blur_mask = Gaussian()(image)
+            _blur_mask = GaussianV2()(image)
             qc_mask = Otsu()(image, mask=_blur_mask)
         # Mask by ROI and QC, if applied.
         # Use white as background for masked areas.
         if qc_mask is not None:
             qc_img = sf.slide.img_as_ubyte(qc_mask)
             mask = ~cv2.resize(qc_img, (image.shape[1], image.shape[0]))
         if roi_mask is not None:
@@ -2268,28 +2301,28 @@
     WHITE = (255, 255, 255)
 
     def __init__(
         self,
         path: str,
         tile_px: int,
         tile_um: Union[str, int],
-        stride_div: int = 1,
+        stride_div: float = 1,
         enable_downsample: bool = True,
         report_dir: Optional[str] = None,
         pb: Optional[Progress] = None,
         **kwargs
     ) -> None:
         '''Initializer.
 
         Args:
             path (str): Path to slide.
             tile_px (int): Size of tiles to extract, in pixels.
             tile_um (int or str): Size of tiles to extract, in microns (int) or
                 magnification (str, e.g. "20x").
-            stride_div (int, optional): Stride divisor for tile extraction
+            stride_div (float, optional): Stride divisor for tile extraction
                 (1 = no tile overlap; 2 = 50% overlap, etc). Defaults to 1.
             enable_downsample (bool, optional): Allow use of downsampled
                 layers in the slide image pyramid, which greatly improves
                 tile extraction speed. Defaults to True.
             pb (Progress, optional): Progress bar; will update
                 progress bar during tile extraction if provided.
                 Defaults to None.
@@ -2503,16 +2536,17 @@
                 Discard tiles with this fraction of grayspace. If 1, will not
                 perform grayspace filtering.
             grayspace_threshold (float, optional): Range 0-1. Defaults to 0.05.
                 Pixels in HSV format with saturation below this threshold are
                 considered grayspace.
             normalizer (str, optional): Normalization for image tiles.
                 Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
             full_core (bool, optional): Extract an entire detected core, rather
                 than subdividing into image tiles. Defaults to False.
             shuffle (bool): Shuffle images during extraction.
             num_threads (int): Number of threads to allocate to workers.
             yolo (bool, optional): Export yolo-formatted tile-level ROI
                 annotations (.txt) in the tile directory. Requires that
@@ -2562,16 +2596,17 @@
                 Discard tiles with this fraction of grayspace. If 1, will not
                 perform grayspace filtering.
             grayspace_threshold (float, optional): Range 0-1. Defaults to 0.05.
                 Pixels in HSV format with saturation below this threshold are
                 considered grayspace.
             normalizer (str, optional): Normalization to use on image tiles.
                 Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, will use slideflow.slide.norm_tile.jpg
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
             num_threads (int, optional): Number of threads for pool. Unused if
                 `pool` is specified.
             pool (:obj:`multiprocessing.Pool`, optional): Multiprocessing pool
                 to use. By using a shared pool, a slide no longer needs to spin
                 up its own new pool for tile extraction, decreasing tile
                 extraction time for large datasets. Defaults to None (create a
@@ -2600,17 +2635,15 @@
         # Setup normalization
         norm = None if not normalizer else sf.norm.autoselect(
             method=normalizer,
             source=normalizer_source
         )
         # Detect CPU cores if num_threads not specified
         if num_threads is None:
-            num_threads = os.cpu_count()
-            if num_threads is None:
-                num_threads = 8
+            num_threads = sf.util.num_cpu(default=8)
 
         # Shuffle TMAs
         if shuffle:
             random.shuffle(self.object_rects)
 
         # Set whitespace / grayspace fraction to defaults if not provided
         if whitespace_fraction is None:
```

## slideflow/slide/backends/cucim.py

```diff
@@ -234,15 +234,16 @@
         self.dimensions = tuple(self.properties['shape'][0:2][::-1])
         self.levels = []
         for lev in range(self.level_count):
             self.levels.append({
                 'dimensions': self.level_dimensions[lev],
                 'width': self.level_dimensions[lev][0],
                 'height': self.level_dimensions[lev][1],
-                'downsample': self.level_downsamples[lev]
+                'downsample': self.level_downsamples[lev],
+                'level': lev
             })
 
     @property
     def mpp(self):
         return self._mpp
 
     @property
@@ -321,16 +322,16 @@
         region = self.reader.read_region(
             location=base_level_dim,
             size=(int(extract_size[0]), int(extract_size[1])),
             level=downsample_level,
             num_workers=self.num_workers,
         )
         if resize_factor:
-            target_size = (int(extract_size[0] * resize_factor),
-                           int(extract_size[1] * resize_factor))
+            target_size = (int(np.round(extract_size[0] * resize_factor)),
+                           int(np.round(extract_size[1] * resize_factor)))
             if not __cv2_resize__:
                 region = resize(np.asarray(region), target_size)
         # Final conversions
         if flatten and region.shape[-1] == 4:
             region = region[:, :, 0:3]
         if convert and convert.lower() in ('jpg', 'jpeg', 'png', 'numpy'):
             region = cucim2numpy(region)
```

## slideflow/slide/backends/vips.py

```diff
@@ -53,21 +53,31 @@
 
     # Return from buffer, if present.
     if (__vipsreader_path__ == path
        and __vipsreader_args__ == args
        and __vipsreader_kwargs__ == kwargs):
         return __vipsreader__
 
-    # Read a JPEG image.
+    # Read a JPEG/PNG/TIFF image.
     if path_to_ext(path).lower() in ('jpg', 'jpeg', 'png'):
-        reader = _JPGVIPSReader(path, *args, **kwargs)
+        reader = _SingleLevelVIPSReader(path, *args, **kwargs)
 
     # Read a slide image.
     else:
-        reader = _VIPSReader(path, *args, **kwargs)
+        vips_image = vips.Image.new_from_file(path)
+        if (vips_image.get('vips-loader') == 'tiffload'
+            and 'n-pages' in vips_image.get_fields()
+            and 'image-description' in vips_image.get_fields()
+            and vips_image.get('image-description').startswith('Versa')):
+            reader = _VersaVIPSReader(path, *args, **kwargs)
+        elif (vips_image.get('vips-loader') == 'tiffload'
+              and 'n-pages' in vips_image.get_fields()):
+            reader = _MultiPageVIPSReader(path, *args, **kwargs)
+        else:
+            reader = _VIPSReader(path, *args, **kwargs)
 
     # Buffer args and return.
     __vipsreader_path__ = path
     __vipsreader_args__ = args
     __vipsreader_kwargs__ = kwargs
     __vipsreader__ = reader
     return reader
@@ -119,14 +129,108 @@
         bands=3,
         format="uchar"
     )
     vips_image = vips_image.resize(target_px/crop_width)
     return vips2numpy(vips_image)
 
 
+def vips_padded_crop(image, x, y, width, height):
+    bg = [255]
+    if x+width <= image.width and y+height <= image.height:
+        return image.crop(x, y, width, height)
+    elif x+width > image.width and y+height <= image.height:
+        cropped = image.crop(x, y, image.width-x, height)
+        return cropped.gravity('west', width, height, background=bg)
+    elif x+width <= image.width and y+height > image.height:
+        cropped = image.crop(x, y, width, image.height-y)
+        return cropped.gravity('north', width, height, background=bg)
+    elif x+width > image.width and y+height > image.height:
+        cropped = image.crop(x, y, image.width-x, image.height-y)
+        return cropped.gravity('north-west', width, height, background=bg)
+    else:
+        raise errors.SlideError(
+            "Unable to interpret padded crop for image {} at location {}, {} "
+            "and width/height {}, {}.".format(
+                image, x, y, width, height
+            ))
+
+def detect_mpp(
+    path: str,
+    loaded_image: Optional["vips.image.Image"] = None,
+) -> Optional[float]:
+
+    # --- Search VIPS fields ------------------------------------------
+
+    # Load the image with Vips, if not already loaded
+    if loaded_image is None:
+        loaded_image = vips.Image.new_from_file(path)
+
+    vips_fields = loaded_image.get_fields()
+
+    if OPS_MPP_X in vips_fields:
+        return float(loaded_image.get(OPS_MPP_X))
+
+    # Search for MPP using SCN format
+    if (sf.util.path_to_ext(path).lower() == 'svs'
+            and 'image-description' in vips_fields):
+        img_des = loaded_image.get('image-description')
+        _mpp = re.findall(r'(?<=MPP\s\=\s)0\.\d+', img_des)[0]
+        if _mpp is not None:
+            log.debug(
+                f"Using MPP {_mpp} from 'image-description' for SCN"
+                "-converted SVS format"
+            )
+            return float(_mpp)
+
+    # Search for MPP via TIFF EXIF field
+    if (sf.util.path_to_ext(path).lower() in ('tif', 'tiff')
+            and 'xres' in vips_fields):
+        xres = loaded_image.get('xres')  # 4000.0
+        if (xres == 4000.0
+            and 'resolution-unit' in vips_fields
+            and loaded_image.get('resolution-unit') == 'cm'):
+            # xres = xres # though resolution from tiffinfo
+            # says 40000 pixels/cm, for some reason the xres
+            # val is 4000.0, so multipley by 10.
+            # Convert from pixels/cm to cm/pixels, then convert
+            # to microns by multiplying by 1000
+            mpp_x = (1/xres) * 1000
+            log.debug(
+                f"Using MPP {mpp_x} per TIFF 'xres' field"
+                f" {loaded_image.get('xres')} and "
+                f"{loaded_image.get('resolution-unit')}"
+            )
+            return mpp_x
+
+    # --- Search EXIF & tags ------------------------------------------
+    try:
+        with Image.open(path) as img:
+            # Search exif data
+            exif_data = img.getexif()
+            if exif_data and TIF_EXIF_KEY_MPP in exif_data.keys():
+                _mpp = exif_data[TIF_EXIF_KEY_MPP]
+                log.debug(f"Using MPP {_mpp} per EXIF field {TIF_EXIF_KEY_MPP}")
+                return float(_mpp)
+
+            # Search image tags
+            if hasattr(img, 'tag') and TIF_EXIF_KEY_MPP in img.tag.keys():
+                _mpp = img.tag[TIF_EXIF_KEY_MPP][0]
+                log.debug(
+                    f"Using MPP {_mpp} per EXIF {TIF_EXIF_KEY_MPP}"
+                )
+                return float(_mpp)
+    except UnidentifiedImageError:
+        pass
+
+    return None
+
+
+
+# -----------------------------------------------------------------------------
+
 def tile_worker(
     c: List[int],
     args: SimpleNamespace
 ) -> Optional[Union[str, Dict]]:
     '''Multiprocessing worker for WSI. Extracts tile at given coordinates.'''
 
     if args.has_segmentation:
@@ -259,136 +363,150 @@
         image = draw_roi(image, coords)
 
     return_dict.update({'image': image})
     if args.yolo:
         return_dict.update({'yolo': yolo_anns})
     return return_dict
 
+# -----------------------------------------------------------------------------
+
 
 class _VIPSReader:
 
     has_levels = True
 
     def __init__(
         self,
         path: str,
         mpp: Optional[float] = None,
+        *,
         cache_kw: Optional[Dict[str, Any]] = None,
-        ignore_missing_mpp: bool = False
+        ignore_missing_mpp: bool = False,
+        pad_missing: bool = True,
+        loaded_image: Optional["vips.Image"] = None
     ) -> None:
-        '''Wrapper for Libvips to preserve cross-compatible functionality.'''
+        """Libvips slide reader.
+
+        Args:
+            path (str): Path to slide.
+            mpp (float, optional): Forcibly set microns-per-pixel.
 
+        Keyword args:
+            cache_kw (Dict, Optional): Optional keyword arguments for setting
+                up a libvips cache. Keyword arguments are passed to
+                ``pyvips.image.tilecache(**cache_kw)``. If not specified,
+                tile cache is not used. Defaults to None.
+            ignore_missing_mpp (bool): If MPP information cannot be found,
+                do not raise an error. Defaults to False.
+            pad_missing (bool): If an image crop is out-of-bounds for a slide
+                (e.g., an edge tile), pad the image with black. If False,
+                will raise an error if an out-of-bounds area is requested.
+                Defaults to False.
+
+        """
         self.path = path
+        self.pad_missing = pad_missing
         self.cache_kw = cache_kw if cache_kw else {}
         self.loaded_downsample_levels = {}  # type: Dict[int, "vips.Image"]
-        loaded_image = self._load_downsample_level(0)
+        if loaded_image is None:
+            loaded_image = vips.Image.new_from_file(path)
+        self.vips_loader = loaded_image.get('vips-loader')
 
         # Load image properties
         self.properties = {}
         for field in loaded_image.get_fields():
             self.properties.update({field: loaded_image.get(field)})
         self.dimensions = (
             int(self.properties[OPS_WIDTH]),
             int(self.properties[OPS_HEIGHT])
         )
         # If Openslide MPP is not available, try reading from metadata
         if mpp is not None:
             log.debug(f"Setting MPP to {mpp}")
             self.properties[OPS_MPP_X] = mpp
         elif OPS_MPP_X not in self.properties.keys():
-            log.debug(
-                "Microns-Per-Pixel (MPP) not found, Searching EXIF"
-            )
-            try:
-                with Image.open(path) as img:
-                    if TIF_EXIF_KEY_MPP in img.tag.keys():
-                        _mpp = img.tag[TIF_EXIF_KEY_MPP][0]
-                        log.debug(
-                            f"Using MPP {_mpp} per EXIF {TIF_EXIF_KEY_MPP}"
-                        )
-                        self.properties[OPS_MPP_X] = _mpp
-                    elif (sf.util.path_to_ext(path).lower() == 'svs'
-                          and 'image-description' in loaded_image.get_fields()):
-                          img_des = loaded_image.get('image-description')
-                          _mpp = re.findall(r'(?<=MPP\s\=\s)0\.\d+', img_des)
-                          if _mpp is not None:
-                            log.debug(
-                                f"Using MPP {_mpp} from 'image-description' for SCN"
-                                "-converted SVS format"
-                            )
-                            self.properties[OPS_MPP_X] = _mpp[0]
-                    elif (sf.util.path_to_ext(path).lower() in ('tif', 'tiff')
-                          and 'xres' in loaded_image.get_fields()):
-                        xres = loaded_image.get('xres')  # 4000.0
-                        if (xres == 4000.0
-                           and loaded_image.get('resolution-unit') == 'cm'):
-                            # xres = xres # though resolution from tiffinfo
-                            # says 40000 pixels/cm, for some reason the xres
-                            # val is 4000.0, so multipley by 10.
-                            # Convert from pixels/cm to cm/pixels, then convert
-                            # to microns by multiplying by 1000
-                            mpp_x = (1/xres) * 1000
-                            self.properties[OPS_MPP_X] = str(mpp_x)
-                            log.debug(
-                                f"Using MPP {mpp_x} per TIFF 'xres' field"
-                                f" {loaded_image.get('xres')} and "
-                                f"{loaded_image.get('resolution-unit')}"
-                            )
-                    else:
-                        name = path_to_name(path)
-                        log.warning(
-                            f"Missing Microns-Per-Pixel (MPP) for {name}"
-                        )
-            except AttributeError:
-                if ignore_missing_mpp:
-                    mpp = DEFAULT_JPG_MPP
-                    log.debug(f"Could not detect microns-per-pixel; using default {mpp}")
-                    self.properties[OPS_MPP_X] = mpp
-                else:
-                    raise errors.SlideMissingMPPError(
-                        f'Could not detect microns-per-pixel for slide: {path}'
-                    )
-            except UnidentifiedImageError:
-                log.error(
-                    f"PIL error; unable to read slide {path_to_name(path)}."
+            log.debug("Microns-Per-Pixel (MPP) not found, Searching EXIF")
+            mpp = detect_mpp(path, loaded_image)
+            if mpp is not None:
+                self.properties[OPS_MPP_X] = mpp
+            elif ignore_missing_mpp:
+                self.properties[OPS_MPP_X] = DEFAULT_JPG_MPP
+                log.debug(f"Could not detect microns-per-pixel; using default "
+                          f"{DEFAULT_JPG_MPP}")
+            else:
+                raise errors.SlideMissingMPPError(
+                    f'Could not detect microns-per-pixel for slide: {path}'
                 )
 
+        # Load levels
+        self._load_levels(loaded_image)
+
+    @property
+    def mpp(self):
+        return self.properties[OPS_MPP_X]
+
+    def _load_levels(self, vips_image: Optional["vips.Image"]):
+        """Load downsample levels."""
+
+        if vips_image is None:
+            vips_image = vips.Image.new_from_file(self.path)
+
         if OPS_LEVEL_COUNT in self.properties:
             self.level_count = int(self.properties[OPS_LEVEL_COUNT])
             # Calculate level metadata
             self.levels = []   # type: List[Dict[str, Any]]
             for lev in range(self.level_count):
-                width = int(loaded_image.get(OPS_LEVEL_WIDTH(lev)))
-                height = int(loaded_image.get(OPS_LEVEL_HEIGHT(lev)))
-                downsample = float(loaded_image.get(OPS_LEVEL_DOWNSAMPLE(lev)))
+                width = int(vips_image.get(OPS_LEVEL_WIDTH(lev)))
+                height = int(vips_image.get(OPS_LEVEL_HEIGHT(lev)))
+                downsample = float(vips_image.get(OPS_LEVEL_DOWNSAMPLE(lev)))
                 self.levels += [{
                     'dimensions': (width, height),
                     'width': width,
                     'height': height,
-                    'downsample': downsample
+                    'downsample': downsample,
+                    'level': lev
                 }]
+        elif 'n-pages' in self.properties and OPS_LEVEL_COUNT not in self.properties:
+            log.debug("Attempting to read non-standard multi-page TIFF")
+            # This is a multipage tiff without openslide metadata.
+            # Ignore the last 2 pages, which per our experimentation,
+            # are likely to be the slide label and image thumbnail.
+            self.level_count = min(int(self.properties['n-pages']) - 3, 1)
+            # Calculate level metadata
+            self.levels = []
+            for lev in range(self.level_count):
+                temp_img = vips.Image.new_from_file(self.path, page=lev)
+                width = int(temp_img.get('width'))
+                height = int(temp_img.get('height'))
+                downsample = float(self.dimensions[0] / width)
+                self.levels += [{
+                    'dimensions': (width, height),
+                    'width': width,
+                    'height': height,
+                    'downsample': downsample,
+                    'level': lev
+                }]
+            self.levels = sorted(self.levels, key=lambda x: x['width'], reverse=True)
+
         else:
             self.level_count = 1
             self.levels = [{
                     'dimensions': self.dimensions,
                     'width': self.dimensions[0],
                     'height': self.dimensions[1],
-                    'downsample': 1
+                    'downsample': 1,
+                    'level': 0
                 }]
         self.level_downsamples = [lev['downsample'] for lev in self.levels]
         self.level_dimensions = [lev['dimensions'] for lev in self.levels]
 
-    @property
-    def mpp(self):
-        return self.properties[OPS_MPP_X]
-
     def _load_downsample_level(self, level: int) -> "vips.Image":
         image = self.read_level(level=level)
         if self.cache_kw:
-            image = image.tilecache(**self.cache_kw)
+            image = image.tilecache(**self.cache_kw)  # type: ignore
         self.loaded_downsample_levels.update({
             level: image
         })
         return image
 
     def best_level_for_downsample(
         self,
@@ -428,17 +546,23 @@
             return False
 
     def read_level(
         self,
         fail: bool = True,
         access=vips.enums.Access.RANDOM,
         to_numpy: bool = False,
+        level: Optional[int] = None,
         **kwargs
     ) -> Union[vips.Image, np.ndarray]:
         """Read a pyramid level."""
+
+        if self.properties['vips-loader'] == 'tiffload' and level is not None:
+            kwargs['page'] = self.levels[level]['level']
+        elif level is not None:
+            kwargs['level'] = level
         image = vips.Image.new_from_file(self.path, fail=fail, access=access, **kwargs)
         if to_numpy:
             return vips2numpy(image)
         else:
             return image
 
     def read_region(
@@ -464,20 +588,24 @@
         """
         base_level_x, base_level_y = base_level_dim
         extract_width, extract_height = extract_size
         downsample_factor = self.level_downsamples[downsample_level]
         downsample_x = int(base_level_x / downsample_factor)
         downsample_y = int(base_level_y / downsample_factor)
         image = self.get_downsampled_image(downsample_level)
-        region = image.crop(
+        crop_args = (
             downsample_x,
             downsample_y,
             extract_width,
             extract_height
         )
+        if self.pad_missing:
+            region = vips_padded_crop(image, *crop_args)
+        else:
+            region = image.crop(*crop_args)
         # Final conversions
         if flatten and region.bands == 4:
             region = region.flatten()
         if resize_factor is not None:
             region = region.resize(resize_factor)
         if convert and convert.lower() in ('jpg', 'jpeg'):
             return vips2jpg(region)
@@ -513,20 +641,24 @@
             In this case, the returned image will be cropped.
         """
         target_downsample = window_size[0] / target_size[0]
         ds_level = self.best_level_for_downsample(target_downsample)
         image = self.get_downsampled_image(ds_level)
         resize_factor = self.level_downsamples[ds_level] / target_downsample
         image = image.resize(resize_factor)
-        image = image.crop(
+        crop_args = (
             int(top_left[0] / target_downsample),
             int(top_left[1] / target_downsample),
             min(target_size[0], image.width),
             min(target_size[1], image.height)
         )
+        if self.pad_missing:
+            image = vips_padded_crop(image, *crop_args)
+        else:
+            image = image.crop(*crop_args)
         # Final conversions
         if flatten and image.bands == 4:
             image = image.flatten()
         if convert and convert.lower() in ('jpg', 'jpeg'):
             return vips2jpg(image)
         elif convert and convert.lower() == 'png':
             return vips2png(image)
@@ -540,79 +672,174 @@
         width: int = 512,
         fail: bool = True,
         access = vips.enums.Access.RANDOM,
         **kwargs
     ) -> np.ndarray:
         """Return thumbnail of slide as numpy array."""
 
-        if (OPS_VENDOR in self.properties and self.properties[OPS_VENDOR] == 'leica'):
+        if ((OPS_VENDOR in self.properties and self.properties[OPS_VENDOR] == 'leica')
+           or (self.vips_loader == 'tiffload')):
             thumbnail = self.read_level(fail=fail, access=access, **kwargs)
         else:
             thumbnail = vips.Image.thumbnail(self.path, width)
         try:
             return vips2numpy(thumbnail)
         except vips.error.Error as e:
             raise sf.errors.SlideLoadError(f"Error loading slide thumbnail: {e}")
 
-class _JPGVIPSReader(_VIPSReader):
+
+class _MultiPageVIPSReader(_VIPSReader):
+
+    def _load_levels(self, vips_image: Optional["vips.Image"]):
+        """Load downsample levels."""
+        log.debug("Attempting to read levels from non-standard multi-page TIFF")
+        # This is a multipage tiff without openslide metadata.
+        # Ignore the last 2 pages, which per our experimentation,
+        # are likely to be the slide label and image thumbnail.
+        self.level_count = int(self.properties['n-pages'])
+        # Calculate level metadata
+        self.levels = []
+        for lev in range(self.level_count):
+            temp_img = vips.Image.new_from_file(self.path, page=lev)
+            width = int(temp_img.get('width'))
+            height = int(temp_img.get('height'))
+            downsample = float(self.dimensions[0] / width)
+            self.levels += [{
+                'dimensions': (width, height),
+                'width': width,
+                'height': height,
+                'downsample': downsample,
+                'level': lev
+            }]
+        self.levels = sorted(self.levels, key=lambda x: x['width'], reverse=True)
+        log.debug(f"Read {self.level_count} levels.")
+        self.level_downsamples = [lev['downsample'] for lev in self.levels]
+        self.level_dimensions = [lev['dimensions'] for lev in self.levels]
+
+
+class _VersaVIPSReader(_VIPSReader):
+
+    def _load_levels(self, vips_image: Optional["vips.Image"]):
+        """Load downsample levels."""
+        log.debug("Attempting to read levels from Versa multi-page image")
+        # This is a multipage tiff without openslide metadata.
+        # Ignore the last 2 pages, which per our experimentation,
+        # are likely to be the slide label and image thumbnail.
+        all_lev = self.level_count = max(int(self.properties['n-pages']) - 2, 1)
+        # Calculate level metadata
+        self.levels = []
+        for lev in range(self.level_count):
+            temp_img = vips.Image.new_from_file(self.path, page=lev)
+            width = int(temp_img.get('width'))
+            height = int(temp_img.get('height'))
+            downsample = float(self.dimensions[0] / width)
+            self.levels += [{
+                'dimensions': (width, height),
+                'width': width,
+                'height': height,
+                'downsample': downsample,
+                'level': lev
+            }]
+        self.levels = sorted(self.levels, key=lambda x: x['width'], reverse=True)
+        log.debug(f"Read {self.level_count} of {all_lev} levels.")
+        self.level_downsamples = [lev['downsample'] for lev in self.levels]
+        self.level_dimensions = [lev['dimensions'] for lev in self.levels]
+
+    def thumbnail(self, width: int = 512, *args, **kwargs) -> np.ndarray:
+        """Return thumbnail of slide as numpy array."""
+        vips_image = vips.Image.new_from_file(self.path, page=1)
+        np_image = vips2numpy(vips_image)
+        width_height_ratio = np_image.shape[1] / np_image.shape[0]
+        height = int(width / width_height_ratio)
+        return cv2.resize(np_image, (width, height))
+
+
+class _SingleLevelVIPSReader(_VIPSReader):
     '''Wrapper for JPG files, which do not possess separate levels, to
     preserve openslide-like functions.'''
 
     has_levels = False
 
     def __init__(
         self,
         path: str,
         mpp: Optional[float] = None,
         cache_kw = None,
         ignore_missing_mpp: bool = True,
-        pad_missing: bool = True
+        pad_missing: bool = True,
+        loaded_image: Optional["vips.Image"] = None
     ) -> None:
         self.path = path
         self.pad_missing = pad_missing
-        self.full_image = vips.Image.new_from_file(path)
+        if loaded_image is None:
+            loaded_image = vips.Image.new_from_file(path)
         self.cache_kw = cache_kw if cache_kw else {}
-        if not self.full_image.hasalpha():
-            self.full_image = self.full_image.addalpha()
+        if not loaded_image.hasalpha():
+            loaded_image = loaded_image.addalpha()
         self.properties = {}
-        for field in self.full_image.get_fields():
-            self.properties.update({field: self.full_image.get(field)})
+        for field in loaded_image.get_fields():
+            self.properties.update({field: loaded_image.get(field)})
         width = int(self.properties[OPS_WIDTH])
         height = int(self.properties[OPS_HEIGHT])
         self.dimensions = (width, height)
+        self.vips_loader = loaded_image.get('vips-loader')
         self.level_count = 1
         self.loaded_downsample_levels = {
-            0: self.full_image
+            0: loaded_image
         }
         # Calculate level metadata
         self.levels = [{
             'dimensions': (width, height),
             'width': width,
             'height': height,
             'downsample': 1,
+            'level': 0
         }]
         self.level_downsamples = [1]
         self.level_dimensions = [(width, height)]
 
-        # MPP data
+        # If MPP is not provided, try reading from metadata
         if mpp is not None:
             log.debug(f"Setting MPP to {mpp}")
             self.properties[OPS_MPP_X] = mpp
-        else:
-            try:
-                with Image.open(path) as img:
-                    exif_data = img.getexif()
-                    if TIF_EXIF_KEY_MPP in exif_data.keys():
-                        _mpp = exif_data[TIF_EXIF_KEY_MPP]
-                        log.debug(f"Using MPP {_mpp} per EXIF field {TIF_EXIF_KEY_MPP}")
-                        self.properties[OPS_MPP_X] = _mpp
-                    else:
-                        raise AttributeError
-            except AttributeError:
-                if ignore_missing_mpp:
-                    mpp = DEFAULT_JPG_MPP
-                    log.debug(f"Could not detect microns-per-pixel; using default {mpp}")
-                    self.properties[OPS_MPP_X] = mpp
-                else:
-                    raise errors.SlideMissingMPPError(
-                        f'Could not detect microns-per-pixel for slide: {path}'
-                    )
+        elif OPS_MPP_X not in self.properties.keys():
+            log.debug("Microns-Per-Pixel (MPP) not found, Searching EXIF")
+            mpp = detect_mpp(path, loaded_image)
+            if mpp is not None:
+                self.properties[OPS_MPP_X] = mpp
+            elif ignore_missing_mpp:
+                self.properties[OPS_MPP_X] = DEFAULT_JPG_MPP
+                log.debug(f"Could not detect microns-per-pixel; using default "
+                          f"{DEFAULT_JPG_MPP}")
+            else:
+                raise errors.SlideMissingMPPError(
+                    f'Could not detect microns-per-pixel for slide: {path}'
+                )
+
+    def _load_downsample_level(self, level: int = 0) -> "vips.Image":
+        if level:
+            raise ValueError(f"_SingleLevelVipsReader does not support levels")
+        image = self.read_level()
+        if self.cache_kw:
+            image = image.tilecache(**self.cache_kw)  # type: ignore
+        self.loaded_downsample_levels.update({
+            level: image
+        })
+        return image
+
+    def read_level(
+        self,
+        fail: bool = True,
+        access=vips.enums.Access.RANDOM,
+        to_numpy: bool = False,
+        level: Optional[int] = None,
+        **kwargs
+    ) -> Union[vips.Image, np.ndarray]:
+        """Read a pyramid level."""
+        if level:
+            raise ValueError(f"_SingleLevelVipsReader does not support levels")
+        return super().read_level(
+            fail=fail,
+            access=access,
+            to_numpy=to_numpy,
+            **kwargs
+        )
```

## slideflow/slide/qc/__init__.py

```diff
@@ -1,4 +1,6 @@
 from .otsu import Otsu
 from .gaussian import Gaussian
+from .gaussian_v2 import GaussianV2
 from .saver import Save, Load
-from .strided_dl import StridedDL
+from .strided_dl import StridedDL
+from .deepfocus import DeepFocus
```

## slideflow/slide/qc/gaussian.py

```diff
@@ -2,14 +2,16 @@
 
 import numpy as np
 import slideflow as sf
 import skimage
 from slideflow import errors
 from typing import Union, Optional
 
+# -----------------------------------------------------------------------------
+
 class Gaussian:
 
     def __init__(
         self,
         mpp: Optional[float] = None,
         sigma: int = 3,
         threshold: float = 0.02
@@ -28,16 +30,16 @@
 
                 .. code-block:: python
 
                     import slideflow as sf
                     from slideflow.slide import qc
 
                     wsi = sf.WSI(...)
-                    otsu = qc.Otsu()
-                    wsi.qc(otsu)
+                    gaussian = qc.Gaussian()
+                    wsi.qc(gaussian)
 
         Args:
             mpp (float): Microns-per-pixel at which to perform filtering.
                 Defaults to 4 times the tile extraction MPP (e.g. for a
                 tile_px/tile_um combination at 10X effective magnification,
                 where tile_px=tile_um, the default blur_mpp would be 4, or
                 effective magnification 2.5x).
@@ -106,21 +108,25 @@
         gaussian = skimage.filters.gaussian(img_laplace, sigma=self.sigma)
         blur_mask = gaussian <= self.threshold
 
         # Assign blur burden value
         existing_qc_mask = mask
         if mask is None and isinstance(wsi, sf.WSI):
             existing_qc_mask = wsi.qc_mask
-        if existing_qc_mask is not None:
-            blur_mask = skimage.transform.resize(blur_mask, existing_qc_mask.shape)
-            blur_mask = blur_mask.astype(bool)
-            blur = np.count_nonzero(
-                np.logical_and(
-                    blur_mask,
-                    np.logical_xor(blur_mask, existing_qc_mask)
-                )
-            )
-            if isinstance(wsi, sf.WSI):
-                wsi.blur_burden = blur / (blur_mask.shape[0] * blur_mask.shape[1])
-                sf.log.debug(f"Blur burden: {wsi.blur_burden}")
-
-        return blur_mask
+        if existing_qc_mask is not None and isinstance(wsi, sf.WSI):
+            wsi.blur_burden = blur_burden(blur_mask, existing_qc_mask)
+            sf.log.debug(f"Blur burden: {wsi.blur_burden}")
+
+        return blur_mask
+
+# -----------------------------------------------------------------------------
+
+def blur_burden(blur_mask, existing_mask):
+    blur_mask = skimage.transform.resize(blur_mask, existing_mask.shape)
+    blur_mask = blur_mask.astype(bool)
+    blur = np.count_nonzero(
+        np.logical_and(
+            blur_mask,
+            np.logical_xor(blur_mask, existing_mask)
+        )
+    )
+    return blur / (blur_mask.shape[0] * blur_mask.shape[1])
```

## slideflow/slide/qc/otsu.py

```diff
@@ -14,14 +14,29 @@
         (~mask).astype(np.uint8),
         (image.shape[1], image.shape[0]),
         interpolation=cv2.INTER_NEAREST
     )
     return cv2.bitwise_or(image, image, mask=resized_mask)
 
 
+def _get_level_for_otsu(wsi: "sf.WSI", min_size: int = 500) -> int:
+    """Find the smallest downsample level of a minimum size."""
+    smallest_dim = np.array([min(L['dimensions']) for L in wsi.levels])
+    level_ids = np.array([L['level'] for L in wsi.levels])
+    sorted_idx = np.argsort(smallest_dim)
+    try:
+        best_idx = np.where(smallest_dim[sorted_idx] > min_size)[0][0]
+    except IndexError:
+        # If the slide is smaller than the target minimum dimension,
+        # use the full slide image
+        best_idx = sorted_idx[-1]
+    return level_ids[sorted_idx][best_idx]
+
+# -----------------------------------------------------------------------------
+
 class Otsu:
 
     def __init__(self, slide_level: Optional[int] = None):
         """Prepare Otsu's thresholding algorithm for filtering a slide.
 
         This method is used to detect areas of tissue and remove background.
 
@@ -43,15 +58,15 @@
 
             .. code-block:: python
 
                     import slideflow as sf
                     from slideflow.slide import qc
 
                     wsi = sf.WSI(...)
-                    gaussian = qc.Gaussian()
+                    gaussian = qc.GaussianV2()
                     otsu = qc.Otsu()
                     wsi.qc([gaussian, otsu])
 
         Examples
             Apply Otsu's thresholding to a slide.
 
                 .. code-block:: python
@@ -83,16 +98,17 @@
         Args:
             wsi (sf.WSI): Whole-slide image.
 
         Returns:
             np.ndarray: RGB thumbnail of the whole-slide image.
         """
         if self.level is None:
-            # Otsu's thresholding can be done on the lowest downsample level
-            level = wsi.slide.level_count - 1
+            # Otsu's thresholding can be done on the smallest downsample level,
+            # with the smallest dimension being at least 500 pixels
+            level = _get_level_for_otsu(wsi, min_size=500)
         else:
             level = self.level
 
         try:
             if wsi.slide.has_levels:
                 thumb = wsi.slide.read_level(level=level, to_numpy=True)
             else:
```

## slideflow/slide/qc/strided_dl.py

```diff
@@ -1,22 +1,26 @@
 import numpy as np
-import slideflow as sf
 
-from typing import Callable, Union
-from tqdm import tqdm
+from contextlib import contextmanager
+from typing import Callable, Union, Optional, TYPE_CHECKING
+from .strided_qc import _StridedQC
 
+if TYPE_CHECKING:
+    import slideflow as sf
 
-class StridedDL:
+
+class StridedDL(_StridedQC):
 
     def __init__(
         self,
         model: Callable,
         pred_idx: int,
         tile_px: int,
         tile_um: Union[str, int],
+        *,
         buffer: int = 8,
         verbose: bool = False,
         pred_threshold: float = 0.5,
         **wsi_kwargs
     ):
         """QC function which uses a deep learning model to generate a QC mask.
 
@@ -67,66 +71,83 @@
 
                     wsi = sf.WSI(...)
                     deepfocus = DeepFocus()
                     wsi.qc(deepfocus)
 
 
         Args:
-            dest (str, optional): Path in which to save the qc mask.
-                If None, will save in the same directory as the slide.
-                Defaults to None.
+            model (callable): Deep learning model.
+            pred_idx (int): Index of the model output to interpret as the
+                final prediction.
+            tile_px (int): Tile size.
+            tile_um (str or float): Tile size, in microns (int) or
+                magnification (str).
+
+        Keyword arguments:
+            verbose (bool): Show a progress bar during calculation.
+            buffer (int): Number of tiles (width and height) to extract and
+                process simultaneously. Extracted tile size (width/height)
+                will be  ``tile_px * buffer``. Defaults to 8.
+            grayspace_fraction (float): Grayspace fraction when extracting
+                tiles from slides. Defaults to 1 (disables).
+            pred_threshold (float): Predictions below this value are masked.
+            kwargs (Any): All remaining keyword arguments are passed to
+                :meth:`slideflow.WSI.build_generator()`.
+
         """
-        self.buffer = buffer
-        self.kernel = tile_px
-        self.tile_um = tile_um
-        self.verbose = verbose
-        self.wsi_kwargs = wsi_kwargs
+        super().__init__(
+            tile_px=tile_px,
+            tile_um=tile_um,
+            buffer=buffer,
+            verbose=verbose,
+            lazy_iter=True,
+            deterministic=False,
+            **wsi_kwargs
+        )
         self.model = model
         self.pred_idx = pred_idx
         self.pred_threshold = pred_threshold
 
+    def build_mask(self, x, y) -> np.ndarray:
+        """Build the base, empty QC mask."""
+        return np.ones((x, y), dtype=np.float32)
+
+    def apply(self, image: np.ndarray) -> np.ndarray:
+        """Predict focus value of an image tile using DeepFocus model."""
+        y_pred = self.model(image, training=False)[:, self.pred_idx].numpy()
+        return y_pred.reshape(self.buffer, self.buffer)
+
+    def collate_mask(self, mask: np.ndarray):
+        """Convert the mask from predictions to bool using a threshold."""
+        if self.pred_threshold is not None:
+            return mask > self.pred_threshold
+        else:
+            return mask
+
+    def preprocess(self, image: np.ndarray):
+        """Apply preprocessing to an image."""
+        return np.clip(image.astype(np.float32) / 255, 0, 1)
+
+    @contextmanager
+    def _set_threshold(self, threshold: Optional[Union[bool, float]]):
+        """Temporariliy set or disable the prediction threshold."""
+        _orig_threshold = self.pred_threshold
+        if isinstance(threshold, float):
+            # Set the threshold to a given threshold
+            self.pred_threshold = threshold
+        elif threshold is False:
+            # Disable thresholding (return raw values)
+            self.pred_threshold = None
+
+        yield
+
+        # Return the threshold to irs original value
+        self.pred_threshold = _orig_threshold
+
     def __call__(
         self,
         wsi: "sf.WSI",
-    ) -> np.ndarray:
-
-        # Initialize whole-slide reader.
-        b = self.buffer
-        k = self.kernel
-        qc_wsi = sf.WSI(wsi.path, tile_px=(k * b), tile_um=self.tile_um, verbose=False)
-        existing_mask = wsi.qc_mask
-        if existing_mask is not None:
-            qc_wsi.apply_qc_mask(existing_mask)
-
-        # Build tile generator.
-        dts = qc_wsi.build_generator(
-            shuffle=False,
-            show_progress=False,
-            img_format='numpy',
-            **self.wsi_kwargs)()
-
-        # Generate prediction for slide.
-        focus_mask = np.ones((qc_wsi.grid.shape[1] * b,
-                              qc_wsi.grid.shape[0] * b),
-                             dtype=np.float32)
-        if self.verbose:
-            pb = tqdm(dts, desc="Generating...", total=qc_wsi.estimated_num_tiles)
-        else:
-            pb = dts
-        for item in pb:
-            img = np.clip(item['image'].astype(np.float32) / 255, 0, 1)
-            sz = img.itemsize
-            grid_i = item['grid'][1]
-            grid_j = item['grid'][0]
-            batch = np.lib.stride_tricks.as_strided(img,
-                                                    shape=(b, b, k, k, 3),
-                                                    strides=(k * sz * 3 * k * b,
-                                                            k * sz * 3,
-                                                            sz * 3 * k * b,
-                                                            sz * 3,
-                                                            sz))
-            batch = batch.reshape(batch.shape[0] * batch.shape[1], *batch.shape[2:])
-            y_pred = self.model(batch)[:, self.pred_idx].numpy()
-            predictions = y_pred.reshape(b, b)
-            focus_mask[grid_i * b: grid_i * b + b, grid_j * b: grid_j * b + b] = predictions
+        threshold: Optional[Union[bool, float]] = None
+    ) -> Optional[np.ndarray]:
 
-        return focus_mask > self.pred_threshold
+        with self._set_threshold(threshold):
+            return super().__call__(wsi)
```

## slideflow/stats/metrics.py

```diff
@@ -316,16 +316,14 @@
 def concordance_index(y_true: np.ndarray, y_pred: np.ndarray) -> float:
     '''Calculates concordance index from a given y_true and y_pred.'''
     E = y_pred[:, -1]
     y_pred = y_pred[:, :-1]
     y_pred = y_pred.flatten()
     E = E.flatten()
     y_true = y_true.flatten()
-    # Need -1 * concordance index, since these are log hazard ratios
-    y_pred = - y_pred
     return c_index(y_true, y_pred, E)
 
 
 def cph_metrics(
     df: DataFrame,
     level: str = 'tile',
     label: str = '',
```

## slideflow/stats/slidemap.py

```diff
@@ -485,18 +485,19 @@
                 to center of grid space. If 'centroid', for each grid, will
                 calculate which tile is nearest to centroid tile_meta.
                 Defaults to 'nearest'.
             tile_meta (dict, optional): Tile metadata, used for tile_select.
                 Dictionary should have slide names as keys, mapped to list of
                 metadata (length of list = number of tiles in slide).
                 Defaults to None.
-            normalizer ((str or `slideflow.norm.StainNormalizer`), optional):
+            normalizer ((str or :class:`slideflow.norm.StainNormalizer`), optional):
                 Normalization strategy to use on image tiles. Defaults to None.
-            normalizer_source (str, optional): Path to normalizer source image.
-                If None, normalizer will use slideflow.slide.norm_tile.jpg.
+            normalizer_source (str, optional): Stain normalization preset or
+                path to a source image. Valid presets include 'v1', 'v2', and
+                'v3'. If None, will use the default present ('v3').
                 Defaults to None.
 
         """
         if self.ftrs is None and tfrecords is None:
             raise ValueError(
                 "If SlideMap was not created using DatasetFeatures, then the "
                 "`tfrecords` argument (list of TFRecord paths) must be supplied "
@@ -506,15 +507,17 @@
                and tfrecords is None):
             raise ValueError(
                 "The DatasetFeatures object used to create this SlideMap "
                 "did not have paths to TFRecords stored. Please supply a list "
                 "of TFRecord paths to the `tfrecords` argument "
                 "of `SlideMap.build_mosaic()`"
             )
-        elif self.ftrs is not None and len(self.ftrs.tfrecords):
+        elif (tfrecords is None
+             and self.ftrs is not None
+             and len(self.ftrs.tfrecords)):
             return sf.Mosaic(self, tfrecords=self.ftrs.tfrecords, **kwargs)
         else:
             return sf.Mosaic(self, tfrecords=tfrecords, **kwargs)
 
     def cluster(self, n_clusters: int) -> None:
         """Performs K-means clustering on data and adds to metadata labels.
 
@@ -745,16 +748,19 @@
         cmap: Optional[Dict] = None,
         xlim: Tuple[float, float] = (-0.05, 1.05),
         ylim: Tuple[float, float] = (-0.05, 1.05),
         xlabel: Optional[str] = None,
         ylabel: Optional[str] = None,
         legend: Optional[str] = None,
         ax: Optional["Axes"] = None,
+        loc: Optional[str] = 'center right',
+        ncol: Optional[int] = 1,
         categorical: Union[str, bool] = 'auto',
-        **scatter_kwargs: Any
+        legend_kwargs: Optional[Dict] = None,
+        **scatter_kwargs: Any,
     ) -> None:
         """Plots calculated map.
 
         Args:
             subsample (int, optional): Subsample to only include this many
                 tiles on plot. Defaults to None.
             title (str, optional): Title for plot.
@@ -764,23 +770,32 @@
             ylim (list, optional): List of float indicating limit for y-axis.
                 Defaults to (-0.05, 1.05).
             xlabel (str, optional): Label for x axis. Defaults to None.
             ylabel (str, optional): Label for y axis. Defaults to None.
             legend (str, optional): Title for legend. Defaults to None.
             ax (matplotlib.axes.Axes, optional): Figure axis. If not supplied,
                 will prepare a new figure axis.
+            loc (str, optional): Location for legend, as defined by
+                matplotlib.axes.Axes.legend(). Defaults to 'center right'.
+            ncol (int, optional): Number of columns in legend, as defined
+                by matplotlib.axes.Axes.legend(). Defaults to 1.
             categorical (str, optional): Specify whether labels are categorical.
                 Determines the colormap.  Defaults to 'auto' (will attempt to
                 automatically determine from the labels).
+            legend_kwargs (dict, optional): Dictionary of additional keyword
+                arguments to the matplotlib.axes.Axes.legend() function.
             **scatter_kwargs (optional): Additional keyword arguments to the
-                seaborn scatterplot function.
+                 seaborn scatterplot function.
         """
         import seaborn as sns
         import matplotlib.pyplot as plt
 
+        if legend_kwargs is None:
+            legend_kwargs = dict()
+
         # Make plot
         if ax is None:
             fig = plt.figure(figsize=(6, 4.5))
             ax = fig.add_subplot(111)
 
         # Subsampling
         if subsample:
@@ -826,17 +841,18 @@
             ax=ax,
             **scatter_kwargs
         )
         ax.set_ylim(*((None, None) if not ylim else ylim))
         ax.set_xlim(*((None, None) if not xlim else xlim))
         if 'hue' in scatter_kwargs:
             ax.legend(
-                loc='center right',
-                ncol=1,
-                title=legend
+                loc=loc,
+                ncol=ncol,
+                title=legend,
+                **legend_kwargs
             )
         umap_2d.set(xlabel=xlabel, ylabel=ylabel)
         if title:
             ax.set_title(title)
 
     def plot_3d(
         self,
```

## slideflow/stats/stats_utils.py

```diff
@@ -85,13 +85,24 @@
     return clipped, (_min, _max), (mins, maxs)
 
 def normalize(
     array: np.ndarray,
     norm_range: Tuple[np.ndarray, np.ndarray],
     norm_clip: Tuple[np.ndarray, np.ndarray],
 ) -> np.ndarray:
+    """Normalize and clip an array."""
     _min, _max = norm_range
     mins, maxs = norm_clip
     clipped = np.clip(array, mins, maxs)
     clipped -= _min
     clipped /= (_max - _min)
     return clipped
+
+def denormalize(
+    array: np.ndarray,
+    norm_range: Tuple[np.ndarray, np.ndarray],
+) -> np.ndarray:
+    """De-normalize an array."""
+    _min, _max = norm_range
+    transformed = array * (_max - _min)
+    transformed += _min
+    return transformed
```

## slideflow/studio/__init__.py

```diff
@@ -108,21 +108,22 @@
         self._defer_tile_refresh = None
         self._should_close_slide = False
         self._should_close_model = False
         self._bg_logo           = None
         self.low_memory         = low_memory
 
         # Interface.
-        self._show_about        = False
-        self._show_performance  = False
-        self._show_tile_preview = False
-        self._tile_preview_is_new = True
+        self._show_about                = False
+        self._show_tile_preview         = False
+        self._tile_preview_is_new       = True
         self._tile_preview_image_is_new = True
-        self._show_overlays     = True
-        self.theme              = theme
+        self._show_overlays             = True
+        self._show_mpp_zoom_popup       = False
+        self._input_mpp                 = 1.
+        self.theme                      = theme
 
         # Widget interface.
         self.wsi                = None
         self.wsi_thumb          = None
         self.viewer             = None
         self.saliency           = None
         self.box_x              = None
@@ -316,14 +317,43 @@
                 if imgui_utils.button('Copy', width=self.button_w/2):
                     pyperclip.copy(about_text)
                 imgui.same_line(imgui.get_content_region_max()[0] + self.spacing - self.button_w/2)
                 if imgui_utils.button('Close', width=self.button_w/2):
                     self._show_about = False
                 imgui.end_popup()
 
+    def _draw_mpp_zoom_dialog(self):
+        """Show a dialog that prompts the user to specify microns-per-pixel."""
+        if not self._show_mpp_zoom_popup:
+            return
+
+        window_size = (self.font_size * 18, self.font_size * 7)
+        self.center_next_window(*window_size)
+        imgui.set_next_window_size(*window_size)
+        _, opened = imgui.begin('Zoom to Microns-Per-Pixel (MPP)', closable=True, flags=imgui.WINDOW_NO_RESIZE)
+        if not opened:
+            self._show_mpp_zoom_popup = False
+
+        imgui.text("Zoom the current view to a given MPP.")
+        imgui.separator()
+        imgui.text('')
+        imgui.same_line(self.font_size*4)
+        with imgui_utils.item_width(self.font_size*4):
+            _changed, self._input_mpp = imgui.input_float('MPP##input_mpp', self._input_mpp, format='%.3f')
+        imgui.same_line()
+        if self._input_mpp:
+            mag = f'{10/self._input_mpp:.1f}x'
+        else:
+            mag = '-'
+        imgui.text(mag)
+        if self.sidebar.full_button("Zoom", width=-1):
+            self.viewer.zoom_to_mpp(window_size[0] / 2, window_size[1] / 2, self._input_mpp)
+            self._show_mpp_zoom_popup = False
+        imgui.end()
+
     def _draw_control_pane(self) -> None:
         """Draw the control pane and widgets."""
         self.sidebar.draw()
 
     def _draw_empty_background(self):
         """Render an empty background with the Studio logo."""
         if self._bg_logo is None:
@@ -460,16 +490,14 @@
             if imgui.begin_menu('View', True):
                 if imgui.menu_item('Fullscreen', 'Ctrl+F')[0]:
                     self.toggle_fullscreen()
                 imgui.separator()
 
                 # --- Show sub-menu -------------------------------------------
                 if imgui.begin_menu('Show', True):
-                    if imgui.menu_item('Performance', 'Ctrl+Shift+P', selected=self._show_performance)[0]:
-                        self._show_performance = not self._show_performance
                     if imgui.menu_item('Tile Preview', 'Ctrl+Shift+T', selected=self._show_tile_preview)[0]:
                         self._show_tile_preview = not self._show_tile_preview
                     imgui.separator()
                     if imgui.menu_item('Thumbnail', selected=(has_wsi and self.viewer.show_thumbnail), enabled=has_wsi)[0]:
                         self.viewer.show_thumbnail = not self.viewer.show_thumbnail
                     if imgui.menu_item('Scale', selected=(has_wsi and self.viewer.show_scale), enabled=has_wsi)[0]:
                         self.viewer.show_scale = not self.viewer.show_scale
@@ -490,14 +518,16 @@
                     self.decrease_font_size()
 
                 imgui.separator()
                 if imgui.menu_item('Increase Tile Zoom', 'Ctrl+]')[1]:
                     self.increase_tile_zoom()
                 if imgui.menu_item('Decrease Tile Zoom', 'Ctrl+[')[1]:
                     self.decrease_tile_zoom()
+                if imgui.menu_item('Zoom to MPP', 'Ctrl+/')[1]:
+                    self.ask_zoom_to_mpp()
                 if imgui.menu_item('Reset Tile Zoom', 'Ctrl+\\')[1]:
                     self.reset_tile_zoom()
 
                 # Widgets with "View" menu.
                 for w in self.widgets:
                     if hasattr(w, 'view_menu_options'):
                         imgui.separator()
@@ -537,15 +567,15 @@
     def _draw_status_bar(self) -> None:
         """Draw the bottom status bar."""
 
         h = self.status_bar_height
         r = self.pixel_ratio
         y_pos = int((self.content_frame_height - (h * r)) / r)
         imgui.set_next_window_position(0-2, y_pos)
-        imgui.set_next_window_size(self.content_frame_width+4, h)
+        imgui.set_next_window_size(self.content_width+4, h)
         imgui.push_style_color(imgui.COLOR_WINDOW_BACKGROUND, *self.theme.main_background)
         imgui.push_style_var(imgui.STYLE_WINDOW_PADDING, [10, 5])
 
         imgui.begin('Status bar', closable=True, flags=(imgui.WINDOW_NO_RESIZE | imgui.WINDOW_NO_COLLAPSE | imgui.WINDOW_NO_TITLE_BAR | imgui.WINDOW_NO_MOVE | imgui.WINDOW_NO_SCROLLBAR))
 
         # Backend
         backend = sf.slide_backend()
@@ -639,16 +669,14 @@
             imgui.end_child()
             imgui.same_line()
             imgui.end()
 
     def _glfw_key_callback(self, _window, key, _scancode, action, _mods):
         """Callback for handling keyboard input."""
         super()._glfw_key_callback(_window, key, _scancode, action, _mods)
-        if self._control_down and self._shift_down and action == glfw.PRESS and key == glfw.KEY_P:
-            self._show_performance = not self._show_performance
         if self._control_down and self._shift_down and action == glfw.PRESS and key == glfw.KEY_T:
             self._show_tile_preview = not self._show_tile_preview
         if self._control_down and action == glfw.PRESS and key == glfw.KEY_Q:
             self._exit_trigger = True
         if self._control_down and action == glfw.PRESS and key == glfw.KEY_O:
             self.ask_load_slide()
         if self._control_down and not self._shift_down and action == glfw.PRESS and key == glfw.KEY_P:
@@ -661,14 +689,16 @@
             self.heatmap_widget.show = True
         if self._control_down and action == glfw.RELEASE and key == glfw.KEY_SPACE:
             self.heatmap_widget.show = False
         if self._control_down and action == glfw.PRESS and key == glfw.KEY_LEFT_BRACKET:
             self.decrease_tile_zoom()
         if self._control_down and action == glfw.PRESS and key == glfw.KEY_RIGHT_BRACKET:
             self.increase_tile_zoom()
+        if self._control_down and action == glfw.PRESS and key == glfw.KEY_SLASH:
+            self.ask_zoom_to_mpp()
         if self._control_down and action == glfw.PRESS and key == glfw.KEY_BACKSLASH:
             self.reset_tile_zoom()
 
         self.slide_widget.keyboard_callback(key, action)
         for widget in self.widgets:
             if hasattr(widget, 'keyboard_callback'):
                 widget.keyboard_callback(key, action)
@@ -703,57 +733,71 @@
 
     def _reload_wsi(
         self,
         path: Optional[str] = None,
         stride: Optional[int] = None,
         use_rois: bool = True,
         **kwargs
-    ) -> None:
+    ) -> bool:
         """Reload the currently loaded Whole-Slide Image.
 
         Args:
             path (str, optional): Path to the slide to reload. If not provided,
                 will reload the currently loaded slide.
             stride (int, optional): Stride to use for the loaded slide. If not
                 provided, will use the stride value from the currently loaded
                 slide.
             use_rois (bool): Use ROIs from the loaded project, if available.
+
+        Returns:
+            bool: True if slide loaded successfully, False otherwise.
+
         """
         if self.wsi is None and path is None:
             return
         if path is None:
             path = self.wsi.path
         if stride is None and self.wsi is None:
             stride = 1
         elif stride is None:
             stride = self.wsi.stride_div
         if self.P and use_rois:
             rois = self.P.dataset().rois()
         else:
             rois = None
         if sf.slide_backend() == 'cucim':
-            kwargs['num_workers'] = os.cpu_count()
-        self.wsi = sf.WSI(
-            path,
-            tile_px=(self.tile_px if self.tile_px else 256),
-            tile_um=(self.tile_um if self.tile_um else 512),
-            stride_div=stride,
-            rois=rois,
-            cache_kw=dict(
-                tile_width=512,
-                tile_height=512,
-                max_tiles=-1,
-                threaded=True,
-                persistent=True
-            ),
-            verbose=False,
-            mpp=self.slide_widget.manual_mpp,
-            **kwargs)
-        self.set_viewer(SlideViewer(self.wsi, **self._viewer_kwargs()))
-        self.set_title(os.path.basename(self.wsi.path))
+            kwargs['num_workers'] = sf.util.num_cpu(default=4)
+        try:
+            self.wsi = sf.WSI(
+                path,
+                tile_px=(self.tile_px if self.tile_px else 256),
+                tile_um=(self.tile_um if self.tile_um else 512),
+                stride_div=stride,
+                rois=rois,
+                cache_kw=dict(
+                    tile_width=512,
+                    tile_height=512,
+                    max_tiles=-1,
+                    threaded=True,
+                    persistent=True
+                ),
+                verbose=False,
+                mpp=self.slide_widget.manual_mpp,
+                **kwargs)
+        except sf.errors.IncompatibleBackendError:
+            self.create_toast(
+                title="Incompatbile slide",
+                message='Slide type "{}" is incompatible with the {} backend.'.format(sf.util.path_to_ext(path), sf.slide_backend()),
+                icon='error'
+            )
+            return False
+        else:
+            self.set_viewer(SlideViewer(self.wsi, **self._viewer_kwargs()))
+            self.set_title(os.path.basename(self.wsi.path))
+            return True
 
     def _render_prediction_message(self, message: str) -> None:
         """Render a prediction string to below the tile bounding box.
 
         Args:
             message (str): Message to render.
         """
@@ -833,14 +877,36 @@
                         imgui.text('This is dim')
 
         """
         imgui.push_style_color(imgui.COLOR_TEXT, *self.theme.dim)
         yield
         imgui.pop_style_color(1)
 
+    @contextmanager
+    def highlighted(self, enable: bool = True):
+        """Render highlighted text.
+
+        Args:
+            enable (bool): Whether to enable highlighting.
+
+        Examples
+            Render highlighted text.
+
+                .. code-block:: python
+
+                    with studio.highlighted(True):
+                        imgui.text('This is highlighted')
+
+        """
+        if enable:
+            imgui.push_style_color(imgui.COLOR_BUTTON, *self.theme.button_active)
+        yield
+        if enable:
+            imgui.pop_style_color(1)
+
     def collapsing_header(self, text, **kwargs):
         """Render a collapsing header using the active theme.
 
         Examples
             Render a collapsing header that is open by default.
 
                 .. code-block:: python
@@ -1162,14 +1228,15 @@
         if paths is not None and len(paths) >= 1:
             self.autoload(paths[0], ignore_errors=True)
 
         self._clear_textures()
         self._draw_control_pane()
         self._draw_menu_bar()
         self._draw_about_dialog()
+        self._draw_mpp_zoom_dialog()
 
         user_input = self._handle_user_input()
 
         # Re-generate WSI view if the window size changed, or if we don't
         # yet have a SlideViewer initialized.
         if window_changed:
             self._content_width  = self.content_width
@@ -1223,18 +1290,14 @@
         else:
             self.args.viewer = self.viewer
         # ---------------------------------------------------------------------
 
         # Render control pane contents.
         self._render_control_pane_contents()
 
-        # Render user widgets.
-        for widget in self.widgets:
-            if hasattr(widget, 'render'):
-                widget.render()
 
         if self.is_skipping_frames():
             pass
         elif self._defer_rendering > 0:
             self._defer_rendering -= 1
         else:
             self._async_renderer.set_args(**self.args)
@@ -1270,14 +1333,22 @@
             if 'message' not in self.result:
                 self.result.message = str(self.result.error)
         if 'message' in self.result or self.message:
             _msg = self.message if 'message' not in self.result else self.result['message']
             tex = text_utils.get_texture(_msg, size=self.gl_font_size, max_width=max_w, max_height=max_h, outline=2)
             tex.draw(pos=middle_pos, align=0.5, rint=True, color=1)
 
+        # Render user widgets.
+        for widget in self.widgets:
+            if hasattr(widget, 'render'):
+                widget.render()
+
+        # Render slide widget tile boxes (for tile extraction preview)
+        self.slide_widget.early_render()
+
         # Render the tile view and status bar.
         self._draw_tile_view()
         self._draw_status_bar()
 
         # Draw prediction message next to box on main display.
         if (self._use_model
            and self._predictions is not None
@@ -1346,14 +1417,19 @@
     def has_uq(self) -> bool:
         """Check if the current model supports uncertainty quantification."""
         return (self._model_path is not None
                 and self._model_config is not None
                 and 'uq' in self._model_config['hp']
                 and self._model_config['hp']['uq'])
 
+    def ask_zoom_to_mpp(self) -> None:
+        """Prompt the user to zoom to a specific microns-per-pixel (MPP)."""
+        if self.viewer and isinstance(self.viewer, SlideViewer):
+            self._show_mpp_zoom_popup = True
+
     def increase_tile_zoom(self) -> None:
         """Increase zoom of tile view two-fold."""
         self.tile_zoom *= 2
 
     def decrease_tile_zoom(self) -> None:
         """Decrease zoom of tile view by half."""
         self.tile_zoom /= 2
@@ -1428,15 +1504,15 @@
             # Update widgets
             log.debug("Updating widgets")
             self.model_widget.reset()
             self.model_widget.cur_model = model
             self.model_widget.use_model = True
             self.model_widget.use_uncertainty = 'uq' in config['hp'] and config['hp']['uq']
             if normalizer is not None and hasattr(self, 'slide_widget'):
-                self.slide_widget.show_model_normalizer()
+                self.slide_widget.add_model_normalizer_option()
                 self.slide_widget.norm_idx = len(self.slide_widget._normalizer_methods)-1
             if self.wsi:
                 log.debug(f"Loading slide... tile_px={self.tile_px}, tile_um={self.tile_um}")
                 self.slide_widget.load(
                     self.wsi.path,
                     mpp=self.slide_widget.manual_mpp,
                     ignore_errors=ignore_errors
@@ -1451,14 +1527,19 @@
             # Update viewer
             self._show_tile_preview = True
             log.debug("Updating viewer with tile_px={}, tile_um={}".format(self.tile_px, self.tile_um))
             if self.viewer and not isinstance(self.viewer, SlideViewer):
                 self.viewer.set_tile_px(self.tile_px)
                 self.viewer.set_tile_um(self.tile_um)
 
+            # Trigger user widgets
+            for widget in self.widgets:
+                if hasattr(widget, '_on_model_load'):
+                    widget._on_model_load()
+
             self.create_toast(f"Loaded model at {model}", icon="success")
 
         except Exception as e:
             self.model_widget.cur_model = None
             if model == '':
                 log.debug("Exception raised: no model loaded.")
                 self.result = EasyDict(message='No model loaded')
@@ -1488,14 +1569,19 @@
             stride (int, optional): Stride for tiles. 1 is non-overlapping
                 tiles, 2 is tiles with 50% overlap, etc. Defaults to 1.
             ignore_errors (bool): Do not fail if an error is encountered.
                 Defaults to False.
         """
         self.slide_widget.load(slide, **kwargs)
 
+        # Trigger user widgets
+        for widget in self.widgets:
+            if hasattr(widget, '_on_slide_load'):
+                widget._on_slide_load()
+
     def print_error(self, error: str) -> None:
         """Print the given error message."""
         error = str(error)
         if error != self._last_error_print:
             print('\n' + error + '\n')
             self._last_error_print = error
 
@@ -1586,14 +1672,15 @@
 
     def __init__(self, viz: Studio):
         self.viz                = viz
         self.expanded           = False
         self.selected           = None
         self.buttonbar_width    = 72
         self.navbutton_width    = 70
+        self.imagebutton_width  = 64
         self._button_tex        = dict()
         self._pane_w_div        = 15
         self.navbuttons         = ['project', 'slide', 'model', 'heatmap']
 
         self.add_widgets(viz.widgets)
         self._load_button_textures()
 
@@ -1678,15 +1765,15 @@
         cy -= viz.menu_bar_height
         end_px = start_px + self.navbutton_width
         if ((cx < 0 or cx > self.navbutton_width) or (cy < start_px or cy > end_px)) and self.selected != name:
             tex = self._button_tex[tex_name].gl_id
         else:
             tex = self._button_tex[f'{tex_name}_highlighted'].gl_id
         imgui.set_cursor_position((0, start_px))
-        if imgui.image_button(tex, 64, 64):
+        if imgui.image_button(tex, self.imagebutton_width, self.imagebutton_width):
             if name == self.selected or self.selected is None or not self.expanded:
                 self.expanded = not self.expanded
             self.selected = name
         if self.selected == name:
             draw_list = imgui.get_window_draw_list()
             draw_list.add_line(2, viz.menu_bar_height+start_px, 2, viz.menu_bar_height+start_px+self.navbutton_width, imgui.get_color_u32_rgba(1,1,1,1), 2)
 
@@ -1806,28 +1893,30 @@
         imgui.push_style_color(imgui.COLOR_BUTTON, 0, 0, 0, 0)
         imgui.push_style_color(imgui.COLOR_BUTTON_HOVERED, *self.theme.button_hovered)
         imgui.push_style_color(imgui.COLOR_BUTTON_ACTIVE, *self.theme.button_active)
         result = imgui.image_button(tex, viz.font_size, viz.font_size)
         imgui.pop_style_color(3)
         return result
 
-    def large_image_button(self, image_name, size=64):
+    def large_image_button(self, image_name, size=None):
         """Render a small button for the sidebar.
 
         Args:
             image_name (str): Name of the image to render on the button.
                 Valid names include 'gear', 'circle_lightning', 'circle_plus',
                 'pencil', 'folder', 'floppy', 'model_loaded', 'extensions',
                 'project', 'slide', 'model', and 'heatmap'.
             size (int): Simage button. Defaults to 64.
 
         Returns:
             bool: If the button was clicked.
 
         """
+        if size is None:
+            size = self.imagebutton_width
         tex = self._button_tex[f'{image_name}'].gl_id
         return imgui.image_button(tex, size, size)
 
     def draw(self):
         """Draw the sidebar and render all widgets."""
         viz = self.viz
```

## slideflow/studio/_renderer.py

```diff
@@ -354,15 +354,15 @@
                 )
             res.inference_time = time.time() - _inference_start
 
 #----------------------------------------------------------------------------
 
 class AsyncRenderer:
 
-    """Renderer to assist with tile-evel model predictions."""
+    """Renderer to assist with tile-level model predictions."""
 
     def __init__(self):
         self._closed        = False
         self._is_async      = False
         self._cur_args      = None
         self._cur_result    = None
         self._cur_stamp     = 0
@@ -376,16 +376,16 @@
         self._umap_encoders = None
         self._live_updates  = False
         self.tile_px        = None
         self.extract_px     = None
         self._addl_render   = []
 
         if sf.util.torch_available:
-            import torch
-            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+            from slideflow.model import torch_utils
+            self.device = torch_utils.get_device()
         else:
             self.device = None
 
     def close(self):
         self._closed = True
         self._renderer_obj = None
         if self._process is not None:
@@ -483,16 +483,16 @@
         self._model_path = None
         self._model = None
         self._saliency = None
 
     @staticmethod
     def _process_fn(args_queue, result_queue, model_path, live_updates):
         if sf.util.torch_available:
-            import torch
-            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+            from slideflow.model import torch_utils
+            device = torch_utils.get_device()
         else:
             device = None
         renderer_obj = Renderer(device=device)
         if model_path:
             _model, _saliency, _umap_encoders = _load_model_and_saliency(model_path, device=device)
             renderer_obj._model = _model
             renderer_obj._saliency = _saliency
```

## slideflow/studio/gui/annotator.py

```diff
@@ -23,36 +23,44 @@
         self._name_prompting = False
         self._prompt_pos = None
         self._keyboard_focus = False
 
     def capture(
         self,
         x_range: Tuple[int, int],
-        y_range: Tuple[int, int]
+        y_range: Tuple[int, int],
+        pixel_ratio: float = 1
     ) -> Tuple[Optional[List[Tuple[int, int]]], Union[bool, str]]:
         """Captures a mouse annotation in the given range.
 
         Args:
             x_range (tuple(int, int)): Range of pixels to capture an annotation,
                 in the horizontal dimension.
             y_range (tuple(int, int)): Range of pixels to capture an annotation,
                 in the horizontal dimension.
+            pixel_ratio (float, optional): Ratio of points to pixels.
+                Defaults to 1.
 
         Returns:
             A list of tuple with (x, y) coordinates for the annotation.
 
             A boolean indicating whether the annotation is finished (True)
             or still being drawn (False). If ``AnnotationCapture`` was
             initialized with `named=True`, this will instead be the name
             of the annotation given by the user.
         """
         min_x, max_x = x_range[0], x_range[1]
         min_y, max_y = y_range[0], y_range[1]
         mouse_down = imgui.is_mouse_down(self.mouse_idx)
         mouse_x, mouse_y = imgui.get_mouse_pos()
+
+        if pixel_ratio != 1:
+            mouse_x *= pixel_ratio
+            mouse_y *= pixel_ratio
+
         in_range = (max_x >= mouse_x) and (mouse_x >= min_x) and (max_y >= mouse_y) and (mouse_y >= min_y)
 
         # First, check if the annotation is finished and we are simply
         # waiting for a name prompt.
         if self._name_prompting:
             imgui.set_cursor_pos(self._prompt_pos)
             imgui.push_style_var(imgui.STYLE_ALPHA, 255)
@@ -83,19 +91,25 @@
                 wx, wy = imgui.get_window_position()
                 x = ann_array[:, 0].mean() + x_range[0]
                 y = ann_array[:, 1].max() + y_range[0]
                 self._prompt_pos = (x - 50 - wx, y - wy)
                 self._keyboard_focus = True
                 self._name_prompting = True
                 return self.annotation_points, False
-            else:
+            elif len(self.annotation_points) >= 3:
                 to_return = self.annotation_points
                 self.annotation_points = []
                 return to_return, True
+            else:
+                # Discard capture if there are less than 3 points
+                return None, False
         elif not self.clicking and not in_range:
             return None, False
         else:
             self.clicking = True
             adj_x = min(max(mouse_x - min_x, 0), max_x - min_x)
             adj_y = min(max(mouse_y - min_y, 0), max_y - min_y)
             self.annotation_points.append((adj_x, adj_y))
-            return self.annotation_points, False
+            if len(self.annotation_points) >= 3:
+                return self.annotation_points, False
+            else:
+                return None, False
```

## slideflow/studio/gui/gl_utils.py

```diff
@@ -172,15 +172,53 @@
     gl.glDrawArrays(mode, 0, vertices.shape[0])
 
     gl.glPopMatrix()
     gl.glPopAttrib()
     gl.glPopClientAttrib()
 
 
+def create_buffer(vertices):
+    vbo = gl.glGenBuffers(1)
+    gl.glBindBuffer(gl.GL_ARRAY_BUFFER, vbo)
+    gl.glBufferData(gl.GL_ARRAY_BUFFER, vertices.nbytes, vertices, gl.GL_STATIC_DRAW)
+    gl.glBindBuffer(gl.GL_ARRAY_BUFFER, 0)
+    return vbo
+
+
+def draw_buffer(vbo, size):
+    gl.glBindBuffer(gl.GL_ARRAY_BUFFER, vbo)
+    gl.glEnableClientState(gl.GL_VERTEX_ARRAY)
+    gl.glVertexPointer(2, gl.GL_FLOAT, 0, None)
+
+    gl.glMultiDrawArrays(gl.GL_LINE_LOOP, [i*4 for i in range(size)], [4 for _ in range(size)], size)
+
+    gl.glDisableClientState(gl.GL_VERTEX_ARRAY)
+    gl.glBindBuffer(gl.GL_ARRAY_BUFFER, 0)
+
+
+def draw_rois(vertices, *, color=1, alpha=1, linewidth=2, vbo=None):
+    """Draw multiple ROIs, reducing the number of OpenGL calls with VBO."""
+    assert vertices.ndim == 3 and vertices.shape[2] == 2
+    color = np.broadcast_to(np.asarray(color, dtype='float32'), [3])
+
+    # Set the color and alpha
+    gl.glColor4f(color[0] * alpha, color[1] * alpha, color[2] * alpha, alpha)
+    gl.glLineWidth(linewidth)
+
+    if vbo is not None:
+        draw_buffer(vbo, size=vertices.shape[0])
+    else:
+        for i in range(vertices.shape[0]):
+            draw_roi(vertices[i])
+
+    gl.glLineWidth(1)
+
+
 def draw_roi(vertices, *, color=1, alpha=1, linewidth=2):
+    """Draw a single ROI using the fixed render pipeline."""
     assert vertices.ndim == 2 and vertices.shape[1] == 2
     color = np.broadcast_to(np.asarray(color, dtype='float32'), [3])
     gl.glLineWidth(linewidth)
     gl.glBegin(gl.GL_LINE_STRIP)
     gl.glColor4f(color[0] * alpha, color[1] * alpha, color[2] * alpha, alpha)
     for vertex in vertices:
         gl.glVertex2f(*vertex)
@@ -188,14 +226,15 @@
     gl.glEnd()
     gl.glLineWidth(1)
 
     gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)
     gl.glBindRenderbuffer(gl.GL_RENDERBUFFER, 0)
     gl.glBindTexture(gl.GL_TEXTURE_2D, 0)
 
+
 # -----------------------------------------------------------------------------
 
 def draw_rect(*, pos=0, pos2=None, size=None, align=0, rint=False, color=1, alpha=1, rounding=0, mode=gl.GL_TRIANGLE_FAN, anchor='center'):
     assert pos2 is None or size is None
     assert anchor in ('center', 'topleft')
     pos = np.broadcast_to(np.asarray(pos, dtype='float32'), [2])
     pos2 = np.broadcast_to(np.asarray(pos2, dtype='float32'), [2]) if pos2 is not None else None
```

## slideflow/studio/gui/window.py

```diff
@@ -98,18 +98,18 @@
             imgui.set_next_window_size(toast.width, 0)
 
             # Render with imgui
             imgui.begin(f'toast{_id}', flags=imgui.WINDOW_NO_TITLE_BAR | imgui.WINDOW_NO_RESIZE | imgui.WINDOW_NO_SCROLLBAR)
             if toast.icon:
                 self.icon(toast.icon, sameline=True)
             if toast.title:
-                imgui.text(toast.title)
                 if toast.spinner:
-                    imgui.same_line()
-                    imgui_utils.spinner()
+                    imgui.text(f"{toast.title}{imgui_utils.spinner_text()}")
+                else:
+                    imgui.text(toast.title)
                 if toast.message:
                     imgui.separator()
             if toast.message:
                 imgui.push_text_wrap_pos()
                 imgui.text(toast.message)
                 if toast.spinner and not toast.title:
                     imgui.same_line()
```

## slideflow/studio/gui/viewer/_slide.py

```diff
@@ -416,33 +416,52 @@
         # Refresh ROIs
         self._refresh_rois()
 
     def _refresh_rois(self) -> None:
         """Refresh the ROIs for the given location and zoom."""
         self.rois = []
         for roi_idx, roi in enumerate(self.wsi.rois):
-            c = np.copy(roi.coordinates)
-            c[:, 0] = c[:, 0] - int(self.origin[0])
-            c[:, 0] = c[:, 0] / self.view_zoom
-            c[:, 0] = c[:, 0] + self.view_offset[0] + self.x_offset
-            c[:, 1] = c[:, 1] - int(self.origin[1])
-            c[:, 1] = c[:, 1] / self.view_zoom
-            c[:, 1] = c[:, 1] + self.view_offset[1] + self.y_offset
-            out_of_view_max = np.any(np.amax(c, axis=0) < 0)
-            out_of_view_min = np.any(np.amin(c, axis=0) > np.array([self.width+self.x_offset, self.height+self.y_offset]))
-            if not (out_of_view_min or out_of_view_max):
+            c = self._scale_roi_to_view(roi.coordinates)
+            if c is not None:
                 self.rois += [(roi_idx, c)]
 
     def _render_rois(self) -> None:
         """Render the ROIs with OpenGL."""
         for roi_idx, roi in self.rois:
             c = [1, 0, 0] if roi_idx in self.selected_rois else 0
             gl_utils.draw_roi(roi, color=1, alpha=0.7, linewidth=5)
             gl_utils.draw_roi(roi, color=c, alpha=1, linewidth=3)
 
+    def _scale_roi_to_view(self, roi: np.ndarray) -> Optional[np.ndarray]:
+        roi = np.copy(roi)
+        roi[:, 0] = roi[:, 0] - int(self.origin[0])
+        roi[:, 0] = roi[:, 0] / self.view_zoom
+        roi[:, 0] = roi[:, 0] + self.view_offset[0] + self.x_offset
+        roi[:, 1] = roi[:, 1] - int(self.origin[1])
+        roi[:, 1] = roi[:, 1] / self.view_zoom
+        roi[:, 1] = roi[:, 1] + self.view_offset[1] + self.y_offset
+        out_of_view_max = np.any(np.amax(roi, axis=0) < 0)
+        out_of_view_min = np.any(np.amin(roi, axis=0) > np.array([self.width+self.x_offset, self.height+self.y_offset]))
+        if not (out_of_view_min or out_of_view_max):
+            return roi
+        else:
+            return None
+
+    def _scale_rois_to_view(self, rois):
+        rois = np.copy(rois)
+        rois[:, :, 0] = rois[:, :, 0] - int(self.origin[0])
+        rois[:, :, 0] = rois[:, :, 0] / self.view_zoom
+        rois[:, :, 0] = rois[:, :, 0] + self.view_offset[0] + self.x_offset
+        rois[:, :, 1] = rois[:, :, 1] - int(self.origin[1])
+        rois[:, :, 1] = rois[:, :, 1] / self.view_zoom
+        rois[:, :, 1] = rois[:, :, 1] + self.view_offset[1] + self.y_offset
+        out_of_view_max = np.any(np.amax(rois, axis=1) < 0, axis=1)
+        out_of_view_min = np.any(np.amin(rois, axis=1) > np.array([self.width+self.x_offset, self.height+self.y_offset]), axis=1)
+        return rois[~(out_of_view_min | out_of_view_max)]
+
     def grid_in_view(self, wsi=None):
         """Returns coordinates of WSI grid currently in view."""
         if wsi is None:
             wsi = self.wsi
         wsi_stride = int(wsi.full_extract_px / wsi.stride_div)
         xi_start = int(self.origin[0] / wsi_stride)
         yi_start = int(self.origin[1] / wsi_stride)
@@ -583,33 +602,50 @@
 
     def zoom(self, cx: int, cy: int, dz: float) -> None:
         """Zoom the slide display.
 
         Args:
             cx (int): Zoom focus location, X coordinate, without offset.
             cy (int): Zoom focus location, Y coordinate, without offset.
-            dz (float): Amount to zoom.
+            dz (float): Amount to zoom (relative).
         """
-        wsi_x, wsi_y = self.display_coords_to_wsi_coords(cx, cy, offset=False)
         new_zoom = min(self.view_zoom * dz,
                         max(self.dimensions[0] / self.width,
                             self.dimensions[1] / self.height))
 
+        self.zoom_to(cx, cy, new_zoom)
+
+    def zoom_to(self, cx: int, cy: int, z: float) -> None:
+        """Zoom the slide display.
+
+        Args:
+            cx (int): Zoom focus location, X coordinate, without offset.
+            cy (int): Zoom focus location, Y coordinate, without offset.
+            z (float): Amount to zoom (absolute).
+        """
+        wsi_x, wsi_y = self.display_coords_to_wsi_coords(cx, cy, offset=False)
+        new_zoom = min(z,
+                       max(self.dimensions[0] / self.width,
+                           self.dimensions[1] / self.height))
+
         # Limit maximum zoom level
         if new_zoom * self.wsi.mpp < 0.01:
             return
 
         self.view_zoom = new_zoom
         new_origin = [wsi_x - (cx * self.wsi_window_size[0] / self.width),
                       wsi_y - (cy * self.wsi_window_size[1] / self.height)]
 
         view_params = self._calculate_view_params(new_origin)
         if view_params != self.view_params:
             self._refresh_view_full(view_params=view_params)
 
+    def zoom_to_mpp(self, cx: int, cy: int, mpp: float) -> None:
+        self.zoom_to(cx, cy, mpp / self.wsi.mpp)
+
 # -----------------------------------------------------------------------------
 
 @contextmanager
 def log_vips_error(left_edge, top_edge, extract_w, extract_h):
     try:
         yield
     except Exception:
```

## slideflow/studio/gui/viewer/_viewer.py

```diff
@@ -229,23 +229,26 @@
         self._tex_to_delete = []
 
     def render_overlay(
         self,
         overlay: np.ndarray,
         dim: Optional[Tuple[int, int]],
         offset: Tuple[int, int] = (0, 0)
-    ) -> None:
+    ) -> int:
         """Render an image as an overlay on the WSI.
 
         Args:
             overlay (np.ndarray): Overlay image to render on the WSI.
             dim ((int, int), optional): Dimensions of the overlay in the full
                 WSI coordinate space (level=0). Defaults to the full WSI size.
             offset ((int, int)): Offset for the overlay in the full WSI
                 coordinate space (level=0). Defaults to 0, 0 (no offset).
+
+        Returns:
+            int: ID of the texture created for the overlay.
         """
         if dim is None:
             dim = self.dimensions
         overlay_params = EasyDict(
             dim=tuple(dim),
             offset=tuple(offset),
             view_zoom=self.view_zoom,
@@ -299,14 +302,15 @@
             if self._overlay_tex_obj is not None:
                 self._tex_to_delete += [self._overlay_tex_obj]
             self._overlay_tex_obj = gl_utils.Texture(image=self._overlay_tex_img, bilinear=False, mipmap=False)  # type: ignore
         else:
             self._overlay_tex_obj.update(self._overlay_tex_img)
         assert self._overlay_tex_obj is not None
         self._overlay_tex_obj.draw(pos=self.overlay_pos, zoom=self.h_zoom, align=0.5, rint=True, anchor='topleft')
+        return self._overlay_tex_obj.gl_id
 
     def set_normalizer(self, normalizer: sf.norm.StainNormalizer) -> None:
         """Set the internal WSI normalizer.
 
         Args:
             normalizer (sf.norm.StainNormalizer): Stain normalizer.
         """
```

## slideflow/studio/widgets/extensions.py

```diff
@@ -1,7 +1,8 @@
+import traceback
 import numpy as np
 import imgui
 import textwrap
 from PIL import Image
 from os.path import join, dirname, abspath
 
 from ..gui import imgui_utils, gl_utils
@@ -18,14 +19,15 @@
     def __init__(self, viz):
         self.viz                = viz
         self._show_err_popup    = False
 
         self.stylegan = any([w.tag == 'stylegan' for w in viz.widgets])
         self.mosaic = any([w.tag == 'mosaic' for w in viz.widgets])
         self.segment = any([w.tag == 'segment' for w in viz.widgets])
+        self.mil = any([w.tag == 'mil' for w in viz.widgets])
 
         _off_path = join(dirname(abspath(__file__)), '..', 'gui', 'buttons', 'small_button_verified.png')
         self._official_tex      = gl_utils.Texture(
             image=np.array(Image.open(_off_path)), bilinear=True, mipmap=True
         )
 
     def update_extensions(self):
@@ -51,14 +53,22 @@
         viz = self.viz
         from ..widgets.segment import SegmentWidget
         if not any(isinstance(w, SegmentWidget) for w in viz.widgets):
             viz.add_widgets(SegmentWidget)
         else:
             viz.remove_widget(SegmentWidget)
 
+    def update_mil(self):
+        viz = self.viz
+        from ..widgets.mil import MILWidget
+        if not any(isinstance(w, MILWidget) for w in viz.widgets):
+            viz.add_widgets(MILWidget)
+        else:
+            viz.remove_widget(MILWidget)
+
     def extension_checkbox(self, title, description, check_value, official=False):
         viz = self.viz
         height = imgui.get_text_line_height_with_spacing() * 3
         imgui.begin_child(f'##{title}', height=height)
         with viz.bold_font():
             imgui.text(title)
         imgui.text_colored(description, *viz.theme.dim)
@@ -69,17 +79,21 @@
         else:
             imgui.text('')
         imgui.same_line(imgui.get_content_region_max()[0] - viz.font_size - viz.spacing * 1.5)
         result = imgui.checkbox(f'##{title}_checkbox', check_value)
         imgui.end_child()
         return result
 
-    def show_extension_error(self, message):
+    def show_extension_error(self, message, full_trace=None):
         self._show_err_popup = True
         self._err_msg = message
+        if full_trace:
+            print(full_trace)
+        else:
+            print(message)
 
     def draw_error_popup(self):
         """Show an error message that an extension failed to load."""
         wrapped = textwrap.wrap(self._err_msg, width=45)
         lh = imgui.get_text_line_height_with_spacing()
         window_size = (self.viz.font_size * 18, lh * len(wrapped) + self.viz.font_size * 4)
         self.viz.center_next_window(*window_size)
@@ -118,15 +132,15 @@
                 check_value=self.stylegan,
                 official=True
             )
             if _c1:
                 try:
                     self.update_stylegan()
                 except Exception as e:
-                    self.show_extension_error(str(e))
+                    self.show_extension_error(str(e), traceback.format_exc())
                     self.stylegan = False
             imgui.separator()
 
             _c3, self.segment = self.extension_checkbox(
                 'Cell Segmentation',
                 description='Segment cells with Cellpose.',
                 check_value=self.segment,
@@ -138,12 +152,25 @@
                 except ImportError as e:
                     self.show_extension_error(
                         'Cellpose is not installed. Cellpose can be installed '
                         'with "pip install cellpose"'
                     )
                     self.segment = False
                 except Exception as e:
-                    self.show_extension_error(str(e))
+                    self.show_extension_error(str(e), traceback.format_exc())
                     self.segment = False
 
+            _c4, self.mil = self.extension_checkbox(
+                'Multiple-Instance Learning',
+                description='MIL support with attention heatmaps.',
+                check_value=self.mil,
+                official=True
+            )
+            if _c4:
+                try:
+                    self.update_mil()
+                except Exception as e:
+                    self.show_extension_error(str(e), traceback.format_exc())
+                    self.mil = False
+
         if self._show_err_popup:
             self.draw_error_popup()
```

## slideflow/studio/widgets/heatmap.py

```diff
@@ -83,27 +83,23 @@
 
     def is_categorical(self):
         """Check if model is a categorical model."""
         return self.viz.model_widget.is_categorical()
 
     def _generate(self):
         """Create and generate a heatmap asynchronously."""
-
         sw = self.viz.slide_widget
         self._create_heatmap()
         self._triggered = True
         self._generating = True
         self._heatmap_grid, self._heatmap_thread = self.viz.heatmap.generate(
             asynchronous=True,
-            grayspace_fraction=sw.gs_fraction,
-            grayspace_threshold=sw.gs_threshold,
-            whitespace_fraction=sw.ws_fraction,
-            whitespace_threshold=sw.ws_threshold,
             lazy_iter=self.viz.low_memory,
             callback=self.refresh_heatmap_grid,
+            **sw.get_tile_filter_params(),
         )
 
     def load(self, obj: Union[str, "sf.Heatmap"]):
         """Load a heatmap from a saved *.npz file."""
         if isinstance(obj, str) and self.viz._model_config:
             if self.viz.heatmap is None:
                 self._create_heatmap()
@@ -191,14 +187,16 @@
         )
         _thread = threading.Thread(target=self._generate)
         _thread.start()
         self.show = True
 
     def _get_all_outcome_names(self):
         config = self.viz._model_config
+        if config is None:
+            raise ValueError("Model is not loaded.")
         if config['model_type'] != 'categorical':
             return config['outcomes']
         if len(config['outcomes']) > 1:
             return [config['outcome_labels'][outcome][o] for outcome in config['outcomes'] for o in config['outcome_labels'][outcome]]
         else:
             return [config['outcome_labels'][str(oidx)] for oidx in range(len(config['outcome_labels']))]
 
@@ -299,22 +297,23 @@
         viz = self.viz
         _cmap_changed = False
         _alpha_changed = False
         _gain_changed = False
         _uq_predictions_switched = False
 
         # Predictions and UQ.
-        imgui_utils.vertical_break()
-        _uq_predictions_switched = self.draw_outcome_selection()
-        imgui_utils.vertical_break()
+        if viz._model_config is not None:
+            imgui_utils.vertical_break()
+            _uq_predictions_switched = self.draw_outcome_selection()
+            imgui_utils.vertical_break()
 
         # Display options (colormap, opacity, etc).
         if viz.collapsing_header('Display', default=False):
             with imgui_utils.item_width(viz.font_size * 5):
-                _clicked, self.show = imgui.checkbox('##saliency', self.show)
+                _clicked, self.show = imgui.checkbox('##show_heatmap', self.show)
                 if _clicked:
                     self.render_heatmap()
                     if self.show:
                         self.viz.slide_widget.show_slide_filter  = False
                         self.viz.slide_widget.show_tile_filter   = False
 
             # Colormap.
@@ -373,28 +372,20 @@
             or (self.heatmap_uncertainty != self._old_uncertainty and self.use_uncertainty)
             or _uq_predictions_switched):
             self.render_heatmap()
 
     @imgui_utils.scoped_by_object_id
     def __call__(self, show=True):
         viz = self.viz
-        config = viz._model_config
 
         if self._generating:
             self.refresh_generating_heatmap()
 
         if show:
             viz.header("Heatmap")
-
-        if show and not config:
-            imgui_utils.padded_text('No model has been loaded.', vpad=[int(viz.font_size/2), int(viz.font_size)])
-            if viz.sidebar.full_button("Load a Model"):
-                viz.ask_load_model()
-            if viz.sidebar.full_button("Download a Model"):
-                viz.model_widget._show_download = True
-
-        elif show:
             self.draw_heatmap_thumb()
             txt = "Generate" if not self._triggered else "Generating..."
-            if viz.sidebar.full_button(txt, enabled=(not self._triggered and viz.wsi)):
+            if viz.sidebar.full_button(txt, enabled=(not self._triggered and viz.wsi and viz._model_config is not None)):
                 self.generate()
+            if viz._model_config is None and imgui.is_item_hovered():
+                imgui.set_tooltip("No model loaded.")
             self.draw_display_options()
```

## slideflow/studio/widgets/mosaic.py

```diff
@@ -106,27 +106,27 @@
         self.pool = None
         self.viz.viewer = None
 
     def refresh_mosaic_resolution(self):
         self.mosaic.generate_grid(**self.mosaic_kwargs)
         self.mosaic.plot(pool=self.pool)
 
-    def load(self, obj, tfrecords=None, slides=None, **kwargs):
+    def load(self, obj, tfrecords=None, slides=None, normalizer=None, **kwargs):
         """Load a UMAP from a file or SlideMap object."""
         if isinstance(obj, str):
             try:
                 self.load_umap_from_path(obj, **kwargs)
             except Exception:
                 self.viz.create_toast("Failed to load mosaic map.", icon='error')
                 return
         elif isinstance(obj, sf.SlideMap):
             self.load_umap_from_slidemap(obj, **kwargs)
         else:
             raise ValueError(f"Unrecognized argument: {obj}")
-        self.generate(tfrecords=tfrecords, slides=slides)
+        self.generate(tfrecords=tfrecords, slides=slides, normalizer=normalizer)
 
     def load_umap_from_slidemap(self, slidemap, subsample=5000):
         """Load a UMAP from a SlideMap object."""
         df = slidemap.data
         self.slidemap = slidemap
         self.coords = np.stack((df.x.values, df.y.values), axis=1)
         if subsample and self.coords.shape[0] > subsample:
@@ -145,32 +145,33 @@
                 self.coords = self.coords[idx]
             self._plot_coords()
             self.slidemap = sf.SlideMap.load(path)
             log.info(f"Loaded UMAP; displaying {self.coords.shape[0]} points.")
         else:
             raise ValueError(f"Could not find UMAP as path {path}")
 
-    def generate(self, tfrecords=None, slides=None):
+    def generate(self, tfrecords=None, slides=None, normalizer=None):
         """Build the mosaic."""
         if self.slidemap is None:
             raise ValueError("Cannot generate mosaic; no SlideMap loaded.")
         if tfrecords is None and self.slidemap.tfrecords is None:
             raise ValueError(
                 "TFRecords not found and not provided. Please provide paths "
                 "to TFRecords with generate_mosaic(tfrecords=...)"
             )
         if tfrecords is None:
             tfrecords = self.slidemap.tfrecords
         if self.pool is None:
-            ctx = mp.get_context('fork')
-            self.pool = ctx.Pool(
-                os.cpu_count(),
-                initializer=sf.util.set_ignore_sigint
-            )
-        self.mosaic = OpenGLMosaic(self.slidemap, tfrecords=tfrecords, **self.mosaic_kwargs)
+            self.pool = mp.dummy.Pool(sf.util.num_cpu(default=4))
+        self.mosaic = OpenGLMosaic(
+            self.slidemap,
+            tfrecords=tfrecords,
+            normalizer=normalizer,
+            **self.mosaic_kwargs
+        )
         self.mosaic.plot()
         self.viz.set_viewer(MosaicViewer(self.mosaic, slides=slides, **self.viz._viewer_kwargs()))
 
     def late_render(self):
         """Render UMAP plot annotations late, to ensure they are on top."""
         for _ in range(len(self._late_render_annotations)):
             annotation, name, kwargs = self._late_render_annotations.pop()
@@ -246,14 +247,15 @@
                     imgui.get_color_u32_rgba(1, 0, 0, 1)
                 )
 
             # Capture mouse input
             new_annotation, annotation_name = self.annotator.capture(
                 x_range=(tx+left_x, tx+right_x),
                 y_range=(ty+top_y, ty+bottom_y),
+                pixel_ratio=viz.pixel_ratio
             )
             imgui.end()
 
             # Render in-progress annotations
             if new_annotation is not None:
                 self.render_annotation(new_annotation, origin=(tx+left_x, ty+top_y))
             if annotation_name:
@@ -279,15 +281,14 @@
         imgui.text_colored('Tiles plotted', *viz.theme.dim)
         imgui_utils.right_align(n_tiles)
         imgui.text(n_tiles)
 
 
     def draw_config_popup(self):
         viz = self.viz
-        has_model = viz._model_config is not None
 
         if self._show_popup:
             cx, cy = imgui.get_cursor_pos()
             imgui.set_next_window_position(viz.sidebar.full_width, cy)
             imgui.begin(
                 '##mosaic_popup',
                 flags=(imgui.WINDOW_NO_TITLE_BAR | imgui.WINDOW_NO_RESIZE | imgui.WINDOW_NO_MOVE)
```

## slideflow/studio/widgets/slide.py

```diff
@@ -1,65 +1,85 @@
 import os
 import cv2
 import imgui
 import numpy as np
 import threading
-import contextlib
 import glfw
 from shapely.geometry import Point
 from shapely.geometry import Polygon
 from tkinter.filedialog import askopenfilename
+from typing import Optional
 
 from .._renderer import CapturedException
 from ..utils import EasyDict
 from ..gui import imgui_utils, text_utils, gl_utils
 from ..gui.annotator import AnnotationCapture
 
 import slideflow as sf
 
 #----------------------------------------------------------------------------
 
 class SlideWidget:
-    def __init__(self, viz):
+    def __init__(self, viz: "sf.studio.Studio") -> None:
+        """Widget for slide processing control and information display.
+
+        Args:
+            viz (:class:`slideflow.studio.Studio`): The parent Slideflow Studio
+                object.
+
+        """
         self.viz                    = viz
         self.cur_slide              = None
         self.user_slide             = ''
         self.normalize_wsi          = False
         self.norm_idx               = 0
         self.qc_idx                 = 0
         self.qc_mask                = None
         self.alpha                  = 1.0
         self.stride                 = 1
-        self.show_slide_filter      = False
-        self.show_tile_filter       = False
-        self.gs_fraction            = sf.slide.DEFAULT_GRAYSPACE_FRACTION
-        self.gs_threshold           = sf.slide.DEFAULT_GRAYSPACE_THRESHOLD
-        self.ws_fraction            = sf.slide.DEFAULT_WHITESPACE_FRACTION
-        self.ws_threshold           = sf.slide.DEFAULT_WHITESPACE_THRESHOLD
         self.num_total_rois         = 0
         self._filter_grid           = None
         self._filter_thread         = None
         self._capturing_ws_thresh   = None
         self._capturing_gs_thresh   = None
         self._capturing_stride      = None
         self._use_rois              = True
         self._rendering_message     = "Calculating tile filter..."
         self._show_filter_controls  = False
         self._show_mpp_popup        = False
         self._input_mpp             = 1.0
         self._mpp_reload_kwargs     = dict()
 
+        # Tile & slide filtering
+        self.apply_tile_filter      = True
+        self.apply_slide_filter     = False
+        self.show_tile_filter       = False
+        self.show_slide_filter      = False
+        self.gs_fraction            = sf.slide.DEFAULT_GRAYSPACE_FRACTION
+        self.gs_threshold           = sf.slide.DEFAULT_GRAYSPACE_THRESHOLD
+        self.ws_fraction            = sf.slide.DEFAULT_WHITESPACE_FRACTION
+        self.ws_threshold           = sf.slide.DEFAULT_WHITESPACE_THRESHOLD
+
         # ROI Annotation params
         self.annotator              = AnnotationCapture(named=False)
         self.capturing              = False
         self.editing                = False
         self.annotations            = []
         self._mouse_down            = False
         self._late_render           = []
 
+        # Tile extraction preview
+        self.preview_tiles          = False
+        self.tile_color             = 0
+        self._tile_box_coords       = []
+        self._vbo                   = None
+        self._scaled_rois           = None
+        self._last_processing_params = None
+        self._last_rois             = None
+
         self._all_normalizer_methods = [
             'reinhard',
             'reinhard_fast',
             'reinhard_mask',
             'reinhard_fast_mask',
             'macenko',
             'vahadane_spams',
@@ -73,210 +93,413 @@
             'Macenko',
             'Vahadane (SPAMS)',
             'Vahadane (Sklearn)',
             'Augment']
         self._normalizer_methods = self._all_normalizer_methods
         self._normalizer_methods_str = self._all_normalizer_methods_str
         self._qc_methods_str    = ["Otsu threshold", "Blur filter", "Blur + Otsu"]
-        self._qc_methods        = ['otsu', 'blur', 'both']
+        self._tile_colors       = ['Black', 'White', 'Red', 'Green', 'Blue']
+
+        self._tile_colors_rgb   = [0, 1, (1, 0, 0), (0, 1, 0), (0, 0, 1)]
+        _gaussian = sf.slide.qc.GaussianV2()
+        _otsu = sf.slide.qc.Otsu()
+        self._qc_methods        = [_otsu, _gaussian, [_otsu, _gaussian]]
         self.load('', ignore_errors=True)
 
     @property
-    def show_overlay(self):
+    def show_overlay(self) -> bool:
+        """Whether any overlay is currently being shown."""
         return self.show_slide_filter or self.show_tile_filter
 
     @property
-    def _thread_is_running(self):
+    def _thread_is_running(self) -> bool:
+        """Whether a thread is currently running."""
         return self._filter_thread is not None and self._filter_thread.is_alive()
 
     # --- Internal ------------------------------------------------------------
 
-    def _filter_thread_worker(self):
+    def _filter_thread_worker(self) -> None:
+        """Worker thread for calculating tile filter."""
         if self.viz.wsi is not None:
             self.viz.set_message(self._rendering_message)
-            if self.viz.low_memory or sf.slide_backend() == 'cucim':
-                mp_kw = dict(lazy_iter=True)
+
+            # Optimize the multiprocessing/multithreading method
+            # based on the slide reading backend and whether we are operating
+            # in low memory mode.
+            if self.viz.low_memory and sf.slide_backend() == 'cucim':
+                mp_kw = dict(lazy_iter=True, num_threads=os.cpu_count())
+            elif self.viz.low_memory:
+                mp_kw = dict(lazy_iter=True, num_processes=1)
+            elif sf.slide_backend() == 'cucim':
+                mp_kw = dict(num_processes=os.cpu_count())
             else:
-                mp_kw = dict()
+                mp_kw = dict(num_processes=min(32, os.cpu_count()))
+
+            # Build a tile generator that will yield tiles along with their
+            # whitespace and grayspace fractions.
             generator = self.viz.wsi.build_generator(
                 img_format='numpy',
                 grayspace_fraction=sf.slide.FORCE_CALCULATE_GRAYSPACE,
                 grayspace_threshold=self.gs_threshold,
                 whitespace_fraction=sf.slide.FORCE_CALCULATE_WHITESPACE,
                 whitespace_threshold=self.ws_threshold,
                 shuffle=False,
                 dry_run=True,
                 **mp_kw)
+            # If the generator is None, then the slide has no tiles.
             if not generator:
                 self.viz.clear_message(self._rendering_message)
                 return
+
             # Returns boolean grid, where:
             #   True = tile will be extracted
             #   False = tile will be discarded (failed QC)
             self._filter_grid = np.transpose(self.viz.wsi.grid).astype(bool)
             self._ws_grid = np.zeros_like(self._filter_grid, dtype=np.float)
             self._gs_grid = np.zeros_like(self._filter_grid, dtype=np.float)
-            self.render_overlay(self._filter_grid, correct_wsi_dim=True)
+
+            # Render the tile filter grid as an overlay.
+            if self.show_tile_filter:
+                self.render_overlay(self._filter_grid, correct_wsi_dim=True)
+
+            # Iterate over the tiles and update the filter grid.
             for tile in generator():
                 x = tile['grid'][0]
                 y = tile['grid'][1]
                 gs = tile['gs_fraction']
                 ws = tile['ws_fraction']
                 try:
                     self._ws_grid[y][x] = ws
                     self._gs_grid[y][x] = gs
                     if gs > self.gs_fraction or ws > self.ws_fraction:
                         self._filter_grid[y][x] = False
-                        self.render_overlay(self._filter_grid, correct_wsi_dim=True)
+                        if self.show_tile_filter:
+                            self.render_overlay(self._filter_grid, correct_wsi_dim=True)
+                        self._update_tile_coords()
                 except TypeError:
                     # Occurs when the _ws_grid is reset, e.g. the slide was re-loaded.
                     sf.log.debug("Aborting tile filter calculation")
                     self.viz.clear_message(self._rendering_message)
                     return
             self.viz.clear_message(self._rendering_message)
 
-    def _join_filter_thread(self):
+    def _join_filter_thread(self) -> None:
+        """Join the filter thread if it is running."""
         if self._filter_thread is not None:
             self._filter_thread.join()
         self._filter_thread = None
 
-    def _reset_tile_filter_and_join_thread(self):
+    def _reset_tile_filter_and_join_thread(self) -> None:
+        """Reset the tile filter and join the filter thread if it is running."""
         self._join_filter_thread()
         if self.viz.viewer is not None:
             self.viz.viewer.clear_overlay_object()
         self._filter_grid = None
         self._filter_thread = None
         self._ws_grid = None
         self._gs_grid = None
 
-    def _start_filter_thread(self):
+    def _start_filter_thread(self) -> None:
+        """Start the filter thread."""
         self._join_filter_thread()
         self._filter_thread = threading.Thread(target=self._filter_thread_worker)
         self._filter_thread.start()
 
-    def _refresh_gs_ws(self):
+    def _refresh_gs_ws(self) -> None:
+        """Refresh the grayspace and whitespace grids."""
         self._join_filter_thread()
         if self._ws_grid is not None:
             # Returns boolean grid, where:
             #   True = tile will be extracted
             #   False = tile will be discarded (failed QC)
             self._filter_grid = np.transpose(self.viz.wsi.grid).astype(bool)
             for y in range(self._ws_grid.shape[0]):
                 for x in range(self._ws_grid.shape[1]):
                     ws = self._ws_grid[y][x]
                     gs = self._gs_grid[y][x]
                     if gs > self.gs_fraction or ws > self.ws_fraction:
                         self._filter_grid[y][x] = False
-            if self.show_tile_filter:
-                self.render_overlay(self._filter_grid, correct_wsi_dim=True)
+            self.update_tile_filter()
+            self.update_tile_filter_display()
+            self._update_tile_coords()
+            self.update_params()
+
+    def _render_tile_boxes(self) -> None:
+        """Render boxes around where tiles would be extracted."""
+        if self.viz.wsi is None:
+            return
+        if not len(self._tile_box_coords):
+            return
+        scaled_rois = self.viz.viewer._scale_rois_to_view(self._tile_box_coords).astype(np.float32)
+        if self._scaled_rois is None or not np.all(self._scaled_rois == scaled_rois):
+            self._scaled_rois = scaled_rois
+            self._vbo = gl_utils.create_buffer(scaled_rois.flatten())
+        c = self._tile_colors_rgb[self.tile_color]
+        gl_utils.draw_rois(scaled_rois, color=c, linewidth=2, alpha=1, vbo=self._vbo)
+
+    def _update_tile_coords(self) -> None:
+        """Update the expected coordinates for tiles that will be extracted.
+
+        Expected tile coordinates are based on the slide grid (which is filtered
+        by ROIs and slide-level filters / QC) and the current tile filter grid
+        (which may be asynchronously updating). Bounding box coordinates for
+        each tile are calculated and stored in self._tile_box_coords.
 
-    # --- ROI annotation functions --------------------------------------------
+        """
+        viz = self.viz
+        width = viz.wsi.full_extract_px
+        indices = viz.wsi.coord[:, 2:4]
+        mask = viz.wsi.grid[indices[:, 0], indices[:, 1]]
+        if self._filter_grid is not None:
+            mask = (mask & self._filter_grid[indices[:, 1], indices[:, 0]])
+        filtered_coords = viz.wsi.coord[mask]
+        _coords = np.zeros((filtered_coords.shape[0], 4, 2))  # Preallocate space for coordinates
+        _coords[:, :, 0] = filtered_coords[:, np.newaxis, 0] + np.array([0, width, width, 0])
+        _coords[:, :, 1] = filtered_coords[:, np.newaxis, 1] + np.array([0, 0, width, width])
+        if _coords.size:
+            self._tile_box_coords = _coords
+        else:
+            self._tile_box_coords = np.array()
+
+    # --- Callbacks and render triggers ---------------------------------------
+
+    def early_render(self) -> None:
+        """Render elements with OpenGL (before other UI elements are drawn).
 
-    def late_render(self):
+        Triggers after the slide has been rendered, but before other UI elements are drawn.
+
+        """
+        if self.preview_tiles:
+            self._render_tile_boxes()
+
+    def late_render(self) -> None:
+        """Render elements with OpenGL (after other UI elements are drawn).
+
+        Triggers after the slide has been rendered and all other UI elements
+        are drawn.
+
+        """
         for _ in range(len(self._late_render)):
             annotation, name, kwargs = self._late_render.pop()
             gl_utils.draw_roi(annotation, **kwargs)
             if isinstance(name, str):
                 tex = text_utils.get_texture(
                     name,
                     size=self.viz.gl_font_size,
                     max_width=self.viz.viewer.width,
                     max_height=self.viz.viewer.height,
                     outline=2
                 )
                 text_pos = (annotation.mean(axis=0))
                 tex.draw(pos=text_pos, align=0.5, rint=True, color=1)
 
-    def render_annotation(self, annotation, origin, name=None, color=1, alpha=1, linewidth=3):
-        kwargs = dict(color=color, linewidth=linewidth, alpha=alpha)
-        self._late_render.append((np.array(annotation) + origin, name, kwargs))
+    def keyboard_callback(self, key: int, action: int) -> None:
+        """Handle keyboard events.
+
+        Args:
+            key (int): The key that was pressed. See ``glfw.KEY_*``.
+            action (int): The action that was performed (e.g. ``glfw.PRESS``,
+                ``glfw.RELEASE``, ``glfw.REPEAT``).
 
-    def keyboard_callback(self, key, action):
+        """
         if (key == glfw.KEY_DELETE and action == glfw.PRESS):
-            if self.editing and hasattr(self.viz.viewer, 'selected_rois'):
+            if (self.editing
+               and self.viz.viewer is not None
+               and hasattr(self.viz.viewer, 'selected_rois')):
                 for idx in self.viz.viewer.selected_rois:
                     self.viz.wsi.remove_roi(idx)
                 self.viz.viewer.deselect_roi()
                 self.viz.viewer.refresh_view()
+                self.num_total_rois = len(self.viz.wsi.rois)
 
+    # --- ROI annotation functions --------------------------------------------
 
-    def check_for_selected_roi(self):
-        mouse_down = imgui.is_mouse_down(0)
+    def render_annotation(
+        self,
+        annotation: np.ndarray,
+        origin: np.ndarray,
+        name: Optional[str] = None,
+        color: float = 1,
+        alpha: float = 1,
+        linewidth: int = 3
+    ):
+        """Render an annotation with OpenGL.
+
+        Annotation is prepared and appended to a list of annotations to be
+        rendered at the end of frame generation.
+
+        Args:
+            annotation (np.ndarray): An array of shape (N, 2) containing the
+                coordinates of the vertices of the annotation.
+            origin (np.ndarray): An array of shape (2,) containing the
+                coordinates of the origin of the annotation.
+            name (str): A name to display with the annotation.
+            color (float, tuple[float, float, float]): The color of the
+                annotation. Defaults to 1 (white).
+            alpha (float): The opacity of the annotation. Defaults to 1.
+            linewidth (int): The width of the annotation. Defaults to 3.
 
-        # Mouse is newly up
-        if not mouse_down:
+        """
+        kwargs = dict(color=color, linewidth=linewidth, alpha=alpha)
+        self._late_render.append((np.array(annotation) + origin, name, kwargs))
+
+    def _check_for_selected_roi(self) -> Optional[int]:
+        """Check if a ROI is selected and return its index.
+
+        Returns:
+            int: The index of the selected ROI, or None if no ROI is selected.
+
+        """
+        if self.viz.viewer is None:
+            return None
+        elif not imgui.is_mouse_down(0):
+            # Mouse is newly up
             self._mouse_down = False
-            return
-        # Mouse is already down
+            return None
         elif self._mouse_down:
-            return
-        # Mouse is newly down
+            # Mouse is already down
+            return None
         else:
+            # Mouse is newly down
             self._mouse_down = True
             mouse_point = Point(imgui.get_mouse_pos())
             for roi_idx, roi_array in self.viz.viewer.rois:
                 try:
                     roi_poly = Polygon(roi_array)
                 except ValueError:
                     continue
                 if roi_poly.contains(mouse_point):
                     return roi_idx
+            return None
+
+    def _process_roi_capture(self) -> None:
+        """Process a newly captured ROI.
 
-    def _process_roi_capture(self):
+        If the ROI is valid, it is added to the slide and rendered.
+
+        """
         viz = self.viz
         if self.capturing:
             new_annotation, annotation_name = self.annotator.capture(
                 x_range=(viz.viewer.x_offset, viz.viewer.x_offset + viz.viewer.width),
                 y_range=(viz.viewer.y_offset, viz.viewer.y_offset + viz.viewer.height),
+                pixel_ratio=viz.pixel_ratio
             )
 
             # Render in-progress annotations
             if new_annotation is not None:
                 self.render_annotation(new_annotation, origin=(viz.viewer.x_offset, viz.viewer.y_offset))
             if annotation_name:
                 wsi_coords = []
                 for c in new_annotation:
                     _x, _y = viz.viewer.display_coords_to_wsi_coords(c[0], c[1], offset=False)
                     int_coords = (int(_x), int(_y))
                     if int_coords not in wsi_coords:
                         wsi_coords.append(int_coords)
                 wsi_coords = np.array(wsi_coords)
                 viz.wsi.load_roi_array(wsi_coords)
+                self.num_total_rois = len(viz.wsi.rois)
                 viz.viewer.refresh_view()
 
         # Edit ROIs
         if self.editing:
-            selected_roi = self.check_for_selected_roi()
+            selected_roi = self._check_for_selected_roi()
             if imgui.is_mouse_down(1):
                 viz.viewer.deselect_roi()
             elif selected_roi is not None:
                 viz.viewer.deselect_roi()
                 viz.viewer.select_roi(selected_roi)
 
-    def _set_roi_button_style(self):
+    def _set_roi_button_style(self) -> None:
+        """Set the style for the ROI buttons."""
         imgui.push_style_color(imgui.COLOR_BUTTON, 0, 0, 0, 0)
         imgui.push_style_var(imgui.STYLE_ITEM_SPACING, [0, 0])
 
-    def _end_roi_button_style(self):
+    def _end_roi_button_style(self) -> None:
+        """End the style for the ROI buttons."""
         imgui.pop_style_color(1)
         imgui.pop_style_var(1)
 
-    @contextlib.contextmanager
-    def highlighted(self, boolean):
-        if boolean:
-            imgui.push_style_color(imgui.COLOR_BUTTON, *self.viz.theme.button_active)
-        yield
-        if boolean:
-            imgui.pop_style_color(1)
-
     # --- Public interface ----------------------------------------------------
 
-    def load(self, slide, stride=None, ignore_errors=False, mpp=None, **kwargs):
-        """Load a slide."""
+    def get_tile_filter_params(self) -> dict:
+        """Return the current tile filter (whitespace/grayspace) parameters.
+
+        Returns:
+            dict: A dictionary containing the tile extraction parameters,
+                including the whitespace/grayspace thresholds and fractions.
+
+        """
+        return dict(
+            grayspace_fraction=(self.gs_fraction if self.apply_tile_filter else 1),
+            grayspace_threshold=self.gs_threshold,
+            whitespace_fraction=(self.ws_fraction if self.apply_tile_filter else 1),
+            whitespace_threshold=self.ws_threshold,
+        )
+
+    def get_slide_processing_params(self) -> dict:
+        """Return the current slide processing parameters.
+
+        Returns:
+            dict: A dictionary containing the slide processing parameters,
+                including the tile extraction parameters, the QC method, and
+                the stride.
+
+        """
+        return dict(
+            grayspace_fraction=(self.gs_fraction if self.apply_tile_filter else 1),
+            grayspace_threshold=self.gs_threshold,
+            whitespace_fraction=(self.ws_fraction if self.apply_tile_filter else 1),
+            whitespace_threshold=self.ws_threshold,
+            qc=(None if not self.apply_slide_filter else self._qc_methods[self.qc_idx]),
+            stride=self.stride
+        )
+
+    def update_params(self) -> None:
+        """Log the current slide processing parameters."""
+        self._last_processing_params = self.get_slide_processing_params()
+        if self.viz.wsi is not None:
+            self._last_rois = len(self.viz.wsi.rois)
+        else:
+            self._last_rois = None
 
+    def params_changed(self) -> bool:
+        """Check if the slide processing parameters have changed.
+
+        Returns:
+            bool: True if the slide processing parameters have changed,
+                False otherwise.
+
+        """
+        return (self.get_slide_processing_params() != self._last_processing_params
+                or (self.viz.wsi is not None and len(self.viz.wsi.rois) != self._last_rois))
+
+    def load(
+        self,
+        slide: str,
+        stride: Optional[int] = None,
+        ignore_errors: bool = False,
+        mpp: Optional[float] = None,
+        **kwargs: Optional[dict]
+    ) -> None:
+        """Load a slide.
+
+        Args:
+            slide (str): The path to the slide to load.
+            stride (int, optional): The stride to use when extracting tiles.
+                If a slide is currently loaded and this value is not None, this
+                will override the current stride. Defaults to None.
+            ignore_errors (bool, optional): Whether to ignore errors when
+                loading the slide. Defaults to False.
+            mpp (float, optional): The microns per pixel of the slide. Used
+                if the slide does not contain microns per pixel metadata
+                (e.g. JPG/PNG images). Defaults to None.
+            **kwargs: Additional keyword arguments to pass to the slide loader.
+
+        """
         viz = self.viz
         if slide == '':
             viz.result = EasyDict(message='No slide loaded')
             return
 
         # Wait until current ops are complete
         self._reset_tile_filter_and_join_thread()
@@ -297,21 +520,23 @@
             self._use_rois = True
             viz.set_message(f'Loading {name}...')
             sf.log.debug(f"Loading slide {slide}...")
             viz.defer_rendering()
             if stride is not None:
                 self.stride = stride
             try:
-                viz._reload_wsi(
+                success = viz._reload_wsi(
                     slide,
                     stride=self.stride,
                     use_rois=self._use_rois,
                     ignore_missing_mpp=False,
                     **kwargs
                 )
+                if not success:
+                    return
             except sf.errors.SlideMissingMPPError:
                 self.cur_slide = None
                 self.user_slide = slide
                 self._show_mpp_popup = True
                 self._mpp_reload_kwargs = dict(
                     slide=slide,
                     stride=stride,
@@ -327,47 +552,75 @@
             max_width = int(min(800 - viz.spacing*2, (800 - viz.spacing*2) / hw_ratio))
             viz.wsi_thumb = np.asarray(viz.wsi.thumb(width=max_width, low_res=True))
             viz.clear_message(f'Loading {name}...')
             if not viz.sidebar.expanded:
                 viz.sidebar.selected = 'slide'
                 viz.sidebar.expanded = True
 
+            # Load tile coordinates.
+            self._update_tile_coords()
+
         except Exception as e:
             self.cur_slide = None
             self.user_slide = slide
             viz.clear_message()
             viz.result = EasyDict(error=CapturedException())
             sf.log.warn(f"Error loading slide {slide}: {e}")
             viz.create_toast(f"Error loading slide {slide}", icon="error")
             if not ignore_errors:
                 raise
 
-    def preview_qc_mask(self, mask):
-        assert isinstance(mask, np.ndarray)
-        assert mask.dtype == bool
-        assert len(mask.shape) == 2
+    def preview_qc_mask(self, mask: np.ndarray) -> None:
+        """Preview a slide filter (QC) mask.
+
+        Args:
+            mask (np.ndarray): The slide filter mask.
+
+        """
+        if not isinstance(mask, np.ndarray):
+            raise ValueError("mask must be a numpy array")
+        if not mask.dtype == bool:
+            raise ValueError("mask must have dtype bool")
+        if not len(mask.shape) == 2:
+            raise ValueError("mask must be 2D")
         self.qc_mask = ~mask
         self.show_slide_filter = True
         self.update_slide_filter()
+        self.update_slide_filter_display()
 
-    def render_slide_filter(self):
+    def render_slide_filter(self) -> None:
         """Render the slide filter (QC) to screen."""
+        if self.qc_mask is None:
+            return
         self.viz.heatmap_widget.show = False
         if self.viz.viewer is not None:
             self.viz.viewer.clear_overlay_object()
         self.viz._overlay_wsi_dim = None
         self.render_overlay(self.qc_mask, correct_wsi_dim=False)
 
-    def render_overlay(self, mask, correct_wsi_dim=False):
+    def render_overlay(
+        self,
+        mask: np.ndarray,
+        correct_wsi_dim: bool = False
+    ) -> None:
         """Renders boolean mask as an overlay, where:
 
             True = show tile from slide
             False = show black box
+
+        Args:
+            mask (np.ndarray): The boolean mask to render.
+            correct_wsi_dim (bool, optional): Whether to correct the overlay
+                dimensions to match the WSI dimensions. Defaults to False.
+
         """
-        assert mask.dtype == bool
+        if not isinstance(mask, np.ndarray):
+            raise ValueError("mask must be a numpy array")
+        if not mask.dtype == bool:
+            raise ValueError("mask must have dtype bool")
         alpha = (~mask).astype(np.uint8) * 255
         black = np.zeros(list(mask.shape) + [3], dtype=np.uint8)
         overlay = np.dstack((black, alpha))
         if correct_wsi_dim:
             self.viz.overlay = overlay
             full_extract = int(self.viz.wsi.tile_um / self.viz.wsi.mpp)
             wsi_stride = int(full_extract / self.viz.wsi.stride_div)
@@ -384,72 +637,116 @@
                 target_shape = (int((2000 / overlay.shape[0]) * overlay.shape[1]), 2000)
                 overlay = cv2.resize(overlay, target_shape)
 
             self.viz.overlay = overlay
             self.viz._overlay_wsi_dim = None
             self.viz._overlay_offset_wsi_dim = (0, 0)
 
-    def show_model_normalizer(self):
+    def add_model_normalizer_option(self) -> None:
+        """Add the model normalizer option to the dropdown."""
         self._normalizer_methods = self._all_normalizer_methods + ['model']
         self._normalizer_methods_str = self._all_normalizer_methods_str + ['<Model>']
 
-    def update_slide_filter(self, method=None):
+    def update_slide_filter(self, method: Optional[str] = None) -> None:
+        """Update the slide filter (QC) mask.
+
+        This will update the slide filter mask and the tile filter mask (if
+        applicable), but will not render the slide filter to screen.
+
+        Args:
+            method (str, optional): The slide filter method to use.
+                Defaults to None.
+
+        """
         if not self.viz.wsi:
             return
         self._join_filter_thread()
 
         # Update the slide QC
-        if self.show_slide_filter and self.viz.wsi is not None:
-            self.viz.heatmap_widget.show = False
+        if self.apply_slide_filter and self.viz.wsi is not None:
             if method is not None:
                 self.viz.wsi.remove_qc()
                 self.qc_mask = ~np.asarray(self.viz.wsi.qc(method), dtype=bool)
         else:
             self.qc_mask = None
 
         # Update the tile filter since the QC method has changed
         self._reset_tile_filter_and_join_thread()
-        if self.show_tile_filter:
+        if self.apply_tile_filter:
             self.update_tile_filter()
 
+        self._update_tile_coords()
+
+    def update_slide_filter_display(self) -> None:
+        """Update the slide filter (QC) display.
+
+        This will render the slide filter to screen.
+
+        """
+        if not self.viz.wsi:
+            return
+
+        if self.show_slide_filter and self.viz.wsi is not None:
+            self.viz.heatmap_widget.show = False
+
         # Render the slide filter
         if self.show_slide_filter and not self.show_tile_filter:
             self.render_slide_filter()
 
-    def update_tile_filter(self):
+    def update_tile_filter(self) -> None:
+        """Update the tile filter mask.
+
+        This will update the tile filter mask, but will not render the tile
+        filter to screen.
+
+        """
+        # If there is an existing tile filter update thread,
+        # let that finish before executing a new tile filter update.
+        self._join_filter_thread()
+        if self.apply_tile_filter:
+            # If this is the first request, start the tile filter thread.
+            if self._filter_grid is None and self.viz.wsi is not None:
+                self._start_filter_thread()
+        else:
+            self._filter_grid = None
+
+    def update_tile_filter_display(self) -> None:
+        """Update the tile filter display.
+
+        This will render the tile filter to screen.
+
+        """
         if self.show_tile_filter:
+            # Hide the heatmap overlay, if one exists.
             self.viz.heatmap_widget.show = False
-            self._join_filter_thread()
+            # Remove any other existing slide overlays.
             if self.viz.viewer is not None:
                 self.viz.viewer.clear_overlay_object()
             if not self.show_slide_filter:
                 self.viz.overlay = None
-            if self._filter_grid is None and self.viz.wsi is not None:
-                self._start_filter_thread()
-            elif self._filter_grid is not None:
-                # Render tile filter
-                self.viz.heatmap_widget.show = False
-                self._join_filter_thread()
+            # Otherwise, render the existing tile filter.
+            if self._filter_grid is not None:
                 self.render_overlay(self._filter_grid, correct_wsi_dim=True)
         else:
             if self.viz.viewer is not None:
                 self.viz.viewer.clear_overlay_object()
             if self.show_slide_filter:
                 self.render_slide_filter()
 
     # --- Widget --------------------------------------------------------------
 
-    def draw_info(self):
+    def draw_info(self) -> None:
+        """Draw the info section."""
         viz = self.viz
         height = imgui.get_text_line_height_with_spacing() * 12 + viz.spacing
         if viz.wsi is not None:
             width, height = viz.wsi.dimensions
-            if self._filter_grid is not None and self.show_tile_filter:
+            if self._filter_grid is not None and self.apply_tile_filter:
                 est_tiles = int(self._filter_grid.sum())
-            elif self.show_slide_filter:
+            elif self.apply_slide_filter or (viz.wsi.has_rois() and viz.wsi.roi_method != 'ignore'):
                 est_tiles = viz.wsi.estimated_num_tiles
             else:
                 est_tiles = viz.wsi.grid.shape[0] * viz.wsi.grid.shape[1]
             vals = [
                 f"{width} x {height}",
                 f'{viz.wsi.mpp:.4f} ({int(10 / (viz.wsi.slide.level_downsamples[0] * viz.wsi.mpp)):d}x)',
                 viz.wsi.vendor if viz.wsi.vendor is not None else '-',
@@ -472,17 +769,36 @@
                 if x != 0:
                     imgui.same_line(viz.font_size * (8 + (x - 1) * 6))
                 if x == 0:
                     imgui.text_colored(col, *viz.theme.dim)
                 else:
                     imgui.text(col)
 
+        # Show the loaded tile_px and tile_um
+        imgui.text_colored("Tile size (current)", *viz.theme.dim)
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Slide has been loaded at this tile size, in pixels (px) and microns (um).")
+        imgui.same_line(viz.font_size * (8 + (x - 1) * 6))
+        imgui.text("{} px, {} um".format(viz.wsi.tile_px, viz.wsi.tile_um))
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Loaded tile size: {}, {}".format(
+                f'{viz.wsi.tile_px} x {viz.wsi.tile_px} pixels',
+                f'{viz.wsi.tile_um} x {viz.wsi.tile_um} microns'
+            ))
+
         imgui_utils.vertical_break()
 
-    def draw_filtering_popup(self):
+    def draw_filtering_popup(self) -> None:
+        """Draw the tile filtering popup.
+
+        This will render the tile filtering popup to screen, which allows the
+        user to select the tile filtering method and parameters (grayspace
+        and whitespace fraction/thresholds).
+
+        """
         viz = self.viz
         cx, cy = imgui.get_cursor_pos()
         imgui.set_next_window_position(viz.sidebar.full_width, cy - viz.font_size)
         imgui.set_next_window_size(viz.font_size*17, viz.font_size*3 + viz.spacing*1.5)
         imgui.push_style_color(imgui.COLOR_WINDOW_BACKGROUND, *viz.theme.popup_background)
         imgui.push_style_color(imgui.COLOR_BORDER, *viz.theme.popup_border)
         imgui.begin(
@@ -534,83 +850,155 @@
             if imgui.is_mouse_released() and self._capturing_gs_thresh:
                 self.gs_threshold = self._capturing_gs_thresh
                 self.ws_threshold = self._capturing_ws_thresh
                 self._capturing_ws_thresh = None
                 self._capturing_gs_thresh = None
                 self._reset_tile_filter_and_join_thread()
                 self.update_tile_filter()
+                self.update_tile_filter_display()
+                self._update_tile_coords()
+                self.update_params()
         imgui.end()
         imgui.pop_style_color(2)
 
-    def draw_slide_processing(self):
+    def draw_slide_processing(self) -> None:
+        """Draw the slide processing section.
+
+        This will render the slide processing section to screen, which allows
+        the user to select the tile-level processing and slide-level processing
+        (QC) methods. It also allows the user to select the stride, which
+        controls the overlap between tiles during extraction.
+
+        """
         viz = self.viz
 
         # Stride
-        imgui.text_colored('Stride', *viz.theme.dim)
+        imgui.text('Stride')
         imgui.same_line(imgui.get_content_region_max()[0] - 1 - viz.font_size*7)
         with imgui_utils.item_width(viz.font_size * 7):
             _stride_changed, _stride = imgui.slider_int('##stride',
                                                         self.stride,
                                                         min_value=1,
                                                         max_value=16,
                                                         format='Stride %d')
             if _stride_changed:
                 self._capturing_stride = _stride
             if imgui.is_mouse_released() and self._capturing_stride:
                 # Refresh stride
                 self.stride = self._capturing_stride
                 self._capturing_stride = None
+                self.apply_tile_filter = False
+                self.apply_slide_filter = False
                 self.show_tile_filter = False
                 self.show_slide_filter = False
                 self._reset_tile_filter_and_join_thread()
                 self.viz.clear_overlay()
                 self.viz._reload_wsi(stride=self.stride, use_rois=self._use_rois)
+                self._update_tile_coords()
 
         # Tile filtering
-        with viz.dim_text():
-            _filter_clicked, self.show_tile_filter = imgui.checkbox('Tile filter', self.show_tile_filter)
-            if _filter_clicked:
-                self.update_tile_filter()
+        _filter_clicked, self.apply_tile_filter = imgui.checkbox('Tile filter', self.apply_tile_filter)
+        if _filter_clicked and not self.apply_tile_filter and self.show_tile_filter:
+            self.viz.viewer.clear_overlay_object()
+            self.viz.overlay = None
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Set tile-level filtering strategy")
         imgui.same_line(imgui.get_content_region_max()[0] - 1 - viz.font_size*7)
         if viz.sidebar.small_button('ellipsis'):
             self._show_filter_controls = not self._show_filter_controls
         if self._show_filter_controls:
             self.draw_filtering_popup()
 
         # Slide filtering
-        with viz.dim_text():
-            _qc_clicked, self.show_slide_filter = imgui.checkbox('Slide filter', self.show_slide_filter)
+        _qc_clicked, self.apply_slide_filter = imgui.checkbox('Slide filter (QC)', self.apply_slide_filter)
+        if _qc_clicked and not self.apply_slide_filter:
+            self.viz.wsi.remove_qc()
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Set slide-level filtering strategy (quality control)")
         imgui.same_line(imgui.get_content_region_max()[0] - 1 - viz.font_size*7)
-        with imgui_utils.item_width(viz.font_size * 7):
+        with imgui_utils.item_width(viz.font_size * 7), imgui_utils.grayed_out(not self.apply_slide_filter):
             _qc_method_clicked, self.qc_idx = imgui.combo("##qc_method", self.qc_idx, self._qc_methods_str)
-        if _qc_clicked or _qc_method_clicked:
+        if _qc_clicked or (_qc_method_clicked and self.apply_slide_filter):
             self.update_slide_filter(method=self._qc_methods[self.qc_idx])
+            self.update_slide_filter_display()
+
+        imgui_utils.vertical_break()
+
+    def draw_display_options(self) -> None:
+        """Draw the display options section.
+
+        This will render the display options section to screen, which allows
+        the user to preivew tile extraction (as bounding box outlines), stain
+        normalization, tile filtering, and slide filtering (QC) masks.
+
+        """
+        viz = self.viz
+
+        # Show tile outlines
+        _preview_clicked, self.preview_tiles = imgui.checkbox("Tile outlines", self.preview_tiles)
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Show tile outlines")
+        imgui.same_line(imgui.get_content_region_max()[0] - 1 - viz.font_size*7)
+        with imgui_utils.item_width(viz.font_size * 7), imgui_utils.grayed_out(not self.preview_tiles):
+            _color_clicked, self.tile_color = imgui.combo("##tile_color", self.tile_color, self._tile_colors)
 
         # Normalizing
-        with viz.dim_text():
-            _norm_clicked, self.normalize_wsi = imgui.checkbox('Normalize', self.normalize_wsi)
+        _norm_clicked, self.normalize_wsi = imgui.checkbox('Normalizer', self.normalize_wsi)
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Preview stain normalization (does not affect model predictions)")
         viz._normalize_wsi = self.normalize_wsi
         if self.normalize_wsi and viz.viewer:
             viz.viewer.set_normalizer(viz._normalizer)
         elif viz.viewer:
             viz.viewer.clear_normalizer()
 
         imgui.same_line(imgui.get_content_region_max()[0] - 1 - viz.font_size*7)
-        with imgui_utils.item_width(viz.font_size * 7):
+        with imgui_utils.item_width(viz.font_size * 7), imgui_utils.grayed_out(not self.normalize_wsi):
             _norm_method_clicked, self.norm_idx = imgui.combo("##norm_method", self.norm_idx, self._normalizer_methods_str)
-        if _norm_clicked or _norm_method_clicked:
+        if _norm_clicked or (_norm_method_clicked and self.normalize_wsi):
             # Update the normalizer
             method = self._normalizer_methods[self.norm_idx]
             if method == 'model':
                 self.viz._normalizer = sf.util.get_model_normalizer(self.viz._model_path)
             else:
                 self.viz._normalizer = sf.norm.autoselect(method, source='v3')
             viz._refresh_view = True
 
-    def draw_mpp_popup(self):
+        # Show slide-level filtering
+        with imgui_utils.grayed_out(not self.apply_slide_filter):
+            _show_qc_clicked, self.show_slide_filter = imgui.checkbox("Show QC", self.show_slide_filter)
+        if _show_qc_clicked and self.show_slide_filter and self.apply_slide_filter:
+            self.show_tile_filter = False
+            self.update_slide_filter_display()
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Show slide filter (quality control) mask")
+
+        # Show only extracted tiles
+        with imgui_utils.grayed_out(not self.apply_tile_filter):
+            _s, self.show_tile_filter = imgui.checkbox("Show tile-level filter", self.show_tile_filter)
+        if _s and not self.show_tile_filter and self.apply_tile_filter:
+            self.viz.viewer.clear_overlay_object()
+            self.viz.overlay = None
+        elif _s and self.apply_tile_filter:
+            self.show_slide_filter = False
+            self.update_tile_filter_display()
+        if imgui.is_item_hovered():
+            imgui.set_tooltip("Only show extracted tiles, hiding other tiles with a black mask.")
+
+        preview_button_text = "Preview tile extraction" if not self._thread_is_running else f"Calculating{imgui_utils.spinner_text()}"
+        _params_changed = self.params_changed()
+        if (viz.sidebar.full_button(preview_button_text, enabled=(not self._thread_is_running and _params_changed))
+            or (_preview_clicked and self.preview_tiles and _params_changed)):
+            self.preview_tiles = True
+            self.update_tile_filter()
+            self.update_tile_filter_display()
+            self._update_tile_coords()
+            self.update_params()
+
+    def draw_mpp_popup(self) -> None:
         """Prompt the user to specify microns-per-pixel for a slide."""
         window_size = (self.viz.font_size * 18, self.viz.font_size * 8.25)
         self.viz.center_next_window(*window_size)
         imgui.set_next_window_size(*window_size)
         _, opened = imgui.begin('Microns-per-pixel (MPP) Not Found', closable=True, flags=imgui.WINDOW_NO_RESIZE)
         if not opened:
             self._show_mpp_popup = False
@@ -629,17 +1017,22 @@
             mag = '-'
         imgui.text(mag)
         if self.viz.sidebar.full_button("Use MPP", width=-1):
             self.load(mpp=self._input_mpp, **self._mpp_reload_kwargs)
             self._show_mpp_popup = False
         imgui.end()
 
-
     @imgui_utils.scoped_by_object_id
     def __call__(self, show=True):
+        """Draw the widget in a sidebar.
+
+        Args:
+            show (bool): Whether to draw the widget. Defaults to True.
+
+        """
         viz = self.viz
 
         if show:
             viz.header("Slide")
 
         if show and viz.wsi is None:
             imgui_utils.padded_text('No slide has been loaded.', vpad=[int(viz.font_size/2), int(viz.font_size)])
@@ -654,26 +1047,26 @@
             if viz.collapsing_header('ROIs', default=True):
                 imgui.text_colored('ROIs', *viz.theme.dim)
                 imgui.same_line(viz.font_size * 8)
                 imgui.text(str(self.num_total_rois))
                 self._set_roi_button_style()
 
                 # Add button.
-                with self.highlighted(self.capturing):
+                with viz.highlighted(self.capturing):
                     if viz.sidebar.large_image_button('circle_plus', size=viz.font_size*3):
                         self.capturing = not self.capturing
                         self.editing = False
                         if self.capturing:
                             viz.create_toast(f'Capturing new ROIs. Right click and drag to create a new ROI.', icon='info')
                     if imgui.is_item_hovered():
                         imgui.set_tooltip("Add ROI")
                 imgui.same_line()
 
                 # Edit button.
-                with self.highlighted(self.editing):
+                with viz.highlighted(self.editing):
                     if viz.sidebar.large_image_button('pencil', size=viz.font_size*3):
                         self.editing = not self.editing
                         if self.editing:
                             viz.create_toast(f'Editing ROIs. Click to select an ROI, and press <Del> to remove.', icon='info')
                         else:
                             viz.viewer.deselect_roi()
                         self.capturing = False
@@ -702,14 +1095,16 @@
                 if imgui.is_item_hovered():
                     imgui.set_tooltip("Load ROIs")
                 self._end_roi_button_style()
 
                 imgui_utils.vertical_break()
             if viz.collapsing_header('Slide Processing', default=False):
                 self.draw_slide_processing()
+            if viz.collapsing_header('Display', default=False):
+                self.draw_display_options()
             self._process_roi_capture()
         else:
             self.capturing = False
             self.editing = False
 
         if self._show_mpp_popup:
-            self.draw_mpp_popup()
+            self.draw_mpp_popup()
```

## slideflow/tfrecord/iterator_utils.py

```diff
@@ -48,16 +48,15 @@
                 ratio_indices[:self.ratios.shape[0]],
                 p=self.ratios
             )
             if (self.shard is not None
                and (global_idx % self.shard[1] != self.shard[0])):
                 continue
             try:
-                for _ in range(8):
-                    yield next(iterators[choice])
+                yield next(iterators[choice])
             except (StopIteration, EmptyIterator):
                 if iterators:
                     del iterators[choice]
                     del self.loaders[choice]
                     self.ratios = np.delete(self.ratios, choice)
                     self.ratios = self.ratios / self.ratios.sum()
```

## slideflow/util/__init__.py

```diff
@@ -68,15 +68,28 @@
 
 # Outcome labels
 Labels = Union[Dict[str, str], Dict[str, int], Dict[str, List[float]]]
 
 # Normalizer fit keyword arguments
 NormFit = Union[Dict[str, np.ndarray], Dict[str, List]]
 
+# --- Detect CPU cores --------------------------------------------------------
+
+def num_cpu(default: Optional[int] = None) -> Optional[int]:
+    try:
+        return len(os.sched_getaffinity(0))
+    except Exception as e:
+        count = os.cpu_count()
+        if count is None and default is not None:
+            return default
+        else:
+            return count
+
 # --- Configure logging--------------------------------------------------------
+
 log = logging.getLogger('slideflow')
 log.setLevel(logging.DEBUG)
 
 
 def setLoggingLevel(level):
     """Set the logging level.
 
@@ -200,15 +213,15 @@
 def about(console=None) -> None:
     """Print a summary of the slideflow version and active backends.
 
     Example
         >>> sf.about()
         ╭=======================╮
         │       Slideflow       │
-        │    Version: 1.5.0     │
+        │    Version: 2.1.0     │
         │  Backend: tensorflow  │
         │ Slide Backend: cucim  │
         │ https://slideflow.dev │
         ╰=======================╯
 
     Args:
         console (rich.console.Console, optional): Active console, if one exists.
@@ -604,19 +617,64 @@
 def load_json(filename: str) -> Any:
     '''Reads JSON data from file.'''
     with open(filename, 'r') as data_file:
         return json.load(data_file)
 
 
 def write_json(data: Any, filename: str) -> None:
-    '''Writes data to JSON file.'''
+    """Write data to JSON file."""
     with open(filename, "w") as data_file:
         json.dump(data, data_file, indent=1)
 
 
+def log_manifest(
+    train_tfrecords: Optional[List[str]] = None,
+    val_tfrecords: Optional[List[str]] = None,
+    *,
+    labels: Optional[Dict[str, Any]] = None,
+    filename: Optional[str] = None
+) -> str:
+    """Saves the training manifest in CSV format and returns as a string.
+
+    Args:
+        train_tfrecords (list(str)], optional): List of training TFRecords.
+            Defaults to None.
+        val_tfrecords (list(str)], optional): List of validation TFRecords.
+            Defaults to None.
+        labels (dict, optional): TFRecord outcome labels. Defaults to None.
+        filename (str, optional): Path to CSV file to save. Defaults to None.
+
+    Returns:
+        str: Saved manifest in str format.
+    """
+    out = ''
+    if filename:
+        save_file = open(os.path.join(filename), 'w')
+        writer = csv.writer(save_file)
+        writer.writerow(['slide', 'dataset', 'outcome_label'])
+    if train_tfrecords or val_tfrecords:
+        if train_tfrecords:
+            for tfrecord in train_tfrecords:
+                slide = sf.util.path_to_name(tfrecord)
+                outcome_label = labels[slide] if labels else 'NA'
+                out += ' '.join([slide, 'training', str(outcome_label)])
+                if filename:
+                    writer.writerow([slide, 'training', outcome_label])
+        if val_tfrecords:
+            for tfrecord in val_tfrecords:
+                slide = sf.util.path_to_name(tfrecord)
+                outcome_label = labels[slide] if labels else 'NA'
+                out += ' '.join([slide, 'validation', str(outcome_label)])
+                if filename:
+                    writer.writerow([slide, 'validation', outcome_label])
+    if filename:
+        save_file.close()
+    return out
+
+
 def get_slides_from_model_manifest(
     model_path: str,
     dataset: Optional[str] = None
 ) -> List[str]:
     """Get list of slides from a model manifest.
 
     Args:
@@ -661,15 +719,15 @@
 
 
 def get_model_config(model_path: str) -> Dict:
     """Loads model configuration JSON file."""
 
     if exists(join(model_path, 'params.json')):
         config = load_json(join(model_path, 'params.json'))
-    elif exists(join(dirname(model_path), 'params.json')):
+    elif exists(model_path) and exists(join(dirname(model_path), 'params.json')):
         if not (sf.util.torch_available
                 and sf.util.path_to_ext(model_path) == 'zip'):
             log.warning(
                 "Hyperparameters not in model directory; loading from parent"
                 " directory. Please move params.json into model folder."
             )
         config = load_json(join(dirname(model_path), 'params.json'))
@@ -907,118 +965,135 @@
             writer.writerow(row)
 
     # Delete the old results log file
     if exists(f"{results_log_path}.temp"):
         os.remove(f"{results_log_path}.temp")
 
 
-def location_heatmap(
+def map_values_to_slide_grid(
     locations: np.ndarray,
     values: np.ndarray,
-    slide: str,
-    tile_px: int,
-    tile_um: Union[int, str],
-    outdir: str,
+    wsi: "sf.WSI",
+    background: str = 'min',
     *,
     interpolation: Optional[str] = 'bicubic',
-    cmap: str = 'inferno',
-    norm: Optional[str] = None,
-    background: str = 'min'
-) -> Dict[str, Dict[str, float]]:
-    """Generate a heatmap for a slide.
+):
+    """Map heatmap values to a slide grid, using tile location information."""
 
-    Args:
-        locations (np.ndarray): Array of shape ``(n_tiles, 2)`` containing x, y
-            coordinates for all image tiles. Coordinates represent the center
-            for an associated tile, and must be in a grid.
-        values (np.ndarray): Array of shape ``(n_tiles,)`` containing heatmap
-            values for each tile.
-        slide (str): Path to corresponding slide.
-        tile_px (int): Tile pixel size.
-        tile_um (int, str): Tile micron or magnification size.
-        outdir (str): Directory in which to save heatmap.
-
-    Keyword args:
-        interpolation (str, optional): Interpolation strategy for smoothing
-            heatmap. Defaults to 'bicubic'.
-        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
-            valid matplotlib colormap. Defaults to 'inferno'.
-        norm (str, optional): Normalization strategy for assigning heatmap
-            values to colors. Either 'two_slope', or any other valid value
-            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
-            If 'two_slope', normalizes values less than 0 and greater than 0
-            separately. Defaults to None.
-    """
-
-    import matplotlib.pyplot as plt
-    import matplotlib.colors as mcol
-
-    slide_name = sf.util.path_to_name(slide)
-    log.info(f'Generating heatmap for [green]{slide}[/]...')
-    log.debug(f"Plotting {len(values)} values")
-    wsi = sf.slide.WSI(slide, tile_px, tile_um, verbose=False)
     no_interpolation = (interpolation is None or interpolation == 'nearest')
 
-    stats = {
-        slide_name: {
-            'mean': np.mean(values),
-            'median': np.median(values)
-        }
-    }
-
     # Slide coordinate information
     loc_grid_dict = {(c[0], c[1]): (c[2], c[3]) for c in wsi.coord}
 
     # Determine the heatmap background
     grid = np.empty((wsi.grid.shape[1], wsi.grid.shape[0]))
     if background == 'mask' and not no_interpolation:
         raise ValueError(
             "'mask' background is not compatible with interpolation method "
             "'{}'. Expected: None or 'nearest'".format(interpolation)
         )
     elif background == 'mask':
         grid[:] = np.nan
     elif background == 'min':
         grid[:] = np.min(values)
-    elif background == 'mean': 
+    elif background == 'mean':
         grid[:] = np.mean(values)
     elif background == 'median':
         grid[:] = np.median(values)
     elif background == 'max':
         grid[:] = np.max(values)
     else:
         raise ValueError(f"Unrecognized value for background: {background}")
 
     if not isinstance(locations, np.ndarray):
         locations = np.array(locations)
-    
+
     # Transform from coordinates as center locations to top-left locations.
     locations = locations - int(wsi.full_extract_px/2)
-    
+
     for i, wsi_dim in enumerate(locations):
         try:
             idx = loc_grid_dict[tuple(wsi_dim)]
         except (IndexError, KeyError):
             raise errors.CoordinateAlignmentError(
                 "Error plotting value at location {} for slide {}. The heatmap "
                 "grid is not aligned to the slide coordinate grid. Ensure "
                 "that tile_px (got: {}) and tile_um (got: {}) match the given "
                 "location values. If you are using data stored in TFRecords, "
                 "verify that the TFRecord was generated using the same "
                 "tile_px and tile_um.".format(
-                    tuple(wsi_dim), slide, tile_px, tile_um
+                    tuple(wsi_dim), wsi.path, wsi.tile_px, wsi.tile_um
                 )
             )
         grid[idx[1]][idx[0]] = values[i]
 
     # Mask out background, if interpolation is not used and background == 'mask'
     if no_interpolation and background == 'mask':
         masked_grid = np.ma.masked_invalid(grid)
     else:
         masked_grid = grid
+    return masked_grid
+
+
+def location_heatmap(
+    locations: np.ndarray,
+    values: np.ndarray,
+    slide: str,
+    tile_px: int,
+    tile_um: Union[int, str],
+    outdir: str,
+    *,
+    interpolation: Optional[str] = 'bicubic',
+    cmap: str = 'inferno',
+    norm: Optional[str] = None,
+    background: str = 'min'
+) -> Dict[str, Dict[str, float]]:
+    """Generate a heatmap for a slide.
+
+    Args:
+        locations (np.ndarray): Array of shape ``(n_tiles, 2)`` containing x, y
+            coordinates for all image tiles. Coordinates represent the center
+            for an associated tile, and must be in a grid.
+        values (np.ndarray): Array of shape ``(n_tiles,)`` containing heatmap
+            values for each tile.
+        slide (str): Path to corresponding slide.
+        tile_px (int): Tile pixel size.
+        tile_um (int, str): Tile micron or magnification size.
+        outdir (str): Directory in which to save heatmap.
+
+    Keyword args:
+        interpolation (str, optional): Interpolation strategy for smoothing
+            heatmap. Defaults to 'bicubic'.
+        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+            valid matplotlib colormap. Defaults to 'inferno'.
+        norm (str, optional): Normalization strategy for assigning heatmap
+            values to colors. Either 'two_slope', or any other valid value
+            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+            If 'two_slope', normalizes values less than 0 and greater than 0
+            separately. Defaults to None.
+    """
+
+    import matplotlib.pyplot as plt
+    import matplotlib.colors as mcol
+
+    slide_name = sf.util.path_to_name(slide)
+    log.info(f'Generating heatmap for [green]{slide}[/]...')
+    log.debug(f"Plotting {len(values)} values")
+    wsi = sf.slide.WSI(slide, tile_px, tile_um, verbose=False)
+
+    stats = {
+        slide_name: {
+            'mean': np.mean(values),
+            'median': np.median(values)
+        }
+    }
+
+    masked_grid = map_values_to_slide_grid(
+        locations, values, wsi, background=background, interpolation=interpolation
+    )
 
     fig = plt.figure(figsize=(18, 16))
     ax = fig.add_subplot(111)
     fig.subplots_adjust(bottom=0.25, top=0.95)
     gca = plt.gca()
     gca.tick_params(
         axis='x',
@@ -1081,25 +1156,24 @@
             (str, e.g. "20x").
         outdir (str): Path to directory in which to save images.
 
     Returns:
         Dictionary mapping slide names to dict of statistics
         (mean, median)
     """
-    loc_dict = sf.io.get_locations_from_tfrecord(tfrecord, as_dict=False)
-    if tile_dict.keys() != loc_dict.keys():
-        td_len = len(list(tile_dict.keys()))
-        loc_len = len(list(loc_dict.keys()))
+    locations = sf.io.get_locations_from_tfrecord(tfrecord)
+    if len(tile_dict) != len(locations):
         raise errors.TFRecordsError(
-            f'tile_dict length ({td_len}) != TFRecord length ({loc_len}).'
+            f'tile_dict length ({len(tile_dict)}) != TFRecord length '
+            f'({len(locations)}).'
         )
 
     return location_heatmap(
-        locations=np.array([loc_dict[loc] for loc in loc_dict]),
-        values=np.array([tile_dict[loc] for loc in loc_dict]),
+        locations=np.array(locations),
+        values=np.array([tile_dict[loc] for loc in range(len(locations))]),
         slide=slide,
         tile_px=tile_px,
         tile_um=tile_um,
         outdir=outdir,
         **kwargs
     )
```

## slideflow/util/tfrecord2idx.py

```diff
@@ -38,68 +38,129 @@
     tfrecord_file: str
         Path to the TFRecord file.
 
     index_file: str
         Path where to store the index file.
     """
     infile = open(tfrecord_file, "rb")
-    out_array = []
+    start_bytes_array = []
+    loc_array = []
+    idx = 0
+    datum_bytes = bytearray(1024 * 1024)
 
     while True:
         cur = infile.tell()
-        try:
-            byte_len = infile.read(8)
-            if len(byte_len) == 0:
-                break
-            infile.read(4)
-            proto_len = struct.unpack("q", byte_len)[0]
-            infile.read(proto_len)
-            infile.read(4)
-            out_array += [[cur, infile.tell() - cur]]
-        except Exception:
-            print("Failed to parse TFRecord.")
+        byte_len = infile.read(8)
+        if len(byte_len) == 0:
             break
+        infile.read(4)
+        proto_len = struct.unpack("q", byte_len)[0]
+
+        if proto_len > len(datum_bytes):
+            try:
+                _fill = int(proto_len * 1.5)
+                datum_bytes = datum_bytes.zfill(_fill)
+            except OverflowError:
+                raise OverflowError(
+                    f'Error reading tfrecord {tfrecord_file}'
+                )
+        datum_bytes_view = memoryview(datum_bytes)[:proto_len]
+        if infile.readinto(datum_bytes_view) != proto_len:
+            raise RuntimeError(
+                f"Failed to read record {idx} of file {tfrecord_file}"
+            )
+        infile.read(4)
+        start_bytes_array += [[cur, infile.tell() - cur]]
+
+        # Process record bytes, to read location information.
+        try:
+            record = process_record_from_bytes(datum_bytes_view)
+        except errors.TFRecordsError:
+            raise errors.TFRecordsError(
+                f'Unable to detect TFRecord format: {tfrecord_file}'
+            )
+        if 'loc_x' in record and 'loc_y' in record:
+            loc_array += [[record['loc_x'], record['loc_y']]]
+        elif 'loc_x' in record:
+            loc_array += [[record['loc_x']]]
+        idx += 1
+
     infile.close()
-    save_index(np.array(out_array), index_file)
+    if loc_array:
+        loc_array = np.array(loc_array)
+    save_index(np.array(start_bytes_array), index_file, locations=loc_array)
 
 
-def save_index(index_array: np.ndarray, index_file: str) -> None:
+def save_index(
+    index_array: np.ndarray,
+    index_file: str,
+    locations: Optional[np.ndarray] = None
+) -> None:
     """Save an array as an index file."""
     if 'SF_ALLOW_ZIP' in os.environ and os.environ['SF_ALLOW_ZIP'] == '0':
         np.save(index_file + '.npy', index_array)
     else:
-        np.savez(index_file, index_array)
+        loc_kw = dict()
+        if locations is not None:
+            loc_kw['locations'] = locations
+        np.savez(
+            index_file,
+            arr_0=index_array,
+            **loc_kw
+        )
 
 
 def find_index(tfrecord: str) -> Optional[str]:
+    """Find the index file for a TFRecord."""
     name = sf.util.path_to_name(tfrecord)
     if exists(join(dirname(tfrecord), name+'.index')):
         return join(dirname(tfrecord), name+'.index')
     elif exists(join(dirname(tfrecord), name+'.index.npz')):
         return join(dirname(tfrecord), name+'.index.npz')
     elif exists(join(dirname(tfrecord), name+'.index.npy')):
         return join(dirname(tfrecord), name+'.index.npy')
     else:
         return None
 
 
 def load_index(tfrecord: str) -> Optional[np.ndarray]:
+    """Find and load the index associated with a TFRecord."""
     index_path = find_index(tfrecord)
     if index_path is None:
         raise OSError(f"Could not find index path for TFRecord {tfrecord}")
     if os.stat(index_path).st_size == 0:
         return None
     elif index_path.endswith('npz'):
         return np.load(index_path)['arr_0']
     elif index_path.endswith('npy'):
         return np.load(index_path)
     else:
         return np.loadtxt(index_path, dtype=np.int64)
 
 
+def index_has_locations(index: str) -> bool:
+    """Check if an index file has tile location information stored."""
+    if index.endswith('npy'):
+        return False
+    else:
+        return 'locations' in np.load(index).files
+
+def get_locations_from_index(index: str):
+    if index.endswith('npy'):
+        raise errors.TFRecordsIndexError(
+            f"Index file {index} does not contain location information."
+        )
+    loaded = np.load(index)
+    if 'locations' not in loaded:
+        raise errors.TFRecordsIndexError(
+            f"Index file {index} does not contain location information."
+        )
+    return [tuple(l) for l in loaded['locations']]
+
+
 def get_tfrecord_length(tfrecord: str) -> int:
     """Return the number of records in a TFRecord file.
 
     Uses an index file if available, otherwise iterates through
     the file to find the total record length.
 
     Args:
@@ -213,38 +274,46 @@
     if file.readinto(datum_bytes_view) != length:
         raise RuntimeError("Failed to read the record.")
     if file.readinto(crc_bytes) != 4:
         raise RuntimeError("Failed to read the end token.")
 
     # Process record bytes.
     try:
-        record = process_record(datum_bytes_view)
+        record = process_record_from_bytes(datum_bytes_view)
+    except errors.TFRecordsError:
+        raise errors.TFRecordsError(
+            f'Unable to detect TFRecord format: {tfrecord}'
+        )
+
+    file.close()
+    return record
+
+
+def process_record_from_bytes(bytes_view):
+    try:
+        record = process_record(bytes_view)
     except KeyError:
         feature_description = {
             k: v for k, v in FEATURE_DESCRIPTION.items()
             if k in ('slide', 'image_raw')
         }
         try:
-            record = process_record(datum_bytes_view, description=feature_description)
+            record = process_record(bytes_view, description=feature_description)
         except KeyError:
-            raise errors.TFRecordsError(
-                f'Unable to detect TFRecord format: {tfrecord}'
-            )
+            raise errors.TFRecordsError
 
     # Final parsing.
     if 'slide' in record:
         record['slide'] = bytes(record['slide']).decode('utf-8')
     if 'image_raw' in record:
         record['image_raw'] = bytes(record['image_raw'])
     if 'loc_x' in record:
         record['loc_x'] = record['loc_x'][0]
     if 'loc_y' in record:
         record['loc_y'] = record['loc_y'][0]
-
-    file.close()
     return record
 
 
 def process_record(record, description=None):
     if description is None:
         description = FEATURE_DESCRIPTION
     example = sf.util.example_pb2.Example()
```

## Comparing `slideflow-2.0.5.data/scripts/slideflow-studio` & `slideflow-2.1.0.data/scripts/slideflow-studio`

 * *Files identical despite different names*

## Comparing `slideflow-2.0.5.data/scripts/slideflow-studio.py` & `slideflow-2.1.0.data/scripts/slideflow-studio.py`

 * *Files identical despite different names*

## Comparing `slideflow-2.0.5.dist-info/LICENSE` & `slideflow-2.1.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `slideflow-2.0.5.dist-info/METADATA` & `slideflow-2.1.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: slideflow
-Version: 2.0.5
+Version: 2.1.0
 Summary: Deep learning tools for digital histology
 Home-page: https://github.com/jamesdolezal/slideflow
 Author: James Dolezal
 Author-email: james.dolezal@uchospitals.edu
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
 Classifier: Operating System :: OS Independent
@@ -23,15 +23,15 @@
 Requires-Dist: pandas (<2)
 Requires-Dist: pyvips
 Requires-Dist: fpdf2
 Requires-Dist: lifelines
 Requires-Dist: scikit-image
 Requires-Dist: tqdm
 Requires-Dist: click
-Requires-Dist: protobuf (<=3.20.2)
+Requires-Dist: protobuf (<3.21)
 Requires-Dist: tensorboard
 Requires-Dist: crc32c
 Requires-Dist: h5py
 Requires-Dist: numpy
 Requires-Dist: tabulate
 Requires-Dist: rasterio
 Requires-Dist: smac (==1.4.0)
@@ -68,21 +68,22 @@
 Provides-Extra: dev
 Requires-Dist: sphinx ; extra == 'dev'
 Requires-Dist: sphinx-markdown-tables ; extra == 'dev'
 Requires-Dist: sphinxcontrib-video ; extra == 'dev'
 Provides-Extra: tf
 Requires-Dist: tensorflow (<2.12,>=2.7) ; extra == 'tf'
 Requires-Dist: tensorflow-probability (<0.20) ; extra == 'tf'
-Requires-Dist: tensorflow-datasets ; extra == 'tf'
+Requires-Dist: tensorflow-datasets (<4.9.0) ; extra == 'tf'
 Provides-Extra: torch
 Requires-Dist: torch ; extra == 'torch'
 Requires-Dist: torchvision ; extra == 'torch'
 Requires-Dist: pretrainedmodels ; extra == 'torch'
 Requires-Dist: cellpose (<2.2) ; extra == 'torch'
 Requires-Dist: fastai ; extra == 'torch'
+Requires-Dist: timm (<0.9) ; extra == 'torch'
 
 ![slideflow logo](https://github.com/jamesdolezal/slideflow/raw/master/docs-source/pytorch_sphinx_theme/images/slideflow-banner.png)
 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5703792.svg)](https://doi.org/10.5281/zenodo.5703792)
 [![Python application](https://github.com/jamesdolezal/slideflow/actions/workflows/python-app.yml/badge.svg?branch=master)](https://github.com/jamesdolezal/slideflow/actions/workflows/python-app.yml)
 [![PyPI version](https://badge.fury.io/py/slideflow.svg)](https://badge.fury.io/py/slideflow)
 
 [ArXiv](https://arxiv.org/abs/2304.04142) | [Docs](https://slideflow.dev) | [Slideflow Studio](https://slideflow.dev/studio/) | [Cite](#reference)
@@ -256,15 +257,15 @@
 - [Dolezal et al](https://www.nature.com/articles/s41379-020-00724-3), _Modern Pathology_, 2020
 - [Rosenberg et al](https://ascopubs.org/doi/10.1200/JCO.2020.38.15_suppl.e23529), _Journal of Clinical Oncology_ [abstract], 2020
 - [Howard et al](https://www.nature.com/articles/s41467-021-24698-1), _Nature Communications_, 2021
 - [Dolezal et al](https://www.nature.com/articles/s41467-022-34025-x) _Nature Communications_, 2022
 - [Storozuk et al](https://www.nature.com/articles/s41379-022-01039-1.pdf), _Modern Pathology_ [abstract], 2022
 - [Partin et al](https://doi.org/10.3389/fmed.2023.1058919) _Front Med_, 2022
 - [Dolezal et al](https://ascopubs.org/doi/abs/10.1200/JCO.2022.40.16_suppl.8549) [abstract], 2022
-- [Dolezal et al](https://arxiv.org/abs/2211.06522) [arXiv], 2022
+- [Dolezal et al](https://www.nature.com/articles/s41698-023-00399-4) _npj Precision Oncology_, 2023
 - [Howard et al](https://www.nature.com/articles/s41523-023-00530-5) _npj Breast Cancer_, 2023
 - [Hieromnimon et al](https://doi.org/10.1101/2023.03.22.533810) [bioRxiv], 2023
 
 ## License
 This code is made available under the GPLv3 License and is available for non-commercial academic purposes.
 
 ## Reference
```

## Comparing `slideflow-2.0.5.dist-info/RECORD` & `slideflow-2.1.0.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 slideflow/__init__.py,sha256=Y463ep8Pwe89TUUVaqmFAbKmtQVjjDEEZHIlQ8unTE0,1411
 slideflow/_backend.py,sha256=Yi7LUgiYUUVlGdi6DBYe2raev3LLdYdtR6PtHozFORU,1662
-slideflow/_version.py,sha256=21tWhd6XYGW-DldfXJg2rtF4gwwGPAN39IRS33iyTJc,497
-slideflow/dataset.py,sha256=A8FWqlDeyZjGNHRM1afdMWDFzfs-xk8Wajmb91JGlkw,161693
-slideflow/errors.py,sha256=wmud1rBTRwbI3PYmPz4HClXE2Ly-FnyQSJs0CGVU0LI,3922
-slideflow/heatmap.py,sha256=WVrSejL5bYz1QGjOSPd6uGIwCV-tIdSkR2qPyLUK9gU,40025
-slideflow/mosaic.py,sha256=JcnXKOSNGV0Lvlmpx0e18QSBmaKPoMXxy06kwTOmzKE,26008
-slideflow/project.py,sha256=18Ro0Vr-P9N-PoW8WJsicRZMtbsMTKESjHSFcCWSMCs,171513
+slideflow/_version.py,sha256=HzDkUoR5VGmyZFsawUMDZBU7yg3egUkprXHPyfqukac,497
+slideflow/dataset.py,sha256=iIRgpApK0nFLIvxe3Fq3DxWSjsHdp1SrFpsbtV5LgVc,168259
+slideflow/errors.py,sha256=PUfoSoEjryhMq2ByNT3T2sgUTuDfN0chqMh7Yygu4fA,4029
+slideflow/heatmap.py,sha256=BZw67XaV8jNg043usF83teuBn1vuKrMZ5d-ZoiDiAyk,40271
+slideflow/mosaic.py,sha256=CLWKUAfr-4ox_GYEFScJw2WtDnM-bLJ6ozvI2Ebb2-Q,26054
+slideflow/project.py,sha256=zHE9BGksGcGI_-6LTcFDVSh8rNPXQlqrVOb9sFwL5po,174290
 slideflow/project_utils.py,sha256=LUzkqRmxDYE63MP9XUYnU0j6P-CpQ3-MqZvzp_8Mt9Q,33540
 slideflow/sample_actions.py,sha256=khhho6m0GzAXUD9licPFKlq8oxqY3hdP1qg1B3CTOMY,978
 slideflow/biscuit/__init__.py,sha256=iSjHWIB2GZloqeMxPhNSpb90gBagYm_hX7xut8rKf48,1195
 slideflow/biscuit/delong.py,sha256=3zc0ctK1ZVHaeyFfxOzkKU8doNlzbfPiNkeA18moCs0,4281
 slideflow/biscuit/errors.py,sha256=nAGXgZnJT6GAUs0zw6tNZ3jCfPM_7U3rcu1roSWe1zA,318
 slideflow/biscuit/experiment.py,sha256=kCz4fy_3wnCmkMDCIoCeEF3EUFhZzZnWN_jXUEf7zCM,46859
 slideflow/biscuit/hp.py,sha256=bSuup8s4G9mxpSZ0Wy1ADlpmsNfbBsXzv1dmspAOPGU,1260
 slideflow/biscuit/threshold.py,sha256=k7Of9UwlU4HJjsAGcAwepoanNptFql1bdVhjFfVtE5Q,21373
 slideflow/biscuit/utils.py,sha256=q0lUNv2PFTSbL3Q-1ewqMcq_E8pcwXH9HaBNgpr3dzE,17143
-slideflow/cellseg/__init__.py,sha256=WxBuRrPa5XAPXtTwymnL6QBS-nkG_71LdrZS0AUt7SE,26315
-slideflow/cellseg/seg_utils.py,sha256=NGOTmdmaU2-bbisuo7pwruY0uUl14kT-BXiko1nYdng,5454
+slideflow/cellseg/__init__.py,sha256=yHGB9mMrL8u4AmxHKZKZtvB7L2yfJR-H9yU7hC45BXA,26398
+slideflow/cellseg/seg_utils.py,sha256=nrRy5kMFnloFAcv8CUXgXyuyKUKeEKmjJUY5d01JqeU,5444
 slideflow/clam/__init__.py,sha256=3BiTqyeuQ3pfqDpm4WzyEXlwmhqV19VzNMgutT1gr3E,444
 slideflow/experimental/__init__.py,sha256=XlV9ei1aGs6mm7IpfqzptPrm46WnqNOPSccdQWlhFdA,116
 slideflow/experimental/embedding_search.py,sha256=KnBkLdB6MSod8prKzkCDdOC-aq6PXCk50yJwg9JOViQ,17224
 slideflow/gan/__init__.py,sha256=PBAaQH7NByvQh6BUNk6RzppSOZ37mg6fSH1GBf6aC9A,273
 slideflow/gan/interpolate.py,sha256=ISqxULHO93ngyYrZKO2vF-DwWLCoWHg9FXjyf_g0fDI,24993
 slideflow/gan/utils.py,sha256=S6MpJTf5mK8WLiJBYHHXL_nc-BDvIgqOCBTDvUaOfiw,949
 slideflow/gan/stylegan2/__init__.py,sha256=vOW9PS4ArDZ1k15GT6kRZghBueB8N2tWHQsxHMeKfbI,45
@@ -84,15 +84,15 @@
 slideflow/gan/stylegan3/train.py,sha256=WMl8XC7KG5bnqfyti1Z_M_xEsyJLdU7a9qp-dJ2-5Ks,6901
 slideflow/gan/stylegan3/visualizer.py,sha256=a2kTHaZpjA2NTjaftGkaGPVoOmnUh_XcK2ddpfzRIl4,6095
 slideflow/gan/stylegan3/stylegan3/__init__.py,sha256=z9O8g-t39mGcRWjjSbqwtGm2iAt83ZpvvHWdvvCw8LE,77
 slideflow/gan/stylegan3/stylegan3/generate.py,sha256=tj-NKHp47k_J8Rsjqiwb6EZ_qbJbtb-LuxV752PjWyM,6128
 slideflow/gan/stylegan3/stylegan3/legacy.py,sha256=9lLaWKajDuzUMYdfTo9H3uCH1IgMaQQ5fz7KiFhTOBI,17302
 slideflow/gan/stylegan3/stylegan3/train.py,sha256=Cl-MNCaO4QNmN-U_CY21EETPPCMMO7tt66Tdxfd9Wg0,15354
 slideflow/gan/stylegan3/stylegan3/utils.py,sha256=W6RT6siuxZ6wKDN5OssU234Qoke6MhEwIJ14oMW3kUA,1845
-slideflow/gan/stylegan3/stylegan3/visualizer.py,sha256=NxE8YoF7s_ZXXiZ01Iukyq95ieaU2HdaVVvmBXr27CM,13379
+slideflow/gan/stylegan3/stylegan3/visualizer.py,sha256=DID5O2tl6Y66uIjff7RD5mT3HOUWDraphb8Z7Yr6B14,13387
 slideflow/gan/stylegan3/stylegan3/dnnlib/__init__.py,sha256=Uf_R0ACoyPDy6lNKMxux0lxJeOyIzRlwDEY3cFbRUh0,488
 slideflow/gan/stylegan3/stylegan3/dnnlib/util.py,sha256=llSzEpOEHQac5PNztXGDhx1T0aWmI_CWRHKKH1bjTBQ,17870
 slideflow/gan/stylegan3/stylegan3/embedding/__init__.py,sha256=15Dw-zog2Mqr6wS3Teqj7H__UDy6SlnGmKw0J5ijFiQ,3153
 slideflow/gan/stylegan3/stylegan3/gui_utils/__init__.py,sha256=S0Yxj3BwXip2oUfQCLCk15PBC3dO23tdIkH1btGYNwo,448
 slideflow/gan/stylegan3/stylegan3/gui_utils/gl_utils.py,sha256=hzN1k7p90JrwbiZQ7QbWY9B97JibbJgeqo4nUft4nMk,16361
 slideflow/gan/stylegan3/stylegan3/gui_utils/glfw_window.py,sha256=0KnqbBdSCgtFxXCJ9-OjZsMjidqqT6aRuutq1GLed8c,7848
 slideflow/gan/stylegan3/stylegan3/gui_utils/imgui_utils.py,sha256=HWUSKMdj7X24MkDoGUNlPcRZ5s4W03dVATYbf9IVtUI,7648
@@ -143,31 +143,32 @@
 slideflow/gan/stylegan3/stylegan3/viz/__init__.py,sha256=S0Yxj3BwXip2oUfQCLCk15PBC3dO23tdIkH1btGYNwo,448
 slideflow/gan/stylegan3/stylegan3/viz/capture_widget.py,sha256=pGffowS8YuyTOcNieTlRpb8j4HNpJAmlwPv927LnBAo,3694
 slideflow/gan/stylegan3/stylegan3/viz/equivariance_widget.py,sha256=otdLbzlCnLsiQqtiHcgg5_l4Oc7lgcLlJt4h0uhfUgE,5884
 slideflow/gan/stylegan3/stylegan3/viz/latent_widget.py,sha256=tAqXMlAVJ16WkTbslnTdOVycfcpk97vD2WdZNrZoAuc,4414
 slideflow/gan/stylegan3/stylegan3/viz/layer_widget.py,sha256=TQ5X38faqAP4HSXYJDctPwark5xyQYEVfzP9ujV9EXo,9520
 slideflow/gan/stylegan3/stylegan3/viz/performance_widget.py,sha256=mskCIARHtmZs9j3q5-x9qgdhT-BJ7ke80T7v4LqS_fs,3566
 slideflow/gan/stylegan3/stylegan3/viz/pickle_widget.py,sha256=mI8Les69cJkHkmC6gbtQyYTLyuRfIVLrj71TjgFGq80,9460
-slideflow/gan/stylegan3/stylegan3/viz/renderer.py,sha256=OaSwrKXsRWxvkl95ytUgK1YAO4MQ70E5YePV-R-rk0I,24417
+slideflow/gan/stylegan3/stylegan3/viz/renderer.py,sha256=h7rfiWk3PW_OaOmWVYpwfrePKaJHTqS6ax1G34kf_TE,25341
 slideflow/gan/stylegan3/stylegan3/viz/stylemix_widget.py,sha256=MoZm72ZSLHz2iacQNeGrkjhmEXy6jnnxnFNzKBbcWoQ,5573
 slideflow/gan/stylegan3/stylegan3/viz/thumb_widget.py,sha256=Q3CDEvVzWEmNNhv0QvRWsovHNZX-My8gHcBJ8eJKUuo,5825
 slideflow/gan/stylegan3/stylegan3/viz/trunc_noise_widget.py,sha256=j8Is6WXvSx9iOgQdKfw2zC_V7wPPX3xBjak0gUegTH8,3845
 slideflow/grad/__init__.py,sha256=_cBjPmYSKKW6urH2xZ6llP-gthQoKCsgxfx0Lksq7cs,15662
 slideflow/grad/plot_utils.py,sha256=vkUAan14mAY3UoMCw5oqtgfiifqWcSw5BPYuD9yDIE4,7376
-slideflow/io/__init__.py,sha256=xnHhQDCnsX3cNzz2YvGtob4cG_mo6B6z7JFXcHSNFF0,11715
+slideflow/io/__init__.py,sha256=Nq48suusvJ9leTmbaBFO1Uo0E5o7-5y-ibtm1yC8CtI,12623
 slideflow/io/gaussian.py,sha256=7mhQeBcEQmpKqq_55WST6VE67jQwFf2uq6u7ZVM99PM,10541
 slideflow/io/io_utils.py,sha256=8oneye-50sEu9eZpaCzy2IdkrCGBd3WKlVSA4s8mW9A,9918
-slideflow/io/tensorflow.py,sha256=R52b44ZkKdUUoXHGDh02v28pV1fJ07EJ1LhmYKQQRi0,34511
-slideflow/io/torch.py,sha256=AD-9UXRWlHWXaGt8sgblar7gp9nZ5M1Jow6nMkSw7iQ,45085
+slideflow/io/tensorflow.py,sha256=Kg5cUBUVdnWeVaJerqgtsI2wQJvKYK76lfCLhZlK1gI,39451
+slideflow/io/torch.py,sha256=QyUDLB9B2WDicZo3FBTn2E3FS8oHNEUdihU4XDL6JsA,53442
 slideflow/io/preservedsite/__init__.py,sha256=9chqMmsn_iKvFwnTxZtVr4Pn08dyOeEi4YpgKhrf1Kw,68
 slideflow/io/preservedsite/crossfolds.py,sha256=UD4e0JqgZJeYl9lfrFHkCKhePJHGbsB20ZK6CA3g7bU,8419
-slideflow/mil/__init__.py,sha256=H_nvcxN4HLMS_rnghdaBTqeaG6gb-7sJctm0UVz38Fc,284
+slideflow/mil/__init__.py,sha256=ZUY41rZvcu__uFhDdxl9CRt0tel2XRbfWTCxReZ6Yhw,300
 slideflow/mil/_params.py,sha256=sCpjitfJxCcsx7iTnOHqD2cLXJe0Z-qxzcldUTYWgBI,14819
-slideflow/mil/data.py,sha256=DIUUAPhAk0qthJQT6Oe4cfIxJmR5BXKPTbnXzH_sKoo,5160
-slideflow/mil/eval.py,sha256=doaogpmKGolnA8PRDWWpl6jQldiV5JYQoe6AK6_nFvs,15507
+slideflow/mil/data.py,sha256=jYR8zzuAR1hkNALdgJThglwJ1KM2LAeA4vQfsY_S8mc,5292
+slideflow/mil/eval.py,sha256=RggYC6vvUbuf5gBFSNB9NlXjeXtIzGYGncenzCS86K0,19380
+slideflow/mil/utils.py,sha256=Q6bYk7D_3wsvMqwj3ElbA1AjLz9SFrAHLshXWm2WMQw,5121
 slideflow/mil/clam/__init__.py,sha256=XFJYtb4FK5xQm7DqdmW3gBnG8IHYEjh3S1pzwFcbDKE,3890
 slideflow/mil/clam/create_attention.py,sha256=oGjZEfcT0zRqMhB8XITtFXHdw5Y9lNT9ZeiOcjntFtE,4334
 slideflow/mil/clam/datasets/__init__.py,sha256=lrqpfKa8fvcZHBzpmdJWt1uikjTNV8ZKqqpMIoRJkh4,1131
 slideflow/mil/clam/datasets/dataset_generic.py,sha256=Ulj-XQigidFsdx_DAoJ-RTQnY3jZucDtZH3YJ95Sw4g,14990
 slideflow/mil/clam/utils/__init__.py,sha256=EbG-maRA3elGYWTGMzVZbQfRqf2p9I8ojMHHJkb1MgU,5914
 slideflow/mil/clam/utils/core_utils.py,sha256=hWgMxfLv4gRWwyBzS0vhDRGJ4IF_AUz6IA6SedbII7U,19782
 slideflow/mil/clam/utils/eval_utils.py,sha256=zFFeNXOP-FhUg2HrYJxG_k5BUFwso1tJo_-kpIxAmWY,4150
@@ -175,113 +176,120 @@
 slideflow/mil/clam/utils/loss_utils.py,sha256=crw4fumbNZxcKWYCVBKMSZS6Ig3Cfr_YWfCZpC2rU3A,3497
 slideflow/mil/models/__init__.py,sha256=KbHLo_IiQOJ6wkOarvVur2kvJY2L38xmccgAYJTJOnk,185
 slideflow/mil/models/_utils.py,sha256=OzeCy0V1sY4wzcEI-4XgWa6vZ_knIH8lr5_JvosPQJM,378
 slideflow/mil/models/att_mil.py,sha256=Tor-RDpdQ1OmXo3pd_7wTT3-qZZRxVtIC3EQxsDROPg,3239
 slideflow/mil/models/clam.py,sha256=7XiRuX4dnkaevi_0WYaaTky98xwkmCO9jtnsXgtfuy0,12816
 slideflow/mil/models/mil_fc.py,sha256=TOQdoVmuyf-wiNJgyAvL2MubdcyDFamEO0GHMkNeeHI,3865
 slideflow/mil/models/transmil.py,sha256=CpVxNifuytkGE7oRUcc2nFWJQHWo51xGXHtn8BIvk7A,4137
-slideflow/mil/train/__init__.py,sha256=SNILpMx3nYeBFYmO2Y_PoJAiC0LgdGKRAKD_pE052ec,15008
-slideflow/mil/train/_fastai.py,sha256=Af8wZL5gOaoKYq8helUE5SpfuxfcdZ8XxB24p4XXFTA,8214
+slideflow/mil/train/__init__.py,sha256=R_wFzZvz0zwnyuxA7aCjYSReQpJ5NzmkflQkiEdvdfA,18243
+slideflow/mil/train/_fastai.py,sha256=ICmSAtifUqeGPbyE9HOnCeXkQwg_oMmh3Mgu6jDoskg,9714
 slideflow/mil/train/_legacy.py,sha256=v1s5Gqc46BZggwz7dcNyLuqVIM9lIYgUfNqTfH6d90A,7795
-slideflow/model/__init__.py,sha256=oJpmQVOsQiDUhI7RqD7WC3n5dKxkrOSA5Emkfgs-Vf0,7485
+slideflow/model/__init__.py,sha256=Ptos8JExEd5gIW_yeiXL_PkfehvN32p8aNzj646zdgo,7504
 slideflow/model/adv_utils.py,sha256=qt25QlPxEQk7vJtqc82DBlkw9KgSf8KkJvpC0ATzF7s,1879
-slideflow/model/base.py,sha256=0tOogk4fN9FMkrJCK5K0lgh3_vVI6KQIrfgMKBPQtg4,23588
-slideflow/model/features.py,sha256=0rsbGiPHgPRn_nZllWmltuph8dYOXTAevbqwYsoIALc,55541
-slideflow/model/tensorflow.py,sha256=Fbc7PULI_vqLylpVHF4DYYU0bgRb8If1vpDQnlxnptk,110776
+slideflow/model/base.py,sha256=E5xh3gG0FRbfTEJdnY5_ji5uYQ5GoZ_nBhZgX6PRfP8,22346
+slideflow/model/features.py,sha256=zeQhwjwDMDsfQyFVXApolwEKWC0KvZRB_DItqsQ2l3A,67879
+slideflow/model/tensorflow.py,sha256=PHdJ6zQMa2ght6QKkUo11oeOF0vW3vARHg-JSzHlumc,107500
 slideflow/model/tensorflow_utils.py,sha256=5HYrVE8Ph3pSXjMfSRi7vmJIC_8mD3pQd28LLj1YAEY,22547
-slideflow/model/torch.py,sha256=vwjNP-xIB7MPpmCsjrgBLhckJ0M2OKYMaL6F1MlDFUo,103271
-slideflow/model/torch_utils.py,sha256=wFUA6VgDydOFVqZj-pWRXlp7PPbdV7-W_ZlMi9rszSU,17005
-slideflow/model/extractors/__init__.py,sha256=MEt448aADc_WUGzTSGF5IvW4Neu2atSiAFAyxzc-4uI,394
-slideflow/model/extractors/_factory.py,sha256=CXR4fa08GLM8Q4b_S5qIO1bmtzRX1VDtYrU7PjPFUHI,3708
-slideflow/model/extractors/_factory_tensorflow.py,sha256=zpvP7rOYQEvbuZbSlFY_L7iV0dub53kHS8fAcYNnaHs,4544
-slideflow/model/extractors/_factory_torch.py,sha256=-5kOiu91eBWq_9erNSTIj7DiUYg4edVKl5SPns0_9JQ,5711
+slideflow/model/torch.py,sha256=xEJYEuosPxSuyu-wf7EHK52lvxFqDK5TmzQMAbqAyik,102386
+slideflow/model/torch_utils.py,sha256=BNrTAUQv93LGWBtvr8irL9wK3ScTMhjDtbovqhGf_SM,18288
+slideflow/model/extractors/__init__.py,sha256=qcS7Ea39t867acC7_5NwbQGHL8PkND1qX-FHu1hbh80,575
+slideflow/model/extractors/_factory.py,sha256=ZbYPxMqvH7Stp_LTjyOvGKYaAGWGNCRHBuCVhlNZM5g,10549
+slideflow/model/extractors/_factory_tensorflow.py,sha256=o1WXhFPuG9WFLeAQlqCf9ls_8O_4miDfaG41cKqay2o,6025
+slideflow/model/extractors/_factory_torch.py,sha256=Zjk0hKjtHui1gI8ui4yzSi4fmZBl4fFgHNhYrOAb3vo,6797
 slideflow/model/extractors/_registry.py,sha256=BBudKPe2sOGvnNZFiuFJhjcKhHZdf2gRe0hheTN8ZKo,1570
-slideflow/model/extractors/_slide.py,sha256=nC0OzgF8_11f5ql0oujeVymM8X8DJ6HU71C4ts-Q9s8,2646
-slideflow/model/extractors/ctranspath.py,sha256=fvxFcJFnd0E5KRRlUgQleXwRNfEMpnsncLLzQKRlgrw,25681
-slideflow/model/extractors/retccl.py,sha256=Qm6QGtpCS2Wc7S4EOvxzO9kUbLpJOpBvgMO1lKZI3CE,12045
-slideflow/norm/__init__.py,sha256=W2OuXm74-QNXb1u3idY3XKMjOCV4E9uVLOBLLPjQ_xM,26156
+slideflow/model/extractors/_slide.py,sha256=oFI8elgrS1BN6Zn57YMo7pdFBJC8MSui0nkYm-aNnA0,2751
+slideflow/model/extractors/ctranspath.py,sha256=THbTCm8gNC5FIzTRDxH-Jp_Zo8s6XvvJa_UXEr_HNM8,26421
+slideflow/model/extractors/retccl.py,sha256=VSZOfhlQPmdK9ImspMXjsMMVGkbZr9V01sAWBGP6jdg,12778
+slideflow/model/extractors/simclr.py,sha256=gHT0q6PNG5s0Ej7uqzz8ZdWxCzafFQZmVG6ET7I6K60,5189
+slideflow/model/extractors/_slide/__init__.py,sha256=37b_iyxPPPzEWN9F5tXPzdUQqDQqkSuJXngWZmcBGb0,461
+slideflow/model/extractors/_slide/_tf.py,sha256=6-X3d93g5J-fz-rKBJjUvcVzfmHT168N50BDiM-G0ek,6785
+slideflow/model/extractors/_slide/_torch.py,sha256=YSgtRhUKfiUqMZwdeE6GDcfIUVDw-rOI5hKS8UKHXg4,3893
+slideflow/model/extractors/_slide/_utils.py,sha256=kqgXUBQi3yHOuCpjPH_zqP5G1onnUJg1fIJngvqY1yo,1352
+slideflow/norm/__init__.py,sha256=77jvADImI8LcCq6GxOYSLMPRkVvPCU-s0AiPm32zFxw,26247
 slideflow/norm/augment.py,sha256=k1uWWhhnwtapzHuq2G8_7eKJ92f-tNOrLh811vQXGxw,1605
 slideflow/norm/macenko.py,sha256=eu58kIhs8FK_6xiJr8rlzFOxxlPR1lJ2GMnAPG7Pl-Y,13438
 slideflow/norm/norm_tile.jpg,sha256=Y3wWM8FDrsfYdiMhReDBC90Rf7Io5s8Ga0UJx4oBGQk,177672
 slideflow/norm/reinhard.py,sha256=cYI--gXzLY-xpqPURpJKF8YsPIXcY0tAQ3MgVJacgwM,16964
 slideflow/norm/utils.py,sha256=dA9JzmAhz1tnzKoCBAgbTPX6iLNhfm6phd9Ng5xWrz0,15314
 slideflow/norm/vahadane.py,sha256=YIRyky8TCVqT_H95wijSbwgOPoOMhH9bApKABypLP30,7836
 slideflow/norm/tensorflow/__init__.py,sha256=EKJBEUXsly7J1j_Z5byskYVpQJzMXUXsOWMpJjrq3lM,11998
 slideflow/norm/tensorflow/color.py,sha256=ei3alBtNb-xm1pHhJdtJoVuS_tPkISh1rXZ6Sl_DvwM,5879
-slideflow/norm/tensorflow/macenko.py,sha256=8c-gPeMAuc1aUf0FC4acPgcgtJmUoEM9xA9-ZXB9GB8,22672
+slideflow/norm/tensorflow/macenko.py,sha256=mLVc7Bimze6WeMLJsxZaCiVdMcpXcr4SCg-pC7kKjB8,22671
 slideflow/norm/tensorflow/reinhard.py,sha256=w0DsrQx7bq57u7HpC5bgQZBjOlqNsHOnEfQrfXfcASU,26169
 slideflow/norm/tensorflow/utils.py,sha256=IcTPD8wR5h8Gqso0O-d6mlR-LrU6d1cJdnkj62XIHm0,1685
 slideflow/norm/torch/__init__.py,sha256=xigN3UHKRzO_--2eUzfmKi9F6MgV5rSMuJErCzVt14c,12095
 slideflow/norm/torch/color.py,sha256=dN1FpcvdVLvmkjGdc77rA4r0q0Clhhr_sQR2fEh18Vc,7826
-slideflow/norm/torch/macenko.py,sha256=OrMPj4geCkKYc_MabCZpaVP5QEBplq0vdUdsMcpK9FM,15306
+slideflow/norm/torch/macenko.py,sha256=1rJL9IXYClY0_eTGZqUhdlzJ5wCw8dPSp-B1TWYdzSQ,15060
 slideflow/norm/torch/reinhard.py,sha256=xE74ZJQTDJckRgumwNikRy3UT8KDq1EoumHO3x3BaVc,24997
 slideflow/norm/torch/utils.py,sha256=MstCD6KIJGG2a7xHAfSpLVr95jcoKrXrW2xDxch79pQ,1673
 slideflow/simclr/__init__.py,sha256=o8TiOkbZzvsQo6bISoejBrOscbq_l-akGzbGr92rkK4,458
 slideflow/simclr/simclr/__init__.py,sha256=M1HPoNrZnhd3zOIQyYO1YXGBeA22ZXmi6BeSccg5774,6
-slideflow/simclr/simclr/tf2/__init__.py,sha256=_z4lp94Onev6T60Ny_ScPHYZoLMoNhuwCxxBNkjSXdo,21046
-slideflow/simclr/simclr/tf2/data.py,sha256=wKVweVOUpusJfo9lkGpN5lo_xiDOr-lUfjBtZIlymzA,10701
-slideflow/simclr/simclr/tf2/data_util.py,sha256=wu0vlo_H_yL8aFCWWGpknf1Z3Apu3gsiyaiEGaEPcRY,18550
+slideflow/simclr/simclr/tf2/__init__.py,sha256=XGhgR85OS5dqXFdMTbMTYo3lFp74M3PDGcozb1uxLrQ,21050
+slideflow/simclr/simclr/tf2/data.py,sha256=soFadWFXIcpVnnvXeAWZ-XqZcpXZhcflKSNTdCWrodM,11777
+slideflow/simclr/simclr/tf2/data_util.py,sha256=vKchaKq_pr916kSfHB8EuAGkBWi5TIgl3of2gMM1Cok,19016
 slideflow/simclr/simclr/tf2/lars_optimizer.py,sha256=XWLnxqVxRlUY6PsQOHiCsFB6Lluk3TOokasLH9TTO-o,6505
 slideflow/simclr/simclr/tf2/metrics.py,sha256=RUtmvVamcnS-8_ZXLE2XdALsLgNgVPOUNl17hKPk13Y,2997
 slideflow/simclr/simclr/tf2/model.py,sha256=3p7YnZeZS09XOkPbVJ19dl77h7w3DNuf4c6rd6gqlqA,12256
 slideflow/simclr/simclr/tf2/objective.py,sha256=v5V1UzGCaSzT1i6xDe4UOaUIOhQyZ9sPDlrEP_2XYQ8,4983
-slideflow/simclr/simclr/tf2/resnet.py,sha256=BJgzhGO3TudNubzAAsx3jl8wsUW6DJ8EzivN3qlV9wY,28397
+slideflow/simclr/simclr/tf2/resnet.py,sha256=GgqKqYitzAC_IdDDEr2Tco00HO4AaAg8vj2F8ubIoU4,28265
 slideflow/simclr/simclr/tf2/run.py,sha256=8Ej3YtqrKgpqlh0sjyqbUmKvLPGIRLz7XYi44dpvPE0,6524
-slideflow/simclr/simclr/tf2/utils.py,sha256=BEiKTWmuJ0XbZfG15413xPs_KnNmRg5uYL0QZEzgDYo,9517
-slideflow/slide/__init__.py,sha256=ro-ibaeiE_w4BuVE2aVrbmjtgGnkTmciU-9_qvHOW3o,115204
+slideflow/simclr/simclr/tf2/utils.py,sha256=XF7LqOaf-SImrKDA9t8J2zp-xrwgSFlrGhbnoEG5JL8,9737
+slideflow/slide/__init__.py,sha256=9Nt0Zhq7zl1ZvyyQ6tgJwDvXifr54EUhASxDL67Nbv0,117079
 slideflow/slide/report.py,sha256=4vAxYiLaqHaoLSeLFLxZmSbIhg7Hv3UbhPhQk1xuC6g,19076
 slideflow/slide/slideflow-logo-name-small.jpg,sha256=C9-2QV_cZkmn_FqzvorF5GvrkD__VE-7NSKME58fTR8,30934
 slideflow/slide/utils.py,sha256=5NmVU1LdFdBciwJLg_Sh9xMU4DgXEVKEB-XV7jBf2lc,5516
 slideflow/slide/backends/__init__.py,sha256=hqSG2dXl8jj578W6wa-3uPgKugBqwj2zKO7ugfku71o,1035
-slideflow/slide/backends/cucim.py,sha256=jweC9lnylwQLlg0FbMFFU48VukShrciWx8UoSA8f6PI,14301
-slideflow/slide/backends/vips.py,sha256=yTKwt7Doxd0e5bZkhHMdZrE3vNOR_4blVoV0cyeusxs,22863
-slideflow/slide/qc/__init__.py,sha256=ePMfd6ZlrWdxYiRkS6gsCIMoGfS3hDPuqIyjX7kQT30,117
+slideflow/slide/backends/cucim.py,sha256=Z0D8qm95u31h0K8ZIWOas5IiQB--dAgvyh9oAIzumHg,14351
+slideflow/slide/backends/vips.py,sha256=cpcs9VA1Ztu3Oa2VmQec1nsrlGNwIAtN0OH_bwIRcHg,31738
+slideflow/slide/qc/__init__.py,sha256=vRRGuudr5SE1valAh4d9m802y3hn6GrxnXUkXW1V3VE,186
 slideflow/slide/qc/deepfocus.py,sha256=6wN5GWvXOi6QEIdAYMccuG1YTzLh5-u-yh6CnEBh1TI,8151
 slideflow/slide/qc/deepfocus_qc.py,sha256=TuHOVLYPWUD_8hYS0OFzmLvgDNAoxxuykeEcb4_xuKc,1010
-slideflow/slide/qc/gaussian.py,sha256=JGBXw0A2mDx34EZepK6A8DT22DppkZIj8P3HZcM6QEU,4304
+slideflow/slide/qc/gaussian.py,sha256=Gt4pzppnbgSLhhwZxnofUkv3UZqOdN_1bcweo2TpOvE,4484
 slideflow/slide/qc/gaussian_v2.py,sha256=w3FQGsrfI_wXZhEcU15m8cWDvQB9v4-xLZXtZD1Zdqs,5341
-slideflow/slide/qc/otsu.py,sha256=l1ipll7WH8XSno5jCNf6Jdct6cVmwjiKC7IXD_L1UKA,5232
+slideflow/slide/qc/otsu.py,sha256=Xj0bXBqSsAcuqYuTKvfWrE4JI9iPLkcQNrax3x4ZyUw,5989
 slideflow/slide/qc/saver.py,sha256=g4aT4PGxNjHj0ZCvWx0odkc_b-_pFXvCYEYKEMR4z3Y,3028
-slideflow/slide/qc/strided_dl.py,sha256=rQqqovzbjFQrPp8ZvOjewQqbL1DMZGzfEc68NrPybhA,4676
+slideflow/slide/qc/strided_dl.py,sha256=KwAlpU258zmrZChVSuRsFq9IPmTsVRNZ_-bajAGHn6E,5327
 slideflow/slide/qc/strided_qc.py,sha256=ioSC_GSaZ9fBQrfv6D8A61Tww9pVJLkUhFXl884TvUI,13029
 slideflow/stats/__init__.py,sha256=PoUMiNzTv2ra_wvkFgwz7ZjJRu0G_aD2tebhEc01BKw,390
 slideflow/stats/delong.py,sha256=C6QaDckNp9FDmwjPkoHDv8qTaT_Yaipa-ky-43tDBtM,4293
-slideflow/stats/metrics.py,sha256=h0cR4Yuh1FziuL0RZNGrNlDn69c-Q6ASL2aFTqiRqMw,35785
+slideflow/stats/metrics.py,sha256=byRsQT_TGM5vWhluqxAz3NpKprPijbLvb4CUt89xwR0,35694
 slideflow/stats/plot.py,sha256=qnyOA--h_SmGR0uyGGRXVQhk-Z3wRMM38__leADAtrQ,5757
-slideflow/stats/slidemap.py,sha256=vXS39-CyQ-3WA_ILUQs99GcBTtKjWyUjw2ebVlyMpuQ,43101
-slideflow/stats/stats_utils.py,sha256=jk7vmXMk_cGUrMwT0vGc1JSwnNCXj97LoWS7IzN8G6k,3261
-slideflow/studio/__init__.py,sha256=pwtCiDS_IMebIAytNu8p1XH3OgY_YJRMWz443mE_P1w,77576
+slideflow/stats/slidemap.py,sha256=ObGNcmBShUBhS63mfqED77GALdL_-FxwktDglLTPCgw,43878
+slideflow/stats/stats_utils.py,sha256=lzHdMoHwUBBO_wDwSCjdX0HG9H_QW5ItstaF0kjJ6IY,3553
+slideflow/studio/__init__.py,sha256=VbSKOzjx9MxZPj_lAb5WVhZCQ_XDGx7ENyi79o0E2Uk,80731
 slideflow/studio/__main__.py,sha256=mjuK2AaJaTFqapCI3ZG26_-OkQTSsH4Xyvvt1GLBl8w,2187
-slideflow/studio/_renderer.py,sha256=oPASsmG9hztqFcstTh7MXZdxkriWTWWluQDDI39-XdU,19662
+slideflow/studio/_renderer.py,sha256=wQHKvav6s5x0D-RNs-tMuyK0cpFVOa5NeWX0bcFUPzc,19645
 slideflow/studio/utils.py,sha256=uP3mb3Xq6UgPVdRaJ0365KhTR4-rQQWOrqzQEw14lX4,3686
 slideflow/studio/gui/__init__.py,sha256=9_8wL9Scv8_Cs8HJyJHGvx1vwXErsuvlsAqNZLcJQR0,8
 slideflow/studio/gui/_glfw.py,sha256=D1CberJgfOn0gG9A3SvoLC0HRKvjvnoG0Mky_h45Ljk,14387
-slideflow/studio/gui/annotator.py,sha256=VUqx2cUw754W5JmAyuuw-or7oAm_nW-_R0HgDIqS7fw,4116
-slideflow/studio/gui/gl_utils.py,sha256=HAdhcChvrthkOl5c8DGfP04-G4nahCM7Ukdq188xvzI,11035
+slideflow/studio/gui/annotator.py,sha256=WhaYpc_L0VSqooaIGlAIpzFf2rTOt3mwcjkGOdMYLIk,4608
+slideflow/studio/gui/gl_utils.py,sha256=6qviIZmxSgUk9RT0hnZeqbkETgLmKLechF7HaTpSjnI,12307
 slideflow/studio/gui/imgui_utils.py,sha256=z4ZRyo0fkmXDYpejv_Z1BvUec9aFJb-afGSS0g1XL3M,10029
 slideflow/studio/gui/logo_dark_outline.png,sha256=QkLA4-7QQi0ZD6a0qfXLq-ULgc5ZkuUPopoOGqwp4SE,29775
 slideflow/studio/gui/splash.png,sha256=tHTzjX5d8ssGCBWKTRHg4QPVW7xYQcp47-U8agj8mz0,179066
 slideflow/studio/gui/text_utils.py,sha256=l_e-w3jsSBjuGMgWkzLYdD086gjR8-iQP7UZaUkByYE,5291
 slideflow/studio/gui/theme.py,sha256=IZ6cQ0p_bgYZlj5SCX0CLhsbhrCKB3ft52DS58Q18bk,2241
 slideflow/studio/gui/toast.py,sha256=Ox7hbTZaWw61mvZCivZwUatOLzxJmNh6egoE4vMcyyE,2333
-slideflow/studio/gui/window.py,sha256=dQNilotqrL8TuUA_-Qvd-isyCHlO76AGBvk0XO8dnBU,7885
+slideflow/studio/gui/window.py,sha256=vreHOoceqmkWx00_Ye5k_s2LauyvVGqK8TUmpDu5mbo,7908
 slideflow/studio/gui/buttons/button_circle_lightning.png,sha256=9uQpBhUKHMhAAOWdFdqXJp3kgVanYW1OCUghaYFPEEc,2425
 slideflow/studio/gui/buttons/button_circle_lightning_highlighted.png,sha256=wYnVL8poj-0VsuD1mljZqbhzaACnwkwKZF2GCis_7wE,2127
 slideflow/studio/gui/buttons/button_circle_plus.png,sha256=MkoMiRoGLkVYwSHXM4ssgQx3ffCIG2y3VGcht4MwVYQ,2278
 slideflow/studio/gui/buttons/button_circle_plus_highlighted.png,sha256=1J867ntXNJyPx-1JAKgt36NHXof2EV_Ea7cHQ1EUnjs,2017
 slideflow/studio/gui/buttons/button_extensions.png,sha256=jt1jXQhej6pTE9eyXCNJyk8JDawy8DP3Ll8qu75-WyE,1994
 slideflow/studio/gui/buttons/button_extensions_highlighted.png,sha256=uo7V9Pfq1ThhmJhU_z3fa8qCrIB5n0iSwAZMELDdEP0,1823
 slideflow/studio/gui/buttons/button_floppy.png,sha256=zxAUDGWj4p3r2tXZ7gXfFI_3udOQiEZMSgfrqrKmBzc,1297
 slideflow/studio/gui/buttons/button_floppy_highlighted.png,sha256=LzqfacOlE8_3kPIJx76NulrLyiC2dprK-xW8EhuG2r8,1291
 slideflow/studio/gui/buttons/button_folder.png,sha256=B1pgaXN03rayuDKuxJqqk-NoLBfokGSE7Kx1e-3pZa0,1463
 slideflow/studio/gui/buttons/button_folder_highlighted.png,sha256=xj4rdA4fWz3xJ4Qy38Ce6xpXIu9-ZF-UIAeLmDEbNSg,1440
 slideflow/studio/gui/buttons/button_gear.png,sha256=KKCqVrVqKrnnowiwco3S3PcqxVgHkdIn6OA0eBiUhTc,2776
 slideflow/studio/gui/buttons/button_gear_highlighted.png,sha256=juixh_b96V2611S8saczoJbmckkqgSXpOZWt51IdEAg,2139
 slideflow/studio/gui/buttons/button_heatmap.png,sha256=lgzAe35Rb0M-H2dDUzkr78JgxdolM25pmSJv8zKeUr0,1331
 slideflow/studio/gui/buttons/button_heatmap_highlighted.png,sha256=uMSDJQZhkA4DdEqPCXrTgERSluqCrNYtYxdBOgLSgCY,1271
+slideflow/studio/gui/buttons/button_mil.png,sha256=0RNQk_yhOouQ8O9AE8M6aXmU_Q34MYY8_1Ox1_CQBls,2648
+slideflow/studio/gui/buttons/button_mil_highlighted.png,sha256=jEUUs1gW77YvoMZRKmwuXKMDEDgmXc_uPJYmK7OUSm8,2389
 slideflow/studio/gui/buttons/button_model.png,sha256=FibJMZzpYHJAQGZ0LNNm2gjDxRAeiGsqa8fqsl6qIgo,2403
 slideflow/studio/gui/buttons/button_model_highlighted.png,sha256=jIKkXYaP0_ebzdy4eZWQJgyvWZHHYu15XuV68VFkEZY,2109
 slideflow/studio/gui/buttons/button_model_loaded.png,sha256=4ovKc5f68S6WW7VmOay_m_WETf3JkJkduPwL3dH54lw,3842
 slideflow/studio/gui/buttons/button_model_loaded_highlighted.png,sha256=EokX2JPFbrW9MOZZ5Tc-v_7TZZwlwNEfsMT8lNIiYHo,3622
 slideflow/studio/gui/buttons/button_mosaic.png,sha256=g0J57_WDf7DyPqkrAGSM_zIOMlpytfWyzznqePd1YlU,1662
 slideflow/studio/gui/buttons/button_mosaic_highlighted.png,sha256=DMvGPB6CZ2rRyVoDvOd7EXe_-GyDN-P8K7H3U2RbjYk,1609
 slideflow/studio/gui/buttons/button_pencil.png,sha256=xQKooAZkZ3jdl8ovGX59nbyW9ZkzNaUxi35Hq97eoMw,1164
@@ -308,55 +316,56 @@
 slideflow/studio/gui/icons/error.png,sha256=XQOSYuK4Gipt36VpGPGYse7HbnPnxoqi5oLKxmNvFeU,7568
 slideflow/studio/gui/icons/info.png,sha256=vqiqW4eXPfcrj_m7FyKajAq8r3YI3AkDGzhUp0__jG8,7286
 slideflow/studio/gui/icons/logo.png,sha256=1Vw6Zi1rlfhKtIrN3rOvzKppRnaiaXorYVk-GQPA72s,125763
 slideflow/studio/gui/icons/success.png,sha256=BxXVANTnqknCN79rZabK-gBap_3YrCeCvar8Fxxr0Jc,7181
 slideflow/studio/gui/icons/warn.png,sha256=mAVKiVI85LF10roGtrrOf43DE5gpR4XORYh3z62iACU,6817
 slideflow/studio/gui/viewer/__init__.py,sha256=9wiXSrhD8Raz5AVGdCBrF-Fykn1sNqkqGgjfXtRwNuU,107
 slideflow/studio/gui/viewer/_mosaic.py,sha256=dKDyGt12-f9sju5g0plSJUoOiP4uYAH2emiK3vQFUb0,7874
-slideflow/studio/gui/viewer/_slide.py,sha256=JZHy-MzIOBxn6gEyMcoNx3CmXnplYsiOWirt_2rY5CQ,26041
-slideflow/studio/gui/viewer/_viewer.py,sha256=M_aVUL7tZRyOI2OhI3xiRUVpQe3kunoLAt5kfRMxZSM,14452
+slideflow/studio/gui/viewer/_slide.py,sha256=Y3CyLOaW6UNkFiImvsmBnleySyN3MD868aX5l06zQ4g,27584
+slideflow/studio/gui/viewer/_viewer.py,sha256=BIVqvsb-68Xr1lv5pUxkkgZiVlJiSq4MzimotBfaQ7g,14572
 slideflow/studio/widgets/__init__.py,sha256=M8gSPhj8W4FThgSm5kMF1GuAffF1PkauBfU5xtwAmxI,439
 slideflow/studio/widgets/_utils.py,sha256=jI9Ah-aO4SPN8CdBB0pWV2-bxifLSW6DpGK106938js,735
 slideflow/studio/widgets/capture.py,sha256=ReS_0x1Alz328yTnDy4Wq09XQZ1aFb0HYJoM90CsY_4,4362
-slideflow/studio/widgets/extensions.py,sha256=V7u_T1t5IxiqRfiza9ru5UgvPBUruvLI7dXmTBwlnm8,5412
-slideflow/studio/widgets/heatmap.py,sha256=P_7Bpz3T9MxxFBCeA9moD0TcLRkrbJvV_1QlRVbRriY,16890
+slideflow/studio/widgets/extensions.py,sha256=sgkLX8egaYaLI4hyL5hUMEKbTiROAJiS6uV_otuvrNQ,6388
+slideflow/studio/widgets/heatmap.py,sha256=mqIwE-iQo1C7D5e19O4i8wswszloA5Rx5zOTG7SLnG4,16624
 slideflow/studio/widgets/layer_umap.py,sha256=Dol3NIxJge6Ff2e80Ydjvfh0xc8CBMb34DmM-n1lPmg,3842
+slideflow/studio/widgets/mil.py,sha256=2L5nzHRYrI9woatmA999WTMdu42DFL1Iu0kjF4wCwcU,16225
 slideflow/studio/widgets/model.py,sha256=T50CTM8jt1e_lNrmh4zx7XOQdTh9S6RApmnJp1Gh7fw,24474
-slideflow/studio/widgets/mosaic.py,sha256=u1a_HvZ20PWuzdQ1EVAWIP9HOUE85m42OvLDY0i4pKk,14307
+slideflow/studio/widgets/mosaic.py,sha256=3w-pwoxObkhJbNLEh9Q3KtYLcbcOPDCt7pR-nKaf4FU,14330
 slideflow/studio/widgets/mosaic_experimental.py,sha256=_Z3huwfE3uJeP3-Fs_PwAvJ00T8ekykeWBvTH2AXN4k,2597
 slideflow/studio/widgets/performance.py,sha256=kTk3fuS6M5mQ5K4pmKZvx5sVJC070IHImfPTCQbxNec,4662
 slideflow/studio/widgets/picam.py,sha256=K_z4pt1-VvU0HkWfyO0NsOEqne4utfr8blZjpxoIvRI,6437
 slideflow/studio/widgets/project.py,sha256=naeSkPx8X_euzt2DBU0LLcI7Lod_824ygcf5m6QPrpQ,7726
 slideflow/studio/widgets/seed_map.py,sha256=b4ftLvj5ySevm9zKjhcKN-HOQSYlza1JKEhknw83aa4,5983
 slideflow/studio/widgets/segment.py,sha256=ocfTzY3fansXaQoPGFUxvVBg2nbzbH8KzRtUpzjQ9-E,19875
 slideflow/studio/widgets/settings.py,sha256=L6etk5UsWrzLxa_5c7S70ABTllviiFAKPZFLw4fFE7o,2071
-slideflow/studio/widgets/slide.py,sha256=B5o-96jdTKFVCeD5DxqKrLd8DQrby9-0YjTXvxGOf4U,31018
+slideflow/studio/widgets/slide.py,sha256=ZnSVXigl1vAsORISTfQyIllWDxkCzpnqHhhcoG8A0kU,47938
 slideflow/studio/widgets/stylegan.py,sha256=wV9mYOpEr11jRQd2e-yNHrbyShyjGaUsIKAvNmvjY4k,16402
 slideflow/test/__init__.py,sha256=PmF-W-f1BUtoBiNM50YN8hrYZlF0CuLQSCzI00kDsjM,32357
 slideflow/test/dataset_test.py,sha256=XJOx1T-J5Rcsjwx54kLY6-GuOQEgUtO_-a6sW9oXVvE,12446
 slideflow/test/functional.py,sha256=V6vs6N-ty49oan2KbM80nT4EmMis9-mQ3UZbVuH-woI,9560
 slideflow/test/model_test.py,sha256=rLiz27NDLOukTbTEPUL88a-aPeSe2ePgScatV4VM7_s,8052
 slideflow/test/norm_test.py,sha256=au7oXuHg-LzfX84nJw6-mSEKa4R46JL4eu9j7hW_r4U,12098
 slideflow/test/slide_test.py,sha256=P-cmg5JpL2LgpyrP61lF5YMpZ9vqmR6sSuJjNGFhIso,2909
 slideflow/test/stats_test.py,sha256=_n_9JHZZGSReWCR3usfid1uiFxGJ363UyFL0ah9U7f8,11481
 slideflow/test/utils.py,sha256=Qxmvs8FAxnVLOKjqstQIsE6DKZORPsvpRMVKsZmwMbQ,11752
 slideflow/tfrecord/__init__.py,sha256=YsF14xnyOnYgR2CCaj6zl0_YSeF7z3CqxZ0yO11dll0,891
-slideflow/tfrecord/iterator_utils.py,sha256=IFRjADsWLpfN65GSZp_1F2CEcgPRw77FszXHNcSegvs,2905
+slideflow/tfrecord/iterator_utils.py,sha256=U5m811mxQNTe5goTSZboRN0pIlFABEjX7YZ5AolGCCA,2866
 slideflow/tfrecord/reader.py,sha256=vGf6X-5JRMeUQN1gDDvfXJ131QbHEHhK16bq8CDN_Bw,15505
 slideflow/tfrecord/writer.py,sha256=47JbvdMp9xcx3RgacsIXZ0MlDm2bjREIYWG-XlcFzpQ,5637
 slideflow/tfrecord/tools/__init__.py,sha256=3082iuyLMnQ8Gc2WFwulW1sSJwBqIJPnBNoMT-VuW-o,179
 slideflow/tfrecord/torch/__init__.py,sha256=zl9XVfdCnojlhGDx0mwtqCbjMyj1ypROP-Ut2J_M7eQ,310
 slideflow/tfrecord/torch/dataset.py,sha256=Vc9q1vHG0xTtMu-tsi-kX1c9wM95BNWb9r5CWzh8B0w,7857
-slideflow/util/__init__.py,sha256=AI2iiKFF9EScqoauo5GQqxbWXcXePId48EmDfXfszKk,41943
+slideflow/util/__init__.py,sha256=_EJUPR4RTJxjFej2zZforBTDRtJTaGucNqWAm43XRDI,44423
 slideflow/util/colors.py,sha256=KOfggJhBNe3uHYa4MNQYdJnOb3A-Nyl1lxIMQ6Sqivc,738
 slideflow/util/example_pb2.py,sha256=oU4oQBXz89HAyrehrDeK-uJYqL7eNznjd8y3YDwn0dY,17912
 slideflow/util/log_utils.py,sha256=kyfTOCmSJW7gIW_1nHXiyPydN-jyO8YEBM3wiEb5OUc,4468
 slideflow/util/neptune_utils.py,sha256=rus5wDerStaFQalTbF9VaEbi6OelhkpkUHmwt0ggejQ,4381
 slideflow/util/smac_utils.py,sha256=T_-b1Wv0V2fQVX-FGlnhX1i4bu69imixlbAJd2aNJgs,20061
-slideflow/util/tfrecord2idx.py,sha256=wQjewrIjxXrC3Z9Zp9-jfjrPR-WOHC3lohoftSrxtEk,8064
-slideflow-2.0.5.data/scripts/slideflow-studio,sha256=JqgJ9btQo5hGFiw42aCQBvaot5PCcnJKU-maJK50O40,14085
-slideflow-2.0.5.data/scripts/slideflow-studio.py,sha256=dYg_ktDH_I74oIs8Sj4l56M-03frzO4KisE7WeXAofI,14085
-slideflow-2.0.5.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-slideflow-2.0.5.dist-info/METADATA,sha256=u673RraPx42h1c7Xc2Fti2LBV9Xti9cHJG1GzgDmb20,13038
-slideflow-2.0.5.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-slideflow-2.0.5.dist-info/top_level.txt,sha256=FRikcoh3_TcsLUYbI4LowF3nrEDWcGLaAyNgsi9Lu9M,10
-slideflow-2.0.5.dist-info/RECORD,,
+slideflow/util/tfrecord2idx.py,sha256=qr0nhiRhPy-gmZHQVnrYdjX_0HkiW0KeOLpj0sflrdo,10308
+slideflow-2.1.0.data/scripts/slideflow-studio,sha256=JqgJ9btQo5hGFiw42aCQBvaot5PCcnJKU-maJK50O40,14085
+slideflow-2.1.0.data/scripts/slideflow-studio.py,sha256=dYg_ktDH_I74oIs8Sj4l56M-03frzO4KisE7WeXAofI,14085
+slideflow-2.1.0.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+slideflow-2.1.0.dist-info/METADATA,sha256=DGz322Z1I5Q6rebDsxNwCoqNPc8Y44aqZWTiEeCSBoQ,13125
+slideflow-2.1.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+slideflow-2.1.0.dist-info/top_level.txt,sha256=FRikcoh3_TcsLUYbI4LowF3nrEDWcGLaAyNgsi9Lu9M,10
+slideflow-2.1.0.dist-info/RECORD,,
```

