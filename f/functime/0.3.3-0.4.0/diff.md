# Comparing `tmp/functime-0.3.3-py3-none-any.whl.zip` & `tmp/functime-0.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 62080 bytes, number of entries: 42
--rw-r--r--  2.0 unx       66 b- defN 23-Jul-31 15:46 functime/__init__.py
--rw-r--r--  2.0 unx     6143 b- defN 23-Jul-31 15:46 functime/backtesting.py
--rw-r--r--  2.0 unx     1838 b- defN 23-Jul-31 15:46 functime/conformal.py
--rw-r--r--  2.0 unx     2456 b- defN 23-Jul-31 15:46 functime/conversion.py
--rw-r--r--  2.0 unx     5549 b- defN 23-Jul-31 15:46 functime/cross_validation.py
--rw-r--r--  2.0 unx      123 b- defN 23-Jul-31 15:46 functime/embeddings.py
--rw-r--r--  2.0 unx     1495 b- defN 23-Jul-31 15:46 functime/offsets.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-31 15:46 functime/plotting.py
--rw-r--r--  2.0 unx    20573 b- defN 23-Jul-31 15:46 functime/preprocessing.py
--rw-r--r--  2.0 unx     1996 b- defN 23-Jul-31 15:46 functime/ranges.py
--rw-r--r--  2.0 unx      217 b- defN 23-Jul-31 15:46 functime/base/__init__.py
--rw-r--r--  2.0 unx     8265 b- defN 23-Jul-31 15:46 functime/base/forecaster.py
--rw-r--r--  2.0 unx     1746 b- defN 23-Jul-31 15:46 functime/base/metric.py
--rw-r--r--  2.0 unx     2771 b- defN 23-Jul-31 15:46 functime/base/model.py
--rw-r--r--  2.0 unx     2155 b- defN 23-Jul-31 15:46 functime/base/transformer.py
--rw-r--r--  2.0 unx      349 b- defN 23-Jul-31 15:46 functime/feature_extraction/__init__.py
--rw-r--r--  2.0 unx     4294 b- defN 23-Jul-31 15:46 functime/feature_extraction/calendar.py
--rw-r--r--  2.0 unx     2162 b- defN 23-Jul-31 15:46 functime/feature_extraction/fourier.py
--rw-r--r--  2.0 unx      783 b- defN 23-Jul-31 15:46 functime/forecasting/__init__.py
--rw-r--r--  2.0 unx    12609 b- defN 23-Jul-31 15:46 functime/forecasting/_ar.py
--rw-r--r--  2.0 unx     4893 b- defN 23-Jul-31 15:46 functime/forecasting/_evaluate.py
--rw-r--r--  2.0 unx     2231 b- defN 23-Jul-31 15:46 functime/forecasting/_reduction.py
--rw-r--r--  2.0 unx     6424 b- defN 23-Jul-31 15:46 functime/forecasting/_regressors.py
--rw-r--r--  2.0 unx     8513 b- defN 23-Jul-31 15:46 functime/forecasting/automl.py
--rw-r--r--  2.0 unx     2175 b- defN 23-Jul-31 15:46 functime/forecasting/catboost.py
--rw-r--r--  2.0 unx     3722 b- defN 23-Jul-31 15:46 functime/forecasting/censored.py
--rw-r--r--  2.0 unx    10936 b- defN 23-Jul-31 15:46 functime/forecasting/elite.py
--rw-r--r--  2.0 unx     1010 b- defN 23-Jul-31 15:46 functime/forecasting/knn.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Jul-31 15:46 functime/forecasting/lance.py
--rw-r--r--  2.0 unx     4208 b- defN 23-Jul-31 15:46 functime/forecasting/lightgbm.py
--rw-r--r--  2.0 unx     3958 b- defN 23-Jul-31 15:46 functime/forecasting/linear.py
--rw-r--r--  2.0 unx     1943 b- defN 23-Jul-31 15:46 functime/forecasting/naive.py
--rw-r--r--  2.0 unx     2412 b- defN 23-Jul-31 15:46 functime/forecasting/xgboost.py
--rw-r--r--  2.0 unx      312 b- defN 23-Jul-31 15:46 functime/metrics/__init__.py
--rw-r--r--  2.0 unx     3826 b- defN 23-Jul-31 15:46 functime/metrics/multi_objective.py
--rw-r--r--  2.0 unx     7432 b- defN 23-Jul-31 15:46 functime/metrics/point.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-31 15:46 functime/metrics/probabilistic.py
--rw-r--r--  2.0 unx    34523 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     7412 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3532 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/RECORD
-42 files, 188856 bytes uncompressed, 56454 bytes compressed:  70.1%
+Zip file size: 63325 bytes, number of entries: 42
+-rw-r--r--  2.0 unx       66 b- defN 23-Aug-02 13:57 functime/__init__.py
+-rw-r--r--  2.0 unx     6143 b- defN 23-Aug-02 13:57 functime/backtesting.py
+-rw-r--r--  2.0 unx     1838 b- defN 23-Aug-02 13:57 functime/conformal.py
+-rw-r--r--  2.0 unx     2456 b- defN 23-Aug-02 13:57 functime/conversion.py
+-rw-r--r--  2.0 unx     5549 b- defN 23-Aug-02 13:57 functime/cross_validation.py
+-rw-r--r--  2.0 unx      123 b- defN 23-Aug-02 13:57 functime/embeddings.py
+-rw-r--r--  2.0 unx     1495 b- defN 23-Aug-02 13:57 functime/offsets.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-02 13:57 functime/plotting.py
+-rw-r--r--  2.0 unx    24010 b- defN 23-Aug-02 13:57 functime/preprocessing.py
+-rw-r--r--  2.0 unx     1996 b- defN 23-Aug-02 13:57 functime/ranges.py
+-rw-r--r--  2.0 unx      217 b- defN 23-Aug-02 13:57 functime/base/__init__.py
+-rw-r--r--  2.0 unx     9150 b- defN 23-Aug-02 13:57 functime/base/forecaster.py
+-rw-r--r--  2.0 unx     1746 b- defN 23-Aug-02 13:57 functime/base/metric.py
+-rw-r--r--  2.0 unx     2771 b- defN 23-Aug-02 13:57 functime/base/model.py
+-rw-r--r--  2.0 unx     2155 b- defN 23-Aug-02 13:57 functime/base/transformer.py
+-rw-r--r--  2.0 unx      349 b- defN 23-Aug-02 13:57 functime/feature_extraction/__init__.py
+-rw-r--r--  2.0 unx     4447 b- defN 23-Aug-02 13:57 functime/feature_extraction/calendar.py
+-rw-r--r--  2.0 unx     1475 b- defN 23-Aug-02 13:57 functime/feature_extraction/fourier.py
+-rw-r--r--  2.0 unx      944 b- defN 23-Aug-02 13:57 functime/forecasting/__init__.py
+-rw-r--r--  2.0 unx    12651 b- defN 23-Aug-02 13:57 functime/forecasting/_ar.py
+-rw-r--r--  2.0 unx     4893 b- defN 23-Aug-02 13:57 functime/forecasting/_evaluate.py
+-rw-r--r--  2.0 unx     2231 b- defN 23-Aug-02 13:57 functime/forecasting/_reduction.py
+-rw-r--r--  2.0 unx     6424 b- defN 23-Aug-02 13:57 functime/forecasting/_regressors.py
+-rw-r--r--  2.0 unx     8727 b- defN 23-Aug-02 13:57 functime/forecasting/automl.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Aug-02 13:57 functime/forecasting/catboost.py
+-rw-r--r--  2.0 unx     3722 b- defN 23-Aug-02 13:57 functime/forecasting/censored.py
+-rw-r--r--  2.0 unx    15872 b- defN 23-Aug-02 13:57 functime/forecasting/elite.py
+-rw-r--r--  2.0 unx     1009 b- defN 23-Aug-02 13:57 functime/forecasting/knn.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Aug-02 13:57 functime/forecasting/lance.py
+-rw-r--r--  2.0 unx     4208 b- defN 23-Aug-02 13:57 functime/forecasting/lightgbm.py
+-rw-r--r--  2.0 unx     6426 b- defN 23-Aug-02 13:57 functime/forecasting/linear.py
+-rw-r--r--  2.0 unx     1943 b- defN 23-Aug-02 13:57 functime/forecasting/naive.py
+-rw-r--r--  2.0 unx     2412 b- defN 23-Aug-02 13:57 functime/forecasting/xgboost.py
+-rw-r--r--  2.0 unx      312 b- defN 23-Aug-02 13:57 functime/metrics/__init__.py
+-rw-r--r--  2.0 unx     3826 b- defN 23-Aug-02 13:57 functime/metrics/multi_objective.py
+-rw-r--r--  2.0 unx     7441 b- defN 23-Aug-02 13:57 functime/metrics/point.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-02 13:57 functime/metrics/probabilistic.py
+-rw-r--r--  2.0 unx    34523 b- defN 23-Aug-02 13:58 functime-0.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     7553 b- defN 23-Aug-02 13:58 functime-0.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-02 13:58 functime-0.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Aug-02 13:58 functime-0.4.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3532 b- defN 23-Aug-02 13:58 functime-0.4.0.dist-info/RECORD
+42 files, 200614 bytes uncompressed, 57699 bytes compressed:  71.2%
```

## zipnote {}

```diff
@@ -105,23 +105,23 @@
 
 Filename: functime/metrics/point.py
 Comment: 
 
 Filename: functime/metrics/probabilistic.py
 Comment: 
 
-Filename: functime-0.3.3.dist-info/LICENSE
+Filename: functime-0.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: functime-0.3.3.dist-info/METADATA
+Filename: functime-0.4.0.dist-info/METADATA
 Comment: 
 
-Filename: functime-0.3.3.dist-info/WHEEL
+Filename: functime-0.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: functime-0.3.3.dist-info/top_level.txt
+Filename: functime-0.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: functime-0.3.3.dist-info/RECORD
+Filename: functime-0.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## functime/preprocessing.py

```diff
@@ -1,11 +1,12 @@
 from typing import List, Mapping, Union
 
 import polars as pl
 import polars.selectors as cs
+from scipy import optimize
 from scipy.stats import boxcox_normmax
 from typing_extensions import Literal
 
 from functime.base import transformer
 from functime.base.model import ModelState
 from functime.offsets import _strip_freq_alias
 
@@ -47,15 +48,17 @@
     Parameters
     ----------
     schema : Mapping[str, pl.DataType]
         A dictionary-like object mapping column names to the desired data types.
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
-        X_new = X.cast({pl.col(col).cast(dtype) for col, dtype in schema.items()})
+        X_new = X.with_columns(
+            [pl.col(col).cast(dtype) for col, dtype in schema.items()]
+        )
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 @transformer
@@ -111,15 +114,15 @@
             X.lazy()
             .groupby_dynamic(time_col, every=freq, by=entity_col)
             .agg(agg_exprs[agg_method])
             # Must defensive sort columns otherwise time_col and target_col
             # positions are incorrectly swapped in lazy
             .select([entity_col, time_col, target_col])
             # Impute gaps after reindex
-            .pipe(experimental_impute(impute_method))
+            .pipe(impute(impute_method))
             # Defensive fill null with 0 for impute method `ffill`
             .fill_null(0)
         )
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
@@ -409,22 +412,22 @@
             X = X.with_columns(pl.col(pl.Boolean).cast(pl.Int8) * 2 - 1)
         return X
 
     return transform, invert, transform_new
 
 
 @transformer
-def experimental_impute(
+def impute(
     method: Union[
         Literal["mean", "median", "fill", "ffill", "bfill", "interpolate"],
         Union[int, float],
     ]
 ):
     """
-    [EXPERIMENTAL] Performs missing value imputation on numeric columns of a DataFrame grouped by entity.
+    Performs missing value imputation on numeric columns of a DataFrame grouped by entity.
 
     Parameters
     ----------
     method : Union[str, int, float]
         The imputation method to use.
 
         Supported methods are:\n
@@ -498,48 +501,54 @@
             ]
         )
         for _ in range(order):
             X = X.select([entity_col, time_col, cs.float().diff(n=sp).over(entity_col)])
 
         # Drop null
         artifacts = {
-            "X_new": X.drop_nulls(),
+            "X_new": X.fill_null(strategy="backward"),
             "X_first": X_first.lazy(),
             "X_last": X_last.lazy(),
         }
         return artifacts
 
     def invert(
         state: ModelState, X: pl.LazyFrame, from_last: bool = False
     ) -> pl.LazyFrame:
         artifacts = state.artifacts
         entity_col = X.columns[0]
         time_col = X.columns[1]
         idx_cols = entity_col, time_col
 
         X_cutoff = artifacts["X_last"] if from_last else artifacts["X_first"]
-        X_new = (
-            pl.concat(
-                [
-                    X,
-                    X_cutoff.select(
-                        pl.col(col).cast(dtype) for col, dtype in X.schema.items()
-                    ),
-                ],
-                how="diagonal",
-            )
-            .drop_nulls()
-            .sort(idx_cols)
-        )
+        X_new = pl.concat(
+            [
+                X,
+                X_cutoff.select(
+                    pl.col(col).cast(dtype) for col, dtype in X.schema.items()
+                ),
+            ],
+            how="diagonal",
+        ).sort(idx_cols)
         for _ in range(order):
             X_new = X_new.select(
                 [entity_col, time_col, cs.float().cumsum().over(entity_col)]
             )
 
-        return X.select(idx_cols).join(X_new, on=idx_cols, how="left")
+        X_new = (
+            X.select(idx_cols)
+            # Must drop duplicates to deal with case where
+            # X to be inverted starts with timestamp == cutoff
+            .join(
+                X_new.unique(subset=[entity_col, time_col], keep="last"),
+                on=idx_cols,
+                how="left",
+            )
+        )
+        return X_new
 
     return transform, invert
 
 
 @transformer
 def boxcox(method: str = "mle"):
     """Applies the Box-Cox transformation to numeric columns in a DataFrame.
@@ -551,21 +560,29 @@
 
         Supported methods:\n
         - `mle`: maximum likelihood estimation
         - `pearsonr`: Pearson correlation coefficient
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
+        def optimizer(fun):
+            return optimize.minimize_scalar(
+                fun,
+                bounds=(-2.0, 2.0),
+                method="bounded",
+                options={"maxiter": 200, "xatol": 1e-12},
+            )
+
         idx_cols = X.columns[:2]
         entity_col, time_col = idx_cols
         gb = X.groupby(X.columns[0])
         # Step 1. Compute optimal lambdas
         lmbds = gb.agg(
             PL_NUMERIC_COLS(entity_col, time_col)
-            .apply(lambda x: boxcox_normmax(x, method=method))
+            .apply(lambda x: boxcox_normmax(x, method=method, optimizer=optimizer))
             .cast(pl.Float64())
             .suffix("__lmbd")
         )
         # Step 2. Transform
         cols = X.select(PL_NUMERIC_COLS(entity_col, time_col)).columns
         X_new = X.join(lmbds, on=entity_col, how="left").select(
             idx_cols
@@ -578,25 +595,103 @@
                 for col in cols
             ]
         )
         artifacts = {"X_new": X_new, "lmbds": lmbds}
         return artifacts
 
     def invert(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
-        idx_cols = X.columns[:2]
+        entity_col, time_col = X.columns[:2]
         lmbds = state.artifacts["lmbds"]
-        cols = X.select(PL_NUMERIC_COLS(state.time)).columns
-        X_new = X.join(lmbds, on=X.columns[0], how="left", suffix="__lmbd").select(
-            idx_cols
-            + [
-                pl.when(pl.col(f"{col}__lmbd") == 0)
-                .then(pl.col(col).exp())
-                .otherwise(
-                    (pl.col(f"{col}__lmbd") * pl.col(col) + 1)
-                    ** (1 / pl.col(f"{col}__lmbd"))
-                )
-                for col in cols
-            ]
+        cols = X.select(PL_NUMERIC_COLS(entity_col, time_col)).columns
+        X_new = (
+            X.join(lmbds, on=entity_col, how="left", suffix="__lmbd")
+            .with_columns(
+                [
+                    pl.when(pl.col(f"{col}__lmbd") == 0)
+                    .then(pl.col(col).exp())
+                    .otherwise(
+                        (pl.col(col) * pl.col(f"{col}__lmbd") + 1)
+                        ** (1 / pl.col(f"{col}__lmbd"))
+                    )
+                    for col in cols
+                ]
+            )
+            .select(X.columns)
         )
         return X_new
 
     return transform, invert
+
+
+@transformer
+def detrend(method: Literal["linear", "mean"] = "linear"):
+    """ """
+
+    def transform(X: pl.LazyFrame) -> pl.LazyFrame:
+        entity_col, time_col = X.columns[:2]
+        if method == "linear":
+            x = pl.col(time_col).to_physical()
+            betas = [
+                (pl.cov(col, x) / x.var()).over(entity_col).alias(f"{col}__beta")
+                for col in X.columns[2:]
+            ]
+            alphas = [
+                (
+                    pl.col(col).mean().over(entity_col)
+                    - pl.col(f"{col}__beta") * x.mean()
+                ).alias(f"{col}__alpha")
+                for col in X.columns[2:]
+            ]
+            residuals = [
+                pl.col(col) - pl.col(f"{col}__alpha") - pl.col(f"{col}__beta") * x
+                for col in X.columns[2:]
+            ]
+            X_new = X.with_columns(betas).with_columns(alphas).with_columns(residuals)
+            artifacts = {
+                "_beta": X_new.select([entity_col, cs.ends_with("__beta")])
+                .unique()
+                .collect(streaming=True),
+                "_alpha": X_new.select([entity_col, cs.ends_with("__alpha")])
+                .unique()
+                .collect(streaming=True),
+                "X_new": X_new.select(X.columns),
+            }
+        if method == "mean":
+            _mean = X.groupby(entity_col).agg(
+                pl.col(X.columns[2:]).mean().suffix("__mean")
+            )
+            X_new = X.with_columns(
+                pl.col(X.columns[2:]) - pl.col(X.columns[2:]).mean().over(entity_col)
+            )
+            _mean, X_new = pl.collect_all([_mean, X_new])
+            artifacts = {"_mean": _mean, "X_new": X_new.lazy()}
+        return artifacts
+
+    def invert(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
+        entity_col, time_col = X.columns[:2]
+        if method == "linear":
+            _beta = state.artifacts["_beta"]
+            _alpha = state.artifacts["_alpha"]
+            X_new = (
+                X.join(_beta.lazy(), on=entity_col, how="left")
+                .join(_alpha.lazy(), on=entity_col, how="left")
+                .with_columns(
+                    [
+                        pl.col(col)
+                        + pl.col(f"{col}__alpha")
+                        + pl.col(f"{col}__beta") * pl.col(time_col).to_physical()
+                        for col in X.columns[2:]
+                    ]
+                )
+                .select(X.columns)
+            )
+        else:
+            X_new = (
+                X.join(state.artifacts["_mean"].lazy(), on=entity_col, how="left")
+                .with_columns(
+                    [pl.col(col) + pl.col(f"{col}__mean") for col in X.columns[2:]]
+                )
+                .select(X.columns)
+            )
+        return X_new
+
+    return transform, invert
```

## functime/base/forecaster.py

```diff
@@ -13,14 +13,30 @@
 # The return type of the esimator's curried function
 R = Tuple[TypeVar("fit", bound=Callable), TypeVar("predict", bound=Callable)]
 
 FORECAST_STRATEGIES = Optional[Literal["direct", "recursive", "naive"]]
 DF_TYPE = Union[pl.LazyFrame, pl.DataFrame]
 
 
+SUPPORTED_FREQ = [
+    "1i",
+    "1m",
+    "5m",
+    "10m",
+    "15m",
+    "30m",
+    "1h",
+    "1d",
+    "1w",
+    "1mo",
+    "3mo",
+    "1y",
+]
+
+
 @dataclass(frozen=True)
 class ForecastState(ModelState):
     target: str
     target_schema: Mapping[str, pl.DataType]
     strategy: Optional[str] = "naive"
     features: Optional[List[str]] = None
 
@@ -38,32 +54,40 @@
         Maximum number of horizons to predict directly.
         Only applied if `strategy` equals "direct" or "ensemble".
     strategy : Optional[str]
         Forecasting strategy. Currently supports "recursive", "direct",
         and "ensemble" of both recursive and direct strategies.
     target_transform : Optional[Transformer]
         functime transformer to apply to `y` before fit. The transform is inverted at predict time.
+    feature_transform : Optional[Transformer]
+        functime transformer to apply to `X` before fit and predict.
     **kwargs : Mapping[str, Any]
         Additional keyword arguments passed into underlying sklearn-compatible regressor.
     """
 
     def __init__(
         self,
         freq: Union[str, None],
         lags: int,
         max_horizons: Optional[int] = None,
         strategy: FORECAST_STRATEGIES = None,
         target_transform: Optional[Transformer] = None,
+        feature_transform: Optional[Transformer] = None,
         **kwargs,
     ):
+
+        if freq not in SUPPORTED_FREQ:
+            raise ValueError(f"Offset {freq} not supported")
+
         self.freq = freq
         self.lags = lags
         self.max_horizons = max_horizons
         self.strategy = strategy
         self.target_transform = target_transform
+        self.feature_transform = feature_transform
         self.kwargs = kwargs
         super().__init__()
 
     def __call__(
         self,
         y: DF_TYPE,
         fh: int,
@@ -73,25 +97,41 @@
         self.fit(y=y, X=X)
         return self.predict(fh=fh, X=X_future)
 
     @property
     def name(self):
         return f"{self.__class__.__name__}(strategy={self.strategy})"
 
+    def _transform_X(self, y: DF_TYPE, X: Optional[DF_TYPE] = None):
+        # Feature transform
+        if X is None:
+            X_new = (
+                y.select(y.columns[:2])
+                .pipe(self.feature_transform)
+                .collect(streaming=True)
+                .lazy()
+            )
+        else:
+            X_new = X.pipe(self.feature_transform).collect(streaming=True).lazy()
+        return X_new
+
     def fit(self, y: DF_TYPE, X: Optional[DF_TYPE] = None):
         # Prepare y
         target_transform = self.target_transform
         y: pl.LazyFrame = self._set_string_cache(y.lazy().collect()).lazy()
         if target_transform is not None:
-            y = y.pipe(self.target_transform).collect(streaming=True).lazy()
+            y = y.pipe(target_transform).collect(streaming=True).lazy()
         # Prepare X
         if X is not None:
             if X.columns[0] == y.columns[0]:
                 X = self._enforce_string_cache(X.lazy().collect())
             X = X.lazy()
+        # Feature transform
+        if self.feature_transform is not None:
+            X = self._transform_X(X=X, y=y)
         # Fit AR forecaster
         artifacts = self._fit(y=y, X=X)
         # Prepare artifacts
         cutoffs = y.groupby(y.columns[0]).agg(pl.col(y.columns[1]).max().alias("low"))
         artifacts["__cutoffs"] = cutoffs.collect(streaming=True)
         state = ForecastState(
             entity=y.columns[0],
@@ -112,20 +152,22 @@
 
         state = self.state
         entity_col = state.entity
         time_col = state.time
         target_col = state.target
         # Cutoffs cannot be lazy
         cutoffs: pl.DataFrame = state.artifacts["__cutoffs"]
+        # Get entity, time "index" for forecast
         future_ranges = make_future_ranges(
             time_col=state.time,
             cutoffs=cutoffs,
             fh=fh,
             freq=self.freq,
         )
+        # Prepare X
         if X is not None:
             X = X.lazy()
             # Coerce X (can be panel / time series / cross sectional) into panel
             # and aggregate feature columns into lists
             has_entity = X.columns[0] == state.entity
             has_time = X.columns[1] == state.time
 
@@ -133,20 +175,20 @@
                 X = self._enforce_string_cache(X.lazy().collect()).lazy()
 
             if has_entity and not has_time:
                 X = future_ranges.lazy().join(X, on=entity_col, how="left")
             elif has_time and not has_entity:
                 X = future_ranges.lazy().join(X, on=time_col, how="left")
 
-            # NOTE: Unlike `y_lag` we DO NOT reshape exogenous features
-            # into list columns. This is because .arr[i] with List[cat] does
-            # not seem to support null values
-            # Raises: ComputeError: cannot construct Categorical
-            # from these categories, at least on of them is out of bounds
-            X = X.select(pl.all().exclude(time_col)).lazy()
+        # Feature transform
+        if self.feature_transform is not None:
+            X = self._transform_X(
+                X=X, y=future_ranges.explode(pl.all().exclude(entity_col))
+            )
+
         y_pred_vals = predict_autoreg(self.state, fh=fh, X=X)
         y_pred_vals = y_pred_vals.sort(by=entity_col).select(
             pl.col(y_pred_vals.columns[-1]).alias(target_col)
         )
         y_pred = pl.concat(
             [future_ranges.sort(by=entity_col), y_pred_vals], how="horizontal"
         ).explode(pl.all().exclude(entity_col))
```

## functime/feature_extraction/calendar.py

```diff
@@ -49,24 +49,23 @@
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 @transformer
-def add_holiday_effects(country_codes: List[str], freq: str):
+def add_holiday_effects(country_codes: List[str], as_dummies: bool = False):
     """Extract holiday effects from time column for specified ISO-2 country codes and frequency.
 
     Parameters
     ----------
     country_codes : List[str]
         A list of ISO-2 country codes.
-    freq : str
-        Sampling frequency at which to group data.
-        Must be specified as an offset alias supported by Polars.
+    as_dummies : bool
+        Returns calendar effects as columns of one-hot-encoded dummies.
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
 
         # Get min and max timestamps
         time_col = X.columns[1]
         timestamps = (
@@ -101,14 +100,20 @@
             )
             .with_columns(
                 pl.Series(values=timestamps, name=time_col).cast(X.schema[time_col])
             )
             .lazy()
         )
         X_new = X.join(holidays, how="left", on=time_col)
+        if as_dummies:
+            X_new = (
+                X_new.collect(streaming=True)
+                .to_dummies(columns=holidays.columns[1:])
+                .lazy()
+            )
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 def make_future_calendar_effects(
@@ -139,9 +144,9 @@
     cutoffs = idx.groupby(entity_col).agg(pl.col(time_col).max().alias("low"))
     future_idx = make_future_ranges(
         time_col=time_col,
         cutoffs=cutoffs,
         fh=fh,
         freq=freq,
     ).explode(time_col)
-    transf = add_holiday_effects(country_codes, freq=freq)
+    transf = add_holiday_effects(country_codes)
     return transf(future_idx)
```

## functime/feature_extraction/fourier.py

```diff
@@ -5,40 +5,25 @@
 
 
 @transformer
 def add_fourier_terms(sp: int, K: int):
     """Fourier features for time series seasonality.
 
     Fourier Series terms can be used as explanatory variables for the cases of multiple
-    seasonal periods and or complex / long seasonal periods [1]_, [2]_. For every
-    seasonal period, :math:`sp` and fourier term :math:`k` pair there are 2 fourier
-    terms sin_sp_k and cos_sp_k:
-        - sin_sp_k = :math:`sin(\frac{2 \\pi k t}{sp})`
-        - cos_sp_k = :math:`cos(\frac{2 \\pi k t}{sp})`
+    seasonal periods and or complex / long seasonal periods.
 
-    Where :math:`t` is the number of time steps elapsed from the beginning of the time series.
-
-    The implementation is based on the fourier function from the R forecast package [3]_
+    The implementation is based on the Fourier function from the R [forecast package](https://pkg.robjhyndman.com/forecast/reference/fourier.html).
 
     Parameters
     ----------
     sp: int
         Seasonal period.
     K : int
         Maximum order(s) of Fourier terms.
         Must be less than `sp`.
-
-    References
-    ----------
-    .. [1] Hyndsight - Forecasting with long seasonal periods:
-        https://robjhyndman.com/hyndsight/longseasonality/
-    .. [2] Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and
-        practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3.
-        Accessed on August 14th 2022.
-    .. [3] https://pkg.robjhyndman.com/forecast/reference/fourier.html
     """
 
     if K > sp:
         raise ValueError("`K` must be less than `sp`")
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
         entity_col, time_col = X.columns[:2]
@@ -49,13 +34,13 @@
             for k in range(1, n_cos_terms + 1)
         ]
         sin_terms = [
             np.sin(2 * np.pi * k * pl.col("fourier_coef")).alias(f"sin_{sp}_{k}")
             for k in range(1, n_sin_terms + 1)
         ]
         X_new = X.with_columns(
-            (pl.col(time_col).arg_sort().over(entity_col) / sp).alias("fourier_coef")
-        ).with_columns(cos_terms + sin_terms)
+            (pl.col(time_col).to_physical().over(entity_col) / sp).alias("fourier_coef")
+        ).select([entity_col, time_col, *cos_terms, *sin_terms])
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
```

## functime/forecasting/__init__.py

```diff
@@ -8,15 +8,24 @@
 )
 from .catboost import catboost
 from .censored import censored_model, zero_inflated_model
 from .elite import elite
 from .knn import knn
 from .lance import ann
 from .lightgbm import flaml_lightgbm, lightgbm
-from .linear import elastic_net, lasso, linear_model, ridge
+from .linear import (
+    elastic_net,
+    elastic_net_cv,
+    lasso,
+    lasso_cv,
+    linear_model,
+    ridge,
+    ridge_cv,
+)
+from .naive import naive
 from .xgboost import xgboost
 
 __all__ = [
     "ann",
     "auto_elastic_net",
     "auto_knn",
     "auto_lasso",
@@ -26,13 +35,17 @@
     "catboost",
     "censored_model",
     "elastic_net",
     "elite",
     "flaml_lightgbm",
     "knn",
     "lasso",
+    "lasso_cv",
+    "ridge_cv",
+    "elastic_net_cv",
     "lightgbm",
     "linear_model",
     "ridge",
     "xgboost",
+    "naive",
     "zero_inflated_model",
 ]
```

## functime/forecasting/_ar.py

```diff
@@ -337,19 +337,21 @@
 
 def predict_autoreg(
     state,
     fh: int,
     X: Optional[Union[pl.DataFrame, pl.LazyFrame]] = None,
 ) -> pl.DataFrame:
     strategy = state.strategy
+    time_col = state.time
     predict_kwargs = {
         "state": state,
         "fh": fh,
-        "X": X.lazy().collect() if X is not None else None,
+        "X": X.drop(time_col).lazy().collect() if X is not None else None,
     }
+
     if strategy == "recursive":
         y_pred = predict_recursive(**predict_kwargs)
     elif strategy == "direct":
         y_pred = predict_direct(**predict_kwargs)
     elif strategy == "ensemble":
         target_col = state.target
         y_pred_rec = predict_recursive(**predict_kwargs).rename(
```

## functime/forecasting/automl.py

```diff
@@ -2,15 +2,15 @@
 from functools import partial
 from typing import Any, Mapping, Optional, Union
 
 import polars as pl
 from flaml import tune
 from typing_extensions import Literal
 
-from functime.base.forecaster import FORECAST_STRATEGIES, Forecaster
+from functime.base.forecaster import FORECAST_STRATEGIES, SUPPORTED_FREQ, Forecaster
 from functime.base.transformer import Transformer
 from functime.forecasting.knn import knn
 from functime.forecasting.lightgbm import lightgbm
 from functime.forecasting.linear import elastic_net, lasso, linear_model, ridge
 
 
 class AutoForecaster(Forecaster):
@@ -42,50 +42,56 @@
         Equivalent to `config` in [FLAML](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#search-space)
     points_to_evaluate : Optional[dict]
         Equivalent to `points_to_evaluate` in [FLAML](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#warm-start)
     num_samples : int
         Number of hyper-parameter sets to test. -1 means unlimited (until `time_budget` is exhausted.)
     target_transform : Optional[Transformer]
         functime transformer to apply to `y` before fit. The transform is inverted at predict time.
+    feature_transform : Optional[Transformer]
+        functime transformer to apply to `X` before fit and predict.
     **kwargs : Mapping[str, Any]
         Additional keyword arguments passed into underlying sklearn-compatible regressor.
     """
 
     def __init__(
         self,
-        # NOTE: MUST EXPLICITLY SPECIFIC FREQ IN ORDER FOR
-        # CROSS-VALIDATION y_pred and y_test time index to match up
         freq: Union[str, None],
         min_lags: int = 3,
         max_lags: int = 12,
         max_horizons: Optional[int] = None,
         strategy: FORECAST_STRATEGIES = None,
         test_size: int = 1,
         step_size: int = 1,
         n_splits: int = 5,
         time_budget: int = 5,
         search_space: Optional[Mapping[str, Any]] = None,
         points_to_evaluate: Optional[Mapping[str, Any]] = None,
         num_samples: int = -1,
         target_transform: Optional[Transformer] = None,
+        feature_transform: Optional[Transformer] = None,
         **kwargs,
     ):
+
+        if freq not in SUPPORTED_FREQ:
+            raise ValueError(f"Offset {freq} not supported")
+
         self.freq = freq
         self.min_lags = min_lags
         self.max_lags = max_lags
         self.max_horizons = max_horizons
         self.strategy = strategy
         self.test_size = test_size
         self.step_size = step_size
         self.n_splits = n_splits
         self.time_budget = time_budget
         self.search_space = search_space
         self.points_to_evaluate = points_to_evaluate
         self.num_samples = num_samples
         self.target_transform = target_transform
+        self.feature_transform = feature_transform
         self.kwargs = kwargs
 
     @property
     @abstractmethod
     def forecaster(self):
         pass
```

## functime/forecasting/elite.py

```diff
@@ -1,22 +1,28 @@
+from functools import partial
 from typing import Any, Mapping, Optional, Union
 
 import polars as pl
+import polars.selectors as cs
 from sklearn.linear_model import LassoLarsIC
 from tqdm import tqdm
+from typing_extensions import Literal
 
 from functime.backtesting import backtest
 from functime.base.forecaster import Forecaster
 from functime.base.metric import METRIC_TYPE
 from functime.conversion import X_to_numpy, y_to_numpy
 from functime.cross_validation import expanding_window_split
-from functime.forecasting.linear import lasso, linear_model, ridge
+from functime.feature_extraction import add_fourier_terms
+from functime.forecasting.knn import knn
+from functime.forecasting.linear import lasso_cv, linear_model, ridge_cv
 from functime.forecasting.naive import naive
-from functime.metrics import rmse
+from functime.metrics import mae
 from functime.offsets import freq_to_sp
+from functime.preprocessing import coerce_dtypes, detrend, diff, scale
 
 
 class elite(Forecaster):
     """Global ELITE forecasting procedure with expanding windows for cross-validation.
 
     Uses `sklearn.linear_model.LassoLarsIC` for stacking.
 
@@ -25,33 +31,34 @@
 
     Parameters
     ----------
     freq : str
         Offset alias supported by Polars.
     lags : int
         Number of lagged target variables.
-    max_fh : Optional[int]
-        Max forecast horizon (required for `naive` forecaster).
-        If None, defaults to `test_size`.
+    max_fh : int
+        Max forecast horizon.
+        `fh` in predict cannot exceed this value.
     sp : Optional[int]
         Seasonal periods; length of one seasonal cycle.
     forecasters : Optional[Mapping[str, Forecaster]]
         Mapping of name to forecaster class to fit.
         A `naive` forecaster is always fit as the fallback.
     model_kwargs : Optional[Mapping[str, Mapping[str, Any]]]
         Mapping of forecaster name to model kwargs passed into the underlying sklearn-compatible regressor.
+    ensemble_strategy : Literal["lasso", "log_lasso", "mean"]
+        Strategy to stack base forecasts.
     top_k : Optional[int]
         Select top k performing forecasters from cross-validation to ensemble.
         Defaults to 4 if None.
     scoring : Optional[metric]
-        If None, defaults to RMSE.
+        If None, defaults to MAE.
     test_size : Optional[int]
         Number of test samples for each split.
-        If None, defaults to equal to one seasonal period given `freq`
-        (e.g. `test_size=12` if freq is monthly `1mo`).
+        If None, defaults to `max_fh`.
     step_size : Optional[int]
         Step size between windows.
     n_splits : Optional[int]
         Number of splits. Defaults to 3.
     **kwargs : Mapping[str, Any]
         Additional keyword arguments passed into the final stacking regressor (i.e. `sklearn.linear_model.LassoLarsIC`)
     """
@@ -60,50 +67,117 @@
         self,
         freq: Union[str, None],
         lags: int,
         max_fh: Optional[int] = None,
         sp: Optional[int] = None,
         forecasters: Optional[Mapping[str, Forecaster]] = None,
         model_kwargs: Optional[Mapping[str, Mapping[str, Any]]] = None,
+        ensemble_strategy: Literal["lasso", "log_lasso", "mean"] = "mean",
         top_k: Optional[int] = None,
         scoring: Optional[METRIC_TYPE] = None,
         test_size: Optional[int] = None,
         step_size: Optional[int] = None,
         n_splits: Optional[int] = None,
         **kwargs,
     ):
         self.max_fh = max_fh
-        self.sp = sp
+        self.sp = sp or freq_to_sp(freq=freq)[0]
         self.forecasters = forecasters or {
-            # # "Seasonality" models
-            # "knn": knn,
+            # "Seasonality" models
+            "knn": partial(knn, n_neighbors=lags // 2),
+            "knn_scaled": partial(knn, n_neighbors=lags // 2),
+            "knn_detrend_linear": partial(
+                knn, n_neighbors=lags // 2, target_transform=detrend(method="linear")
+            ),
             # AR linear models
             "linear": linear_model,
-            "ridge": ridge,
-            "lasso": lasso,
-            # # AR models with Fourier terms
-            # # AR models with box-cox scaling
-            # "linear_boxcox": linear_model,
-            # "ridge_boxcox": lasso,
-            # "lasso_boxcox": ridge,
-            # # AR models with box-cox scaling and Fourier terms
-            # "linear_boxcox_fourier": linear_model,
-            # "ridge_boxcox_fourier": lasso,
-            # "lasso_boxcox_fourier": ridge,
-            # # Linear detrended AR models
-            # "linear_detrend": linear_model,
-            # "ridge_detrend": lasso,
-            # "lasso_detrend": ridge,
+            "ridge": ridge_cv,
+            "lasso": lasso_cv,
+            # AR linear models without drift
+            "linear_no_drift": partial(linear_model, fit_intercept=False),
+            "ridge_no_drift": partial(ridge_cv, fit_intercept=False),
+            "lasso_no_drift": partial(lasso_cv, fit_intercept=False),
+            # AR models with local scaling
+            "linear_scaled": partial(linear_model, target_transform=scale()),
+            "ridge_scaled": partial(ridge_cv, target_transform=scale()),
+            "lasso_scaled": partial(lasso_cv, target_transform=scale()),
+            # AR models with first differences
+            "linear_diff": partial(linear_model, target_transform=diff(order=1)),
+            "ridge_diff": partial(ridge_cv, target_transform=diff(order=1)),
+            "lasso_diff": partial(lasso_cv, target_transform=diff(order=1)),
+            # AR models with Fourier terms (defaults to K=6)
+            "linear_fourier": partial(
+                linear_model, feature_transform=add_fourier_terms(sp=self.sp, K=6)
+            ),
+            "ridge_fourier": partial(
+                ridge_cv, feature_transform=add_fourier_terms(sp=self.sp, K=6)
+            ),
+            "lasso_fourier": partial(
+                lasso_cv, feature_transform=add_fourier_terms(sp=self.sp, K=6)
+            ),
+            "linear_scaled_fourier": partial(
+                linear_model,
+                target_transform=scale(),
+                feature_transform=add_fourier_terms(sp=self.sp, K=6),
+            ),
+            "ridge_scaled_fourier": partial(
+                ridge_cv,
+                target_transform=scale(),
+                feature_transform=add_fourier_terms(sp=self.sp, K=6),
+            ),
+            "lasso_scaled_fourier": partial(
+                lasso_cv,
+                target_transform=scale(),
+                feature_transform=add_fourier_terms(sp=self.sp, K=6),
+            ),
+            # Linear detrended AR models
+            "linear_detrend_linear": partial(
+                linear_model, target_transform=detrend(method="linear")
+            ),
+            "ridge_detrend_linear": partial(
+                ridge_cv, target_transform=detrend(method="linear")
+            ),
+            "lasso_detrend_linear": partial(
+                lasso_cv, target_transform=detrend(method="linear")
+            ),
+            # Mean detrended models
+            "linear_detrend_mean": partial(
+                linear_model, target_transform=detrend(method="mean")
+            ),
+            "ridge_detrend_mean": partial(
+                ridge_cv, target_transform=detrend(method="mean")
+            ),
+            "lasso_detrend_mean": partial(
+                lasso_cv, target_transform=detrend(method="mean")
+            ),
+            # Linear detrended fourier AR models
+            "linear_detrend_linear_fourier": partial(
+                linear_model,
+                target_transform=detrend(method="linear"),
+                feature_transform=add_fourier_terms(sp=self.sp, K=12),
+            ),
+            "ridge_detrend_linear_fourier": partial(
+                ridge_cv,
+                target_transform=detrend(method="linear"),
+                feature_transform=add_fourier_terms(sp=self.sp, K=12),
+            ),
+            "lasso_detrend_linear_fourier": partial(
+                lasso_cv,
+                target_transform=detrend(method="linear"),
+                feature_transform=add_fourier_terms(sp=self.sp, K=12),
+            ),
         }
+
         self.model_kwargs = model_kwargs or {}
-        self.top_k = top_k or 4
+        self.ensemble_strategy = ensemble_strategy
+        self.top_k = top_k or 12
         self.scoring = scoring
-        self.test_size = test_size
-        self.step_size = step_size
-        self.n_splits = n_splits
+        self.test_size = test_size or max_fh
+        self.step_size = step_size or self.test_size
+        self.n_splits = n_splits or 3
         super().__init__(freq=freq, lags=lags, **kwargs)
 
     def _get_X_stack(
         self,
         y_pred: pl.DataFrame,
         best_models: pl.DataFrame,
         X: Optional[pl.DataFrame] = None,
@@ -133,53 +207,55 @@
         )
         if X is not None:
             X_stack = (
                 X_stack.lazy()
                 .join(X.lazy(), on=[entity_col, time_col], how="left")
                 .collect(streaming=True)
             )
+        X_stack = X_stack.with_columns(
+            trend=pl.col(time_col).arg_sort().over(entity_col)
+        )
         return X_stack
 
     def _fit(self, y: pl.LazyFrame, X: Optional[pl.LazyFrame] = None):
         freq = self.freq
         lags = self.lags
         top_k = self.top_k
         model_kwargs = self.model_kwargs
-        score = self.scoring or rmse
+        score = self.scoring or mae
         metric_name = score.__name__
         entity_col, time_col, target_col = y.columns
 
         # 1. Cross validation
-        sp = self.sp or freq_to_sp(freq=freq)
-        test_size = self.test_size or sp
-        max_fh = self.max_fh or test_size
-        step_size = self.step_size or sp
-        n_splits = self.n_splits or 3
+        test_size = self.test_size
+        max_fh = self.max_fh
+        step_size = self.step_size
+        n_splits = self.n_splits
         cv = expanding_window_split(
             test_size=test_size,
             step_size=step_size,
             n_splits=n_splits,
         )
         cv_y_preds = {}
+        schema = y.schema
+        forecasters = {**self.forecasters, "naive": naive}
         # NOTE: Parallelized version available in Cloud
-        for model_name, forecaster_cls in (pbar := tqdm(self.forecasters.items())):
+        for model_name, forecaster_cls in (pbar := tqdm(forecasters.items())):
             pbar.set_description(f"Cross-validating [forecaster={model_name}]")
             # TODO: Investigate using residuals to quantify model uncertainty
-            forecaster = forecaster_cls(
-                freq=freq, lags=lags, **model_kwargs.get(model_name, {})
-            )
+            if model_name != "naive":
+                forecaster = forecaster_cls(
+                    freq=freq, lags=lags, **model_kwargs.get(model_name, {})
+                )
+            else:
+                forecaster = forecaster_cls(freq=freq, max_fh=test_size)
             y_preds = backtest(forecaster=forecaster, y=y, cv=cv, residualize=False)
-            cv_y_preds[model_name] = y_preds
-
-        # 2. Fit naive (fallback)
-        cv_y_preds["naive"] = backtest(
-            forecaster=naive(freq=freq, max_fh=test_size), y=y, cv=cv, residualize=False
-        )
+            cv_y_preds[model_name] = y_preds.pipe(coerce_dtypes(schema)).collect()
 
-        # 3. Score individual forecasters
+        # 2. Score individual forecasters
         cv_scores = []
         for model_name, y_preds in (pbar := tqdm(cv_y_preds.items())):
             pbar.set_description(f"Scoring [forecaster={model_name}]")
             for split in range(n_splits):
                 y_pred = y_preds.filter(pl.col("split") == split).drop("split").lazy()
                 y_true = (
                     y_pred.select([entity_col, time_col])
@@ -190,96 +266,128 @@
                     [
                         pl.lit(split).alias("split"),
                         pl.lit(model_name).alias("model_name"),
                     ]
                 )
                 cv_scores.append(scores)
 
-        # 4. Select top N best models from CV
+        # 3. Select top N best models from CV
         scores = (
             pl.concat(cv_scores)
             .sort([entity_col, metric_name, "split"])
             .set_sorted([entity_col, metric_name, "split"])
         )
         best_models = (
             scores.lazy()
             .groupby([entity_col, "model_name"])
             # Compute average score across splits
             .agg(pl.col(metric_name).mean())
             .sort([entity_col, metric_name])
             .set_sorted([entity_col, metric_name])
             # Select top K scores
             .groupby(entity_col, maintain_order=True)
-            .agg([pl.col("model_name").head(top_k), metric_name])
+            .agg([pl.col("model_name").head(top_k), pl.col(metric_name).head(top_k)])
             .collect(streaming=True)
         )
         full_y_preds = pl.concat(
             [
                 y_preds.lazy().with_columns(pl.lit(model_name).alias("model_name"))
                 for model_name, y_preds in cv_y_preds.items()
             ]
         )
 
-        # 5. Prepare ensemble (stacked regression) inputs
+        # 4. Prepare ensemble (stacked regression) inputs
         X_stack = self._get_X_stack(y_pred=full_y_preds, best_models=best_models, X=X)
         y_stack = (
             X_stack.select([entity_col, time_col])
             .lazy()
             .join(y.lazy(), on=[entity_col, time_col], how="left")
             .collect(streaming=True)
         )
 
-        # 6. Fit final regressor
-        regressor = LassoLarsIC(**self.kwargs).fit(
-            X=X_to_numpy(X_stack), y=y_to_numpy(y_stack)
-        )
+        final_regressor = None
+        if self.ensemble_strategy in ["lasso", "log_lasso"]:
+            # 5. Fit final regressor
+            final_regressor = LassoLarsIC(**self.kwargs).fit(
+                X=X_to_numpy(X_stack), y=y_to_numpy(y_stack)
+            )
 
-        # 7. Fit forecasters
-        forecasters = {}
-        for model_name, forecaster_cls in (pbar := tqdm(self.forecasters.items())):
+        # 6. Fit forecasters
+        fitted_forecasters = {}
+        for model_name, forecaster_cls in (pbar := tqdm(forecasters.items())):
             pbar.set_description(f"Refitting [forecaster={model_name}]")
-            forecaster = forecaster_cls(
-                freq=freq, lags=lags, **model_kwargs.get(model_name, {})
-            ).fit(y=y)
-            forecasters[model_name] = forecaster
-        forecasters["naive"] = naive(freq=freq, max_fh=max_fh).fit(y=y)
+            if model_name != "naive":
+                forecaster = forecaster_cls(
+                    freq=freq, lags=lags, **model_kwargs.get(model_name, {})
+                ).fit(y=y)
+            else:
+                forecaster = forecaster_cls(freq=freq, max_fh=max_fh).fit(y=y)
+            fitted_forecasters[model_name] = forecaster
 
         artifacts = {
             "scores": scores,
             "best_models": best_models,
-            "forecasters": forecasters,
-            "final_regressor": regressor,
+            "forecasters": fitted_forecasters,
+            "final_regressor": final_regressor,
         }
         return artifacts
 
     def predict(self, fh: int, X: Optional[pl.LazyFrame] = None):
         state = self.state
         entity_col = state.entity
         time_col = state.time
         target_col = state.target
+        schema = state.target_schema
 
         # 1. Get individual forecasts
         forecasters = state.artifacts["forecasters"]
         forecasts = {}
         for model_name, forecaster in (pbar := tqdm(forecasters.items())):
             pbar.set_description(f"Forecast [forecaster={model_name}]")
-            forecasts[model_name] = forecaster.predict(fh=fh)
+            y_pred = forecaster.predict(fh=fh).pipe(coerce_dtypes(schema)).collect()
+            forecasts[model_name] = y_pred
 
         # 2. Prepare ensemble (stacked regression) input
         best_models = state.artifacts["best_models"]
         full_y_pred = pl.concat(
             [
                 y_pred.lazy().with_columns(pl.lit(model_name).alias("model_name"))
                 for model_name, y_pred in forecasts.items()
             ]
         )
         X_stack = self._get_X_stack(y_pred=full_y_pred, best_models=best_models, X=X)
 
-        # 3. Predict using final regressor
-        final_regressor = state.artifacts["final_regressor"]
-        y_pred_values = final_regressor.predict(X=X_to_numpy(X_stack))
-        y_pred = (
-            X_stack.select([entity_col, time_col])
-            .with_columns(pl.lit(y_pred_values).alias(target_col))
-            .pipe(self._reset_string_cache)
+        # 3. Predict using final stacker
+        if self.ensemble_strategy == "mean":
+            y_pred = X_stack.select(
+                [
+                    entity_col,
+                    time_col,
+                    (pl.sum_horizontal(cs.starts_with("model_")) / self.top_k).alias(
+                        target_col
+                    ),
+                ]
+            )
+        else:
+            final_regressor = state.artifacts["final_regressor"]
+            y_pred_values = final_regressor.predict(X=X_to_numpy(X_stack))
+            y_pred = (
+                X_stack.select([entity_col, time_col])
+                .with_columns(pl.lit(y_pred_values).alias(target_col))
+                .pipe(coerce_dtypes(schema))
+                .collect()
+            )
+
+        random_walk_series = (
+            best_models.select([entity_col, pl.col("model_name").list.first()])
+            .filter(pl.col("model_name") == "naive")
+            .get_column(entity_col)
+        )
+        naive_forecasts = forecasts["naive"].filter(
+            pl.col(entity_col).is_in(random_walk_series)
         )
-        return y_pred
+        ensemble_forecasts = y_pred.filter(
+            ~pl.col(entity_col).is_in(random_walk_series)
+        )
+        y_pred = pl.concat([naive_forecasts, ensemble_forecasts])
+
+        return y_pred.pipe(self._reset_string_cache)
```

## functime/forecasting/knn.py

```diff
@@ -19,15 +19,15 @@
     return regress
 
 
 class knn(Forecaster):
     """Autoregressive k-nearest neighbors.
 
     Reference:
-    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
+    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html
     """
 
     def _fit(self, y: pl.LazyFrame, X: Optional[pl.LazyFrame] = None):
         regress = _knn(**self.kwargs)
         return fit_autoreg(
             regress=regress,
             y=y,
```

## functime/forecasting/linear.py

```diff
@@ -51,14 +51,46 @@
             regressor=ElasticNet(**kwargs, tol=0.001, copy_X=False, max_iter=10000)
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
+def _lasso_cv(**kwargs):
+    def regress(X: pl.DataFrame, y: pl.DataFrame):
+        from sklearn.linear_model import LassoCV
+
+        regressor = SklearnRegressor(
+            regressor=LassoCV(**kwargs, max_iter=10000, n_jobs=-1),
+        )
+        return regressor.fit(X=X, y=y)
+
+    return regress
+
+
+def _ridge_cv(**kwargs):
+    def regress(X: pl.DataFrame, y: pl.DataFrame):
+        from sklearn.linear_model import RidgeCV
+
+        regressor = SklearnRegressor(regressor=RidgeCV(**kwargs))
+        return regressor.fit(X=X, y=y)
+
+    return regress
+
+
+def _elastic_net_cv(**kwargs):
+    def regress(X: pl.DataFrame, y: pl.DataFrame):
+        from sklearn.linear_model import ElasticNetCV
+
+        regressor = SklearnRegressor(regressor=ElasticNetCV(**kwargs, n_jobs=-1))
+        return regressor.fit(X=X, y=y)
+
+    return regress
+
+
 class linear_model(Forecaster):
     """Autoregressive linear forecaster.
 
     Reference:
     https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
     """
 
@@ -135,8 +167,65 @@
         return fit_autoreg(
             regress=regress,
             y=y,
             X=X,
             lags=self.lags,
             max_horizons=self.max_horizons,
             strategy=self.strategy,
+        )
+
+
+class lasso_cv(Forecaster):
+    """Autoregressive LassoCV forecaster.
+
+    Reference:
+    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.LassoCV
+    """
+
+    def _fit(self, y: pl.LazyFrame, X: Optional[pl.LazyFrame] = None):
+        regress = _lasso_cv(**self.kwargs)
+        return fit_autoreg(
+            regress=regress,
+            y=y,
+            X=X,
+            lags=self.lags,
+            max_horizons=self.max_horizons,
+            strategy=self.strategy,
+        )
+
+
+class ridge_cv(Forecaster):
+    """Autoregressive RidgeCV forecaster.
+
+    Reference:
+    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.RidgeCV
+    """
+
+    def _fit(self, y: pl.LazyFrame, X: Optional[pl.LazyFrame] = None):
+        regress = _ridge_cv(**self.kwargs)
+        return fit_autoreg(
+            regress=regress,
+            y=y,
+            X=X,
+            lags=self.lags,
+            max_horizons=self.max_horizons,
+            strategy=self.strategy,
+        )
+
+
+class elastic_net_cv(Forecaster):
+    """Autoregressive ElasticNetCV forecaster.
+
+    Reference:
+    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNetCV
+    """
+
+    def _fit(self, y: pl.LazyFrame, X: Optional[pl.LazyFrame] = None):
+        regress = _elastic_net_cv(**self.kwargs)
+        return fit_autoreg(
+            regress=regress,
+            y=y,
+            X=X,
+            lags=self.lags,
+            max_horizons=self.max_horizons,
+            strategy=self.strategy,
         )
```

## functime/metrics/point.py

```diff
@@ -155,15 +155,15 @@
     -------
     scores : pl.DataFrame
         Score per series.
     """
     num = 2 * (pl.col("pred") - pl.col("actual")).abs()
     denom = 0.0001 + pl.col("actual").abs() + pl.col("pred").abs()
     pct_error = (100 / pl.col("pred").len()) * (num / denom).sum()
-    return _score(y_true, y_pred, pct_error, "smape")
+    return _score(y_true, y_pred, pct_error, "smape_original")
 
 
 @metric
 def mase(
     y_true: pl.DataFrame, y_pred: pl.DataFrame, y_train: pl.DataFrame, sp: int = 1
 ):
     """Return mean absolute scaled error (MASE).
```

## Comparing `functime-0.3.3.dist-info/LICENSE` & `functime-0.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `functime-0.3.3.dist-info/METADATA` & `functime-0.4.0.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: functime
-Version: 0.3.3
+Version: 0.4.0
 Summary: The easiest way to run and scale time-series machine learning in the Cloud.
 Author-email: functime Team <team@functime.ai>, Chris Lo <chris@functime.ai>, Daryl Lim <daryl@functime.ai>
 Project-URL: Homepage, https://github.com/descendant-ai/functime
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Science/Research
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
@@ -23,15 +23,15 @@
 Requires-Dist: holidays
 Requires-Dist: joblib
 Requires-Dist: kaleido (==0.2.1)
 Requires-Dist: lightgbm
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: plotly
-Requires-Dist: polars (==0.18.7)
+Requires-Dist: polars (==0.18.11)
 Requires-Dist: pyarrow
 Requires-Dist: pylance
 Requires-Dist: pynndescent (==0.5.8)
 Requires-Dist: rich (>=12.0.0)
 Requires-Dist: scikit-learn (==1.2.2)
 Requires-Dist: scipy
 Requires-Dist: tqdm
@@ -53,15 +53,15 @@
 Requires-Dist: pytest-timeout ; extra == 'test'
 Requires-Dist: pytest ; extra == 'test'
 
 <div align="center">
     <h1>Time-series machine learning and embeddings at scale</h1>
 <br />
 
-![functime](https://github.com/descendant-ai/functime/raw/main/static/images/functime_banner.png)
+![functime](https://github.com/descendant-ai/functime/raw/main/docs/img/banner.png)
 [![Python](https://img.shields.io/pypi/pyversions/functime)](https://pypi.org/project/functime/)
 [![PyPi](https://img.shields.io/pypi/v/functime?color=blue)](https://pypi.org/project/functime/)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![GitHub Publish to PyPI](https://github.com/descendant-ai/functime/actions/workflows/publish.yml/badge.svg)](https://github.com/descendant-ai/functime/actions/workflows/publish.yml)
 [![GitHub Build Docs](https://github.com/descendant-ai/functime/actions/workflows/docs.yml/badge.svg)](https://docs.functime.ai/)
 [![GitHub Run Quickstart](https://github.com/descendant-ai/functime/actions/workflows/quickstart.yml/badge.svg)](https://github.com/descendant-ai/functime/actions/workflows/quickstart.yml)
 
@@ -103,24 +103,32 @@
 y = pl.read_parquet("https://github.com/descendant-ai/functime/raw/main/data/commodities.parquet")
 entity_col, time_col = y.columns[:2]
 
 # Time series split
 y_train, y_test = y.pipe(train_test_split(test_size=3))
 
 # Fit-predict
-forecaster = lightgbm(freq="1mo", lags=24, max_horizons=3, strategy="ensemble")
+forecaster = lightgbm(freq="1mo", lags=24)
 forecaster.fit(y=y_train)
 y_pred = forecaster.predict(fh=3)
 
 # functime ❤️ functional design
 # fit-predict in a single line
 y_pred = lightgbm(freq="1mo", lags=24)(y=y_train, fh=3)
 
 # Score forecasts in parallel
 scores = mase(y_true=y_test, y_pred=y_pred, y_train=y_train)
+
+# Forecast with target transforms and feature transforms
+forecaster = lightgbm(
+    freq="1mo",
+    lags=24,
+    target_transform=scale(),
+    feature_transform=add_fourier_terms(sp=12, K=6)
+)
 ```
 
 View the [full walkthrough](https://docs.functime.ai/forecasting.md) on forecasting with `functime`.
 
 ## Embeddings
 
 Currently in closed-beta for `functime` Cloud.
@@ -143,15 +151,15 @@
 Currently in closed-beta for `functime` Cloud.
 Contact us for a demo via [Calendly](https://calendly.com/functime).
 
 Deploy and train forecasters the moment you call any `.fit` method.
 Run the `functime list` CLI command to list all deployed models.
 Finally, track data and forecasts usage using `functime usage` CLI command.
 
-![Example CLI usage](static/gifs/functime_cli_usage.gif)
+![Example CLI usage](docs/img/functime_cli_usage.gif)
 
 You can reuse a deployed model for predictions anywhere using the `stub_id` variable.
 Note: the `.from_deployed` model class must be the same as during `.fit`.
 ```python
 forecaster = LightGBM.from_deployed(stub_id)
 y_pred = forecaster.predict(fh=3)
 ```
```

## Comparing `functime-0.3.3.dist-info/RECORD` & `functime-0.4.0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -2,41 +2,41 @@
 functime/backtesting.py,sha256=rEn299g57Mwa4-JUdgJBfvK3KwZs10dufmdNeK8aE8U,6143
 functime/conformal.py,sha256=SLLPOEEzOdzVdFzCpNfmETmr4fHKqIhtJC-Xce6mHyo,1838
 functime/conversion.py,sha256=VUn6bbsXTaRAR1n9fDQpgLf4KanvbCIEMctIw5FG5tg,2456
 functime/cross_validation.py,sha256=uTkcotiLATM_qA0zfgWAIPaD4fJW9LSqiBABfAbeqvY,5549
 functime/embeddings.py,sha256=rupMM8Fs6-nExoWYHS-PWFHiEHXsrNBvP4o-PPzACos,123
 functime/offsets.py,sha256=GUCVLFs0xBSj_gsSaBz19ibBr_nquJVUxtuosObBucU,1495
 functime/plotting.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime/preprocessing.py,sha256=l6EdFHIp3DFjUYZACJbEk0_Tf0LGwEN1ZoiMz80VmeE,20573
+functime/preprocessing.py,sha256=yYwLXi0banSsIuaiKDm8IlhvLAzMsN9rvwHvpYV3Vl0,24010
 functime/ranges.py,sha256=aZ3CFVRhoAVqw2EAnK532Tara3n66Ka7c_Qz1FJJEQQ,1996
 functime/base/__init__.py,sha256=YKzO_7RyxYbn3-jTa4gipQxrwnzUQN2_d2edjDehLmw,217
-functime/base/forecaster.py,sha256=5JwdxqxE0UnukC8o1K8xhSRfyKkh7J6vQ_iV_qlf7mw,8265
+functime/base/forecaster.py,sha256=FdwLyVUorXK1NVjZp9BXE1XI2hVN8isL0MErjPISKp0,9150
 functime/base/metric.py,sha256=__ByYz8ojkLJMjbTB1fTg2KDC5wEl-h_s1AxDSIsjkM,1746
 functime/base/model.py,sha256=mL8GFL7qNNb4rufm3ZbLzxyDPoDuoHgQhpZ0t20AHv4,2771
 functime/base/transformer.py,sha256=qSuKYisdWL1qFufJlr50zNg-NryMX5q7MP61I1DgPHw,2155
 functime/feature_extraction/__init__.py,sha256=mSHvtROzMYq19_-emVmIEP87qDzQjC1RtsmzVRgGRX4,349
-functime/feature_extraction/calendar.py,sha256=z6WsFPma0KV0zAq93sZ3vlofaG_LMht47ihry8y4GEg,4294
-functime/feature_extraction/fourier.py,sha256=2mQ61zhynGmBxSyyOqcjTQC_wYe0-6Vu8_0neUcWzNU,2162
-functime/forecasting/__init__.py,sha256=IRwMvzG0WkYRXwKm45NIpRvhrdX6duEMHrCOQ6woP74,783
-functime/forecasting/_ar.py,sha256=RwdvXWNXD888i0n2RSUqsqFC4W647kWd30i57v9TK_s,12609
+functime/feature_extraction/calendar.py,sha256=iKMhPqWWYXE20XFxBZhKL5mOtP3ut3j5pkUUck4sa2w,4447
+functime/feature_extraction/fourier.py,sha256=CMZGaFSrNw36eWbo4Q-FbJ3Uu091jnUjsP6ENgSKV0k,1475
+functime/forecasting/__init__.py,sha256=pPr61j5xDrp9Q6hczdauv0bAFnDndhbkFxtuHtIK_BU,944
+functime/forecasting/_ar.py,sha256=B5OZ8TMfb1KLnA9Drk5CRaeYAVm4fevCo9ohZUPXAlM,12651
 functime/forecasting/_evaluate.py,sha256=V8XmiXA0loE-7DUkixJiC-I8DSxGkFjqjx5Dqg_4-Hs,4893
 functime/forecasting/_reduction.py,sha256=FLYt6FXJCIcg5j60BiJNX8HvE8LHnkBdoc0BvYyDEtA,2231
 functime/forecasting/_regressors.py,sha256=_UihMNrrCShfmABv5HKhx9JOSR_b-r-voacKXv-6pLI,6424
-functime/forecasting/automl.py,sha256=Wan9SoRzPdi7M8RkkniuHclxw1PO34XbC2EOPiwESW8,8513
+functime/forecasting/automl.py,sha256=6JjysFh3Kwmyj5qhLZqsV3vQQDf1bo7f_HdxHj22R7I,8727
 functime/forecasting/catboost.py,sha256=nF8I4AxJH7qDIu2BUGE_l7w-guNt8iYuNKH9oy-N3tU,2175
 functime/forecasting/censored.py,sha256=fKTO9I0c9k_TKuPwxk4sYdo9793Lge0R0QVncFNCwUY,3722
-functime/forecasting/elite.py,sha256=g-G7eo0oipbxEU38tUvOcUuwAUFM5wqOls6mVeaT1e4,10936
-functime/forecasting/knn.py,sha256=NA2-dCTHtVMUonuQMCU8i-bZuScB2H6mRVCXB_uUytQ,1010
+functime/forecasting/elite.py,sha256=DFLv07CBlrrnVVLnM9g3j9vo-5Ew-0jkOpj4klSGexA,15872
+functime/forecasting/knn.py,sha256=Vmj2dvNMQxZNhZ3Ago98vUL7ifMZgi9dmFyfrjWzp08,1009
 functime/forecasting/lance.py,sha256=LiEhbLrd6DpMoFhYjbjyPXWhHxNEdM_v_XdpPcgpwiU,3703
 functime/forecasting/lightgbm.py,sha256=1TPdFXhtXfp6cl7fQGzmHg1G0OUpAoSXZkvbiP1amKE,4208
-functime/forecasting/linear.py,sha256=adMbCOpqNZZDLICTULY8Ya6dDTCJacmeY0-76SC7odI,3958
+functime/forecasting/linear.py,sha256=LoQTWRoAJw9GdVwo7fk-JkUxVnvEBAOuvWDAvkNb2Ak,6426
 functime/forecasting/naive.py,sha256=Sla8qTn18j9rTctzMJKxXIZrYbHBIQSGfkqMXGTIby4,1943
 functime/forecasting/xgboost.py,sha256=kguM3IvqtJKSD3tysxgCiOe-NwApBew5tOO9_zxt4-I,2412
 functime/metrics/__init__.py,sha256=4DE4A-UtFY8AfTAEmVGqrbs2tXQwtO7Ogq-BZJp4zr8,312
 functime/metrics/multi_objective.py,sha256=EP9sGQ4HSCRN1SQazoJfntcovMP6RAVf7hZMXgcA2gE,3826
-functime/metrics/point.py,sha256=4rgQ0RJ8UR0xRxCOxxO2wuSHZFhj26XgLK4VBm3GHtQ,7432
+functime/metrics/point.py,sha256=w6H_gABzUmx3M0kQl4t0tWGb6koMH4deTBwU9fP2B10,7441
 functime/metrics/probabilistic.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime-0.3.3.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
-functime-0.3.3.dist-info/METADATA,sha256=InSiHm3EBC1RMw8wb_UiItS2trfbJr6TFonvWSu0HRc,7412
-functime-0.3.3.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
-functime-0.3.3.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
-functime-0.3.3.dist-info/RECORD,,
+functime-0.4.0.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
+functime-0.4.0.dist-info/METADATA,sha256=ym7w2zW3cjzn_C9OJxhkXco3GEWWjIpdDjP-RQFwupw,7553
+functime-0.4.0.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+functime-0.4.0.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
+functime-0.4.0.dist-info/RECORD,,
```

