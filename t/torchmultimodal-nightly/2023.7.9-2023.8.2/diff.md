# Comparing `tmp/torchmultimodal_nightly-2023.7.9-py39-none-any.whl.zip` & `tmp/torchmultimodal_nightly-2023.8.2-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,90 +1,90 @@
-Zip file size: 160759 bytes, number of entries: 88
--rw-r--r--  2.0 unx      306 b- defN 23-Jul-09 00:13 torchmultimodal/__init__.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/models/__init__.py
--rw-r--r--  2.0 unx    34200 b- defN 23-Jul-09 00:13 torchmultimodal/models/gpt.py
--rw-r--r--  2.0 unx     1629 b- defN 23-Jul-09 00:13 torchmultimodal/models/late_fusion.py
--rw-r--r--  2.0 unx     8828 b- defN 23-Jul-09 00:13 torchmultimodal/models/omnivore.py
--rw-r--r--  2.0 unx     3666 b- defN 23-Jul-09 00:13 torchmultimodal/models/two_tower.py
--rw-r--r--  2.0 unx     8698 b- defN 23-Jul-09 00:13 torchmultimodal/models/video_gpt.py
--rw-r--r--  2.0 unx    13705 b- defN 23-Jul-09 00:13 torchmultimodal/models/video_vqvae.py
--rw-r--r--  2.0 unx     4731 b- defN 23-Jul-09 00:13 torchmultimodal/models/vqvae.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/models/albef/__init__.py
--rw-r--r--  2.0 unx     2687 b- defN 23-Jul-09 00:13 torchmultimodal/models/albef/image_encoder.py
--rw-r--r--  2.0 unx    12835 b- defN 23-Jul-09 00:13 torchmultimodal/models/albef/model.py
--rw-r--r--  2.0 unx     3643 b- defN 23-Jul-09 00:13 torchmultimodal/models/albef/multimodal_encoder.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/models/clip/__init__.py
--rw-r--r--  2.0 unx    11987 b- defN 23-Jul-09 00:13 torchmultimodal/models/clip/image_encoder.py
--rw-r--r--  2.0 unx     6885 b- defN 23-Jul-09 00:13 torchmultimodal/models/clip/model.py
--rw-r--r--  2.0 unx     4851 b- defN 23-Jul-09 00:13 torchmultimodal/models/clip/text_encoder.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/models/dalle2/__init__.py
--rw-r--r--  2.0 unx     4731 b- defN 23-Jul-09 00:13 torchmultimodal/models/dalle2/dalle2_decoder.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/models/dalle2/adm/__init__.py
--rw-r--r--  2.0 unx    22492 b- defN 23-Jul-09 00:13 torchmultimodal/models/dalle2/adm/adm.py
--rw-r--r--  2.0 unx     7001 b- defN 23-Jul-09 00:13 torchmultimodal/models/dalle2/adm/attention_block.py
--rw-r--r--  2.0 unx     7638 b- defN 23-Jul-09 00:13 torchmultimodal/models/dalle2/adm/res_block.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/models/flava/__init__.py
--rw-r--r--  2.0 unx    10585 b- defN 23-Jul-09 00:13 torchmultimodal/models/flava/image_encoder.py
--rw-r--r--  2.0 unx    26154 b- defN 23-Jul-09 00:13 torchmultimodal/models/flava/model.py
--rw-r--r--  2.0 unx     2297 b- defN 23-Jul-09 00:13 torchmultimodal/models/flava/text_encoder.py
--rw-r--r--  2.0 unx     3239 b- defN 23-Jul-09 00:13 torchmultimodal/models/flava/transformer.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/models/mdetr/__init__.py
--rw-r--r--  2.0 unx     6128 b- defN 23-Jul-09 00:13 torchmultimodal/models/mdetr/image_encoder.py
--rw-r--r--  2.0 unx    16672 b- defN 23-Jul-09 00:13 torchmultimodal/models/mdetr/model.py
--rw-r--r--  2.0 unx     5433 b- defN 23-Jul-09 00:13 torchmultimodal/models/mdetr/text_encoder.py
--rw-r--r--  2.0 unx    16850 b- defN 23-Jul-09 00:13 torchmultimodal/models/mdetr/transformer.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/modules/__init__.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/modules/diffusion/__init__.py
--rw-r--r--  2.0 unx     8452 b- defN 23-Jul-09 00:13 torchmultimodal/modules/diffusion/cfguidance.py
--rw-r--r--  2.0 unx     5752 b- defN 23-Jul-09 00:13 torchmultimodal/modules/diffusion/ddim.py
--rw-r--r--  2.0 unx     7226 b- defN 23-Jul-09 00:13 torchmultimodal/modules/diffusion/ddpm.py
--rw-r--r--  2.0 unx     3782 b- defN 23-Jul-09 00:13 torchmultimodal/modules/diffusion/predictors.py
--rw-r--r--  2.0 unx    12620 b- defN 23-Jul-09 00:13 torchmultimodal/modules/diffusion/schedules.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/modules/encoders/__init__.py
--rw-r--r--  2.0 unx     7084 b- defN 23-Jul-09 00:13 torchmultimodal/modules/encoders/bert_text_encoder.py
--rw-r--r--  2.0 unx     1883 b- defN 23-Jul-09 00:13 torchmultimodal/modules/encoders/embedding_encoder.py
--rw-r--r--  2.0 unx     3859 b- defN 23-Jul-09 00:13 torchmultimodal/modules/encoders/mil_encoder.py
--rw-r--r--  2.0 unx     3288 b- defN 23-Jul-09 00:13 torchmultimodal/modules/encoders/swin_transformer_3d_encoder.py
--rw-r--r--  2.0 unx     2111 b- defN 23-Jul-09 00:13 torchmultimodal/modules/encoders/weighted_embedding_encoder.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/modules/fusions/__init__.py
--rw-r--r--  2.0 unx     2263 b- defN 23-Jul-09 00:13 torchmultimodal/modules/fusions/attention_fusion.py
--rw-r--r--  2.0 unx      977 b- defN 23-Jul-09 00:13 torchmultimodal/modules/fusions/concat_fusion.py
--rw-r--r--  2.0 unx     8722 b- defN 23-Jul-09 00:13 torchmultimodal/modules/fusions/deepset_fusion.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/__init__.py
--rw-r--r--  2.0 unx      791 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/activation.py
--rw-r--r--  2.0 unx    17251 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/attention.py
--rw-r--r--  2.0 unx    11764 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/codebook.py
--rw-r--r--  2.0 unx     9498 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/conv.py
--rw-r--r--  2.0 unx     2254 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/mlp.py
--rw-r--r--  2.0 unx     7107 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/multi_head_attention.py
--rw-r--r--  2.0 unx     1512 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/normalizations.py
--rw-r--r--  2.0 unx     6433 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/position_embedding.py
--rw-r--r--  2.0 unx     4720 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/text_embedding.py
--rw-r--r--  2.0 unx    15503 b- defN 23-Jul-09 00:13 torchmultimodal/modules/layers/transformer.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/modules/losses/__init__.py
--rw-r--r--  2.0 unx     5280 b- defN 23-Jul-09 00:13 torchmultimodal/modules/losses/albef.py
--rw-r--r--  2.0 unx     8166 b- defN 23-Jul-09 00:13 torchmultimodal/modules/losses/contrastive_loss_with_temperature.py
--rw-r--r--  2.0 unx     6068 b- defN 23-Jul-09 00:13 torchmultimodal/modules/losses/diffusion.py
--rw-r--r--  2.0 unx    17264 b- defN 23-Jul-09 00:13 torchmultimodal/modules/losses/flava.py
--rw-r--r--  2.0 unx     6295 b- defN 23-Jul-09 00:13 torchmultimodal/modules/losses/mdetr.py
--rw-r--r--  2.0 unx     1182 b- defN 23-Jul-09 00:13 torchmultimodal/modules/losses/vqvae.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/transforms/__init__.py
--rw-r--r--  2.0 unx     2122 b- defN 23-Jul-09 00:13 torchmultimodal/transforms/bert_text_transform.py
--rw-r--r--  2.0 unx     8292 b- defN 23-Jul-09 00:13 torchmultimodal/transforms/clip_transform.py
--rw-r--r--  2.0 unx     3076 b- defN 23-Jul-09 00:13 torchmultimodal/transforms/diffusion_transforms.py
--rw-r--r--  2.0 unx    12098 b- defN 23-Jul-09 00:13 torchmultimodal/transforms/flava_transform.py
--rw-r--r--  2.0 unx     3597 b- defN 23-Jul-09 00:13 torchmultimodal/transforms/video_transform.py
--rw-r--r--  2.0 unx      208 b- defN 23-Jul-09 00:13 torchmultimodal/utils/__init__.py
--rw-r--r--  2.0 unx      475 b- defN 23-Jul-09 00:13 torchmultimodal/utils/assertion.py
--rw-r--r--  2.0 unx     2621 b- defN 23-Jul-09 00:13 torchmultimodal/utils/attention.py
--rw-r--r--  2.0 unx     6586 b- defN 23-Jul-09 00:13 torchmultimodal/utils/common.py
--rw-r--r--  2.0 unx     1423 b- defN 23-Jul-09 00:13 torchmultimodal/utils/diffusion_utils.py
--rw-r--r--  2.0 unx     1586 b- defN 23-Jul-09 00:13 torchmultimodal/utils/distributed.py
--rw-r--r--  2.0 unx      601 b- defN 23-Jul-09 00:13 torchmultimodal/utils/file_io.py
--rw-r--r--  2.0 unx    13583 b- defN 23-Jul-09 00:13 torchmultimodal/utils/generate.py
--rw-r--r--  2.0 unx     1519 b- defN 23-Jul-09 00:14 torchmultimodal_nightly-2023.7.9.dist-info/LICENSE
--rw-r--r--  2.0 unx     4967 b- defN 23-Jul-09 00:14 torchmultimodal_nightly-2023.7.9.dist-info/METADATA
--rw-r--r--  2.0 unx    12590 b- defN 23-Jul-09 00:14 torchmultimodal_nightly-2023.7.9.dist-info/NOTICES
--rw-r--r--  2.0 unx       93 b- defN 23-Jul-09 00:14 torchmultimodal_nightly-2023.7.9.dist-info/WHEEL
--rw-r--r--  2.0 unx       16 b- defN 23-Jul-09 00:14 torchmultimodal_nightly-2023.7.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     8741 b- defN 23-Jul-09 00:14 torchmultimodal_nightly-2023.7.9.dist-info/RECORD
-88 files, 524084 bytes uncompressed, 146477 bytes compressed:  72.1%
+Zip file size: 161660 bytes, number of entries: 88
+-rw-r--r--  2.0 unx      306 b- defN 23-Aug-02 00:10 torchmultimodal/__init__.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/models/__init__.py
+-rw-r--r--  2.0 unx    34200 b- defN 23-Aug-02 00:10 torchmultimodal/models/gpt.py
+-rw-r--r--  2.0 unx     1629 b- defN 23-Aug-02 00:10 torchmultimodal/models/late_fusion.py
+-rw-r--r--  2.0 unx     8828 b- defN 23-Aug-02 00:10 torchmultimodal/models/omnivore.py
+-rw-r--r--  2.0 unx     3666 b- defN 23-Aug-02 00:10 torchmultimodal/models/two_tower.py
+-rw-r--r--  2.0 unx     8698 b- defN 23-Aug-02 00:10 torchmultimodal/models/video_gpt.py
+-rw-r--r--  2.0 unx    13705 b- defN 23-Aug-02 00:10 torchmultimodal/models/video_vqvae.py
+-rw-r--r--  2.0 unx     4731 b- defN 23-Aug-02 00:10 torchmultimodal/models/vqvae.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/models/albef/__init__.py
+-rw-r--r--  2.0 unx     2687 b- defN 23-Aug-02 00:10 torchmultimodal/models/albef/image_encoder.py
+-rw-r--r--  2.0 unx    12835 b- defN 23-Aug-02 00:10 torchmultimodal/models/albef/model.py
+-rw-r--r--  2.0 unx    10464 b- defN 23-Aug-02 00:10 torchmultimodal/models/albef/multimodal_encoder.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/models/clip/__init__.py
+-rw-r--r--  2.0 unx    11987 b- defN 23-Aug-02 00:10 torchmultimodal/models/clip/image_encoder.py
+-rw-r--r--  2.0 unx     6885 b- defN 23-Aug-02 00:10 torchmultimodal/models/clip/model.py
+-rw-r--r--  2.0 unx     4851 b- defN 23-Aug-02 00:10 torchmultimodal/models/clip/text_encoder.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/models/dalle2/__init__.py
+-rw-r--r--  2.0 unx     4731 b- defN 23-Aug-02 00:10 torchmultimodal/models/dalle2/dalle2_decoder.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/models/dalle2/adm/__init__.py
+-rw-r--r--  2.0 unx    22492 b- defN 23-Aug-02 00:10 torchmultimodal/models/dalle2/adm/adm.py
+-rw-r--r--  2.0 unx     7001 b- defN 23-Aug-02 00:10 torchmultimodal/models/dalle2/adm/attention_block.py
+-rw-r--r--  2.0 unx     7638 b- defN 23-Aug-02 00:10 torchmultimodal/models/dalle2/adm/res_block.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/models/flava/__init__.py
+-rw-r--r--  2.0 unx    10585 b- defN 23-Aug-02 00:10 torchmultimodal/models/flava/image_encoder.py
+-rw-r--r--  2.0 unx    26154 b- defN 23-Aug-02 00:10 torchmultimodal/models/flava/model.py
+-rw-r--r--  2.0 unx     2256 b- defN 23-Aug-02 00:10 torchmultimodal/models/flava/text_encoder.py
+-rw-r--r--  2.0 unx    11343 b- defN 23-Aug-02 00:10 torchmultimodal/models/flava/transformer.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/models/mdetr/__init__.py
+-rw-r--r--  2.0 unx     6128 b- defN 23-Aug-02 00:10 torchmultimodal/models/mdetr/image_encoder.py
+-rw-r--r--  2.0 unx    16672 b- defN 23-Aug-02 00:10 torchmultimodal/models/mdetr/model.py
+-rw-r--r--  2.0 unx     5433 b- defN 23-Aug-02 00:10 torchmultimodal/models/mdetr/text_encoder.py
+-rw-r--r--  2.0 unx    16850 b- defN 23-Aug-02 00:10 torchmultimodal/models/mdetr/transformer.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/modules/__init__.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/modules/diffusion/__init__.py
+-rw-r--r--  2.0 unx     8452 b- defN 23-Aug-02 00:10 torchmultimodal/modules/diffusion/cfguidance.py
+-rw-r--r--  2.0 unx     5752 b- defN 23-Aug-02 00:10 torchmultimodal/modules/diffusion/ddim.py
+-rw-r--r--  2.0 unx     7226 b- defN 23-Aug-02 00:10 torchmultimodal/modules/diffusion/ddpm.py
+-rw-r--r--  2.0 unx     3782 b- defN 23-Aug-02 00:10 torchmultimodal/modules/diffusion/predictors.py
+-rw-r--r--  2.0 unx    12620 b- defN 23-Aug-02 00:10 torchmultimodal/modules/diffusion/schedules.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/modules/encoders/__init__.py
+-rw-r--r--  2.0 unx     7123 b- defN 23-Aug-02 00:10 torchmultimodal/modules/encoders/bert_text_encoder.py
+-rw-r--r--  2.0 unx     1883 b- defN 23-Aug-02 00:10 torchmultimodal/modules/encoders/embedding_encoder.py
+-rw-r--r--  2.0 unx     3859 b- defN 23-Aug-02 00:10 torchmultimodal/modules/encoders/mil_encoder.py
+-rw-r--r--  2.0 unx     3288 b- defN 23-Aug-02 00:10 torchmultimodal/modules/encoders/swin_transformer_3d_encoder.py
+-rw-r--r--  2.0 unx     2111 b- defN 23-Aug-02 00:10 torchmultimodal/modules/encoders/weighted_embedding_encoder.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/modules/fusions/__init__.py
+-rw-r--r--  2.0 unx     2263 b- defN 23-Aug-02 00:10 torchmultimodal/modules/fusions/attention_fusion.py
+-rw-r--r--  2.0 unx      977 b- defN 23-Aug-02 00:10 torchmultimodal/modules/fusions/concat_fusion.py
+-rw-r--r--  2.0 unx     8722 b- defN 23-Aug-02 00:10 torchmultimodal/modules/fusions/deepset_fusion.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/__init__.py
+-rw-r--r--  2.0 unx     1393 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/activation.py
+-rw-r--r--  2.0 unx    17251 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/attention.py
+-rw-r--r--  2.0 unx    11764 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/codebook.py
+-rw-r--r--  2.0 unx     9498 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/conv.py
+-rw-r--r--  2.0 unx     2254 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/mlp.py
+-rw-r--r--  2.0 unx     7291 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/multi_head_attention.py
+-rw-r--r--  2.0 unx     1512 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/normalizations.py
+-rw-r--r--  2.0 unx     6433 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/position_embedding.py
+-rw-r--r--  2.0 unx     4720 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/text_embedding.py
+-rw-r--r--  2.0 unx      688 b- defN 23-Aug-02 00:10 torchmultimodal/modules/layers/transformer.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/modules/losses/__init__.py
+-rw-r--r--  2.0 unx     5280 b- defN 23-Aug-02 00:10 torchmultimodal/modules/losses/albef.py
+-rw-r--r--  2.0 unx     8166 b- defN 23-Aug-02 00:10 torchmultimodal/modules/losses/contrastive_loss_with_temperature.py
+-rw-r--r--  2.0 unx     6068 b- defN 23-Aug-02 00:10 torchmultimodal/modules/losses/diffusion.py
+-rw-r--r--  2.0 unx    17264 b- defN 23-Aug-02 00:10 torchmultimodal/modules/losses/flava.py
+-rw-r--r--  2.0 unx     6295 b- defN 23-Aug-02 00:10 torchmultimodal/modules/losses/mdetr.py
+-rw-r--r--  2.0 unx     1182 b- defN 23-Aug-02 00:10 torchmultimodal/modules/losses/vqvae.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/transforms/__init__.py
+-rw-r--r--  2.0 unx     2122 b- defN 23-Aug-02 00:10 torchmultimodal/transforms/bert_text_transform.py
+-rw-r--r--  2.0 unx     8292 b- defN 23-Aug-02 00:10 torchmultimodal/transforms/clip_transform.py
+-rw-r--r--  2.0 unx     3076 b- defN 23-Aug-02 00:10 torchmultimodal/transforms/diffusion_transforms.py
+-rw-r--r--  2.0 unx    12098 b- defN 23-Aug-02 00:10 torchmultimodal/transforms/flava_transform.py
+-rw-r--r--  2.0 unx     3597 b- defN 23-Aug-02 00:10 torchmultimodal/transforms/video_transform.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Aug-02 00:10 torchmultimodal/utils/__init__.py
+-rw-r--r--  2.0 unx      475 b- defN 23-Aug-02 00:10 torchmultimodal/utils/assertion.py
+-rw-r--r--  2.0 unx     2621 b- defN 23-Aug-02 00:10 torchmultimodal/utils/attention.py
+-rw-r--r--  2.0 unx     6877 b- defN 23-Aug-02 00:10 torchmultimodal/utils/common.py
+-rw-r--r--  2.0 unx     1423 b- defN 23-Aug-02 00:10 torchmultimodal/utils/diffusion_utils.py
+-rw-r--r--  2.0 unx     1586 b- defN 23-Aug-02 00:10 torchmultimodal/utils/distributed.py
+-rw-r--r--  2.0 unx      601 b- defN 23-Aug-02 00:10 torchmultimodal/utils/file_io.py
+-rw-r--r--  2.0 unx    13583 b- defN 23-Aug-02 00:10 torchmultimodal/utils/generate.py
+-rw-r--r--  2.0 unx     1519 b- defN 23-Aug-02 00:11 torchmultimodal_nightly-2023.8.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     4967 b- defN 23-Aug-02 00:11 torchmultimodal_nightly-2023.8.2.dist-info/METADATA
+-rw-r--r--  2.0 unx    12590 b- defN 23-Aug-02 00:11 torchmultimodal_nightly-2023.8.2.dist-info/NOTICES
+-rw-r--r--  2.0 unx       93 b- defN 23-Aug-02 00:11 torchmultimodal_nightly-2023.8.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       16 b- defN 23-Aug-02 00:11 torchmultimodal_nightly-2023.8.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     8742 b- defN 23-Aug-02 00:11 torchmultimodal_nightly-2023.8.2.dist-info/RECORD
+88 files, 525270 bytes uncompressed, 147378 bytes compressed:  71.9%
```

## zipnote {}

```diff
@@ -240,26 +240,26 @@
 
 Filename: torchmultimodal/utils/file_io.py
 Comment: 
 
 Filename: torchmultimodal/utils/generate.py
 Comment: 
 
-Filename: torchmultimodal_nightly-2023.7.9.dist-info/LICENSE
+Filename: torchmultimodal_nightly-2023.8.2.dist-info/LICENSE
 Comment: 
 
-Filename: torchmultimodal_nightly-2023.7.9.dist-info/METADATA
+Filename: torchmultimodal_nightly-2023.8.2.dist-info/METADATA
 Comment: 
 
-Filename: torchmultimodal_nightly-2023.7.9.dist-info/NOTICES
+Filename: torchmultimodal_nightly-2023.8.2.dist-info/NOTICES
 Comment: 
 
-Filename: torchmultimodal_nightly-2023.7.9.dist-info/WHEEL
+Filename: torchmultimodal_nightly-2023.8.2.dist-info/WHEEL
 Comment: 
 
-Filename: torchmultimodal_nightly-2023.7.9.dist-info/top_level.txt
+Filename: torchmultimodal_nightly-2023.8.2.dist-info/top_level.txt
 Comment: 
 
-Filename: torchmultimodal_nightly-2023.7.9.dist-info/RECORD
+Filename: torchmultimodal_nightly-2023.8.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchmultimodal/models/albef/multimodal_encoder.py

```diff
@@ -4,18 +4,187 @@
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 
 from typing import Callable, Optional
 
 from torch import nn, Tensor
-from torchmultimodal.modules.layers.transformer import TransformerCrossAttentionLayer
+from torchmultimodal.modules.layers.attention import MultiHeadAttention, SelfAttention
+from torchmultimodal.modules.layers.mlp import MLP
+from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
 from torchmultimodal.utils.attention import get_extended_attention_mask
 
 
+class TransformerCrossAttentionLayer(nn.Module):
+    """Transformer layer with self-attention on inputs and cross-attention on an encoder's outputs.
+    Can be used in a transformer decoder or an encoder with cross-attention. Similar to
+    ``nn.TransformerDecoderLayer``, but generalized for use in an encoder with cross-attention as well.
+    Uses a custom ``MultiHeadAttention`` that supports n-dimensional inputs including sequences,
+    images, video.
+
+    Attributes:
+        d_model (int): size of hidden dimension of input
+        n_head (int): number of attention heads
+        dim_feedforward (int): size of hidden dimension of feedforward network
+        dropout (float): dropout probability for all dropouts. Defaults to 0.
+        activation (Callable): activation function in feedforward network. Defaults to ``nn.ReLU``.
+        layer_norm_eps (float): the eps value in layer norms. Default is 1e-12.
+        norm_first (bool): if True, layer norm is done prior to each of self-attention, cross-attention,
+            and feedforward. Otherwise, layer norm is done after.
+
+    Args:
+        hidden_states (Tensor): input tensor of shape [b, d1, ..., dn, c] to calculate self-attention on.
+        encoder_hidden_states (Tensor): input tensor of shape [b, d1, ..., dn, c] to calculate
+            cross-attention on.
+        attention_mask (Tensor, optional): mask to be applied to self-attention inputs, ``hidden_states``.
+            See ``MultiHeadAttention`` for shape requirements.
+        cross_attention_mask (Tensor, optional): mask to be applied to cross-attention inputs,
+            ``encoder_hidden_states``. See ``MultiHeadAttention`` for shape requirements.
+    """
+
+    def __init__(
+        self,
+        d_model: int,
+        n_head: int,
+        dim_feedforward: int,
+        dropout: float = 0.0,
+        activation: Callable[..., nn.Module] = nn.ReLU,
+        layer_norm_eps: float = 1e-12,
+        norm_first: bool = False,
+    ) -> None:
+        super().__init__()
+        # attention block
+        self.attention = MultiHeadAttention(
+            dim_q=d_model,
+            dim_kv=d_model,
+            n_head=n_head,
+            attn_module=SelfAttention(dropout),
+        )
+        self.attention_dropout = nn.Dropout(dropout)
+        # cross attention block
+        self.cross_attention = MultiHeadAttention(
+            dim_q=d_model,
+            dim_kv=d_model,
+            n_head=n_head,
+            attn_module=SelfAttention(dropout),
+        )
+        self.cross_attention_dropout = nn.Dropout(dropout)
+        # feedforward block
+        self.feedforward = MLP(
+            d_model, d_model, dim_feedforward, dropout=dropout, activation=activation
+        )
+        self.feedforward_dropout = nn.Dropout(dropout)
+        # layernorms
+        self.attention_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
+        self.cross_attention_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
+        self.feedforward_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
+        self.norm_first = norm_first
+
+    def _self_attention_block(
+        self, hidden_states: Tensor, attention_mask: Optional[Tensor] = None
+    ) -> Tensor:
+        output = self.attention(
+            hidden_states, attention_mask=attention_mask, return_attn_weights=False
+        )
+        output = self.attention_dropout(output)
+        return output
+
+    def _cross_attention_block(
+        self,
+        hidden_states: Tensor,
+        encoder_hidden_states: Tensor,
+        cross_attention_mask: Optional[Tensor] = None,
+    ) -> Tensor:
+        output = self.cross_attention(
+            hidden_states,
+            encoder_hidden_states,
+            attention_mask=cross_attention_mask,
+            return_attn_weights=False,
+        )
+        output = self.cross_attention_dropout(output)
+        return output
+
+    def _feedforward_block(self, hidden_states: Tensor) -> Tensor:
+        h = self.feedforward(hidden_states)
+        h = self.feedforward_dropout(h)
+        return h
+
+    def _forward_prenorm(
+        self,
+        hidden_states: Tensor,
+        encoder_hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        cross_attention_mask: Optional[Tensor] = None,
+    ) -> Tensor:
+        x = hidden_states
+        kv = encoder_hidden_states
+        inputs = self.attention_layernorm(x)
+        attn_output = self._self_attention_block(inputs, attention_mask=attention_mask)
+        attn_residual = attn_output + x
+        attn_norm_output = self.cross_attention_layernorm(attn_residual)
+        cross_attention_output = self._cross_attention_block(
+            attn_norm_output, kv, cross_attention_mask
+        )
+        cross_attention_residual = cross_attention_output + attn_norm_output
+        cross_attention_norm_output = self.feedforward_layernorm(
+            cross_attention_residual
+        )
+        ff_residual = cross_attention_norm_output + self._feedforward_block(
+            cross_attention_norm_output
+        )
+        return ff_residual
+
+    def _forward_postnorm(
+        self,
+        hidden_states: Tensor,
+        encoder_hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        cross_attention_mask: Optional[Tensor] = None,
+    ) -> Tensor:
+        x = hidden_states
+        kv = encoder_hidden_states
+        attn_output = self._self_attention_block(x, attention_mask=attention_mask)
+        attn_residual = attn_output + x
+        attn_norm_output = self.attention_layernorm(attn_residual)
+        cross_attention_output = self._cross_attention_block(
+            attn_norm_output, kv, cross_attention_mask
+        )
+        cross_attention_residual = cross_attention_output + attn_norm_output
+        cross_attention_norm_output = self.cross_attention_layernorm(
+            cross_attention_residual
+        )
+        ff_residual = cross_attention_norm_output + self._feedforward_block(
+            cross_attention_norm_output
+        )
+        outputs = self.feedforward_layernorm(ff_residual)
+        return outputs
+
+    def forward(
+        self,
+        hidden_states: Tensor,
+        encoder_hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        cross_attention_mask: Optional[Tensor] = None,
+    ) -> Tensor:
+        if self.norm_first:
+            return self._forward_prenorm(
+                hidden_states,
+                encoder_hidden_states,
+                attention_mask,
+                cross_attention_mask,
+            )
+        else:
+            return self._forward_postnorm(
+                hidden_states,
+                encoder_hidden_states,
+                attention_mask,
+                cross_attention_mask,
+            )
+
+
 class ALBEFMultimodalEncoder(nn.Module):
     """
     Construct multimodal embeddings from image embeddings, text embeddings, and text attention mask.
 
     The ALBEFMultimodalEncoder is similar to ALBEFTextEncoder, with the addition of image-text cross attention in encoder layers.
 
     Args:
```

## torchmultimodal/models/flava/image_encoder.py

```diff
@@ -7,20 +7,20 @@
 import math
 import warnings
 from functools import partial
 from typing import Any, Callable, Dict, Optional, Tuple
 
 import torch
 from torch import nn, Tensor
-from torchmultimodal.models.flava.transformer import init_transformer_weights
-from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
-from torchmultimodal.modules.layers.transformer import (
+from torchmultimodal.models.flava.transformer import (
+    init_transformer_weights,
     TransformerEncoder,
-    TransformerOutput,
 )
+from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
+from torchmultimodal.modules.layers.transformer import TransformerOutput
 from torchmultimodal.modules.losses.flava import Pooler
 
 
 def to_2tuple(x: int) -> Tuple[int, int]:
     return (x, x)
```

## torchmultimodal/models/flava/model.py

```diff
@@ -13,21 +13,21 @@
 from functools import partial
 from typing import Any, Callable, List, Optional, Tuple, Union
 
 import torch
 from torch import nn, Tensor
 from torchmultimodal.models.flava.image_encoder import flava_image_encoder
 from torchmultimodal.models.flava.text_encoder import flava_text_encoder
-from torchmultimodal.models.flava.transformer import FLAVATransformerWithoutEmbeddings
-from torchmultimodal.modules.layers.mlp import MLP
-from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
-from torchmultimodal.modules.layers.transformer import (
+from torchmultimodal.models.flava.transformer import (
+    FLAVATransformerWithoutEmbeddings,
     TransformerEncoder,
-    TransformerOutput,
 )
+from torchmultimodal.modules.layers.mlp import MLP
+from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
+from torchmultimodal.modules.layers.transformer import TransformerOutput
 from torchmultimodal.modules.losses.flava import (
     FLAVAPretrainingLoss,
     FLAVAPretrainingLossOutput,
     Pooler,
 )
 from torchmultimodal.utils.common import load_module_from_url, ModelOutput
 from typing_extensions import Literal
```

## torchmultimodal/models/flava/text_encoder.py

```diff
@@ -4,19 +4,21 @@
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 from functools import partial
 from typing import Callable
 
 from torch import nn
-from torchmultimodal.models.flava.transformer import init_transformer_weights
+from torchmultimodal.models.flava.transformer import (
+    init_transformer_weights,
+    TransformerEncoder,
+)
 from torchmultimodal.modules.encoders.bert_text_encoder import BERTTextEncoder
 from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
 from torchmultimodal.modules.layers.text_embedding import BERTTextEmbeddings
-from torchmultimodal.modules.layers.transformer import TransformerEncoder
 from torchmultimodal.modules.losses.flava import Pooler
 
 
 def flava_text_encoder(
     # TransformerEncoder params
     num_hidden_layers: int = 12,
     hidden_size: int = 768,
```

## torchmultimodal/models/flava/transformer.py

```diff
@@ -1,18 +1,21 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 from functools import partial
-from typing import Any, Callable, Optional
+from typing import Any, Callable, Optional, Tuple, Union
 
 import torch
 from torch import nn, Tensor
+from torchmultimodal.modules.layers.attention import MultiHeadAttention, SelfAttention
+from torchmultimodal.modules.layers.mlp import MLP
+from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
 
 from torchmultimodal.modules.layers.transformer import TransformerOutput
 
 
 class FLAVATransformerWithoutEmbeddings(nn.Module):
     # TODO(asg): Add support for pretrained checkpoint loading
     def __init__(
@@ -71,14 +74,231 @@
             last_hidden_state=sequence_output,
             pooler_output=pooled_output,
             hidden_states=encoder_output.hidden_states,
             attentions=encoder_output.attentions,
         )
 
 
+class TransformerEncoderLayer(nn.Module):
+    """Transformer encoder layer is made up of multihead self-attention and feedforward blocks,
+    based on the architecture in "Attention Is All You Need" (Vaswani et al. 2017). Similar to
+    ``nn.TransformerEncoderLayer``, but uses a custom ``MultiHeadAttention`` that supports
+    n-dimensional inputs (including sequences, images, video) and head-masking.
+
+    Attributes:
+        d_model (int): size of hidden dimension of input
+        n_head (int): number of attention heads
+        dim_feedforward (int): size of hidden dimension of feedforward network
+        dropout (float): dropout probability for all dropouts. Defaults to 0.
+        activation (Callable): activation function in feedforward network. Defaults to ``nn.ReLU``.
+        layer_norm_eps (float): the eps value in layer norms. Default is 1e-12.
+        norm_first (bool): if True, layer norm is done prior to each of self-attention, cross-attention,
+            and feedforward. Otherwise, layer norm is done after.
+
+    Args:
+        hidden_states (Tensor): input tensor of shape [b, d1, ..., dn, c] to calculate self-attention on.
+        attention_mask (Tensor, optional): mask to be applied to self-attention inputs, ``hidden_states``. See
+            ``MultiHeadAttention`` for shape requirements.
+        head_mask (Tensor, optional): mask to be applied to self-attention inputs after softmax and dropout,
+            before matrix multiplication with values. See ``MultiHeadAttention`` for shape requirements.
+        return_attn_weights (bool, optional): return attention probabilities in addition to attention output.
+            Defaults to False.
+    """
+
+    def __init__(
+        self,
+        d_model: int,
+        n_head: int,
+        dim_feedforward: int,
+        dropout: float = 0.0,
+        activation: Callable[..., nn.Module] = nn.ReLU,
+        layer_norm_eps: float = 1e-12,
+        norm_first: bool = False,
+    ) -> None:
+        super().__init__()
+        # attention block
+        self.attention = MultiHeadAttention(
+            dim_q=d_model,
+            dim_kv=d_model,
+            n_head=n_head,
+            attn_module=SelfAttention(dropout),
+        )
+        self.attention_dropout = nn.Dropout(dropout)
+        # feedforward block
+        self.feedforward = MLP(
+            d_model, d_model, dim_feedforward, dropout=dropout, activation=activation
+        )
+        self.feedforward_dropout = nn.Dropout(dropout)
+        # layernorms
+        self.attention_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
+        self.feedforward_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
+        self.norm_first = norm_first
+
+    def _attention_block(
+        self,
+        hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        head_mask: Optional[Tensor] = None,
+    ) -> Tuple[Tensor, Tensor]:
+        output, attn_weights = self.attention(
+            hidden_states,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+            return_attn_weights=True,
+        )
+        output = self.attention_dropout(output)
+        return output, attn_weights
+
+    def _feedforward_block(self, hidden_states: Tensor) -> Tensor:
+        h = self.feedforward(hidden_states)
+        h = self.feedforward_dropout(h)
+        return h
+
+    def _forward_prenorm(
+        self,
+        hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        head_mask: Optional[Tensor] = None,
+        return_attn_weights: bool = False,
+    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+        x = hidden_states
+        inputs = self.attention_layernorm(x)
+        attn_output, attn_weights = self._attention_block(
+            inputs,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+        )
+        attn_residual = attn_output + x
+        ff_residual = attn_residual + self._feedforward_block(
+            self.feedforward_layernorm(attn_residual)
+        )
+        if return_attn_weights:
+            return ff_residual, attn_weights
+        else:
+            return ff_residual
+
+    def _forward_postnorm(
+        self,
+        hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        head_mask: Optional[Tensor] = None,
+        return_attn_weights: bool = False,
+    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+        x = hidden_states
+        attn_output, attn_weights = self._attention_block(
+            x,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+        )
+        attn_residual = attn_output + x
+        attn_residual = self.attention_layernorm(attn_residual)
+        ff_residual = attn_residual + self._feedforward_block(attn_residual)
+        outputs = self.feedforward_layernorm(ff_residual)
+        if return_attn_weights:
+            return outputs, attn_weights
+        else:
+            return outputs
+
+    def forward(
+        self,
+        hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        head_mask: Optional[Tensor] = None,
+        return_attn_weights: bool = False,
+    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+        if self.norm_first:
+            return self._forward_prenorm(
+                hidden_states,
+                attention_mask,
+                head_mask,
+                return_attn_weights,
+            )
+        else:
+            return self._forward_postnorm(
+                hidden_states,
+                attention_mask,
+                head_mask,
+                return_attn_weights,
+            )
+
+
+class TransformerEncoder(nn.Module):
+    def __init__(
+        self,
+        n_layer: int,
+        d_model: int,
+        n_head: int,
+        dim_feedforward: int,
+        dropout: float = 0.0,
+        activation: Callable[..., nn.Module] = nn.ReLU,
+        layer_norm_eps: float = 1e-12,
+        norm_first: bool = False,
+        final_layer_norm_eps: Optional[float] = None,
+    ):
+        super().__init__()
+        self.layer = nn.ModuleList(
+            [
+                TransformerEncoderLayer(
+                    d_model,
+                    n_head,
+                    dim_feedforward,
+                    dropout,
+                    activation,
+                    layer_norm_eps,
+                    norm_first,
+                )
+                for _ in range(n_layer)
+            ]
+        )
+        self.final_layer_norm = None
+        if final_layer_norm_eps:
+            self.final_layer_norm = Fp32LayerNorm(d_model, eps=final_layer_norm_eps)
+
+    def forward(
+        self,
+        hidden_states: Tensor,
+        attention_mask: Optional[Tensor] = None,
+        head_mask: Optional[Tensor] = None,
+        return_attn_weights: bool = False,
+        return_hidden_states: bool = False,
+    ) -> TransformerOutput:
+
+        all_hidden_states = [] if return_hidden_states else None
+        all_self_attentions = [] if return_attn_weights else None
+
+        for layer_module in self.layer:
+            if return_hidden_states:
+                all_hidden_states.append(hidden_states)
+
+            layer_outputs = layer_module(
+                hidden_states,
+                attention_mask=attention_mask,
+                head_mask=head_mask,
+                return_attn_weights=return_attn_weights,
+            )
+
+            if return_attn_weights:
+                hidden_states = layer_outputs[0]
+                all_self_attentions.append(layer_outputs[1])
+            else:
+                hidden_states = layer_outputs
+
+        if return_hidden_states:
+            all_hidden_states.append(hidden_states)
+
+        if self.final_layer_norm is not None:
+            hidden_states = self.final_layer_norm(hidden_states)
+
+        return TransformerOutput(
+            last_hidden_state=hidden_states,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attentions,
+        )
+
+
 def init_transformer_weights(module: nn.Module, initializer_range: float) -> None:
     """Initialize the weights"""
     if isinstance(module, (nn.Linear, nn.Conv2d)):
         # Slightly different from the TF version which uses truncated_normal for initialization
         # cf https://github.com/pytorch/pytorch/pull/5617
         module.weight.data.normal_(mean=0.0, std=initializer_range)
         if module.bias is not None:
```

## torchmultimodal/modules/encoders/bert_text_encoder.py

```diff
@@ -4,19 +4,17 @@
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 from typing import Callable, Optional
 
 import torch
 from torch import nn, Tensor
+from torchmultimodal.models.flava.transformer import TransformerEncoder
 from torchmultimodal.modules.layers.text_embedding import BERTTextEmbeddings
-from torchmultimodal.modules.layers.transformer import (
-    TransformerEncoder,
-    TransformerOutput,
-)
+from torchmultimodal.modules.layers.transformer import TransformerOutput
 from torchmultimodal.utils.attention import get_extended_attention_mask
 
 
 class BERTTextEncoder(nn.Module):
     """
     General text transformer encoder with embeddings, following BERT.
     Can be constructed with any user-provided embeddings and encoder.
```

## torchmultimodal/modules/layers/activation.py

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 import torch
+import torch.nn.functional as F
 from torch import nn, Tensor
 
 
 class SiLU(nn.Module):
     r"""Sigmoid Linear Unit
 
     .. math:: \text{SiLU}(x) = x * \sigma(1.702 * x)
@@ -18,7 +19,26 @@
     Approximation of the exact GeLU for greater forward speed. Note that this is different from
     ``torch.nn.SiLU`` by the coefficient ``1.702`` from the paper:
     `"Gaussian error linear units"<https://arxiv.org/pdf/1606.08415.pdf>`_.
     """
 
     def forward(self, x: Tensor) -> Tensor:
         return torch.sigmoid(1.702 * x) * x
+
+
+class GEGLU(nn.Module):
+    """Gated Linear Unit with GELU activation function
+
+    .. math:: \text{GEGLU}(a,b) = a * \text{GELU}(b)
+
+    where :math:`a` is the first half of the input matrices and :math:`b` is
+    the second half, as descibed in the paper:
+    `"GLU Variants Improve Transformer"<https://arxiv.org/pdf/2002.05202.pdf>`.
+    """
+
+    def __init__(self, dim: int = -1):
+        super().__init__()
+        self.split_dim = dim
+
+    def forward(self, x: Tensor) -> Tensor:
+        x, gate = x.chunk(2, dim=self.split_dim)
+        return x * F.gelu(gate)
```

## torchmultimodal/modules/layers/multi_head_attention.py

```diff
@@ -85,28 +85,31 @@
 
     Args:
         dim_q (int): query embedding dimension
         dim_kv (int): key, value embedding dimension,
             same as dim_q for SA; equals to encoder dimension for cross-attention
         num_heads (int): number of attention heads
         dropout (float): dropout rate
+        add_bias (bool): if true, adds a learnable bias to query, key, value.
+            Defaults to True.
     """
 
     def __init__(
         self,
         dim_q: int,
         dim_kv: int,
         num_heads: int,
         dropout: float = 0.0,
+        add_bias: bool = True,
     ) -> None:
         super().__init__()
         self.num_heads = num_heads
-        self.q_proj = nn.Linear(dim_q, dim_q)
-        self.k_proj = nn.Linear(dim_kv, dim_q)
-        self.v_proj = nn.Linear(dim_kv, dim_q)
+        self.q_proj = nn.Linear(dim_q, dim_q, bias=add_bias)
+        self.k_proj = nn.Linear(dim_kv, dim_q, bias=add_bias)
+        self.v_proj = nn.Linear(dim_kv, dim_q, bias=add_bias)
         self.output_proj = nn.Linear(dim_q, dim_q)
         self.dropout = dropout
 
     def forward(
         self,
         query: Tensor,
         key: Tensor,
```

## torchmultimodal/modules/layers/transformer.py

```diff
@@ -3,405 +3,18 @@
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 # Code for some of the transformers components in this file are initialized
 # from their counterparts in Hugging Face Transformers library.
 
-from typing import Callable, List, NamedTuple, Optional, Tuple, Union
+from typing import List, NamedTuple, Optional
 
-from torch import nn, Tensor
-from torchmultimodal.modules.layers.attention import MultiHeadAttention, SelfAttention
-from torchmultimodal.modules.layers.mlp import MLP
-from torchmultimodal.modules.layers.normalizations import Fp32LayerNorm
+from torch import Tensor
 
 
 class TransformerOutput(NamedTuple):
     last_hidden_state: Optional[Tensor] = None
     pooler_output: Optional[Tensor] = None
     hidden_states: Optional[List[Tensor]] = None
     attentions: Optional[List[Tensor]] = None
     image_labels: Optional[Tensor] = None
-
-
-class TransformerCrossAttentionLayer(nn.Module):
-    """Transformer layer with self-attention on inputs and cross-attention on an encoder's outputs.
-    Can be used in a transformer decoder or an encoder with cross-attention. Similar to
-    ``nn.TransformerDecoderLayer``, but generalized for use in an encoder with cross-attention as well.
-    Uses a custom ``MultiHeadAttention`` that supports n-dimensional inputs including sequences,
-    images, video.
-
-    Attributes:
-        d_model (int): size of hidden dimension of input
-        n_head (int): number of attention heads
-        dim_feedforward (int): size of hidden dimension of feedforward network
-        dropout (float): dropout probability for all dropouts. Defaults to 0.
-        activation (Callable): activation function in feedforward network. Defaults to ``nn.ReLU``.
-        layer_norm_eps (float): the eps value in layer norms. Default is 1e-12.
-        norm_first (bool): if True, layer norm is done prior to each of self-attention, cross-attention,
-            and feedforward. Otherwise, layer norm is done after.
-
-    Args:
-        hidden_states (Tensor): input tensor of shape [b, d1, ..., dn, c] to calculate self-attention on.
-        encoder_hidden_states (Tensor): input tensor of shape [b, d1, ..., dn, c] to calculate
-            cross-attention on.
-        attention_mask (Tensor, optional): mask to be applied to self-attention inputs, ``hidden_states``.
-            See ``MultiHeadAttention`` for shape requirements.
-        cross_attention_mask (Tensor, optional): mask to be applied to cross-attention inputs,
-            ``encoder_hidden_states``. See ``MultiHeadAttention`` for shape requirements.
-    """
-
-    def __init__(
-        self,
-        d_model: int,
-        n_head: int,
-        dim_feedforward: int,
-        dropout: float = 0.0,
-        activation: Callable[..., nn.Module] = nn.ReLU,
-        layer_norm_eps: float = 1e-12,
-        norm_first: bool = False,
-    ) -> None:
-        super().__init__()
-        # attention block
-        self.attention = MultiHeadAttention(
-            dim_q=d_model,
-            dim_kv=d_model,
-            n_head=n_head,
-            attn_module=SelfAttention(dropout),
-        )
-        self.attention_dropout = nn.Dropout(dropout)
-        # cross attention block
-        self.cross_attention = MultiHeadAttention(
-            dim_q=d_model,
-            dim_kv=d_model,
-            n_head=n_head,
-            attn_module=SelfAttention(dropout),
-        )
-        self.cross_attention_dropout = nn.Dropout(dropout)
-        # feedforward block
-        self.feedforward = MLP(
-            d_model, d_model, dim_feedforward, dropout=dropout, activation=activation
-        )
-        self.feedforward_dropout = nn.Dropout(dropout)
-        # layernorms
-        self.attention_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
-        self.cross_attention_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
-        self.feedforward_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
-        self.norm_first = norm_first
-
-    def _self_attention_block(
-        self, hidden_states: Tensor, attention_mask: Optional[Tensor] = None
-    ) -> Tensor:
-        output = self.attention(
-            hidden_states, attention_mask=attention_mask, return_attn_weights=False
-        )
-        output = self.attention_dropout(output)
-        return output
-
-    def _cross_attention_block(
-        self,
-        hidden_states: Tensor,
-        encoder_hidden_states: Tensor,
-        cross_attention_mask: Optional[Tensor] = None,
-    ) -> Tensor:
-        output = self.cross_attention(
-            hidden_states,
-            encoder_hidden_states,
-            attention_mask=cross_attention_mask,
-            return_attn_weights=False,
-        )
-        output = self.cross_attention_dropout(output)
-        return output
-
-    def _feedforward_block(self, hidden_states: Tensor) -> Tensor:
-        h = self.feedforward(hidden_states)
-        h = self.feedforward_dropout(h)
-        return h
-
-    def _forward_prenorm(
-        self,
-        hidden_states: Tensor,
-        encoder_hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        cross_attention_mask: Optional[Tensor] = None,
-    ) -> Tensor:
-        x = hidden_states
-        kv = encoder_hidden_states
-        inputs = self.attention_layernorm(x)
-        attn_output = self._self_attention_block(inputs, attention_mask=attention_mask)
-        attn_residual = attn_output + x
-        attn_norm_output = self.cross_attention_layernorm(attn_residual)
-        cross_attention_output = self._cross_attention_block(
-            attn_norm_output, kv, cross_attention_mask
-        )
-        cross_attention_residual = cross_attention_output + attn_norm_output
-        cross_attention_norm_output = self.feedforward_layernorm(
-            cross_attention_residual
-        )
-        ff_residual = cross_attention_norm_output + self._feedforward_block(
-            cross_attention_norm_output
-        )
-        return ff_residual
-
-    def _forward_postnorm(
-        self,
-        hidden_states: Tensor,
-        encoder_hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        cross_attention_mask: Optional[Tensor] = None,
-    ) -> Tensor:
-        x = hidden_states
-        kv = encoder_hidden_states
-        attn_output = self._self_attention_block(x, attention_mask=attention_mask)
-        attn_residual = attn_output + x
-        attn_norm_output = self.attention_layernorm(attn_residual)
-        cross_attention_output = self._cross_attention_block(
-            attn_norm_output, kv, cross_attention_mask
-        )
-        cross_attention_residual = cross_attention_output + attn_norm_output
-        cross_attention_norm_output = self.cross_attention_layernorm(
-            cross_attention_residual
-        )
-        ff_residual = cross_attention_norm_output + self._feedforward_block(
-            cross_attention_norm_output
-        )
-        outputs = self.feedforward_layernorm(ff_residual)
-        return outputs
-
-    def forward(
-        self,
-        hidden_states: Tensor,
-        encoder_hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        cross_attention_mask: Optional[Tensor] = None,
-    ) -> Tensor:
-        if self.norm_first:
-            return self._forward_prenorm(
-                hidden_states,
-                encoder_hidden_states,
-                attention_mask,
-                cross_attention_mask,
-            )
-        else:
-            return self._forward_postnorm(
-                hidden_states,
-                encoder_hidden_states,
-                attention_mask,
-                cross_attention_mask,
-            )
-
-
-class TransformerEncoderLayer(nn.Module):
-    """Transformer encoder layer is made up of multihead self-attention and feedforward blocks,
-    based on the architecture in "Attention Is All You Need" (Vaswani et al. 2017). Similar to
-    ``nn.TransformerEncoderLayer``, but uses a custom ``MultiHeadAttention`` that supports
-    n-dimensional inputs (including sequences, images, video) and head-masking.
-
-    Attributes:
-        d_model (int): size of hidden dimension of input
-        n_head (int): number of attention heads
-        dim_feedforward (int): size of hidden dimension of feedforward network
-        dropout (float): dropout probability for all dropouts. Defaults to 0.
-        activation (Callable): activation function in feedforward network. Defaults to ``nn.ReLU``.
-        layer_norm_eps (float): the eps value in layer norms. Default is 1e-12.
-        norm_first (bool): if True, layer norm is done prior to each of self-attention, cross-attention,
-            and feedforward. Otherwise, layer norm is done after.
-
-    Args:
-        hidden_states (Tensor): input tensor of shape [b, d1, ..., dn, c] to calculate self-attention on.
-        attention_mask (Tensor, optional): mask to be applied to self-attention inputs, ``hidden_states``. See
-            ``MultiHeadAttention`` for shape requirements.
-        head_mask (Tensor, optional): mask to be applied to self-attention inputs after softmax and dropout,
-            before matrix multiplication with values. See ``MultiHeadAttention`` for shape requirements.
-        return_attn_weights (bool, optional): return attention probabilities in addition to attention output.
-            Defaults to False.
-    """
-
-    def __init__(
-        self,
-        d_model: int,
-        n_head: int,
-        dim_feedforward: int,
-        dropout: float = 0.0,
-        activation: Callable[..., nn.Module] = nn.ReLU,
-        layer_norm_eps: float = 1e-12,
-        norm_first: bool = False,
-    ) -> None:
-        super().__init__()
-        # attention block
-        self.attention = MultiHeadAttention(
-            dim_q=d_model,
-            dim_kv=d_model,
-            n_head=n_head,
-            attn_module=SelfAttention(dropout),
-        )
-        self.attention_dropout = nn.Dropout(dropout)
-        # feedforward block
-        self.feedforward = MLP(
-            d_model, d_model, dim_feedforward, dropout=dropout, activation=activation
-        )
-        self.feedforward_dropout = nn.Dropout(dropout)
-        # layernorms
-        self.attention_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
-        self.feedforward_layernorm = Fp32LayerNorm(d_model, eps=layer_norm_eps)
-        self.norm_first = norm_first
-
-    def _attention_block(
-        self,
-        hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        head_mask: Optional[Tensor] = None,
-    ) -> Tuple[Tensor, Tensor]:
-        output, attn_weights = self.attention(
-            hidden_states,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-            return_attn_weights=True,
-        )
-        output = self.attention_dropout(output)
-        return output, attn_weights
-
-    def _feedforward_block(self, hidden_states: Tensor) -> Tensor:
-        h = self.feedforward(hidden_states)
-        h = self.feedforward_dropout(h)
-        return h
-
-    def _forward_prenorm(
-        self,
-        hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        head_mask: Optional[Tensor] = None,
-        return_attn_weights: bool = False,
-    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-        x = hidden_states
-        inputs = self.attention_layernorm(x)
-        attn_output, attn_weights = self._attention_block(
-            inputs,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-        )
-        attn_residual = attn_output + x
-        ff_residual = attn_residual + self._feedforward_block(
-            self.feedforward_layernorm(attn_residual)
-        )
-        if return_attn_weights:
-            return ff_residual, attn_weights
-        else:
-            return ff_residual
-
-    def _forward_postnorm(
-        self,
-        hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        head_mask: Optional[Tensor] = None,
-        return_attn_weights: bool = False,
-    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-        x = hidden_states
-        attn_output, attn_weights = self._attention_block(
-            x,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-        )
-        attn_residual = attn_output + x
-        attn_residual = self.attention_layernorm(attn_residual)
-        ff_residual = attn_residual + self._feedforward_block(attn_residual)
-        outputs = self.feedforward_layernorm(ff_residual)
-        if return_attn_weights:
-            return outputs, attn_weights
-        else:
-            return outputs
-
-    def forward(
-        self,
-        hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        head_mask: Optional[Tensor] = None,
-        return_attn_weights: bool = False,
-    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-        if self.norm_first:
-            return self._forward_prenorm(
-                hidden_states,
-                attention_mask,
-                head_mask,
-                return_attn_weights,
-            )
-        else:
-            return self._forward_postnorm(
-                hidden_states,
-                attention_mask,
-                head_mask,
-                return_attn_weights,
-            )
-
-
-class TransformerEncoder(nn.Module):
-    def __init__(
-        self,
-        n_layer: int,
-        d_model: int,
-        n_head: int,
-        dim_feedforward: int,
-        dropout: float = 0.0,
-        activation: Callable[..., nn.Module] = nn.ReLU,
-        layer_norm_eps: float = 1e-12,
-        norm_first: bool = False,
-        final_layer_norm_eps: Optional[float] = None,
-    ):
-        super().__init__()
-        self.layer = nn.ModuleList(
-            [
-                TransformerEncoderLayer(
-                    d_model,
-                    n_head,
-                    dim_feedforward,
-                    dropout,
-                    activation,
-                    layer_norm_eps,
-                    norm_first,
-                )
-                for _ in range(n_layer)
-            ]
-        )
-        self.final_layer_norm = None
-        if final_layer_norm_eps:
-            self.final_layer_norm = Fp32LayerNorm(d_model, eps=final_layer_norm_eps)
-
-    def forward(
-        self,
-        hidden_states: Tensor,
-        attention_mask: Optional[Tensor] = None,
-        head_mask: Optional[Tensor] = None,
-        return_attn_weights: bool = False,
-        return_hidden_states: bool = False,
-    ) -> TransformerOutput:
-
-        all_hidden_states = [] if return_hidden_states else None
-        all_self_attentions = [] if return_attn_weights else None
-
-        for layer_module in self.layer:
-            if return_hidden_states:
-                all_hidden_states.append(hidden_states)
-
-            layer_outputs = layer_module(
-                hidden_states,
-                attention_mask=attention_mask,
-                head_mask=head_mask,
-                return_attn_weights=return_attn_weights,
-            )
-
-            if return_attn_weights:
-                hidden_states = layer_outputs[0]
-                all_self_attentions.append(layer_outputs[1])
-            else:
-                hidden_states = layer_outputs
-
-        if return_hidden_states:
-            all_hidden_states.append(hidden_states)
-
-        if self.final_layer_norm is not None:
-            hidden_states = self.final_layer_norm(hidden_states)
-
-        return TransformerOutput(
-            last_hidden_state=hidden_states,
-            hidden_states=all_hidden_states,
-            attentions=all_self_attentions,
-        )
```

## torchmultimodal/utils/common.py

```diff
@@ -183,7 +183,16 @@
             return fn(cls, *inputs, **kwargs)
 
     return inner
 
 
 def get_clones(module: nn.Module, n: int) -> nn.ModuleList:
     return nn.ModuleList([deepcopy(module) for i in range(n)])
+
+
+def init_module_parameters_to_zero(module: nn.Module) -> None:
+    """
+    Sets the parameters of a module to zero. This is a commonly used trick
+    from Fixup initialization, to stabilize training of residual networks.
+    """
+    for p in module.parameters():
+        nn.init.zeros_(p)
```

## Comparing `torchmultimodal_nightly-2023.7.9.dist-info/LICENSE` & `torchmultimodal_nightly-2023.8.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchmultimodal_nightly-2023.7.9.dist-info/METADATA` & `torchmultimodal_nightly-2023.8.2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchmultimodal-nightly
-Version: 2023.7.9
+Version: 2023.8.2
 Summary: PyTorch Multimodal Library
 Home-page: https://github.com/facebookresearch/multimodal
 Author: PyTorch Multimodal Team
 Author-email: torchmultimodal@fb.com
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
```

## Comparing `torchmultimodal_nightly-2023.7.9.dist-info/NOTICES` & `torchmultimodal_nightly-2023.8.2.dist-info/NOTICES`

 * *Files identical despite different names*

## Comparing `torchmultimodal_nightly-2023.7.9.dist-info/RECORD` & `torchmultimodal_nightly-2023.8.2.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -6,63 +6,63 @@
 torchmultimodal/models/two_tower.py,sha256=py988p-rFNncA1LALHejlx1dc5VaB9pQIS9uUyKJPb0,3666
 torchmultimodal/models/video_gpt.py,sha256=SW9Ux2xjM0WcmSPtcIFPBKA3UEGxxVJR8HybzM3pnaQ,8698
 torchmultimodal/models/video_vqvae.py,sha256=k82-wyhbgl7Zi4Fy2T6zPkErr5PdKys92v8bb9c5U-s,13705
 torchmultimodal/models/vqvae.py,sha256=ok2m-KcWwXY3TpA2fKFW_yJqXR5Xc6asxS6Yn73zDgU,4731
 torchmultimodal/models/albef/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/models/albef/image_encoder.py,sha256=kZjjZvT68bf81itqWFw2f5Tt2-9nDMLFB4ZkKra2ypU,2687
 torchmultimodal/models/albef/model.py,sha256=LHm3G8CHF13eTnV0czT79nH_DhXiHSmUBABEcoAb74g,12835
-torchmultimodal/models/albef/multimodal_encoder.py,sha256=y_DRstthWtxMA6LBMZws6oJ5nSz8SjQqsih3IOHnvr0,3643
+torchmultimodal/models/albef/multimodal_encoder.py,sha256=v7XMcP4lcKftzz5lwDesazEVrA4z3UFR2Of3_lBK0Bs,10464
 torchmultimodal/models/clip/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/models/clip/image_encoder.py,sha256=rKGjM5G6bMS2PZIFfYZ0kYB9ELFv0P9fdIuSkjStahI,11987
 torchmultimodal/models/clip/model.py,sha256=F3cqwi44HVM0WgtJRZsr1t74YMnIpUOmTAOUg_P7Blc,6885
 torchmultimodal/models/clip/text_encoder.py,sha256=uYafDrX97dlW5d7ABCXYgqTWgsVt4yAbpz_F7F44coM,4851
 torchmultimodal/models/dalle2/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/models/dalle2/dalle2_decoder.py,sha256=1PDhpFwPVIkWUeFEevugb131rsEvWSJajELGV8frxM0,4731
 torchmultimodal/models/dalle2/adm/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/models/dalle2/adm/adm.py,sha256=g7JTJtfReDKiznVttVk7GwlCI8MK08A5WQmkyYgQqxA,22492
 torchmultimodal/models/dalle2/adm/attention_block.py,sha256=1mAKC-kFXj8qLO9XXkPlEHuOAGGJcjpzYhP842HezY8,7001
 torchmultimodal/models/dalle2/adm/res_block.py,sha256=6UzKOL9xhaL1WxDkgcKuGRXuk9BegGgmsBoU9oS8sMM,7638
 torchmultimodal/models/flava/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchmultimodal/models/flava/image_encoder.py,sha256=OsA68t9mFClQurtB0Z-ta1P_O3ssTsDSjy6GNBVB-_o,10585
-torchmultimodal/models/flava/model.py,sha256=usKe8UmXRb6F7AmczgY4H6ftqoZwGFnO7giw0a-9vLw,26154
-torchmultimodal/models/flava/text_encoder.py,sha256=SH8DdnNLslyvqfS9ex14EsZdBidB-fTGxGkFuSQNfEE,2297
-torchmultimodal/models/flava/transformer.py,sha256=I1vzLVx-6jv82K7piFFQ6BAQBU-39aPADxYLUcIMBb0,3239
+torchmultimodal/models/flava/image_encoder.py,sha256=FyEzFnwOoa6LAYloJmG91AoJguwBl8lMXkckR_oP5bE,10585
+torchmultimodal/models/flava/model.py,sha256=H1hh6AlMJbDD9fYbx4WBk_k1yiHfHNd1PrscKtz79Q0,26154
+torchmultimodal/models/flava/text_encoder.py,sha256=ZoKW93m86nL4CREoeP6L2o5sO4_gH03WGHHhwzxeYVk,2256
+torchmultimodal/models/flava/transformer.py,sha256=eQlitLG3KZrV1EcN___g9AVlRU1Ep8c4DttHycJi_vU,11343
 torchmultimodal/models/mdetr/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/models/mdetr/image_encoder.py,sha256=CEDyn0RADjwG7UONLwOkuIXc10dOok802yqjKM9YsxQ,6128
 torchmultimodal/models/mdetr/model.py,sha256=NMRQpvAN8M-Udo-F0qxkxJX5hvqbxEIaBVbnool-wJU,16672
 torchmultimodal/models/mdetr/text_encoder.py,sha256=4GlA8d1jOWUOe-b-tiNiqgo_n2U4-w8ijhXaiaS7rJc,5433
 torchmultimodal/models/mdetr/transformer.py,sha256=uWhfWcIMpdSJuzhR66nlYb60D1ydsJ-4W-_KtgZAzXU,16850
 torchmultimodal/modules/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/modules/diffusion/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/modules/diffusion/cfguidance.py,sha256=l6X6b98F9jGb2Fe_LMUVM4AC4WirxT1WiqSLg4bvjQE,8452
 torchmultimodal/modules/diffusion/ddim.py,sha256=WlSR0f49hjcWio1Nf36FOOZX402aJuGtYHrCma-KM6I,5752
 torchmultimodal/modules/diffusion/ddpm.py,sha256=VMVjdjXOOHkxiU-yD1vWfp_4oRoJeleaIotpm8VqlTw,7226
 torchmultimodal/modules/diffusion/predictors.py,sha256=busySNE_ruknipRMpMuWMfrD5g4yVShYH52YmVVkerI,3782
 torchmultimodal/modules/diffusion/schedules.py,sha256=4by79UQBYJwr6KNnbPATE1v7E_ZoYTGkzXWdwBfXQjI,12620
 torchmultimodal/modules/encoders/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchmultimodal/modules/encoders/bert_text_encoder.py,sha256=kdpzg3DSlnFqoDO8tEeLkdCqfupDS_3BomoC-s05coQ,7084
+torchmultimodal/modules/encoders/bert_text_encoder.py,sha256=M-lTIJyQ3Gp5qfhMgfY8q4aWWffkpCbmF6noMFkAiGU,7123
 torchmultimodal/modules/encoders/embedding_encoder.py,sha256=X_cAwz_zclpT3JpYb9aOtGEJSdMgKRYZQgGZP4Sw-zs,1883
 torchmultimodal/modules/encoders/mil_encoder.py,sha256=S-bwgXTuUXlvpCz9P74FHxvJsyOepCvzflleJIKzXFw,3859
 torchmultimodal/modules/encoders/swin_transformer_3d_encoder.py,sha256=pPxFJPjA8ntOua3UMe64rg3F_5UFyCfYJJa_uu7ZDjw,3288
 torchmultimodal/modules/encoders/weighted_embedding_encoder.py,sha256=qJ0fg5-wHEsyTafGjwfkn1EjjMiJP0areJoQXFO3IX8,2111
 torchmultimodal/modules/fusions/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/modules/fusions/attention_fusion.py,sha256=p9PiIuThRIO7M4Gsxq_h5Qcu-AFy1Fw8yFt7kJEg-vU,2263
 torchmultimodal/modules/fusions/concat_fusion.py,sha256=NGaYMBqiilOECWpMeiGpYWNSnndJ-jPVGIGu5FQqtnE,977
 torchmultimodal/modules/fusions/deepset_fusion.py,sha256=Z8RG9QwROeisWWw0Mmq33GNMYP4VgWG81FDkbTbY37c,8722
 torchmultimodal/modules/layers/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchmultimodal/modules/layers/activation.py,sha256=qRCBoh42Z6fzMo_COxEE9erOdtiMLeIAifo6P1aFnE8,791
+torchmultimodal/modules/layers/activation.py,sha256=R82978_eXGzJ03lkUAxAyiTENmcMkFpfFoNsIHu4wcQ,1393
 torchmultimodal/modules/layers/attention.py,sha256=53GeQUd-FT3jxNGKq07W9Hjd2pz8rgqi_BMzvr4ezMk,17251
 torchmultimodal/modules/layers/codebook.py,sha256=1Vu-FJvch_0FVUahQ-YV0Z5coIggT0LneWgLcLdQkl8,11764
 torchmultimodal/modules/layers/conv.py,sha256=76-2uMuLH6-UVo6bQpWj3dv0JJmpPhpwvb42GtWvHZs,9498
 torchmultimodal/modules/layers/mlp.py,sha256=K7Ip4mg_P9whG-HceViZehJTdb8cv9rKIzG3ExE7fqM,2254
-torchmultimodal/modules/layers/multi_head_attention.py,sha256=l8EdI8v7DfyCGh0hYdTKzVclh7mLlbdX7zhnTSjCWxg,7107
+torchmultimodal/modules/layers/multi_head_attention.py,sha256=kDIEUtEz-IOg-xpzY-YrlUKv6bIFpi4rzAlHaJT0B_s,7291
 torchmultimodal/modules/layers/normalizations.py,sha256=KpHGiujEE9a9IoiONHK6bjGryFkk23Z3XWqUZ4V8N28,1512
 torchmultimodal/modules/layers/position_embedding.py,sha256=q3r0602n0OMotMBdaXBqaXvkIQKRc8QiJUMRqPtta0w,6433
 torchmultimodal/modules/layers/text_embedding.py,sha256=4HBSAwoCrHSLZs--odsIOzQF2ncQPxFp3G-31jiWfwA,4720
-torchmultimodal/modules/layers/transformer.py,sha256=kcjWYFTzAD56GRdGcPQFuGK13V1aDh9LE8ovk-v1i3k,15503
+torchmultimodal/modules/layers/transformer.py,sha256=ulTsWM4TUErUO7JiwI6kuinA77aJEwaC_RYEMvZ_rJk,688
 torchmultimodal/modules/losses/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/modules/losses/albef.py,sha256=JUnEiYOuLE5tiSItMGOIGe_X_6gGYZI9ENwhRcDSzMw,5280
 torchmultimodal/modules/losses/contrastive_loss_with_temperature.py,sha256=EeYkFM3HsJGzLolQQClc1NauseoxQ2NKpAlpffLPfYk,8166
 torchmultimodal/modules/losses/diffusion.py,sha256=dXrqPFWh-wVjR7EqpT_A2XXdGSoObTNeJU-y50xhewE,6068
 torchmultimodal/modules/losses/flava.py,sha256=5i_7bl2bXTwZeBS3LuKmfdanmNAf_Ax9_5df6GJkrD0,17264
 torchmultimodal/modules/losses/mdetr.py,sha256=xdZadhY6AokmeZj4UZC8obDl9j5lTPKXf9rTsbO87YM,6295
 torchmultimodal/modules/losses/vqvae.py,sha256=pLEMButki7difmX2EWKSKhSXmxKnOy4_UyZE91upWSI,1182
@@ -71,18 +71,18 @@
 torchmultimodal/transforms/clip_transform.py,sha256=jDSFKjzjKpbPuwhzE22cYn5MQFp40nC3xwq2v85DJtc,8292
 torchmultimodal/transforms/diffusion_transforms.py,sha256=QiZIYEhlSyBVXxF_xRcBNz7t41T6ZGqR79xETbEnrKM,3076
 torchmultimodal/transforms/flava_transform.py,sha256=JK_HmsVlerHvSsWhe2SH9YIx9MPsCMhPQyNCvXa1P_0,12098
 torchmultimodal/transforms/video_transform.py,sha256=VBTB7KUGQnjO9T-uOYC1Ww--4I8gP9mSDp3_ig6ibFk,3597
 torchmultimodal/utils/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchmultimodal/utils/assertion.py,sha256=ORPGR1A5aRHu0O008xw5oPDagg7RC-YRHSLG2ojcOBA,475
 torchmultimodal/utils/attention.py,sha256=o5a2dnFH5VqQvYd1aStvJP2Jso-fNy0VS79KfhpUMiA,2621
-torchmultimodal/utils/common.py,sha256=SyaO7eWn0UZraSdE9u_7RNoo0WyK6nSRI3G_rYT-Ie0,6586
+torchmultimodal/utils/common.py,sha256=VKnijxye4haICveottI21tiNwWBKOPvd4oP8wiVtQW4,6877
 torchmultimodal/utils/diffusion_utils.py,sha256=2IAwtWKB8D9Do-vth_oISR7OvaaVGtG2xYoqdsrEAHU,1423
 torchmultimodal/utils/distributed.py,sha256=e6nX4hnIVDSGNL-6aqvjtvoLBejdmvVkTgZgS5ZcvaI,1586
 torchmultimodal/utils/file_io.py,sha256=lq-4vSRZzPzSX_L2KgzVEC3WmeWob7toF6KSFkIvDEc,601
 torchmultimodal/utils/generate.py,sha256=d3DSxRytub8BicpZLtm2gCG2QMmKam_KEyOJUav3cSY,13583
-torchmultimodal_nightly-2023.7.9.dist-info/LICENSE,sha256=duRyurRGfGwtCNAEpO3e-kpCzqgxBuolm9Elasr9BHA,1519
-torchmultimodal_nightly-2023.7.9.dist-info/METADATA,sha256=hrzGMDTTFS9W7hN3hsatw-qjCbkyhenATa12PD-umA4,4967
-torchmultimodal_nightly-2023.7.9.dist-info/NOTICES,sha256=I1LcugX64ADueZokp1jo21mJ-w2bHmxZg3oKRH4MRCc,12590
-torchmultimodal_nightly-2023.7.9.dist-info/WHEEL,sha256=ijjRDmPkGsL9eKpOeSzmTjLbiyw0Dy8TY4QGa2Zy9J8,93
-torchmultimodal_nightly-2023.7.9.dist-info/top_level.txt,sha256=UNjlL7zuIEXrk_tVX96dzKo-1gSLPHaHvxJt3-tQM1c,16
-torchmultimodal_nightly-2023.7.9.dist-info/RECORD,,
+torchmultimodal_nightly-2023.8.2.dist-info/LICENSE,sha256=duRyurRGfGwtCNAEpO3e-kpCzqgxBuolm9Elasr9BHA,1519
+torchmultimodal_nightly-2023.8.2.dist-info/METADATA,sha256=IIsOWkzqAbxUlQQbsICkm0yhk0e6Ry1IRaJ26tQTAak,4967
+torchmultimodal_nightly-2023.8.2.dist-info/NOTICES,sha256=I1LcugX64ADueZokp1jo21mJ-w2bHmxZg3oKRH4MRCc,12590
+torchmultimodal_nightly-2023.8.2.dist-info/WHEEL,sha256=x0YL6OSV_WitIuMP0HsTpwBgQTKZeHzZKOAyhSZVN4o,93
+torchmultimodal_nightly-2023.8.2.dist-info/top_level.txt,sha256=UNjlL7zuIEXrk_tVX96dzKo-1gSLPHaHvxJt3-tQM1c,16
+torchmultimodal_nightly-2023.8.2.dist-info/RECORD,,
```

