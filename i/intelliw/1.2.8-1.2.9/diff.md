# Comparing `tmp/intelliw-1.2.8-py3-none-any.whl.zip` & `tmp/intelliw-1.2.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,85 +1,82 @@
-Zip file size: 148918 bytes, number of entries: 83
+Zip file size: 156791 bytes, number of entries: 80
 -rw-r--r--  2.0 unx    10244 b- defN 22-Oct-19 09:16 intelliw/.DS_Store
--rw-r--r--  2.0 unx      703 b- defN 22-Nov-25 01:41 intelliw/__init__.py
--rw-r--r--  2.0 unx      435 b- defN 22-Nov-11 07:28 intelliw/feature.py
--rwxr-xr-x  2.0 unx     9500 b- defN 22-Nov-11 07:28 intelliw/prepare_env.sh
+-rw-r--r--  2.0 unx      704 b- defN 23-Apr-04 02:49 intelliw/__init__.py
+-rw-r--r--  2.0 unx      607 b- defN 23-Apr-04 02:49 intelliw/feature.py
+-rwxr-xr-x  2.0 unx     9562 b- defN 23-Apr-04 02:49 intelliw/prepare_env.sh
 -rw-r--r--  2.0 unx        0 b- defN 22-Sep-19 01:43 intelliw/algorithms_demo/README.md
 -rw-r--r--  2.0 unx     5402 b- defN 22-Nov-18 06:28 intelliw/algorithms_demo/algorithm.py
--rw-r--r--  2.0 unx      369 b- defN 22-Oct-13 04:55 intelliw/algorithms_demo/algorithm.yaml
+-rw-r--r--  2.0 unx      305 b- defN 23-Apr-04 02:49 intelliw/algorithms_demo/algorithm.yaml
 -rw-r--r--  2.0 unx        0 b- defN 22-Sep-19 01:43 intelliw/algorithms_demo/requirements.txt
 -rw-r--r--  2.0 unx        0 b- defN 22-Sep-19 01:43 intelliw/config/__init__.py
 -rw-r--r--  2.0 unx     9271 b- defN 22-Sep-19 09:35 intelliw/config/cfg_parser.py
--rw-r--r--  2.0 unx     8672 b- defN 22-Nov-11 07:28 intelliw/config/config.py
+-rw-r--r--  2.0 unx     9195 b- defN 23-Apr-04 02:49 intelliw/config/config.py
 -rw-r--r--  2.0 unx        1 b- defN 22-Sep-19 01:43 intelliw/config/loader/__init__.py
 -rw-r--r--  2.0 unx     7455 b- defN 22-Sep-19 01:43 intelliw/config/loader/schema.py
 -rw-r--r--  2.0 unx     2697 b- defN 22-Oct-11 09:22 intelliw/config/loader/yaml_helpers.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Sep-19 01:43 intelliw/core/__init__.py
--rw-r--r--  2.0 unx     7440 b- defN 22-Sep-19 09:35 intelliw/core/al_decorator.py
+-rw-r--r--  2.0 unx     9580 b- defN 23-Apr-04 02:49 intelliw/core/al_decorator.py
 -rw-r--r--  2.0 unx      723 b- defN 22-Nov-22 01:48 intelliw/core/infer.py
 -rw-r--r--  2.0 unx     9609 b- defN 22-Nov-11 07:28 intelliw/core/linkserver.py
--rw-r--r--  2.0 unx    25661 b- defN 22-Nov-22 01:48 intelliw/core/pipeline.py
--rw-r--r--  2.0 unx     3302 b- defN 22-Nov-22 01:48 intelliw/core/recorder.py
--rw-r--r--  2.0 unx      718 b- defN 22-Nov-22 01:48 intelliw/core/trainer.py
--rw-r--r--  2.0 unx        0 b- defN 22-Nov-04 05:45 intelliw/core/logs/iw-algo-fx-user.log
--rw-r--r--  2.0 unx      865 b- defN 22-Nov-04 05:49 intelliw/core/logs/iw-algo-fx.log
+-rw-r--r--  2.0 unx    26440 b- defN 23-Apr-04 02:49 intelliw/core/pipeline.py
+-rw-r--r--  2.0 unx     3398 b- defN 23-Apr-04 02:49 intelliw/core/recorder.py
+-rw-r--r--  2.0 unx      922 b- defN 23-Apr-04 02:49 intelliw/core/trainer.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Mar-14 01:53 intelliw/datasets/__init__.py
--rw-r--r--  2.0 unx    12068 b- defN 22-Nov-25 01:40 intelliw/datasets/datasets.py
--rw-r--r--  2.0 unx     4442 b- defN 22-Nov-11 07:28 intelliw/datasets/datasource_base.py
--rw-r--r--  2.0 unx     8280 b- defN 22-Nov-25 01:40 intelliw/datasets/datasource_intelliv.py
--rw-r--r--  2.0 unx     8099 b- defN 22-Nov-22 01:48 intelliw/datasets/datasource_iwfactorydata.py
--rw-r--r--  2.0 unx    14760 b- defN 22-Nov-22 01:48 intelliw/datasets/datasource_iwimgdata.py
--rw-r--r--  2.0 unx     5195 b- defN 22-Nov-22 01:48 intelliw/datasets/datasource_local_csv.py
--rw-r--r--  2.0 unx    14046 b- defN 22-Nov-22 01:48 intelliw/datasets/datasource_nlp_corpora.py
+-rw-r--r--  2.0 unx     9150 b- defN 23-Apr-04 02:49 intelliw/datasets/dataset_writer.py
+-rw-r--r--  2.0 unx    12269 b- defN 23-Apr-04 02:49 intelliw/datasets/datasets.py
+-rw-r--r--  2.0 unx     4749 b- defN 23-Apr-04 02:49 intelliw/datasets/datasource_base.py
+-rw-r--r--  2.0 unx     6307 b- defN 23-Apr-04 02:49 intelliw/datasets/datasource_intelliv.py
+-rw-r--r--  2.0 unx     7809 b- defN 23-Apr-04 02:49 intelliw/datasets/datasource_iwfactorydata.py
+-rw-r--r--  2.0 unx    14898 b- defN 23-Apr-04 02:49 intelliw/datasets/datasource_iwimgdata.py
+-rw-r--r--  2.0 unx     5109 b- defN 23-Apr-04 02:49 intelliw/datasets/datasource_local_csv.py
+-rw-r--r--  2.0 unx    14012 b- defN 23-Apr-04 02:49 intelliw/datasets/datasource_nlp_corpora.py
 -rw-r--r--  2.0 unx     1986 b- defN 22-Nov-11 07:28 intelliw/datasets/datasource_remote_csv.py
--rw-r--r--  2.0 unx    12687 b- defN 22-Nov-22 01:48 intelliw/datasets/spliter.py
--rw-r--r--  2.0 unx    10284 b- defN 22-Nov-11 07:28 intelliw/docs/Q&A.md
--rw-r--r--  2.0 unx     3178 b- defN 22-Nov-11 07:28 intelliw/docs/README.md
--rw-r--r--  2.0 unx    42744 b- defN 22-Nov-11 07:28 intelliw/docs/instructions.md
+-rw-r--r--  2.0 unx     6752 b- defN 23-Apr-04 02:49 intelliw/datasets/datasource_semantic.py
+-rw-r--r--  2.0 unx    12667 b- defN 23-Apr-04 02:49 intelliw/datasets/spliter.py
+-rw-r--r--  2.0 unx    10278 b- defN 23-Apr-04 02:49 intelliw/docs/Q&A.md
+-rw-r--r--  2.0 unx     3142 b- defN 23-Apr-04 02:49 intelliw/docs/README.md
+-rw-r--r--  2.0 unx    52196 b- defN 23-Apr-04 02:49 intelliw/docs/instructions.md
 -rw-r--r--  2.0 unx      699 b- defN 22-Nov-11 07:28 intelliw/functions/__init__.py
 -rw-r--r--  2.0 unx     5512 b- defN 22-Sep-19 09:35 intelliw/functions/data_filtering.py
--rw-r--r--  2.0 unx    37437 b- defN 22-Nov-22 01:48 intelliw/functions/feature_process.py
+-rw-r--r--  2.0 unx    37567 b- defN 23-Apr-04 02:49 intelliw/functions/feature_process.py
 -rw-r--r--  2.0 unx      619 b- defN 22-Sep-19 01:43 intelliw/functions/general.py
 -rw-r--r--  2.0 unx     1486 b- defN 22-Nov-11 07:28 intelliw/functions/matrix.py
 -rw-r--r--  2.0 unx     1711 b- defN 22-Nov-11 07:28 intelliw/functions/opencv.py
--rw-r--r--  2.0 unx      650 b- defN 22-May-24 09:19 intelliw/functions/post.py
+-rw-r--r--  2.0 unx      650 b- defN 23-Feb-03 02:24 intelliw/functions/post.py
 -rw-r--r--  2.0 unx      779 b- defN 22-Sep-19 09:35 intelliw/functions/select_columns.py
--rw-r--r--  2.0 unx     1383 b- defN 22-Mar-14 01:53 intelliw/infer/conf/syslog.xml
 -rw-r--r--  2.0 unx      171 b- defN 22-Sep-19 01:43 intelliw/interface/__init__.py
--rw-r--r--  2.0 unx     5030 b- defN 22-Nov-22 01:45 intelliw/interface/apihandler.py
--rw-r--r--  2.0 unx     7484 b- defN 22-Nov-22 01:48 intelliw/interface/apijob.py
--rw-r--r--  2.0 unx     7687 b- defN 22-Nov-22 01:48 intelliw/interface/batchjob.py
--rw-r--r--  2.0 unx     5013 b- defN 22-Nov-22 01:48 intelliw/interface/controller.py
--rw-r--r--  2.0 unx     8383 b- defN 22-Nov-11 07:28 intelliw/interface/entrypoint.py
--rw-r--r--  2.0 unx     5164 b- defN 22-Nov-22 01:48 intelliw/interface/goinferjob.py
--rw-r--r--  2.0 unx     1204 b- defN 22-Nov-22 01:48 intelliw/interface/trainjob.py
--rw-r--r--  2.0 unx     2636 b- defN 22-Nov-22 01:48 intelliw/interface/validatejob.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-05 08:11 intelliw/logs/iw-algo-fx-user.log
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-05 08:11 intelliw/logs/iw-algo-fx.log
--rw-r--r--  2.0 unx     6148 b- defN 22-Aug-25 02:21 intelliw/utils/.DS_Store
+-rw-r--r--  2.0 unx     5030 b- defN 23-Apr-04 02:49 intelliw/interface/apihandler.py
+-rw-r--r--  2.0 unx     6998 b- defN 23-Apr-04 02:49 intelliw/interface/apijob.py
+-rw-r--r--  2.0 unx     7606 b- defN 23-Apr-04 02:49 intelliw/interface/batchjob.py
+-rw-r--r--  2.0 unx     4092 b- defN 23-Apr-04 02:49 intelliw/interface/controller.py
+-rw-r--r--  2.0 unx     8647 b- defN 23-Apr-04 02:49 intelliw/interface/entrypoint.py
+-rw-r--r--  2.0 unx     1286 b- defN 23-Apr-04 02:49 intelliw/interface/trainjob.py
+-rw-r--r--  2.0 unx     2463 b- defN 23-Apr-04 02:49 intelliw/interface/validatejob.py
+-rw-r--r--  2.0 unx     8196 b- defN 23-Feb-21 06:42 intelliw/utils/.DS_Store
 -rw-r--r--  2.0 unx      167 b- defN 22-Sep-19 01:43 intelliw/utils/__init__.py
--rw-r--r--  2.0 unx     2049 b- defN 22-Nov-12 03:18 intelliw/utils/check_algorithm_py.py
--rw-r--r--  2.0 unx     4103 b- defN 22-Nov-11 07:28 intelliw/utils/crontab.py
--rw-r--r--  2.0 unx     5267 b- defN 22-Nov-22 01:48 intelliw/utils/debug_controller.py
--rw-r--r--  2.0 unx      550 b- defN 22-Nov-11 07:28 intelliw/utils/exception.py
--rw-r--r--  2.0 unx     8802 b- defN 22-Sep-19 09:35 intelliw/utils/gen_high_performance_cfg.py
+-rw-r--r--  2.0 unx     2048 b- defN 23-Apr-04 02:49 intelliw/utils/check_algorithm_py.py
+-rw-r--r--  2.0 unx     4103 b- defN 23-Mar-21 02:13 intelliw/utils/crontab.py
+-rw-r--r--  2.0 unx     5303 b- defN 23-Apr-04 02:49 intelliw/utils/debug_controller.py
+-rw-r--r--  2.0 unx     3352 b- defN 23-Apr-04 02:49 intelliw/utils/exception.py
 -rw-r--r--  2.0 unx     2233 b- defN 22-Sep-19 09:35 intelliw/utils/gen_model_cfg.py
--rw-r--r--  2.0 unx     1634 b- defN 22-Sep-19 09:35 intelliw/utils/global_val.py
--rw-r--r--  2.0 unx     8106 b- defN 22-Nov-22 01:48 intelliw/utils/iuap_request.py
--rw-r--r--  2.0 unx     3044 b- defN 22-Nov-11 07:28 intelliw/utils/logger.py
--rw-r--r--  2.0 unx     6506 b- defN 22-Nov-11 07:28 intelliw/utils/message.py
--rw-r--r--  2.0 unx     3410 b- defN 22-Sep-19 09:35 intelliw/utils/storage_service.py
--rw-r--r--  2.0 unx     5443 b- defN 22-Nov-22 01:48 intelliw/utils/util.py
+-rw-r--r--  2.0 unx     1826 b- defN 23-Apr-04 02:49 intelliw/utils/global_val.py
+-rw-r--r--  2.0 unx     8800 b- defN 23-Apr-04 02:49 intelliw/utils/iuap_request.py
+-rw-r--r--  2.0 unx     3982 b- defN 23-Apr-04 02:49 intelliw/utils/logger.py
+-rw-r--r--  2.0 unx     6582 b- defN 23-Apr-04 02:49 intelliw/utils/message.py
+-rw-r--r--  2.0 unx     3765 b- defN 23-Apr-04 02:49 intelliw/utils/storage_service.py
+-rw-r--r--  2.0 unx     5621 b- defN 23-Apr-04 02:49 intelliw/utils/util.py
 -rw-r--r--  2.0 unx      466 b- defN 22-Nov-11 07:28 intelliw/utils/database/__init__.py
 -rw-r--r--  2.0 unx     3253 b- defN 22-Nov-24 06:58 intelliw/utils/database/connection.py
 -rw-r--r--  2.0 unx     2023 b- defN 22-Nov-11 07:28 intelliw/utils/database/proxy.py
 -rw-r--r--  2.0 unx     2605 b- defN 22-Nov-11 07:28 intelliw/utils/database/schema.py
--rw-r--r--  2.0 unx      200 b- defN 22-Nov-11 07:28 intelliw/utils/dataprocess/__init__.py
--rw-r--r--  2.0 unx    25295 b- defN 22-Nov-22 01:48 intelliw/utils/dataprocess/engine.py
--rw-r--r--  2.0 unx     4417 b- defN 22-Nov-25 01:40 intelliw/utils/dataprocess/spark.py
--rw-r--r--  2.0 unx    35147 b- defN 22-Nov-25 01:43 intelliw-1.2.8.dist-info/LICENCE
--rw-r--r--  2.0 unx     3946 b- defN 22-Nov-25 01:43 intelliw-1.2.8.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Nov-25 01:43 intelliw-1.2.8.dist-info/WHEEL
--rw-r--r--  2.0 unx       64 b- defN 22-Nov-25 01:43 intelliw-1.2.8.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 22-Nov-25 01:43 intelliw-1.2.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     7227 b- defN 22-Nov-25 01:43 intelliw-1.2.8.dist-info/RECORD
-83 files, 476090 bytes uncompressed, 137412 bytes compressed:  71.1%
+-rw-r--r--  2.0 unx      336 b- defN 23-Apr-04 02:49 intelliw/utils/redis/__init__.py
+-rw-r--r--  2.0 unx     3397 b- defN 23-Apr-04 02:49 intelliw/utils/redis/connection.py
+-rw-r--r--  2.0 unx      200 b- defN 23-Apr-04 02:49 intelliw/utils/spark_process/__init__.py
+-rw-r--r--  2.0 unx    25407 b- defN 23-Apr-04 02:49 intelliw/utils/spark_process/engine.py
+-rw-r--r--  2.0 unx     4420 b- defN 23-Apr-04 02:49 intelliw/utils/spark_process/spark.py
+-rw-r--r--  2.0 unx    35147 b- defN 23-Apr-04 02:49 intelliw-1.2.9.dist-info/LICENCE
+-rw-r--r--  2.0 unx     4069 b- defN 23-Apr-04 02:49 intelliw-1.2.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-04 02:49 intelliw-1.2.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx       64 b- defN 23-Apr-04 02:49 intelliw-1.2.9.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        9 b- defN 23-Apr-04 02:49 intelliw-1.2.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     6977 b- defN 23-Apr-04 02:49 intelliw-1.2.9.dist-info/RECORD
+80 files, 497198 bytes uncompressed, 145691 bytes compressed:  70.7%
```

## zipnote {}

```diff
@@ -57,21 +57,18 @@
 
 Filename: intelliw/core/recorder.py
 Comment: 
 
 Filename: intelliw/core/trainer.py
 Comment: 
 
-Filename: intelliw/core/logs/iw-algo-fx-user.log
+Filename: intelliw/datasets/__init__.py
 Comment: 
 
-Filename: intelliw/core/logs/iw-algo-fx.log
-Comment: 
-
-Filename: intelliw/datasets/__init__.py
+Filename: intelliw/datasets/dataset_writer.py
 Comment: 
 
 Filename: intelliw/datasets/datasets.py
 Comment: 
 
 Filename: intelliw/datasets/datasource_base.py
 Comment: 
@@ -90,14 +87,17 @@
 
 Filename: intelliw/datasets/datasource_nlp_corpora.py
 Comment: 
 
 Filename: intelliw/datasets/datasource_remote_csv.py
 Comment: 
 
+Filename: intelliw/datasets/datasource_semantic.py
+Comment: 
+
 Filename: intelliw/datasets/spliter.py
 Comment: 
 
 Filename: intelliw/docs/Q&A.md
 Comment: 
 
 Filename: intelliw/docs/README.md
@@ -126,17 +126,14 @@
 
 Filename: intelliw/functions/post.py
 Comment: 
 
 Filename: intelliw/functions/select_columns.py
 Comment: 
 
-Filename: intelliw/infer/conf/syslog.xml
-Comment: 
-
 Filename: intelliw/interface/__init__.py
 Comment: 
 
 Filename: intelliw/interface/apihandler.py
 Comment: 
 
 Filename: intelliw/interface/apijob.py
@@ -147,29 +144,20 @@
 
 Filename: intelliw/interface/controller.py
 Comment: 
 
 Filename: intelliw/interface/entrypoint.py
 Comment: 
 
-Filename: intelliw/interface/goinferjob.py
-Comment: 
-
 Filename: intelliw/interface/trainjob.py
 Comment: 
 
 Filename: intelliw/interface/validatejob.py
 Comment: 
 
-Filename: intelliw/logs/iw-algo-fx-user.log
-Comment: 
-
-Filename: intelliw/logs/iw-algo-fx.log
-Comment: 
-
 Filename: intelliw/utils/.DS_Store
 Comment: 
 
 Filename: intelliw/utils/__init__.py
 Comment: 
 
 Filename: intelliw/utils/check_algorithm_py.py
@@ -180,17 +168,14 @@
 
 Filename: intelliw/utils/debug_controller.py
 Comment: 
 
 Filename: intelliw/utils/exception.py
 Comment: 
 
-Filename: intelliw/utils/gen_high_performance_cfg.py
-Comment: 
-
 Filename: intelliw/utils/gen_model_cfg.py
 Comment: 
 
 Filename: intelliw/utils/global_val.py
 Comment: 
 
 Filename: intelliw/utils/iuap_request.py
@@ -216,35 +201,41 @@
 
 Filename: intelliw/utils/database/proxy.py
 Comment: 
 
 Filename: intelliw/utils/database/schema.py
 Comment: 
 
-Filename: intelliw/utils/dataprocess/__init__.py
+Filename: intelliw/utils/redis/__init__.py
+Comment: 
+
+Filename: intelliw/utils/redis/connection.py
+Comment: 
+
+Filename: intelliw/utils/spark_process/__init__.py
 Comment: 
 
-Filename: intelliw/utils/dataprocess/engine.py
+Filename: intelliw/utils/spark_process/engine.py
 Comment: 
 
-Filename: intelliw/utils/dataprocess/spark.py
+Filename: intelliw/utils/spark_process/spark.py
 Comment: 
 
-Filename: intelliw-1.2.8.dist-info/LICENCE
+Filename: intelliw-1.2.9.dist-info/LICENCE
 Comment: 
 
-Filename: intelliw-1.2.8.dist-info/METADATA
+Filename: intelliw-1.2.9.dist-info/METADATA
 Comment: 
 
-Filename: intelliw-1.2.8.dist-info/WHEEL
+Filename: intelliw-1.2.9.dist-info/WHEEL
 Comment: 
 
-Filename: intelliw-1.2.8.dist-info/entry_points.txt
+Filename: intelliw-1.2.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: intelliw-1.2.8.dist-info/top_level.txt
+Filename: intelliw-1.2.9.dist-info/top_level.txt
 Comment: 
 
-Filename: intelliw-1.2.8.dist-info/RECORD
+Filename: intelliw-1.2.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## intelliw/__init__.py

```diff
@@ -1,24 +1,24 @@
 '''
 Author: Hexu
 Date: 2022-04-08 17:13:36
 LastEditors: Hexu
-LastEditTime: 2022-11-25 09:41:28
+LastEditTime: 2023-03-30 09:38:13
 FilePath: /iw-algo-fx/intelliw/__init__.py
 Description: intelliw
 '''
-__version__ = "1.2.8"
+__version__ = "1.2.9"
 
 logo = f"""\033[92m
 -------------------------------------------------------------
          ⓐ                    _    _    ⓘ
          _  _ __    _    ___ | |  | |   _  _     _
         | || '_ \ _| |_ / _ \| |  | |  | |\ \ _ / /
         | || | | |_   _|  __/| |__| |__| | \  _  /
         |_||_| |_| |___|\___| \____\___|_|  \/ \/
 
            intelliw  -- {__version__} Version --
 -------------------------------------------------------------\033[0m
 """
 print(logo, flush=True)
 
-from intelliw.utils import exception
+from intelliw.utils import exception
```

## intelliw/feature.py

```diff
@@ -1,17 +1,20 @@
 '''
 Author: Hexu
 Date: 2022-07-25 10:36:04
 LastEditors: Hexu
-LastEditTime: 2022-09-09 13:51:51
+LastEditTime: 2023-03-21 10:08:46
 FilePath: /iw-algo-fx/intelliw/feature.py
 Description: 统一功能包入口
 '''
 
 from intelliw.interface.apijob import Application
 from intelliw.core.linkserver import linkserver
 from intelliw.utils.logger import _get_algorithm_logger as get_logger
+from intelliw.utils import exception
+from intelliw.utils.exception import FileTransferDevice
+from intelliw.datasets.datasets import get_datasource_writer as OutPutWriter
 
 try:
-    from intelliw.utils.dataprocess.spark import Spark
+    from intelliw.utils.spark_process.spark import Spark
 except ImportError:
     pass
```

## intelliw/prepare_env.sh

```diff
@@ -43,15 +43,15 @@
 export FRAMEWORK_MODE=$REPORTCODE
 
 function get_report_type() {
     if [ "$REPORTCODE" == "importmodel" ]; then
         echo "importmodel"
     elif [ "$REPORTCODE" == "importalg" ]; then
         echo "importalg"
-    elif [ "$REPORTCODE" == "train" ] || [ "$REPORTCODE" == "distributedtrain" ]; then
+    elif [ "$REPORTCODE" == "train" ] || [ "$REPORTCODE" == "distributedtrain" ] || [ "$REPORTCODE" == "analysis" ]; then
         echo "train_fail"
     elif [ "$REPORTCODE" == "batchservice" ] || [ "$REPORTCODE" == "allserver" ]; then
         echo "batchjob-"$TASK
     elif [ "$REPORTCODE" == "apiservice" ]; then
         echo "inferstatus"
     else
         echo "unknow"
@@ -281,14 +281,17 @@
     ;;
 importalg)
     import importalg
     ;;
 train)
     train
     ;;
+analysis)
+    train
+    ;;
 distributedtrain)
     distributedtrain
     ;;
 apiservice)
     apiservice
     ;;
 batchservice)
```

## intelliw/algorithms_demo/algorithm.yaml

```diff
@@ -1,15 +1,11 @@
----
 AlgorithmInformation:
   name: "你好"
   desc: "AlgorithmName"
   example: ""
-  ref: ""
-  isGpu: true
-  enableHighPerformanceInfer: false
   requestMem: "256M"
   algorithm:
     desc: ""
     parameters:
       - desc: 测试超参
         key: demo
         name: 测试超参
```

## intelliw/config/config.py

```diff
@@ -1,22 +1,33 @@
 import os
 import sys
 import inspect
 import json
 from typing import Tuple
 
-FRAMEWORK_MODE = ''  # importmodel/importalg/train/distributedtrain/batchservice/apiservice
+
+class FrameworkMode:
+    Import = "import"
+    Train = "train"
+    Infer = "infer"
+    Batch = "batch"
+    DistTrain = "distributedtrain"
+    Analysis = "analysis"
+
+
+FLAME_PROF_MODE = False
+FRAMEWORK_MODE = ''  # import/train/infer/batch
 
 # 运行模式 SCAFFOLDING 脚手架，SERVER 服务端
 RUNNING_MODE = 'SERVER'
 DOMIAN_IUAP_AIP_CONSOLE = ''    # 接口环境
 REGISTER_CLUSTER_ADDRESS = ''   # 服务的ip➕端口  json: ['','']
 
 # basic
-TENANT_ID = 'rjtfwo7u'
+TENANT_ID = ''
 # 实例 id，代表当前运行实例
 INSTANCE_ID = ''
 # 推理任务 id
 INFER_ID = ''
 # 任务 id，推理任务时与 INFER_ID 相同
 SERVICE_ID = ''
 # 是否专属化
@@ -45,25 +56,24 @@
 INPUT_ADDR = ''
 INPUT_GETROW_ADDR = ''
 INPUT_MODEL_ID = ''
 INPUT_DATA_SOURCE_ID = ''
 INPUT_DATA_SOURCE_TRAIN_TYPE = 2 cv: 0-自有 1-labelme 2-voc 3-coco  nlp: 20-txt 21-csv 22-json
 """
 
-# 数据输出
-OUTPUT_DATA_SOURCE_ID = ''
-OUTPUT_SOURCE_TYPE = 0  # 输出数据源类型，0 空，2 智能分析, 5 数据工场
+# 数据输出 输出数据源类型，0 空，2 智能分析, 5 数据工场
+OUTPUT_DATASET_INFO = '{"sourceType":0}'
 
 # 数据读取
 DATA_SOURCE_READ_SIZE = 10000
 DATA_SOURCE_READ_LIMIT = sys.maxsize
 TRAIN_DATASET_RATIO = 0.8   # 训练集比例
 VALID_DATASET_RATIO = 0.2   # 验证集比例
 TEST_DATASET_RATIO = 0.0    # 测试集比例
-DATA_SPLIT_MODE = 1         # 数据集划分模式, 0 顺序划分，1 全局随机划分，2 根据目标列随机划分
+DATA_SPLIT_MODE = 1         # 数据集划分模式, -1 不分割, 0 顺序划分，1 全局随机划分，2 根据目标列随机划分
 
 
 # cv数据存储文件名
 CV_IMG_FILEPATH = "tmp_local_cv_image_data/"
 CV_IMG_TRAIN_FILEPATH = os.path.join(CV_IMG_FILEPATH, "train/")
 CV_IMG_VAL_FILEPATH = os.path.join(CV_IMG_FILEPATH, "val/")
 CV_IMG_TEST_FILEPATH = os.path.join(CV_IMG_FILEPATH, "test/")
@@ -105,14 +115,18 @@
 
 # 分布式
 DIST_IS_MASTER = False
 
 # Spark
 SPARK_MODE = False
 
+# checkpoint
+CHECKPOINT_MODE = False
+CHECKPOINT_SAVE_MAX = 100
+
 
 def is_server_mode():
     return 'SERVER' == RUNNING_MODE
 
 
 def str2bool(str):
     return True if str.lower() == 'true' else False
@@ -163,14 +177,17 @@
     """
     module = sys.modules[__name__]
 
     if not local_csv and not local_corpus and not dataset_id:
         setattr(module, "SOURCE_TYPE", 0)
         return
 
+    if dataset_id is not None:
+        update_by_env()
+
     _csv_file_check = False
     _nlp_corpus_check = False
     if isinstance(local_csv, list):
         for c in local_csv:
             if c and not os.path.exists(c):
                 break
         else:
@@ -201,44 +218,46 @@
         elif local_corpus:
             setattr(module, "SOURCE_TYPE", 8)
             dataset_info = {"SOURCE_TYPE": 21}
             dataset_info["NLP_CORPORA_PATH"] = local_corpus
             dataset_info["INPUT_DATA_SOURCE_TRAIN_TYPE"] = INPUT_DATA_SOURCE_TRAIN_TYPE
             dataset_list.append(dataset_info)
     else:
-        dataset_info = {}
         from intelliw.utils import iuap_request
         DATASET_URL = os.environ.get('DATASET_BY_ID_ADDRESS')
-        resp = iuap_request.get(DATASET_URL, params={"dataSetId": dataset_id})
-        resp.raise_for_status()
-        body = resp.json
-        if body['status'] == 0:
-            raise Exception(f"get dataset info response: {body}")
-        result = body['data']
-        source = result["SOURCE_TYPE"]
-        if result.get("TENANT_ID"):
-            setattr(module, "TENANT_ID", result.get("TENANT_ID"))
-        if source == 1:
-            setattr(module, "SOURCE_TYPE", 6)
-            dataset_info["DATA_SOURCE_ADDRESS"] = result["DATA_SOURCE_ADDRESS"]
-        else:
-            dataset_info["INPUT_ADDR"] = result["INPUT_ADDR"]
-            dataset_info["INPUT_GETROW_ADDR"] = result["INPUT_GETROW_ADDR"]
-            if source == 2:
+        for i in dataset_id.split(","):
+            dataset_info = {}
+            resp = iuap_request.get(DATASET_URL, params={"dataSetId": dataset_id})
+            resp.raise_for_status()
+            body = resp.json
+            if body['status'] == 0:
+                raise Exception(f"get dataset info response: {body}")
+            result = body['data']
+            source = result["SOURCE_TYPE"]
+            if result.get("TENANT_ID"):
+                setattr(module, "TENANT_ID", result.get("TENANT_ID"))
+            if source == 1:
                 setattr(module, "SOURCE_TYPE", 6)
-                mid = result["INPUT_MODEL_ID"]
+                dataset_info["DATA_SOURCE_ADDRESS"] = result["DATA_SOURCE_ADDRESS"]
+            else:
+                dataset_info["INPUT_ADDR"] = result["INPUT_ADDR"]
                 dataset_info["INPUT_GETROW_ADDR"] = result["INPUT_GETROW_ADDR"]
-                dataset_info["INPUT_MODEL_ID"] = mid
-            elif source == 4:
-                setattr(module, "SOURCE_TYPE", 7)
-                dataset_info["INPUT_DATA_SOURCE_ID"] = result["INPUT_DATA_SOURCE_ID"]
-            elif source == 5:
-                setattr(module, "SOURCE_TYPE", 6)
-                dataset_info["INPUT_DATA_SOURCE_ID"] = result["INPUT_DATA_SOURCE_ID"]
-            elif source == 21:
-                setattr(module, "SOURCE_TYPE", 8)
-                dataset_info["INPUT_DATA_SOURCE_ID"] = result["INPUT_DATA_SOURCE_ID"]
-                dataset_info["NLP_CORPORA_INPUT_TYPE"] = result["NLP_CORPORA_INPUT_TYPE"]
-        dataset_info["SOURCE_TYPE"] = source
-        dataset_list.append(dataset_info)
+                if source == 2:
+                    setattr(module, "SOURCE_TYPE", 6)
+                    mid = result["INPUT_MODEL_ID"]
+                    dataset_info["INPUT_GETROW_ADDR"] = result["INPUT_GETROW_ADDR"]
+                    dataset_info["INPUT_MODEL_ID"] = mid
+                elif source == 4:
+                    setattr(module, "SOURCE_TYPE", 7)
+                    dataset_info["INPUT_DATA_SOURCE_ID"] = result["INPUT_DATA_SOURCE_ID"]
+                elif source == 5:
+                    setattr(module, "SOURCE_TYPE", 6)
+                    dataset_info["INPUT_DATA_SOURCE_ID"] = result["INPUT_DATA_SOURCE_ID"]
+                    dataset_info["INPUT_DATA_META_ADDR"] = result["INPUT_DATA_META_ADDR"]
+                elif source == 21:
+                    setattr(module, "SOURCE_TYPE", 8)
+                    dataset_info["INPUT_DATA_SOURCE_ID"] = result["INPUT_DATA_SOURCE_ID"]
+                    dataset_info["NLP_CORPORA_INPUT_TYPE"] = result["NLP_CORPORA_INPUT_TYPE"]
+            dataset_info["SOURCE_TYPE"] = source
+            dataset_list.append(dataset_info)
     setattr(module, "DATASET_INFO", json.dumps(
         dataset_list, ensure_ascii=False))
```

## intelliw/core/al_decorator.py

```diff
@@ -3,36 +3,42 @@
 
 import inspect
 import json
 import time
 import errno
 import shutil
 import os
+import cProfile
 import zipfile
 import traceback
 from functools import wraps
 import intelliw.utils.message as message
+from intelliw.utils.exception import CheckpointException, FileTransferDevice
 from intelliw.utils.logger import _get_framework_logger, Logger
 from intelliw.config import config
+from intelliw.utils.util import generate_random_str, get_json_encoder
 
 logger = _get_framework_logger()
 
+save_index = 0
+curkey_r = generate_random_str(32)
 
-def zipdir(mpath):
+
+def zipdir(model_path):
     outpath = '/tmp/model.zip'
     with zipfile.ZipFile(outpath, 'w', zipfile.ZIP_DEFLATED) as zipf:
-        if os.path.isdir(mpath):
-            for root, dirs, files in os.walk(mpath):
-                relative_path = root.replace(mpath, "")
+        if os.path.isdir(model_path):
+            for root, dirs, files in os.walk(model_path):
+                relative_path = root.replace(model_path, "")
                 for file in files:
                     logger.info("压缩文件 {}".format(os.path.join(root, file)))
                     zipf.write(os.path.join(root, file),
                                os.path.join(relative_path, file))
-        elif os.path.isfile(mpath):
-            zipf.write(mpath, os.path.basename(mpath))
+        elif os.path.isfile(model_path):
+            zipf.write(model_path, os.path.basename(model_path))
     return outpath
 
 
 def decorator_report_train_info(function, reporter=None):
     from intelliw.utils.util import get_json_encoder
 
     @wraps(function)
@@ -85,91 +91,142 @@
                 data, cls=get_json_encoder(), ensure_ascii=False)))
         return function(*args, **kwargs)
     return wrapper
 
 
 # decorator_save 存储模型文件到云存储
 def decorator_save(function, reporter=None):
-    from intelliw.utils.util import generate_random_str
 
     @wraps(function)
     def wrapper(*args, **kwargs):
         # 分布式训练 slave不需要保存模型
-        if config.FRAMEWORK_MODE == 'distributedtrain' and not config.DIST_IS_MASTER:
+        if config.FRAMEWORK_MODE == config.FrameworkMode.DistTrain and \
+                not config.DIST_IS_MASTER:
             logger.info("分布式训练slave服务不需要保存模型文件")
             return None
 
         # 如果用户输入的是绝对路径，就使用输入的路径
         user_path = args[0]
         if os.path.isabs(user_path):
-            mpath = user_path
+            model_path = user_path
         else:
             hpath = os.path.join('/tmp', generate_random_str(16))
             os.makedirs(hpath)
-            mpath = os.path.join(hpath, user_path)
+            model_path = os.path.join(hpath, user_path)
 
-        abs_path = os.path.abspath(mpath)
-        # 如传入的是目录，则拼接上路径分隔符，以保证获取 dir_path 时包括末级路径
-        if mpath.endswith('/') or mpath.endswith('\\'):
-            abs_path = abs_path + os.sep
-        dir_path = os.path.dirname(abs_path)
-        if not os.path.exists(dir_path):
-            logger.info("目录不存在， 自动创建 {}".format(dir_path))
+        # 创建模型保存文件夹
+        if not os.path.exists(model_path):
+            logger.info("目录不存在， 自动创建 {}".format(model_path))
             try:
-                os.makedirs(dir_path)
+                os.makedirs(model_path)
             except OSError as e:
-                if e.errno == errno.EEXIST and os.path.isdir(dir_path):
+                if e.errno == errno.EEXIST and os.path.isdir(model_path):
                     pass
                 else:
                     logger.error("保存模型错误:  创建目录失败")
                     reporter.report(str(message.CommonResponse(500, "train_save",
-                                                               "保存模型错误:  创建目录失败 {}".format(dir_path))))
-        result = function(mpath)
+                                                               "保存模型错误:  创建目录失败 {}".format(model_path))))
+
+        # 设置save_index
+        global save_index
+        is_checkpoint = False
+        if function.__name__ == 'save_checkpoint':
+            is_checkpoint = True
+            if kwargs.get("save_best_only") is True:
+                save_index = "best"
+            elif "max_to_keep" in kwargs:
+                # save_index类型为str类型，说明之前执行过save()方法或执行过save_checkpoint()方法save_best_only为true的情况，不符合逻辑
+                if isinstance(save_index, str):
+                    raise CheckpointException()
+                max_to_keep = kwargs.get("max_to_keep")
+                if max_to_keep < 0:
+                    save_index = (save_index + 1) % config.CHECKPOINT_SAVE_MAX
+                elif max_to_keep > 0:
+                    save_index = (
+                        save_index + 1) % max_to_keep % config.CHECKPOINT_SAVE_MAX
+        else:
+            save_index = "model"
+        # 执行用户的保存函数
+        result = function(model_path)
+
+        # 上传模型
         if reporter is not None:
-            from intelliw.utils.storage_service import StorageService
+            _, outpath = __push_model_to_cloud(
+                reporter, model_path, save_index, is_checkpoint, kwargs)
             try:
-                outpath = zipdir(os.path.abspath(mpath))
-                curkey = os.path.join(
-                    config.STORAGE_SERVICE_PATH, generate_random_str(32))
-                env_val = os.environ.get("FILE_UP_TYPE").upper()
-                if env_val == "MINIO":
-                    client_type = "Minio"
-                elif env_val == "ALIOSS":
-                    client_type = "AliOss"
-                elif env_val == "HWOBS":
-                    client_type = "HWObs"
-                else:
-                    raise TypeError(f"FILE_UP_TYPE err: {env_val}")
-                uploader = StorageService(curkey, client_type, "upload")
-                logger.info(f"上传模型文件到{client_type}")
-                try:
-                    uploader.upload(outpath)
-                    logger.info(f"上传模型文件到{client_type}成功： {curkey}")
-                    reporter.report(message.CommonResponse(
-                        200, 'train_save', 'success', [curkey]))
-                except:
-                    err_info = traceback.format_exc()
-                    logger.info(f"上传模型文件到{client_type}失败: {err_info}")
-                    reporter.report(str(message.CommonResponse(
-                        500, "train_save", f"保存模型错误 {err_info}")))
-                try:
-                    os.remove(outpath)
-                    shutil.rmtree(hpath, ignore_errors=True)
-                except:
-                    pass
-            except Exception as e:
-                stack_info = traceback.format_exc()
-                reporter.report(str(message.CommonResponse(500, "train_save",
-                                                           "保存模型错误 {}, stack: \n {}".format(e, stack_info))))
+                os.remove(outpath)
+                shutil.rmtree(hpath, ignore_errors=True)
+            except:
+                pass
         else:
-            logger.info("保存模型错误:  reporter is  None")
+            logger.info("保存模型错误: reporter is  None")
+
         return result
     return wrapper
 
 
+def __push_model_to_cloud(reporter, model_path, index, is_checkpoint, kwargs):
+    from intelliw.utils.storage_service import StorageService
+
+    try:
+        outpath = zipdir(os.path.abspath(model_path))
+        save_fn = config.INSTANCE_ID or curkey_r
+        curkey = os.path.join(config.STORAGE_SERVICE_PATH, save_fn, str(index))
+        uploader = StorageService(curkey, config.FILE_UP_TYPE, "upload")
+        logger.info(
+            f"上传模型文件到{config.FILE_UP_TYPE}, 上传地址{uploader.service_url}, checkpoint模式：{is_checkpoint}")
+        try:
+            uploader.upload(outpath)
+
+            logger.info(
+                f"上传模型文件到{config.FILE_UP_TYPE}成功：{curkey}, checkpoint模式：{is_checkpoint}")
+            if is_checkpoint:
+                data = {'modelPath': curkey, 'epoch': kwargs.get(
+                    'epoch'), 'indice': str(kwargs.get('indice'))}
+                request = json.dumps(
+                    data, cls=get_json_encoder(), ensure_ascii=False)
+                businessType = "save_checkpoint"
+            else:
+                request = [curkey]
+                businessType = "save_model"
+
+            reporter.report(message.CommonResponse(
+                200, 'train_save', 'success', request, businessType=businessType))
+        except:
+            err_info = traceback.format_exc()
+            logger.info(
+                f"上传模型文件到{config.FILE_UP_TYPE}失败: {err_info}, checkpoint模式：{is_checkpoint}")
+            reporter.report(str(message.CommonResponse(
+                500, "train_save", f"保存模型错误 {err_info}")))
+    except Exception as e:
+        stack_info = traceback.format_exc()
+        reporter.report(str(message.CommonResponse(500, "train_save",
+                                                   "保存模型错误 {}, stack: \n {}".format(e, stack_info))))
+    return curkey, outpath
+
+
+def flame_prof_process(function):
+    @wraps(function)
+    def wrapper(*args, **kwargs):
+        if not config.FLAME_PROF_MODE:
+            return function(*args, **kwargs)
+        else:
+            prof_file = "./request.prof"
+
+            pr = cProfile.Profile()
+            pr.enable()
+            result = function(*args, **kwargs)
+            pr.disable()
+            pr.dump_stats(prof_file)
+            FileTransferDevice(prof_file, "flame_prof")
+            logger.info("\033[33mFlame Performance Complete: Save file to ./request.prof\033[0m")
+            return result
+    return wrapper
+
+
 def make_decorators_server(instance, reporter=None):
     # report_train_info
     if (hasattr(instance, 'report_train_info')) and inspect.ismethod(instance.report_train_info):
         instance.report_train_info = decorator_report_train_info(
             instance.report_train_info, reporter)
 
     # report_val_info
@@ -177,11 +234,16 @@
         instance.report_val_info = decorator_report_val_info(
             instance.report_val_info, reporter)
 
     # save model
     if (hasattr(instance, 'save')) and inspect.ismethod(instance.save):
         instance.save = decorator_save(instance.save, reporter)
 
+    # save model
+    if (hasattr(instance, 'save_checkpoint')) and inspect.ismethod(instance.save_checkpoint):
+        instance.save_checkpoint = decorator_save(
+            instance.save_checkpoint, reporter)
+
 
 def make_decorators_local(instance):
     if (hasattr(instance, 'get_user_logger')) and inspect.isfunction(instance.get_user_logger):
         instance.get_user_logger = Logger()._get_logger
```

## intelliw/core/pipeline.py

```diff
@@ -7,16 +7,17 @@
 import os
 import time
 import importlib.util
 import sys
 from inspect import signature
 from typing import Tuple
 from intelliw.config.cfg_parser import load_config
-from intelliw.datasets.datasets import DataSets, DataSourceReaderException
+from intelliw.datasets.datasets import DataSets
 from intelliw.utils import exception
+from intelliw.utils.exception import ModelLoadException
 from intelliw.utils.util import prepare_algorithm_parameters, prepare_model_parameters, import_code, get_first_element
 from intelliw.utils.check_algorithm_py import is_valid_algorithm
 import intelliw.utils.message as message
 from intelliw.core.recorder import Recorder
 from intelliw.core.linkserver import linkserver
 from intelliw.utils.logger import _get_framework_logger, _get_algorithm_logger
 from intelliw.config import config
@@ -42,14 +43,15 @@
     pre_train = "pre-train"
     post_train = "post-train"
 
     # 其他
     new_target_name = "new_target_data_unrepeatable_name"
     metadata = "metadata"
     model_type = "modelType"
+    predict_type = "predictType"
     target_name = "targetName"
     target_col = "targetCol"
     target_code = "targetCode"
 
 
 def get_model_yaml():
     return Const.model_yaml
@@ -64,15 +66,14 @@
         self.recorder = reporter if isinstance(reporter, Recorder) else Recorder(
             reporter, self.perodic_interval <= 0)
         self.instance = None
         self.custom_router = []
         self.metadata = []
         self.linkserver = None
         self.alg_describe = ''
-        gl.set("reporter", self.recorder)
 
     def perodic_callback_infer(self):
         logger.debug("定时任务： 上报状态")
         self.recorder.report_infer()
 
     def _check_algorithm_parameters_format(self, alg):
         try:
@@ -116,15 +117,17 @@
 
                 # 算法描述
                 self.alg_describe = model_cfg['Model'].get('desc', '')
 
                 # 算法元数据
                 self.metadata = self.model_cfg.get(Const.metadata, {})
 
-                # 算法类型（分类回归时间序列。。。）
+                # 算法类型 string（分类回归时间序列, classify|regression）
+                self.predict_type = self.metadata.get(Const.predict_type)
+                # 算法类型 int（分类回归时间序列。。。）
                 self.model_type = self.metadata.get(Const.model_type)
                 if self.model_type is not None:
                     gl.set("model_type", self.model_type)
 
                 gl.set("model_yaml_file", model_file)
 
                 model_ps = self.model_cfg.get('parameters')
@@ -168,28 +171,35 @@
                 # linkserver
                 self.linkserver = self.model_cfg.get('linkserver', None)
                 linkserver.config = self.linkserver
 
             # 日志
             parameters["framework_log"] = _get_algorithm_logger()
 
+            if self.predict_type:
+                parameters[Const.predict_type] = self.predict_type
+
             instance = self.module.Algorithm(parameters)
             if instance is None:
                 self._raise_exception(message.err_import_alg_invalid_path)
 
             # 检测自定义路由方法是否存在
             for router in self.custom_router:
                 func = router.get('func')
                 if not hasattr(instance, func):
                     self._raise_exception(message.err_missing_router_func)
 
-            # add decorator to  functions
+            # add decorator to functions
             if config.is_server_mode():
                 make_decorators_server(instance, self.recorder)
 
+            # FRAMEWORK_MODE
+            instance.is_train_mode = config.FRAMEWORK_MODE == config.FrameworkMode.Train
+            instance.is_infer_mode = config.FRAMEWORK_MODE == config.FrameworkMode.Infer
+
             return instance
         except Exception as e:
             self._raise_exception(message.err_import_alg_invalid_path, e)
 
     def _importalg_phase(self, path):
         if not os.path.isfile(os.path.join(path, Const.algorithm_py)):
             self._raise_exception(message.err_import_alg_missing_algorithpy)
@@ -254,16 +264,27 @@
                 gl.set_dict({
                     'feature_process': self.feature_transforms,
                     'timeseries_process': transforms_result[3]
                 })
 
                 # load model
                 if load_model:
-                    model_file = os.path.join(path, self.model_cfg['location'])
-                    self.instance.load(model_file)
+                    try:
+                        load_path: str = self.model_cfg['location']
+                        if os.path.isabs(load_path):
+                            model_file = load_path
+                        else:
+                            if load_path.startswith("./"):
+                                load_path = load_path.rstrip("./")
+                            model_file = os.path.join(path, load_path)
+                        logger.info(f"model load path：{model_file}")
+                        self.instance.load(model_file)
+                    except Exception as e:
+                        raise ModelLoadException(e)
+        # report
         except Exception as e:
             self._raise_exception(message.err_import_model, e)
 
         # report
         self.recorder.report(message.ok_import_model)
 
     def _vaild_transforms_cfg(self, stage, is_split):
@@ -591,29 +612,23 @@
         return self.transform_call_chains(transforms, data)
 
     def _raise_exception(self, msg: message.CommonResponse, e: Exception = None, is_framework_error=True):
         raise_logger = logger if is_framework_error else _get_algorithm_logger()
 
         # 用于传递错误给prepare_env.sh
         err_stack = "\n"
-        if isinstance(e, (exception.DatasetException,
-                          exception.DataSourceDownloadException,
-                          exception.FeatureProcessException,
-                          DataSourceReaderException,
-                          ImportError)):
-            err_stack += str(e)  # 数据集错误不需要调用栈，知道什么问题就行
+        if hasattr(e, "ignore_stack") or isinstance(e, ImportError):
+            err_stack += f"错误描述: {e}"   # 数据集错误不需要调用栈，知道什么问题就行
         elif e:
-            err_stack += f"✖️ {str(e)} ✖️\n{traceback.format_exc()}"
+            err_stack += f"错误描述: {e} \n错误详情: {traceback.format_exc()}"
         else:
             err_stack = ""
-        err_content = "[{}]\n{}".format(msg.msg, err_stack)
+        err_content = "\n错误内容: {}{}".format(msg.msg, err_stack)
         os.environ["ERR_MASSAGE"] = err_content
 
-        if e is None:
-            raise_logger.error(msg.msg)
-        else:
+        if e is not None:
             msg.msg = err_content
-            raise_logger.error(msg.msg + ' %s', e, exc_info=True)
+        raise_logger.error(msg.msg)
 
         # 上报
         self.recorder.report(msg)
         sys.exit()
```

## intelliw/core/recorder.py

```diff
@@ -6,14 +6,15 @@
 import threading
 import traceback
 from intelliw.utils.iuap_request import post_json
 import intelliw.utils.message as message
 from intelliw.utils.util import get_json_encoder
 from intelliw.utils.logger import _get_framework_logger
 from intelliw.config import config
+from intelliw.utils.global_val import gl
 
 logger = _get_framework_logger()
 
 
 class Recorder:
 
     # 单例锁
@@ -21,14 +22,15 @@
 
     def __new__(cls, *args, **kwargs):
         """ 单例，防止调用生成更多环境变量dict """
         if not hasattr(Recorder, "_instance"):
             with Recorder._instance_lock:
                 if not hasattr(Recorder, "_instance"):
                     Recorder._instance = object.__new__(cls)
+                    gl.recorder = Recorder._instance
         return Recorder._instance
 
     def __init__(self, addr: str, not_delay=True):
         self.not_delay = not_delay
         self.duplicate = set()
         self.queue = queue.Queue()
         # 上报地址
```

## intelliw/core/trainer.py

```diff
@@ -1,19 +1,25 @@
-
-#!/usr/bin/env python
+'''
+Author: Hexu
+Date: 2022-11-22 09:48:10
+LastEditors: Hexu
+LastEditTime: 2023-02-22 13:37:41
+FilePath: /iw-algo-fx/intelliw/core/trainer.py
+Description: Train core
+'''
 # coding: utf-8
-
+from intelliw.config import config
 from intelliw.datasets.datasets import DataSets, MultipleDataSets
 from intelliw.core.pipeline import Pipeline
 
 
 class Trainer:
     def __init__(self, path, reporter=None, perodic_interval=-1):
         self.pipeline = Pipeline(reporter, perodic_interval)
-        self.pipeline.importmodel(path, True, False)
+        self.pipeline.importmodel(path, True, config.CHECKPOINT_MODE)
 
     def train(self, datasets: DataSets):
         if not isinstance(datasets, DataSets) and not isinstance(datasets, MultipleDataSets):
             raise TypeError("datasets has a wrong type, required: DataSets, MultipleDataSets, actually: {}"
                             .format(type(datasets).__name__))
 
         return self.pipeline.train(datasets)
```

## intelliw/datasets/datasets.py

```diff
@@ -1,11 +1,11 @@
 '''
 Author: hexu
 Date: 2021-10-14 14:54:05
-LastEditTime: 2022-11-03 14:39:14
+LastEditTime: 2023-03-30 20:20:36
 LastEditors: Hexu
 Description: 数据集
 FilePath: /iw-algo-fx/intelliw/datasets/datasets.py
 '''
 import json
 from typing import List, Tuple, overload
 from intelliw.datasets.spliter import get_set_spliter
@@ -26,23 +26,24 @@
         dsaddr     DATA_SOURCE_ADDRESS
         addr       INPUT_ADDR
         raddr      INPUT_GETROW_ADDR
         mid        INPUT_MODEL_ID
         dsid       INPUT_DATA_SOURCE_ID
         dstt       INPUT_DATA_SOURCE_TRAIN_TYPE
         nlpctypt   NLP_CORPORA_INPUT_TYPE
+        tmaddr    INPUT_DATA_META_ADDR
 
         if SOURCE_TYPE = 1
             NEED: DATA_SOURCE_ADDRESS
         if SOURCE_TYPE = 2
             NEED: INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_MODEL_ID
         if SOURCE_TYPE = 4
             NEED: INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID, INPUT_DATA_SOURCE_TRAIN_TYPE
         if SOURCE_TYPE = 5
-            NEED: INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID
+            NEED: INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID, INPUT_DATA_META_ADDR
         if SOURCE_TYPE = 21
             NEED: INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID, INPUT_DATA_SOURCE_TRAIN_TYPE, NLP_CORPORA_INPUT_TYPE
 
     Returns:
         AbstractDataSource: 数据集class
     """
     if stype == DST.EMPTY:
@@ -57,48 +58,52 @@
         from intelliw.datasets.datasource_intelliv import DataSourceIntelliv
         return DataSourceIntelliv(kwargs['addr'], kwargs['raddr'], kwargs['mid'])
     elif stype == DST.IW_IMAGE_DATA:
         from intelliw.datasets.datasource_iwimgdata import DataSourceIwImgData
         return DataSourceIwImgData(kwargs['addr'], kwargs['raddr'], kwargs['dsid'], kwargs['dstt'])
     elif stype == DST.IW_FACTORY_DATA:
         from intelliw.datasets.datasource_iwfactorydata import DataSourceIwFactoryData
-        return DataSourceIwFactoryData(kwargs['addr'], kwargs['raddr'], kwargs['dsid'])
+        return DataSourceIwFactoryData(kwargs['addr'], kwargs['raddr'], kwargs['tmaddr'], kwargs['dsid'])
     elif stype == DST.NLP_CORPORA:
         from intelliw.datasets.datasource_nlp_corpora import DataSourceNLPCorpora
         return DataSourceNLPCorpora(kwargs['addr'], kwargs['dsid'], kwargs['dstt'], kwargs['nlpctypt'])
     else:
         err_msg = "数据读取失败，无效的数据源类型: {}".format(stype)
-        logger.error(err_msg)
         raise ValueError(err_msg)
 
 
-def get_datasource_writer(output_addr: str) -> AbstractDataSourceWriter:
-    output_datasource_type = config.OUTPUT_SOURCE_TYPE
+def get_datasource_writer(cfg: str = None) -> AbstractDataSourceWriter:
+    output_datasource_type = 0
+    cfg = cfg or config.OUTPUT_DATASET_INFO
+    if cfg:
+        if isinstance(cfg, str):
+            cfg = json.loads(cfg)
+        output_datasource_type = cfg['sourceType']
+
     if output_datasource_type == DST.EMPTY:
         return EmptyDataSourceWriter()
-    elif output_datasource_type == DST.INTELLIV or output_datasource_type == DST.IW_FACTORY_DATA:
-        from intelliw.datasets.datasource_intelliv import DataSourceWriter
-        return DataSourceWriter(output_addr, config.OUTPUT_DATA_SOURCE_ID, config.INFER_ID, config.TENANT_ID)
+    elif output_datasource_type in (DST.INTELLIV, DST.IW_FACTORY_DATA):
+        from intelliw.datasets.dataset_writer import DataSourceWriter
+        return DataSourceWriter(output_config=cfg, writer_type=output_datasource_type)
     else:
-        err_msg = "输出数据源设置失败，无效的数据源类型: {}".format(output_datasource_type)
-        logger.error(err_msg)
+        err_msg = f"输出数据源设置失败，无效的数据源类型: {output_datasource_type}"
         raise ValueError(err_msg)
 
 
 class DataSets:
     def __init__(self, datasource: AbstractDataSource):
         self.datasource = datasource
         self.alldata = list()
         self.column_meta = list()
         self.model_type = gl.get("model_type")  # 分类/回归/ocr/时间序列/文本分类。。。。。
 
     def empty_reader(self, dataset_type=DatasetType.TRAIN):
         return self.datasource.reader(page_size=1, offset=0, limit=0, transform_function=None, dataset_type=dataset_type)
 
-    def reader(self, page_size=100000, offset=0, limit=0, split_transform_function=None):
+    def reader(self, page_size=10000, offset=0, limit=0, split_transform_function=None):
         return self.datasource.reader(page_size, offset, limit, split_transform_function)
 
     @overload
     def data_pipeline(self, split_transform_function,
                       alldata_transform_function, feature_process): pass
 
     def data_pipeline(self, *args):
@@ -255,15 +260,15 @@
             f"Error Source Type: {config.SOURCE_TYPE}")
 
 
 _dependent_param = {
     1: "DATA_SOURCE_ADDRESS",
     2: "INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_MODEL_ID",
     4: "INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID, INPUT_DATA_SOURCE_TRAIN_TYPE",
-    5: "INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID",
+    5: "INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID, INPUT_DATA_META_ADDR",
     21: "INPUT_ADDR, INPUT_GETROW_ADDR, INPUT_DATA_SOURCE_ID, INPUT_DATA_SOURCE_TRAIN_TYPE, NLP_CORPORA_INPUT_TYPE"
 }
 
 
 def _get_dataset_args(cfg: dict) -> dict:
     stype = cfg["SOURCE_TYPE"]
     kwargs = {}
@@ -279,26 +284,26 @@
             kwargs = {"addr": cfg["INPUT_ADDR"],
                       "raddr": f"{raddr}/{mid}/null",
                       "mid": mid}
         elif stype == DST.IW_IMAGE_DATA:
             kwargs = {"addr": cfg["INPUT_ADDR"],
                       "raddr": cfg["INPUT_GETROW_ADDR"],
                       "dsid": cfg["INPUT_DATA_SOURCE_ID"],
-                      "dstt": 3}
+                      "dstt": cfg.get("INPUT_DATA_SOURCE_TRAIN_TYPE", 3)}
         elif stype == DST.IW_FACTORY_DATA:
             kwargs = {"addr": cfg["INPUT_ADDR"],
                       "raddr": cfg["INPUT_GETROW_ADDR"],
-                      "dsid": cfg["INPUT_DATA_SOURCE_ID"]}
+                      "dsid": cfg["INPUT_DATA_SOURCE_ID"],
+                      "tmaddr": cfg["INPUT_DATA_META_ADDR"]}
         elif stype == DST.NLP_CORPORA:
             cpath = cfg.get("NLP_CORPORA_PATH", "")
             if len(cpath) > 0:
                 kwargs = {"addr": cpath, "dsid": "", "nlpctypt": "local",
                           "dstt": cfg.get("INPUT_DATA_SOURCE_TRAIN_TYPE", 22)}
             else:
                 kwargs = {"addr": cfg["INPUT_ADDR"],
                           "dsid": cfg["INPUT_DATA_SOURCE_ID"],
                           "dstt": cfg.get("INPUT_DATA_SOURCE_TRAIN_TYPE", 22),
                           "nlpctypt": cfg["NLP_CORPORA_INPUT_TYPE"]}
     except KeyError:
-        logger.error(errmsg)
         raise DataSourceReaderException(errmsg)
     return stype, kwargs
```

## intelliw/datasets/datasource_base.py

```diff
@@ -1,18 +1,18 @@
 #!/usr/bin/env python
 # coding: utf-8
 '''
 Author: hexu
 Date: 2021-10-14 14:54:05
-LastEditTime: 2022-09-20 15:52:13
+LastEditTime: 2023-03-30 14:13:23
 LastEditors: Hexu
 Description: Algorithm -> DataSourceReader -> DataSource
 FilePath: /iw-algo-fx/intelliw/datasets/datasource_base.py
 '''
-
+import datetime
 from collections.abc import Iterable
 from typing import Iterator
 
 from intelliw.utils.logger import _get_framework_logger
 from abc import ABCMeta, abstractmethod
 
 logger = _get_framework_logger()
@@ -29,14 +29,18 @@
     # 子类
     REMOTE_CSV = 1  # 远程csv
     INTELLIV = 2  # 智能分析
     LOCAL_CSV = 3  # 本地 csv
     IW_IMAGE_DATA = 4  # 图片数据源
     IW_FACTORY_DATA = 5  # 数据工场数据集
     NLP_CORPORA = 21    # nlp语料
+    SEMANTIC = 22    # 语义模型
+
+    SQL = 998
+    USER = 999
 
 
 class AlgorithmsType:
     '''算法类型'''
     CLASSIFICATION = 3  # 分类算法
     TIME_SERIES = 9     # 时间序列
 
@@ -70,31 +74,37 @@
         :param transform_function: 转换函数
         :return: 数据源 iterator
         """
         pass
 
 
 class DataSourceReaderException(Exception):
-    pass
+    def ignore_stack(self):
+        return True
 
 
 class DataSourceWriterException(Exception):
-    pass
+    def ignore_stack(self):
+        return True
 
 
 class AbstractDataSourceWriter(metaclass=ABCMeta):
+    def __init__(self) -> None:
+        self.table_columns = None
 
     @abstractmethod
     def write(self, data, starttime):
         pass
 
 
 class EmptyDataSourceWriter(AbstractDataSourceWriter):
 
-    def write(self, data, starttime):
+    def write(self, data, starttime=None):
+        if starttime is None:
+            starttime = datetime.datetime.now()
         logger.info(
             "datasource output: {}, starttime: {}".format(data, starttime))
         return {'status': 1}
 
 
 class DataSourceEmpty(AbstractDataSource):
     """
@@ -126,15 +136,15 @@
     def __init__(self, meta: list, data: list):
         self.__meta = meta
         self.__data = data
 
     def total(self) -> int:
         return len(self.__data)
 
-    def reader(self, page_size=100000, offset=0, limit=0, transform_function=None) -> Iterable:
+    def reader(self, page_size=10000, offset=0, limit=0, transform_function=None) -> Iterable:
         return self.__Reader(self.__meta, self.__data, page_size, offset, limit, transform_function)
 
     class __Reader(Iterator):
 
         def __init__(self, meta, data, page_size, offset, limit, transform_function):
             self.__meta = meta
             self.__data = data
```

## intelliw/datasets/datasource_intelliv.py

```diff
@@ -1,24 +1,23 @@
 '''
 Author: hexu
 Date: 2021-10-14 14:54:05
-LastEditTime: 2022-11-08 10:14:42
+LastEditTime: 2023-03-27 11:16:22
 LastEditors: Hexu
 Description: 从智能服务获取模型数据集
 FilePath: /iw-algo-fx/intelliw/datasets/datasource_intelliv.py
 '''
-import json
-import math
+from concurrent.futures import ThreadPoolExecutor
+import os
 import time
 
-from intelliw.datasets.datasource_base import AbstractDataSource, DataSourceReaderException, AbstractDataSourceWriter, \
-    DataSourceWriterException
+from intelliw.datasets.datasource_base import AbstractDataSource, DataSourceReaderException
+
 from intelliw.utils import iuap_request
 from intelliw.utils.logger import _get_framework_logger
-from intelliw.utils.util import get_json_encoder
 from intelliw.config import config
 
 logger = _get_framework_logger()
 
 
 class DataSourceIntelliv(AbstractDataSource):
     """
@@ -44,23 +43,21 @@
         if self.__total is not None:
             return self.__total
         response = iuap_request.get(self.get_row_address)
 
         if 200 != response.status or response.json is None:
             msg = "获取行数失败，url: {}, response: {}".format(
                 self.get_row_address, response)
-            logger.error(msg)
             raise DataSourceReaderException(msg)
 
         row_data = response.json
         self.__total = row_data['data']
         if not isinstance(self.__total, int):
             msg = "获取行数返回结果错误, response: {}, request_url: {}".format(
                 row_data, self.get_row_address)
-            logger.error(msg)
             raise DataSourceReaderException(msg)
         return self.__total
 
     def reader(self, page_size=100000, offset=0, limit=0, transform_function=None, dataset_type='train_set'):
         return self.__Reader(self.input_address, self.input_model_id, self.total(), page_size, offset, limit, transform_function)
 
     class __Reader:
@@ -71,91 +68,80 @@
             offset 15, limit 30:
             [15,19][20,39][40,44]
             offset 10 limit 5:
             [10,14]
             """
             self.input_address = input_address
             self.input_model_id = input_model_id
-            self.limit = limit
             self.total_rows = total
-            if limit <= 0:
-                self.limit = total - offset
-            elif offset + limit > total:
-                self.limit = total - offset
-            self.page_size = page_size if page_size < self.limit else self.limit
-            self.total_page = math.ceil(total / self.page_size)
-            self.start_page = math.ceil(
-                offset / self.page_size) if offset > 0 else 1
-            self.end_page = math.ceil((offset + self.limit) / page_size)
-            self.start_index_in_start_page = offset - \
-                (self.start_page - 1) * page_size
-            self.end_index_in_end_page = offset + \
-                self.limit - 1 - (self.end_page - 1) * page_size
-            self.current_page = self.start_page
-
-            self.transform_function = transform_function
-
             self.total_read = 0
-            self.after_transform = 0
+            self.page_size = max(100, page_size)
+            self.page_num = 1
             self.meta = []
+            self.after_transform = 0
+            self.transform_function = transform_function
+            self.worker = min(10, max(1, os.cpu_count()))
+            logger.info(f'数据获取中: 启用的获取线程 {self.worker} 条')
 
         def get_data_bar(self):
             """数据拉取进度条"""
-            if self.current_page % 5 == 1:
-                try:
-                    proportion = round(
-                        (self.total_read/self.total_rows)*100, 2)
-                    logger.info(
-                        f"数据获取中: 共{self.total_rows}条数据, 已获取{self.total_read}条, 进度{proportion}%")
-                except:
-                    pass
+            try:
+                proportion = round(
+                    (self.total_read/self.total_rows)*100, 2)
+                logger.info(
+                    f"数据获取中: 共{self.total_rows}条数据, 已获取{self.total_read}条, 进度{proportion}%")
+            except:
+                pass
 
         @property
         def iterable(self):
             return True
 
         def __iter__(self):
             return self
 
         def __next__(self):
-            if self.current_page > self.end_page:
-                logger.info('共读取原始数据 {} 条，经特征工程处理后数据有 {} 条'.format(
-                    self.total_read, self.after_transform))
+            if self.total_read >= self.total_rows:
+                logger.info('数据下载完成，共读取原始数据 {} 条'.format(self.total_read))
                 raise StopIteration
 
             self.get_data_bar()
 
             try:
-                page = self._read_page(self.current_page, self.page_size)
-                assert page is not None
-
-                # 首尾页需截取有效内容
-                if self.current_page == self.start_page or self.current_page == self.end_page:
-                    start_index = 0
-                    end_index = len(page['result']) - 1
-                    if self.current_page == self.start_page:
-                        start_index = self.start_index_in_start_page
-                        self.meta = page['meta']
-                    if self.current_page == self.end_page:
-                        end_index = self.end_index_in_end_page
-                    page['result'] = page['result'][start_index: end_index + 1]
-
-                self.total_read += len(page['result'])
-                self.current_page += 1
-                if self.transform_function is not None:
-                    transformed = self.transform_function(page)
-                    self.after_transform += len(transformed['result'])
-                    return transformed
-                self.after_transform = self.total_read
-                return page
+                with ThreadPoolExecutor(max_workers=self.worker) as executor:
+                    futures, data_result = [], []
+                    for i in range(self.worker):
+                        futures.append(executor.submit(
+                            self._read_page, self.page_num, self.page_size
+                        ))
+                        self.page_num += 1
+
+                    for f in futures:
+                        data_result.extend(f.result())
+                    self.total_read += len(data_result)
+                    if len(data_result) == 0:
+                        logger.info(
+                            '数据下载完成，共读取原始数据 {} 条'.format(self.total_read))
+                        raise StopIteration
+                    return {"result": data_result, "meta": self.meta}
             except Exception as e:
                 logger.exception("智能分析数据源读取失败, input_address: [{}], model_id: [{}]".
                                  format(self.input_address, self.input_model_id))
                 raise DataSourceReaderException('智能分析数据源读取失败') from e
 
+        def _get_meta(self, meta):
+            if self.meta == []:
+                self.meta = meta
+            return self.meta
+
+        def _data_process(self, data):
+            if len(data) > 0:
+                self._get_meta(data['meta'])
+            return data['result']
+
         def _read_page(self, page_index, page_size):
             """
             调用智能分析接口，分页读取数据
             :param page_index: 页码，从 1 开始
             :param page_size:  每页大小
             :return:
             """
@@ -168,41 +154,10 @@
                     "recordCount": 0
                 },
                 "signDTO": {"ak": config.ACCESS_KEY,
                             "sign": config.ACCESS_SECRET,
                             "ts": int(time.time() * 1000)}
             }
             response = iuap_request.post_json(
-                url=self.input_address, json=request_data, params={"AuthSdkServer": "true"})
+                url=self.input_address, json=request_data, params={"AuthSdkServer": "true"}, timeout=30)
             response.raise_for_status()
-            return response.json
-
-
-class DataSourceWriter(AbstractDataSourceWriter):
-    def __init__(self, source_addr, dataSourceId, inferId, tenantId):
-        self.source_addr = source_addr
-        self.dataSourceId = dataSourceId
-        self.inferId = inferId
-        self.tenantId = tenantId
-
-    def _get_data_hub_dest_cfg(self, start_time, end_time, data):
-        data_dest = {
-            "dataSourceId": self.dataSourceId,
-            "serviceId": self.inferId,
-            "startTime": start_time,
-            "endTime": end_time,
-            "tenantId": self.tenantId,
-            'data': json.dumps(data, cls=get_json_encoder())
-        }
-        return data_dest
-
-    def write(self, data, start_time):
-        try:
-            end_time = int(time.time() * 1000)
-            result = self._get_data_hub_dest_cfg(start_time, end_time, data)
-            res = iuap_request.post_json(url=self.source_addr, json=result)
-            res.raise_for_status()
-            res_data = res.json
-            return res_data
-        except Exception as e:
-            logger.exception("写入数据错误")
-            raise DataSourceWriterException('写入数据错误') from e
+            return self._data_process(response.json)
```

## intelliw/datasets/datasource_iwfactorydata.py

```diff
@@ -1,192 +1,189 @@
 '''
 Author: hexu
 Date: 2021-10-14 14:54:05
-LastEditTime: 2022-11-08 10:00:58
+LastEditTime: 2023-03-27 11:20:17
 LastEditors: Hexu
 Description: 从数据工场获取模型数据集
 FilePath: /iw-algo-fx/intelliw/datasets/datasource_iwfactorydata.py
 '''
-import math
-
+from concurrent.futures import ThreadPoolExecutor
+import os
+import pandas as pd
 from intelliw.datasets.datasource_base import AbstractDataSource, DataSourceReaderException
-from intelliw.utils import iuap_request
+from intelliw.utils import iuap_request, util
 from intelliw.utils.logger import _get_framework_logger
+from intelliw.config import config
+from pandas.api.types import is_numeric_dtype
 
 logger = _get_framework_logger()
 
 
 def err_handler(request, exception):
     print("请求出错,{}".format(exception))
 
 
 class DataSourceIwFactoryData(AbstractDataSource):
     """
     数据工场数据源
     """
 
-    def __init__(self, input_address, get_row_address, table_id):
+    def __init__(self, input_address, get_row_address, get_table_meta, table_id):
         """
         智能分析数据源
         :param input_address:   获取数据 url
         :param get_row_address: 获取数据总条数 url
         :param table_id:   表Id
         """
         self.input_address = input_address
         self.get_row_address = get_row_address
+        self.get_table_meta = get_table_meta
         self.table_id = table_id
         self.__total = None
 
     def total(self):
         """获取数据总行数"""
         if self.__total is not None:
             return self.__total
-        # print(self.get_row_address, self.table_id)
-        response = iuap_request.post_json(self.get_row_address, json={
-                                          'tableid': self.table_id})
-        if 200 != response.status:
+        response = iuap_request.post_json(
+            self.get_row_address, json={'tableid': self.table_id, 'tenantId': config.TENANT_ID})
+        if response.status != 200:
             msg = f"获取行数失败，url: {self.get_row_address}, response: {response}"
-            logger.error(msg)
             raise DataSourceReaderException(msg)
-        # print(response)
         row_data = response.json
-        self.__total = 0
+
         try:
             count = row_data["data"]["count"]
-            if isinstance(count, int):
-                self.__total = count
-        except Exception as e:
-            msg = f"获取行数返回结果错误, response: {row_data}, request_url: {self.get_row_address}, error: {e}"
-            logger.error(msg)
+            self.__total = count if isinstance(count, int) else 0
+            return self.__total
+        except Exception:
+            msg = f"获取行数返回结果错误, response: {row_data}, request_url: {self.get_row_address}"
+            raise DataSourceReaderException(msg)
+
+    def table_meta(self):
+        response = iuap_request.post_json(
+            self.get_table_meta, json={'logicIds': [self.table_id], 'tenantId': config.TENANT_ID})
+        if response.status != 200:
+            msg = f"获取表信息失败，url: {self.get_table_meta}, response: {response}"
+            raise DataSourceReaderException(msg)
+        row_data = response.json
+
+        try:
+            meta = row_data["data"][0]["fieldList"]
+            return [{'type': m['columnType'], 'code':m['columnName']} for m in meta]
+        except Exception:
+            msg = f"获取表信息失败, response: {row_data}, request_url: {self.get_table_meta}"
             raise DataSourceReaderException(msg)
-        return self.__total
 
-    def reader(self, page_size=100, offset=0, limit=0, transform_function=None, dataset_type='train_set'):
-        return self.__Reader(self.input_address, self.table_id, self.total(), page_size, offset, limit, transform_function)
+    def reader(self, page_size=10000, offset=0, limit=0, transform_function=None, dataset_type='train_set'):
+        return self.__Reader(self.input_address, self.table_id, self.table_meta(), self.total(), page_size, transform_function)
 
     class __Reader:
-        def __init__(self, input_address, table_id, total, page_size=100, offset=0, limit=0, transform_function=None):
+        def __init__(self, input_address, table_id, table_meta, total, page_size=10000, transform_function=None):
             """
             eg. 91 elements, page_size = 20, 5 pages as below:
             [0,19][20,39][40,59][60,79][80,90]
             offset 15, limit 30:
             [15,19][20,39][40,44]
             offset 10 limit 5:
             [10,14]
             """
             self.input_address = input_address
             self.table_id = table_id
+            self.meta = table_meta
+            self.num_table_meta = self.get_number_column(self.meta)
             self.total_rows = total
-            self.limit = limit if limit <= 0 or (
-                offset + limit > total) else (total - offset)
-            self.page_size = page_size if page_size < self.limit else self.limit
-            self.total_page = math.ceil(total / self.page_size)
-            self.start_page = math.ceil(
-                offset / self.page_size) if offset > 0 else 1
-            self.end_page = math.ceil((offset + self.limit) / page_size)
-            self.start_index_in_start_page = offset - \
-                (self.start_page - 1) * page_size
-            self.end_index_in_end_page = offset + \
-                self.limit - 1 - (self.end_page - 1) * page_size
-            self.current_page = self.start_page
-            self.transform_function = transform_function
-            self.meta = []
-
+            self.page_size = max(100, page_size)
+            self.page_num = 1
             self.total_read = 0
-            self.after_transform = 0
+            self.transform_function = transform_function
+            self.worker = min(10, max(1, os.cpu_count()))
+            logger.info(f'数据获取中: 启用的获取线程 {self.worker} 条')
 
-            """
-            print("total_page={},start_page={},end_page={},start_index={},end_index={},current_page={}"
-                  .format(self.total_page,
-                          self.start_page,
-                          self.end_page,
-                          self.start_index_in_start_page,
-                          self.end_index_in_end_page,
-                          self.current_page))
-            """
+        def get_number_column(self, table_meta):
+            # 所有数值型字段返回的都是字符型， 需要转一下
+            number_column = []
+            for m in table_meta:
+                _type = m['type'].upper()
+                if _type in util.DB_NUM_TYPE:
+                    number_column.append(m['code'])
+            return number_column
 
         def get_data_bar(self):
             """数据拉取进度条"""
-            if self.current_page % 5 == 1:
-                try:
-                    proportion = round(
-                        (self.total_read/self.total_rows)*100, 2)
-                    logger.info(
-                        f"数据获取中: 共{self.total_rows}条数据, 已获取{self.total_read}条, 进度{proportion}%")
-                except:
-                    pass
+            try:
+                proportion = round(
+                    (self.total_read/self.total_rows)*100, 2)
+                logger.info(
+                    f"数据获取中: {self.total_read}/{self.total_rows}, 进度{proportion}%")
+            except:
+                pass
 
         @property
         def iterable(self):
             return True
 
         def __iter__(self):
             return self
 
         def __next__(self):
-            if self.current_page > self.end_page:
-                logger.info('共读取原始数据 {} 条，经特征工程处理后数据有 {} 条'.format(
-                    self.total_read, self.after_transform))
+            if self.total_read >= self.total_rows:
+                logger.info('数据下载完成，共读取原始数据 {} 条'.format(self.total_read))
                 raise StopIteration
 
             self.get_data_bar()
 
             try:
-                page = self._read_page(self.current_page, self.page_size)
-                if self.current_page == self.start_page or self.current_page == self.end_page:
-                    # 首尾页需截取有效内容
-                    start_index = 0
-                    end_index = len(page['result']) - 1
-                    if self.current_page == self.start_page:
-                        start_index = self.start_index_in_start_page
-                    if self.current_page == self.end_page:
-                        end_index = self.end_index_in_end_page
-                    page['result'] = page['result'][start_index: end_index + 1]
-
-                self.current_page += 1
-                self.total_read += len(page['result'])
-                # 检查是否需要使用转换函数
-                if self.transform_function is not None:
-                    transformed = self.transform_function(page)
-                    self.after_transform += len(transformed["result"])
-                    return transformed
-                self.after_transform = self.total_read
-                return page  # 统一格式
+                with ThreadPoolExecutor(max_workers=self.worker) as executor:
+                    futures, data_result = [], []
+                    for i in range(self.worker):
+                        futures.append(executor.submit(
+                            self._read_page, self.page_num, self.page_size
+                        ))
+                        self.page_num += 1
+
+                    for f in futures:
+                        data_result.extend(f.result())
+                    self.total_read += len(data_result)
+                    if len(data_result) == 0:
+                        logger.info(
+                            '数据下载完成，共读取原始数据 {} 条'.format(self.total_read))
+                        raise StopIteration
+                    return {"result": data_result, "meta": self.meta}
             except Exception as e:
                 logger.exception(
-                    f"智能工场数据源读取失败, input_address: [{self.table_id}]")
-                raise DataSourceReaderException('智能工场数据源读取失败') from e
-
-        def _get_meta(self, data):
-            if self.meta == []:
-                self.meta = [{"code": k} for k in data.keys()]
-            return self.meta
+                    f"智能工场数据源读取失败, input_address: [{self.input_address}, {self.table_id}]")
+                raise DataSourceReaderException(f'智能工场数据源读取失败,{e}')
 
         def _data_process(self, data):
-            result = []
-            if len(data) > 0:
-                self._get_meta(data[0])
-            for d in data:
-                val = []
-                [val.append(d.get(k["code"])) for k in self.meta]
-                result.append(val)
-            return {"result": result, "meta": self.meta}
+            if len(data) == 0:
+                return []
+            # 数据过滤
+            df = pd.DataFrame(data=data)
+            for nm in self.num_table_meta:
+                if not is_numeric_dtype(df[nm]):
+                    df[nm] = pd.to_numeric(df[nm], errors='coerce')
+            # 数据format
+            return df.values.tolist()
 
         def _read_page(self, page_index, page_size):
             """
             数据工场获取数据接口，分页读取数据
             :param page_index: 页码，从 0 开始
             :param page_size:  每页大小
 
             此接口：pageIndex 代表起始下标（不是页）， pagesize代表每页数据的数量， pagecount代表获取几页
                    但是返回的数据类型是[{},{}] 而不是 [[{},{}],[]], 所以保证pageSize和pageCount中某一个数为1的时候， 另一个参数就可以当size使用（很迷惑）
             例如： {'id': self.table_id, 'pageIndex': 1,'pageSize': 10, 'pageCount': 1} 和 {'id': self.table_id, 'pageIndex': 1,'pageSize': 1, 'pageCount': 10} 的结果完全一致
             :return:
             """
-            print("pageIndex: ", page_index, "pageCount:", page_size)
-            request_data = {'id': self.table_id,
-                            'pageIndex': page_index, 'pageCount': page_size}
+            request_data = {
+                'id': self.table_id,
+                'pageIndex': page_index,
+                'pageCount': page_size,
+                'tenantId': config.TENANT_ID
+            }
             response = iuap_request.post_json(
-                url=self.input_address, json=request_data)
+                url=self.input_address, json=request_data, timeout=60)
             response.raise_for_status()
             result = response.json
             return self._data_process(result["data"])
```

## intelliw/datasets/datasource_iwimgdata.py

```diff
@@ -1,15 +1,16 @@
 '''
 Author: Hexu
 Date: 2022-04-25 15:16:48
 LastEditors: Hexu
-LastEditTime: 2022-11-10 11:12:47
+LastEditTime: 2023-03-24 11:11:04
 FilePath: /iw-algo-fx/intelliw/datasets/datasource_iwimgdata.py
 Description: 图片数据集
 '''
+import ssl
 import json
 import math
 import os
 import shutil
 import urllib.request as requests
 import urllib.error
 import socket
@@ -48,24 +49,28 @@
     @classmethod
     def reset_config(cls):
         cls.set_config = {'licenses': cls.licenses, 'info': cls.info,
                           'categories': cls.categories, 'images': [], 'annotations': []}
 
     @classmethod
     def __gen_config(cls, images, annotations):
-        amap = {a['image_id']: a for a in annotations}
-        return {i['file_name']: {'image': i, 'annotation': amap[i['id']]} for i in images}
+        a_map = {}
+        for a in annotations:
+            image_id = a['image_id']
+            a_map[image_id] = a_map.get(image_id, [])
+            a_map[image_id].append(a)
+        return {i['file_name']: {'image': i, 'annotation': a_map[i['id']]} for i in images}
 
     @classmethod
     def gen_config(cls, filename):
         meta = cls.coco_config.get(filename)
         if meta is None:
             return f"image:{filename} annotation not exist"
         cls.set_config['images'].append(meta['image'])
-        cls.set_config['annotations'].append(meta['annotation'])
+        cls.set_config['annotations'].extend(meta['annotation'])
         return None
 
     @classmethod
     def flush(cls, path):
         with open(path, 'w+') as fp:
             json.dump(cls.set_config, fp, ensure_ascii=False)
 
@@ -106,28 +111,26 @@
             return self.__total
         logger = _get_framework_logger()
         params = {'dsId': self.ds_id, 'yTenantId': config.TENANT_ID}
         response = iuap_request.get(self.get_row_address, params=params)
         if 200 != response.status:
             msg = "获取行数失败，url: {}, response: {}".format(
                 self.get_row_address, response)
-            logger.error(msg)
             raise DataSourceReaderException(msg)
 
         row_data = response.json
         self.__total = row_data['data']
 
         if not isinstance(self.__total, int):
             msg = "获取行数返回结果错误, response: {}, request_url: {}".format(
                 row_data, self.get_row_address)
-            logger.error(msg)
             raise DataSourceReaderException(msg)
         return self.__total
 
-    def reader(self, page_size=1000, offset=0, limit=0, transform_function=None, dataset_type='train_set'):
+    def reader(self, page_size=10000, offset=0, limit=0, transform_function=None, dataset_type='train_set'):
         return self.__Reader(self.input_address, self.ds_id, self.ds_type, self.total(), dataset_type, page_size, offset, limit, transform_function)
 
     def download_images(self, images, transform_function=None, dataset_type='train_set'):
         r = self.reader(page_size=1, offset=0, limit=0,
                         transform_function=transform_function, dataset_type=dataset_type)
         r.set_download(images)
         return r()
@@ -141,15 +144,14 @@
             [15,19][20,39][40,44]
             offset 10 limit 5:
             [10,14]
             """
             logger = _get_framework_logger()
             if offset > total:
                 msg = "偏移量大于总条数:偏移 {}, 总条数: {}".format(offset, total)
-                logger.error(msg)
                 raise DataSourceReaderException(msg)
             self.input_address = input_address
             self.ds_id = ds_id
             self.ds_type = ds_type
             self.limit = limit
             self.offset = offset
             self.total = total
@@ -255,16 +257,16 @@
             err_count, success_count, err_msg = 0, 0, ""
             with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:
                 futures = [executor.submit(self.__download, p) for p in page]
                 for f in as_completed(futures):
                     try:
                         filename = f.result()
                         success_count += 1
-                        print("[Framework Log] dataset downloading: ",
-                              f"{success_count}/{self.total}", end='\r', flush=True)
+                        if success_count % 10 == 0:
+                            logger.info(f"dataset downloading: {success_count}/{self.total}")
                     except Exception as e:
                         err_count += 1
                         err_msg = e
                         logger.error(
                             f'dataset download error: {e}, total {err_count}')
 
             if err_count > 5 and success_count / err_count < 1000:
@@ -320,16 +322,18 @@
                     'test_set': os.path.join(abspath, config.CV_IMG_TEST_FILEPATH),
                     'annotations': os.path.join(abspath, config.CV_IMG_ANNOTATION_FILEPATH)}
 
 
 def save_file(url, filepath=None, transform_function=None, timeout=None, is_img=True):
     logger = _get_framework_logger()
     try:
+        # 创建未验证的上下文
+        context = ssl._create_unverified_context()
         request = requests.Request(url=url, method='GET')
-        response = requests.urlopen(request, timeout=timeout)
+        response = requests.urlopen(request, timeout=timeout, context=context)
         status = response.status
         if status == 200:
             if is_img:
                 data = response.read()
                 if transform_function:
                     data = transform_function(data)
                 if filepath:
```

## intelliw/datasets/datasource_local_csv.py

```diff
@@ -1,21 +1,21 @@
 '''
 Author: hexu
 Date: 2021-10-14 14:54:05
-LastEditTime: 2022-11-03 15:06:59
+LastEditTime: 2023-02-23 10:34:09
 LastEditors: Hexu
 Description: 本地csv数据集
 FilePath: /iw-algo-fx/intelliw/datasets/datasource_local_csv.py
 '''
 import time
 from intelliw.utils.exception import DatasetException
 from intelliw.datasets.datasource_base import AbstractDataSource, DataSourceReaderException
 from intelliw.utils.global_val import gl
 from intelliw.utils.logger import _get_framework_logger
-from intelliw.utils.dataprocess import Engine
+from intelliw.utils.spark_process import Engine
 
 
 logger = _get_framework_logger()
 engine = Engine()
 
 
 class DataSourceLocalCsv(AbstractDataSource):
@@ -31,15 +31,14 @@
         if self.__total >= 0:
             return self.__total
         logger.info("start read files: %s", self.file_path)
         start_time = time.time()
         csv = engine.csv(self.file_path).read()
         self.__total = csv.index.size
         if self.__total == 0:
-            logger.error('csv 文件为空')
             raise DataSourceReaderException('csv 文件为空')
         logger.info("total: %s, load time: %s",
                     self.__total, time.time() - start_time)
         return self.__total
 
     def reader(self, pagesize=10000, offset=0, limit=0, transform_function=None, dataset_type='train_set'):
         return self.__DataSourceReaderLocalCsv(self.file_path, pagesize, offset, limit, transform_function)
@@ -121,19 +120,18 @@
 
             def read(self, amount=1):
                 try:
                     self.csv = engine.csv(self.file_path).read(
                         skiprows=self.line_offset, nrows=amount)
                 except DatasetException:
                     if self.line_offset == 0:
-                        logger.error('csv 文件为空')
+                        msg = 'csv 文件为空'
                     else:
                         msg = '超出范围的 offset {}'.format(self.line_offset)
-                    logger.error(msg)
-                    raise DataSourceReaderException()
+                    raise DataSourceReaderException(msg)
 
                 batch = self.csv.values.tolist()
                 if len(batch) < amount:
                     self.is_finish = True
                     self.line_offset += len(batch)
                     return batch
                 self.line_offset += amount
```

## intelliw/datasets/datasource_nlp_corpora.py

```diff
@@ -1,11 +1,11 @@
 '''
 Author: hexu
 Date: 2021-10-14 14:54:05
-LastEditTime: 2022-11-08 15:41:00
+LastEditTime: 2023-03-16 10:38:16
 LastEditors: Hexu
 Description: nlp数据集
 FilePath: /iw-algo-fx/intelliw/datasets/datasource_nlp_corpora.py
 '''
 from random import Random
 import shutil
 import time
@@ -26,18 +26,14 @@
 
 
 class CorpusType:
     csv = 'csv'
     json = 'json'
     txt = 'txt'
 
-    TXT = 20
-    CSV = 21
-    JSON = 22
-
     ItoN = {20: "txt", 21: "csv", 22: "json"}
 
 
 class CorpusInputType:
     local = 'local'
     file = 'file'
     row = 'row'
@@ -330,27 +326,27 @@
                     "totalElements": 14,
                     "totalPages": 14
                 },
                 "msg": "成功",
                 "status": 1
             }
             """
+            page = None
             try:
                 page = self._read_page(self.page_num, self.page_size)
-                assert page is not None
+                assert page is not None, "获取的数据为空"
                 if page['status'] != 1:
                     raise Exception(page['msg'])
                 if page['data']['totalElements'] == 0:
                     raise Exception("nlp 数据获取失败, 无已标注语料")
                 content = page['data']['content']
                 content = [i if i.endswith(
                     "\n") else f"{i}\n" for i in content]
             except Exception as e:
                 errmsg = f"nlp 数据获取失败, input_address: [{self.input_address}], ds_id: [{self.ds_id}], error: [{e}], response: {page}"
-                self.logger.error(errmsg)
                 raise DatasetException(errmsg)
 
             if len(content) == 0:
                 raise StopIteration
             self.page_num += 1
             self.total_read += len(content)
             return content
```

## intelliw/datasets/spliter.py

```diff
@@ -1,11 +1,11 @@
 '''
 Author: hexu
 Date: 2021-10-14 14:54:05
-LastEditTime: 2022-11-07 19:42:51
+LastEditTime: 2023-03-30 09:33:05
 LastEditors: Hexu
 Description: 数据集切分工具
 FilePath: /iw-algo-fx/intelliw/datasets/spliter.py
 '''
 from abc import ABCMeta, abstractmethod
 import math
 from random import Random
@@ -13,42 +13,45 @@
 from intelliw.utils.exception import DatasetException
 from intelliw.config import config
 from intelliw.utils.global_val import gl
 from intelliw.utils.logger import _get_framework_logger
 from intelliw.datasets.datasource_base import DataSourceType
 
 
+class SpliterClass:
+    nothing = -1
+    sequential = 0
+    random = 1
+    random_by_target = 2
+
+
 def get_set_spliter(data, not_spliter=False):
     logger = _get_framework_logger()
     trr, var, ter = config.TRAIN_DATASET_RATIO, config.VALID_DATASET_RATIO, config.TEST_DATASET_RATIO
-    msg = "Dataset Splite Reader: \033[33m{}\033[0m"
-    if not_spliter or config.DATA_SPLIT_MODE == 9:
+    msg = "\033[33mDataset Split Reader: {}\033[0m"
+    if not_spliter or config.DATA_SPLIT_MODE == SpliterClass.nothing:
         logger.info(msg.format("no Spliter"))
         return NoSpliter(data, trr, var, ter)
-    elif config.DATA_SPLIT_MODE == 0:
-        logger.info(msg.format("Sequential Spliter"))
-        return SequentialSpliter(data, trr, var, ter)
-    elif config.DATA_SPLIT_MODE == 1:
+    elif config.DATA_SPLIT_MODE == SpliterClass.random:
         target_col, err = target_verify()
         if err is None:  # 分类算法根据目标列特殊处理
             logger.info(msg.format("Random Spliter by Target"))
             return TargetRandomSpliter(data, trr, var, ter, target_col)
         else:
             logger.info(msg.format("Random Spliter"))
             return ShuffleSpliter(data, trr, var, ter)
-    elif config.DATA_SPLIT_MODE == 2:
+    elif config.DATA_SPLIT_MODE == SpliterClass.random_by_target:
         target_col, err = target_verify()
         if err is not None:
             raise DatasetException(err)
         logger.info("dataset loader: random spliter by target")
         return TargetRandomSpliter(data, trr, var, ter, target_col)
-    else:
-        err_msg = "输出数据源设置失败，数据集划分模式: {}".format(config.DATA_SPLIT_MODE)
-        logger.error(err_msg)
-        raise ValueError(err_msg)
+    else:  # 不指定都用顺序
+        logger.info(msg.format("Sequential Spliter"))
+        return SequentialSpliter(data, trr, var, ter)
 
 
 def target_verify():
     target_col, err = None, None
     if not gl.value_is("model_type", 3):
         err = "只有分类算法才能使用该规则"
         return target_col, err
@@ -297,15 +300,15 @@
 
     def _test_reader(self):
         if self.test_data is None:
             self._set_data()
         return self.test_data
 
     def _set_data(self):
-        from intelliw.utils.dataprocess import Engine
+        from intelliw.utils.spark_process import Engine
         engine = Engine()
 
         df = engine.DataFrame(self.alldata)
         # 边界处理
         if df.shape[1] < self.target_col + 1:
             raise DatasetException(
                 f"数据集不存在目标列, 数据集列数{df.shape[1]}, 目标列下标{self.target_col}")
```

## intelliw/docs/Q&A.md

```diff
@@ -1,9 +1,15 @@
 # 常见问题
 
+
+
+
+
+
+
 ### Q: 算法框架如何获取?
 
 **A**:  安装 -> 获取脚手架
 
 **详情**：
 
 *➡️安装*：`pip install intelliw`
@@ -231,19 +237,19 @@
 ```
 my_model = None
 class Algorithm:
     def __init__(self, parameters):
         pass
         
     def load(self, path):
-        ```
+```
         1 赋值给全局变量：my_model = xxx.load(path)
         2 赋值给类属性：  self.my_model =  xxx.load(path)
         ```
-				
+
 ```
 
 
 
 
 
 ### Q: <span id="setrouter">推理如何设置服务路由？</span>
@@ -259,15 +265,15 @@
   algorithm: pass
   desc: 展示算法
   name: demo
   router:
   - path: "/predict"    # 服务访问地址  127.0.0.1:8080/predict
     func: "myinfer"     # 推理对应的Algorithm类中函数名称
     method: "post"      # 请求方式
-    need_featrue: true  # 是否定义了特征工程 
+    need_feature: true  # 是否定义了特征工程 
     desc: ""            # 描述
 ```
 
 
 
 ➡️*通过装饰器*：
```

## intelliw/docs/README.md

```diff
@@ -1,10 +1,12 @@
 AI 工作坊脚手架
 ===============
 
+
+
 欢迎使用用友AI工作坊脚手架，该脚手架提供了本地开发、调试、打包等基本功能，供开发者在**本地**开发调试。
 
 环境要求
 --------
 
 脚手架依赖 python \>= 3.6，使用
 pip进行包管理。开发者在**本地**调试时，需保证安装配置了正确的 python 环境。
@@ -13,15 +15,15 @@
 ----------
 
 ### 环境准备
 
 - 安装框架
 
   ```
-  pip install intelliw -i https://pypi.python.org/simple/ -U
+  pip install intelliw
   ```
 
 -   初始化算法文件
     生成算法文件框架，参数`name`为算法名称，默认值`example`,
     参数`output_path`为生成算法文件的位置，默认当前文件下。
 
     ```
```

## intelliw/docs/instructions.md

```diff
@@ -1,9 +1,13 @@
 # 算法框架使用手册
 
+
+
+
+
 ## 1.  流程概述
 
 算法平台支持开发者导入算法包或模型包。
 
 ### 1.1  算法导入
 
 开发者自行导入算法包的使用流程如下所示。开发者导入编写的算法包，平台校验确认算法包符合要求后，开发者可以使用导入的算法包创建模型。创建好的模型可以在平台进行训练。模型训练好后，开发者可以将该模型上线，成功上线后，即可使用该服务进行推理。
@@ -183,35 +187,53 @@
         self.parameters = parameters
         self.logger = self.parameters['framework_log']   # 日志
         pass
 
     def load(self, path):
         """
         加载模型
-            不需要调用，只需要在这里实现加载模型的方法，并赋值给一个属性就可以，例如：
+            不需要调用，只需要根据加载模型的方式是上线或checkpoint训练，在这里实现加载模型的方法，并赋值给一个属性就可以，例如：
             self.model = xxxx.load(path)
             上线流程：算法框架会将模型文件放在model/下，然后进行函数的执行
             本地流程：可以通过model.yaml中location字段进行配置，然后会传入path参数
+            训练流程：如果训练方式开启checkpoint训练，算法框架会将checkpoint模型文件放在model/下，然后进行函数的执行
             Args:
                 path : 模型文件路径，根据 model.yaml 文件中的 location 字段指定
             Returns:
                 无
         """
-        pass
+        if self.is_train_mode:
+            pass
+        if self.is_infer_mode:
+            pass
 
     def save(self, path):
         """
         保存模型， 保存为 path 的文件或文件夹会被框架程序自动打包上传到服务器
         如果是path是文件夹，需要加上/， 系统会创建该文件夹
             Args:
                 path : 模型文件路径， 文件名或文件夹名，根据算法自定义
             Returns:
                 无
         """
         pass
+        
+    def save_checkpoint(self, path, **kwargs):
+        """
+        保存checkpoint模型， 保存为 path 的文件或文件夹会被框架程序自动打包上传到服务器
+            Args:
+                path : 模型文件路径
+                save_best_only: 是否只保存最优的模型。如果save_best_only设置为true，则max_to_keep配置失效。如：save_best_only=true
+                max_to_keep：设置checkpoint模型的个数，n<0时全部保存，n>0时保存最近的n个模型。如：max_to_keep=-1
+                epoch： 保存checkpoint模型时的迭代次数。如：epoch=3
+                indice：保存checkpoint模型时的指标数据。如：indice={"loss": "0.01"}
+            Returns:
+                无
+        """
+        pass
 
     def train(self, train_data, val_data, **kwargs):
         """
         模型训练
             Args:
                 self.train_set 或 train_data : 训练数据
                 self.valid_set 或 val_data : 验证数据
@@ -244,14 +266,16 @@
 
 ```
 # 算法包Algorithm调用过程
 # 导入算法包
  -> __init__
 # 训练
  -> __init__ 
+# 如果checkpoint训练则加载模型
+ -> if CHECKPOINT_MODE: load
  -> pre-train transfrom functions 
  -> train(
         -> report_train_info 
         -> sava 
         -> report_val_info
       )
 # 推理
@@ -274,21 +298,50 @@
 
 ##### 2.3.3.1 `__init__ (self, parameters)`
 
 该函数是`Algorithm`的构造函数，`parameters`是开发者在`algorithm.yaml`中定义的超参数。`parameters` 是一个`dict`，其键为描述文件定义的`Algorithm.algorithms.parameters[].key`，其值为用户在训练时配置的超参数值， 在算法框架启动时，会根据配置文件进行参数类型转换，并将实例化后的`logger`放入`parameters`，使用时直接通过`parameters.get('framework_log')`获取。
 
 ##### 2.3.3.2 `load(self, path)`
 
-开发者需要在`load`函数中实现模型加载逻辑，推理上线过程中，算法框架会调用该方法，执行用户定义的模型加载逻辑。模型文件的绝对路径会在`path`参数中传递。该模型文件是用户在训练过程中产生的，平台会在推理时将模型文件加载到path。
+开发者需要在`load`函数中实现模型加载逻辑。
 
+1. 推理上线过程中，算法框架会调用该方法，执行用户定义的模型加载逻辑。模型文件的绝对路径会在`path`参数中传递。该模型文件是用户在训练过程中产生的，平台会在推理时将模型文件加载到path。
+2. checkpoint训练时，算法框架也会调用该方法，执行用户定义的模型加载逻辑。checkpoint模型文件的绝对路径会在`path`参数中传递。
+```python
+# 示例：    
+def load(self, path):
+        if self.is_train_mode:
+            # 加载checkpoint模型逻辑
+            pass
+        if self.is_infer_mode:
+            # 加载训练好的模型逻辑
+            pass
+```
 ##### 2.3.3.3 `save(self, path)`
 
 `save`函数是模型保存函数，开发者需要将用户的模型保存逻辑在该函数中进行实现，并在训练结束后调用该方法保存模型。`path` 是模型保存路径，用户可以使用相对路径保存到算法包所在的某个路径下，平台根据会将该路径下生成的模型文件打包上传，保存至云端。
 
-##### 2.3.3.3 `train(self, **kwargs)`
+##### 2.3.3.4 `save_checkpoint(self, path, **kwargs)`
+
+`save_checkpoint`函数是模型训练过程中保存checkpoint函数，开发者需要将训练过程中保存checkpoint模型逻辑在该函数中实现，同时需要在load(self, path)函数中写明checkpoint训练加载模型的逻辑。
+
+
+注： `save_checkpoint`方法需要在save方法之前调用并且只可调用一次，否则会报异常。
+```txt
+示例：
+save_checkpoint(path, max_to_keep=-1, epoch=0, indice={"loss": 0.01}, save_best_only=true)
+参数：
+path: checkpoint模型保存的路径，用户可以使用相对路径保存到算法包所在的某个路径下，平台根据会将该路径下生成的模型文件打包上传，保存至云端。
+save_best_only: 是否只保存最优的模型,开发者可在需要保存最优模型的位置调用save_checkpoint方法并设置save_best_only参数为True。如果save_best_only设置为true，则max_to_keep配置失效。如：save_best_only=true
+max_to_keep：设置checkpoint模型的个数，n<0时保存全部保存，n>0时保存最近的n个模型。如：max_to_keep=n
+epoch： 保存checkpoint模型时的迭代次数。如：epoch=3
+indice：保存checkpoint模型时的指标数据。如：indice={"loss": "0.012"}
+```
+
+##### 2.3.3.5 `train(self, **kwargs)`
 
 *注*： 以下格式只针对表格数据， cv, nlp 类请参考 [数据集](#dataset)
 
 `train` 函数是训练入口函数，开发者需要在该函数中实现训练逻辑。`self.train_set` 是训练集，该参数是一个`iterable`对象，开发者可以对其迭代来获取训练数据。训练数据格式如下所示：
 
 ```
 {   
@@ -317,15 +370,15 @@
     "column_meta": [Employee_ID,Gender,Age,...]    // 原始数据的列名
     "column_relation_df": Dataframe   // the map between the input columns and output columns, if column is droped, it will not exist in column_relation_df!
 }
 ```
 
 训练结束后，开发者需要在该函数调用`save`方法来保存模型。训练过程的`loss`、`lr`等数据可以调用`report_train_info`进行上报，模型验证结果可以调用`report_val_info`函数进行上报。
 
-##### 2.3.3.4 `infer(self, infer_data)`
+##### 2.3.3.6 `infer(self, infer_data)`
 
 `infer` 函数是推理函数，开发者需要在函数中实现推理逻辑。参数`infer_data`是json输入数据。对于API调用，该参数的格式如下所示：
 
 ```
 推理服务Args:
     infer_data : 推理请求数据, json输入的参数, 类型可以为列表/字符串/字典
 
@@ -410,19 +463,19 @@
       method: "get"
       need_feature: false
       desc: ""
   ```
 
 3. 不设置默认会给algorithm.py下的infer函数设置post方法的路由/predict
 
-##### 2.3.3.5 `report_train_info(self, loss, lr, iter, batchsize=1)`
+##### 2.3.3.7 `report_train_info(self, loss, lr, iter, batchsize=1)`
 
 该函数是训练信息上报函数，开发者不需要实现该函数，只需要在训练过程中调用该函数即可。该函数参数定义如下，其中`loss`是损失函数的`loss`值，`lr`是学习率，`iter`是当前的`iteration`，`batchsize`是每批训练数据大小。
 
-##### 2.3.3.6 `report_val_info(self, **kwargs)`
+##### 2.3.3.8 `report_val_info(self, **kwargs)`
 
 该函数是评估结果上报函数，开发者不需要实现该函数，只需要在训练结束后根据自身需求调用该函数传递评估结果。评估结果参数以 `kwargs` 的形式传递，当前平台支持的参数定义如下表所示。开发者可以传递感兴趣的评估结果进行上报，平台会在模型服务的评估结果页面对上报结果进行展示。
 
 | 参数名                  | 描述             |
 | ----------------------- | ---------------- |
 | truePositiveRate        | 真阳性率         |
 | falsePositiveRate       | 伪阳性率         |
@@ -450,15 +503,15 @@
 ```
 {
     "rocX": [0.1, 0.2],
     "rocY": [0.1, 0,2]
 }
 ```
 
-##### 2.3.3.7 特征工程函数
+##### 2.3.3.9 特征工程函数
 
 开发者可以根据自身算法需要，编写特征工程函数，来对训练数据、推理数据进行预处理、后处理。
 
 特征工程函数需要定义在`Algorithm`类之外，算法框架会根据开发者在`algorithm.yaml`定义的特征工程函数的`key`在`algorithm.py`中查找加载。
 
 一个典型的特征工程函数如下所示：
 
@@ -762,7 +815,181 @@
     |     |--- 1.txt/csv/json
     |     |--- 2.txt/csv/json
     |-- val      验证集
     |     |--- 1.txt/csv/json
     |     |--- 2.txt/csv/json
     |-- test     测试集
 ```
+
+
+
+### 5.4 数据输出
+
+#### 5.4.1 输出到智能输出源
+*使用流程*
+通过训练/推理得到数据 -> 标准化输出数据格式 -> 调用输出sdk   
+注意⚠️：`id`字段、`ts`字段和`batch_no`为关键字，请勿占用，以防数据丢失   
+
+*标准化输出数据格式*   
+
+ - 返回字段是否与智能输出源数据库定义一致
+ - 返回字段的列名是否与每行一一对应
+ - 如果使用数据输出功能，返回数据格式应为
+    ```python
+    output_data = {
+          "meta":[
+              {"code":"col1"},
+              {"code":"col2"},
+              {"code":"col3"}
+          ],
+          "result":[
+              ["col1_value1", "col2_value1", "col3_value1"],
+              ["col1_value2", "col2_value2", "col3_value2"],
+              ["col1_value3", "col2_value3", "col3_value3"],
+          ]
+    }
+    ```
+
+*调用输出sdk*    
+```python
+from intelliw.feature import OutPutWriter
+
+writer = OutPutWriter()
+res_data = writer.write(output_data)
+```
+
+*获取输出表元数据*   
+
+此数据为输出数据表的字段信息：
+```python
+[
+    {'code': 'customerCode', 'type': 'varchar', 'comment': 'customerCode'}, 
+    {'code': 'deliveryDate', 'type': 'date', 'comment': 'deliveryDate'}, 
+    {'code': 'id', 'type': 'varchar', 'comment': '主键'}, 
+    {'code': 'batch_no', 'type': 'varchar', 'comment': '批次号'}
+]
+```
+其中，`id`字段和`batch_no`为关键字，可以忽略，无需写入数据，写入的数据会被覆盖
+
+获取数据表元数据的方式：    
+```python
+from intelliw.feature import OutPutWriter
+
+writer = OutPutWriter()
+# 元数据信息
+columns = writer.table_columns  
+```
+
+
+
+
+
+
+## 6 分布式训练
+
+为了满足大规模模型训练的需求，算法框架考虑支持各种主流AI框架的分布式训练模式。
+目前已适配:
+ - PyTorch模型并行训练
+### 6.1 Torch
+>目前支持模型并行的方式
+想使用torch，算法同学需要对算法进行以下改造：
+
+ - 使用 `torch.distributed.init_process_group` 初始化进程组
+ - 使用` torch.nn.parallel.DistributedDataParallel` 创建 分布式模型
+ - 使用 `torch.utils.data.distributed.DistributedSampler` 创建 DataLoader
+ - 调整其他必要的地方(tensor放到指定device上，Save/Load checkpoint，指标计算等)
+
+#### BN(batch normalization)
+ - 多卡同步计算BN的时候会显著降低并行效率，如果不适用同步BN则精度会受损。
+ - 使用 `torch.nn.SyncBatchNorm.convert_sync_batchnorm()` 同步GPU之间的数据。注意，training和test阶段都需要同步
+#### 数据同步
+使用`Distributed data parallel ` 的时候，需要特殊的函数进行数据的分发: torch.utils.data.distributed.DistributedSampler
+#### 必要的新参数
+为了使用分布式训练，需要首先初始化进程组，其函数原型为
+```
+def init_process_group(backend,
+                       init_method=None,
+                       timeout=default_pg_timeout,
+                       world_size=-1,
+                       rank=-1,
+                       store=None,
+                       group_name=''):
+```
+ - backend ：指定当前进程要使用的通信后端。支持的通信后端有 gloo(CPU分布式训练)，mpi，nccl(GPU分布式训练)
+ - init_method ： 指定当前进程组初始化方式。可选参数，字符串形式。如果未指定 init_method 及 store，则默认为 env://，表示使用读取环境变量的方式进行初始化。该参数与 store 互斥。根据官网介绍，该参数还支持tcp、共享文件等方式
+ - rank ： 所有进程中的第几个进程。int 值。表示当前进程的编号，即优先级。rank=0 的为主进程，即 master 节点, 由节点自己控制无需传值。
+ - world_size ：所有机器中使用的进程数量, 由节点自己控制无需传值
+ - timeout ： 指定每个进程的超时时间
+
+
+声明分布式模型使用 `DistributedDataParallel`
+```
+class DistributedDataParallel(
+        module,
+        device_ids=None,
+        output_device=None,
+        dim=0,
+        broadcast_buffers=True,
+        process_group=None,
+        bucket_cap_mb=25,
+        find_unused_parameters=False,
+        check_reduction=False,
+        gradient_as_bucket_view=False,
+        static_graph=False)
+```
+ - model: model对象
+ - device_ids: 本地GPU的ID（从 0 开始）
+ - bucket_cap_mb: bucket缓存大小，这个参数涉及到pytorch C++的源代码，请尽量不要修改
+
+
+负责分配数据的 `torch.utils.data.distributed.DistributedSampler`
+```
+class DistributedSampler(dataset,
+                         num_replicas=None,
+                         rank=None,
+                         shuffle=True,
+                         seed=0,
+                         drop_last=False)
+```
+ - dataset: DataSet对象
+ - num_replicas: 参与训练的进程数
+ - seed: shuffle用的随机数种子
+ - drop_last: 补全数据标识。如果训练数据不是world_size的整倍数，pytorch将会由这个标识来决定最终的几个数据如何处理
+
+
+#### 推荐运行环境
+```
+dockerhub.yonyoucloud.com/c87e2267-1001-4c70-bb2a-ab41f3b81aa3/intelliw/touch-data-parallel:1.0
+```
+
+
+
+## 7 Spark数据处理
+
+Spark是一款分布式内存计算的统一分析引擎。 其特点就是对任意类型的数据进行自定义计算。
+Spark可以计算:结构化、半结构化、非结构化等各种类型的数据结构，同时也支持使用Python、Java、Scala、R以及SQL语言去开发应用程序计算数据。
+
+在算法框架中，可以通过手动开启的方式进行Spark数据处理
+```
+from intelliw.feature import Spark
+
+# 获取spark session
+sp = Spark.get_spark(master=None, appName=None, conf=None, hive_support=False)
+
+# 通过spark读取文件
+content = sp.read_file(filetype="csv", filepath="xxxx")
+```
+
+### 7.1 初始化
+Spark初始化支持本地模式和Standalone模式，Standalone模式需要用户指定集群信息
+`get_spark()`的`master`参数，可以指定为 "local"运行本地模式 或 "spark://master:7077" 以运行standalone模式。
+
+### 7.2 读取文件
+`read_file()`函数可以读取常见的数据格式
+`filetype`可以根据文件类型进行指定，支持 csv/txt/json
+`filepath`为文件链接，可以是文件夹/文件/文件链接
+
+### 7.3 推荐运行环境
+```
+dockerhub.yonyoucloud.com/c87e2267-1001-4c70-bb2a-ab41f3b81aa3/intelliw/pyspark3.3.0:py38-jdk8
+```
+
```

## intelliw/functions/feature_process.py

```diff
@@ -1,29 +1,29 @@
 # type: ignore
 '''
 Author: hexu
 Date: 2022-02-20 10:49:18
-LastEditTime: 2022-11-08 09:56:41
+LastEditTime: 2023-03-16 17:16:15
 LastEditors: Hexu
 Description: 按列的特征处理
 FilePath: /iw-algo-fx/intelliw/functions/feature_process.py
 '''
 from copy import deepcopy
 import json
 import random
 from intelliw.utils import message
 from intelliw.utils.util import get_json_encoder
 from collections import Iterable
 from intelliw.config import cfg_parser
-from intelliw.config import config as intelliw_core_config
+from intelliw.config import config as fw_config
 from intelliw.utils.logger import _get_framework_logger
 from intelliw.utils.global_val import gl
 from intelliw.utils.exception import FeatureProcessException
 from intelliw.utils.exception import DatasetException
-from intelliw.utils.dataprocess import Engine
+from intelliw.utils.spark_process import Engine
 
 try:
     import numpy as np
     from sklearn.cluster import KMeans
 except ImportError:
     raise ImportError(
         "\033[31mIf use feature process, you need: pip install scikit-learn\033[0m")
@@ -172,31 +172,33 @@
                     "column_relation_df": col_map_df, "column_meta": column_meta})
         logger.info("feature process end")
 
         # 如果是训练需要写入配置
         gl.get('feature_process')[stage] = new_config
         new_config = json.dumps(
             new_config, ensure_ascii=False, cls=get_json_encoder())
-        if not intelliw_core_config.is_server_mode():
-            # 本地测试需要反写入配置文件
-            model_yaml = injection_alg_yaml(
-                model_yaml_file, new_config, stage_infer_map[stage], "model")
-            cfg_parser.dump_config(model_yaml, model_yaml_file)
-            alg_yaml = injection_alg_yaml(
-                alg_yaml_file, new_config, stage_infer_map[stage], "algorithm")
-            cfg_parser.dump_config(alg_yaml, alg_yaml_file)
-            logger.info("injection feature process config to yaml")
-        elif gl.get('reporter'):
-            # 线上的上报到AI工作坊
-            reporter_info = {"modelInstanceId": intelliw_core_config.INSTANCE_ID,
-                             "tenantId": intelliw_core_config.TENANT_ID,
-                             "valuationResult": {stage_infer_map[stage]: new_config}}
-            gl.get('reporter').report(message.CommonResponse(
-                200, 'feature_config', 'feature_config_upload', reporter_info))
-            logger.info("report feature process config to ai-console")
+
+        if fw_config.FRAMEWORK_MODE != fw_config.FrameworkMode.Analysis:
+            if not fw_config.is_server_mode():
+                # 本地测试需要反写入配置文件
+                model_yaml = injection_alg_yaml(
+                    model_yaml_file, new_config, stage_infer_map[stage], "model")
+                cfg_parser.dump_config(model_yaml, model_yaml_file)
+                alg_yaml = injection_alg_yaml(
+                    alg_yaml_file, new_config, stage_infer_map[stage], "algorithm")
+                cfg_parser.dump_config(alg_yaml, alg_yaml_file)
+                logger.info("injection feature process config to yaml")
+            elif gl.recorder is not None:
+                # 线上的上报到AI工作坊
+                reporter_info = {"modelInstanceId": fw_config.INSTANCE_ID,
+                                 "tenantId": fw_config.TENANT_ID,
+                                 "valuationResult": {stage_infer_map[stage]: new_config}}
+                gl.recorder.report(message.CommonResponse(
+                    200, 'feature_config', 'feature_config_upload', reporter_info))
+                logger.info("report feature process config to ai-console")
         return alldata
 
 
 class FeatureProcessor:
     def __init__(self, config_list, mode):
         self.config_list = config_list
         self.mode = mode       # train or infer
@@ -309,17 +311,17 @@
                 cols.extend(tmp_frame.columns)
                 df = pd.concat([df, tmp_frame], axis=1)
                 df.columns = cols
                 if fit and self.mode == 'train':
                     self.config_list[i]['func'] = res_params
             except Exception as e:
                 import traceback
-                traceback.print_exc()
+                err_stack = traceback.format_exc()
                 raise Exception(
-                    f"origin_col_num:{feat.index}, final_num:{col_num}, order of the column:{i}", e)
+                    f"origin_col_num:{feat.index}, final_num:{col_num}, order of the column:{i}, stack:\n {err_stack}")
 
         self.__set_fit(False)
         self.col_map_df = pd.DataFrame(self.col_map_df)
         return df, self.category_cols, self.config_list, self.col_map_df
 
     def __set_fit(self, fit):
         if fit:
```

## intelliw/interface/apihandler.py

```diff
@@ -1,11 +1,11 @@
 '''
 Author: hexu
 Date: 2021-10-25 15:20:34
-LastEditTime: 2022-10-10 13:44:43
+LastEditTime: 2023-01-06 16:23:19
 LastEditors: Hexu
 Description: api处理函数
 FilePath: /iw-algo-fx/intelliw/interface/apihandler.py
 '''
 import time
 import json
 import traceback
```

## intelliw/interface/apijob.py

```diff
@@ -1,18 +1,16 @@
 #!/usr/bin/env python
 # coding: utf-8
 
 import os
 import json
-import signal
 import time
 import datetime
 import threading
 import numpy as np
-from intelliw.core.recorder import Recorder
 from intelliw.utils import message
 from intelliw.config import config
 from intelliw.core.infer import Infer
 from intelliw.utils.logger import _get_framework_logger
 from intelliw.interface import apihandler
 from intelliw.utils.util import get_json_encoder
 
@@ -38,69 +36,56 @@
     """重载flask的jsonencoder, 确保能解析numpy的json"""
     json_encoder = FlaskJSONEncoder
 
 
 logger = _get_framework_logger()
 
 
-childs = []
-
-
-def exit_handler(signum, frame):
-    for child in childs:
-        child.terminate()
-
-
 class Application():
     """推理服务路由类
     example:
         @Application.route("/infer-api", method='get', need_feature=True)
         def infer(self, test_data):
             pass
     args:
         path           访问路由   /infer-api
         method         访问方式，支持 get post push delete head patch options
         need_feature   是否需要使用特征工程, 如果是自定义与推理无关的函数, 请设置False
     """
 
     # Set URL handlers
     HANDLERS = []
-    HAS_INFER = False
 
     def __init__(self, custom_router):
         self.app = Flask(__name__, template_folder=os.path.join(os.path.dirname(__file__), "templates"),
                          static_folder=os.path.join(os.path.dirname(__file__), "static"))
         self.__handler_process(custom_router)
 
     def __call__(self):
         return self.app
 
     @classmethod
     def route(cls, path, **options):
         def decorator(f):
             func = f.__name__
-            if func == 'infer':
-                cls.HAS_INFER = True
             cls.HANDLERS.append((path, apihandler.MainHandler, {'func': func, 'method': options.pop(
                 'method', 'post'), 'need_feature': options.pop('need_feature', True)}))
             return f
         return decorator
 
     def __handler_process(self, router):
         # 加载自定义api, 配置在algorithm.yaml中
         for r in router:
             path, func, method, need_feature = r["path"], r["func"], r.get(
                 "method", "post").lower(), r.get("need_feature", True)
-            if func == 'infer':
-                Application.HAS_INFER = True
             Application.HANDLERS.append((path, apihandler.MainHandler, {
                 'func': func, 'method': method, 'need_feature': need_feature}))
 
         # 检查用户是否完全没有配置路由
-        if len(Application.HANDLERS) == 0 or not Application.HAS_INFER:
+        if len(Application.HANDLERS) == 0:
             Application.HANDLERS.append((r'/predict', apihandler.MainHandler,
                                          {'func': 'infer', 'method': 'post', 'need_feature': True}))  # 默认值
 
         # 集中绑定路由
         _route_cache = dict()
         for r, _, info in Application.HANDLERS:
             f, m, nf = info.get('func'), info.pop(
@@ -152,15 +137,14 @@
                 logger.info(
                     f"eureka server client init success, register:{should_register}, server name: {config.EUREKA_APP_NAME}")
             except Exception as e:
                 logger.error(
                     f"eureka server client init failed, error massage: {e}")
 
     def _flask_server(self):
-        signal.signal(signal.SIGILL, exit_handler)
         if self.PERODIC_INTERVAL > 0:
             timer = threading.Timer(
                 self.PERODIC_INTERVAL, self.perodic_callback)
             timer.daemon = True
             timer.start()
         try:
             from gevent.pywsgi import WSGIServer
@@ -176,21 +160,19 @@
 
                 for _ in range(cpu_count()):
                     p = Process(target=serve_forever)
                     p.start()
                     logger.info(f"multiprocessing server start, pid: {p.pid}")
             else:
                 # 携程io
-                WSGIServer(('0.0.0.0', self.port), self.app, environ={
-                           "wsgi.multithread": True}).serve_forever()
-
+                WSGIServer(('0.0.0.0', self.port), self.app).serve_forever()
         except ImportError:
             logger.warn(
                 "\033[33mIf want use a production WSGI server, you need: pip install gevent\033[0m")
-            self.app.run('0.0.0.0', self.port, threaded=True)
+            self.app.run('0.0.0.0', self.port)
 
     def perodic_callback(self):
         infer = self.app.config["infer"]
         while True:
             infer.perodic_callback()
             time.sleep(self.PERODIC_INTERVAL)
```

## intelliw/interface/batchjob.py

```diff
@@ -71,33 +71,29 @@
     INFER_CONTROLLER = Infer(path, reporter)
     msg = get_msg(True, '定时推理校验通过，上线成功')
     reporter.report(message.CommonResponse(200, batchjob_infer,
                                            '定时推理校验通过，上线成功',
                                            json.dumps(msg, cls=get_json_encoder(), ensure_ascii=False)))
 
 
-def infer_job(reporter, path, dataset_cfg, output_addr, params={}):
+def infer_job(reporter, dataset_cfg, output_dataset_cfg, params={}):
     def input():
         datasets = get_dataset(dataset_cfg)
         alldata = datasets.read_all_data()
         # 保持与训练一致
         if isinstance(datasets, MultipleDataSets):
             return alldata
         elif isinstance(datasets, DataSets):
             return [alldata]
         else:
             return None
 
-    def output(t, r):
-        out_result = {
-            'meta': str(type(r)),
-            'result': r
-        }
-        writer = get_datasource_writer(output_addr)
-        res_data = writer.write(out_result, t)
+    def output(r):
+        writer = get_datasource_writer(output_dataset_cfg)
+        res_data = writer.write(r)
         if res_data['status'] != 1:
             raise DataSourceWriterException(res_data['msg'])
 
     try:
         start_time = int(time.time() * 1000)
 
         # 输入
@@ -106,15 +102,15 @@
         # 批处理
         req = Request(params)
         global INFER_CONTROLLER
         result = INFER_CONTROLLER.pipeline._infer_process(alldata, req)
         logger.info('批处理处理结果 {}'.format(result))
 
         # 输出
-        output(start_time, result)
+        output(result)
     except DataSourceReaderException as e:
         info = f"批处理输入数据错误: {e}"
         stack_info = f"{info}, stack:\n{traceback.format_exc()}"
     except DataSourceWriterException as e:
         info = f"批处理输出数据错误: {e}"
         stack_info = f"{info}, stack:\n{traceback.format_exc()}"
     except Exception as e:
@@ -138,55 +134,56 @@
 class JobServer:
     @staticmethod
     def healthcheck():
         resp = message.HealthCheckResponse(200, "api", 'ok', "")
         return jsonify(resp())
 
     @staticmethod
-    def run(reporter, path, dataset_cfg, output_addr):
+    def run(reporter, dataset_cfg, output_dataset_cfg):
         msg = "server job start"
         logger.info(msg)
         threading.Thread(
             target=infer_job,
-            args=(reporter, path, dataset_cfg, output_addr, request.json)
+            args=(reporter, dataset_cfg,
+                  output_dataset_cfg, request.json)
         ).start()
         return jsonify({"code": "1", "msg": msg})
 
 
 class BatchService:
-    def __init__(self, format, path, dataset_cfg, output_adds, response_addr=None, task='infer'):
+    def __init__(self, format, path, dataset_cfg, output_dataset_cfg, response_addr=None, task='infer'):
         self.reporter = Recorder(response_addr)
         if task != 'infer':
             msg = get_msg(False, '批处理任务任务错误，TASK环境变量必须为infer')
             self.reporter.report(message.CommonResponse(500, batchjob_infer,
                                                         '批处理任务任务错误, TASK环境变量必须为infer',
                                                         json.dumps(msg, cls=get_json_encoder(), ensure_ascii=False)))
         self.format = format
         self.only_server = not format
         self.dataset_cfg = dataset_cfg
-        self.output_adds = output_adds
+        self.output_dataset_cfg = output_dataset_cfg
         self.path = path
 
     def _format_parse(self, _format: str):
         return [{
             'crontab': f.strip(),
             'func': infer_job,
-            'args': (self.reporter, self.path, self.dataset_cfg, self.output_adds)
+            'args': (self.reporter, self.dataset_cfg, self.output_dataset_cfg)
         } for f in _format.split("|")]
 
     def _cronjob(self):
         job_list = self._format_parse(self.format)
         crontab = Crontab(job_list, True)
         crontab.start()
         logger.info("Start cronjob")
 
     def _server(self):
         app = Flask(__name__)
-        args = {"reporter": self.reporter, "path": self.path,
-                "dataset_cfg": self.dataset_cfg, "output_addr": self.output_adds}
+        args = {"reporter": self.reporter, "dataset_cfg": self.dataset_cfg,
+                "output_dataset_cfg": self.output_dataset_cfg}
         app.add_url_rule("/batch-predict", view_func=JobServer.run,
                          methods=["POST"], defaults=args)
         app.add_url_rule(
             "/healthcheck", view_func=JobServer.healthcheck, methods=["POST", "GET"])
         logger.info(
             "Server Start: \n\033[33m[POST] /batch-predict\n[POST, GET] /healthcheck\033[0m"
         )
```

## intelliw/interface/controller.py

```diff
@@ -1,25 +1,23 @@
 #!/usr/bin/env python
 # coding: utf-8
 '''
 Author: hexu
 Date: 2021-10-25 15:20:34
-LastEditTime: 2022-11-12 11:18:12
+LastEditTime: 2023-03-27 14:17:42
 LastEditors: Hexu
 Description: 线上调用的入口文件
 FilePath: /iw-algo-fx/intelliw/interface/controller.py
 '''
-import os
-import argparse
 import traceback
-from intelliw.core.pipeline import get_model_yaml
 from intelliw.core.pipeline import Pipeline
-from intelliw.config.cfg_parser import load_config
 from intelliw.config import config
 from intelliw.utils.logger import _get_framework_logger
+from absl.flags import argparse_flags as argparse
+
 
 logger = _get_framework_logger()
 
 
 class FrameworkArgs:
     def __init__(self, args=None):
         self.path = "" if args is None else args.path
@@ -48,61 +46,39 @@
     parser.add_argument("--port", default=8888, type=int, help="port")
     parser.add_argument("-r", "--response",
                         default=None, type=str, help="response addr, which can be used to report status")
 
     return parser.parse_args()
 
 
-def is_high_performance(module_file_path):
-    if not os.path.isfile(module_file_path):
-        logger.error("未找到 model.yaml, path: {}".format(module_file_path))
-        raise Exception("未找到 model.yaml")
-    cfg = load_config(module_file_path)
-    model_cfg = cfg['Model']
-    if 'enableHighPerformanceInfer' in model_cfg and model_cfg['enableHighPerformanceInfer'] is True:
-        return True
-    return False
-
-
 def main(args):
     try:
         if args.method == "importalg":
+            config.FRAMEWORK_MODE = config.FrameworkMode.Import
             pl = Pipeline(args.response)
             pl.importalg(args.path, False)
         elif args.method == "importmodel":
-            is_high = is_high_performance(
-                os.path.join(args.path, get_model_yaml()))
-            if is_high:
-                from intelliw.interface.goinferjob import InferService
-                infer = InferService(args.name, args.port,
-                                     args.path, args.response)
-                infer.check()
-            else:
-                pl = Pipeline(args.response)
-                pl.importmodel(args.path, False)
+            config.FRAMEWORK_MODE = config.FrameworkMode.Import
+            pl = Pipeline(args.response)
+            pl.importmodel(args.path, False)
         elif args.method == "train":
             from intelliw.interface.trainjob import TrainServer
+            config.FRAMEWORK_MODE = config.FRAMEWORK_MODE or config.FrameworkMode.Train
             train = TrainServer(args.path, config.DATASET_INFO, args.response)
             train.run()
         elif args.method == "apiservice":
-            is_high = is_high_performance(
-                os.path.join(args.path, get_model_yaml()))
-            if is_high:
-                from intelliw.interface.goinferjob import InferService
-                infer = InferService(args.name, args.port,
-                                     args.path, args.response)
-                infer.run()
-            else:
-                from intelliw.interface.apijob import ApiService
-                apiservice = ApiService(args.port, args.path, args.response)
-                apiservice.run()
+            from intelliw.interface.apijob import ApiService
+            config.FRAMEWORK_MODE = config.FrameworkMode.Infer
+            apiservice = ApiService(args.port, args.path, args.response)
+            apiservice.run()
         elif args.method == "batchservice":
             from intelliw.interface.batchjob import BatchService
+            config.FRAMEWORK_MODE = config.FrameworkMode.Batch
             batchservice = BatchService(
-                args.format, args.path, config.DATASET_INFO, args.output, args.response, args.task)
+                args.format, args.path, config.DATASET_INFO, config.OUTPUT_DATASET_INFO, args.response, args.task)
             batchservice.run()
         elif args.method == "validateservice":
             from intelliw.interface.validatejob import ValidateService
             validateservice = ValidateService(
                 args.name, args.port, args.path, args.response)
             validateservice.run()
         exit(0)
```

## intelliw/interface/entrypoint.py

```diff
@@ -1,20 +1,20 @@
 '''
 Author: hexu
 Date: 2021-10-25 15:20:34
-LastEditTime: 2022-10-19 13:44:18
+LastEditTime: 2023-03-27 14:11:42
 LastEditors: Hexu
 Description: 本地使用和安装包的入口文件
 FilePath: /iw-algo-fx/intelliw/interface/entrypoint.py
 '''
-import argparse
 import os
 import zipfile
 import shutil
 from intelliw.config import config
+from absl.flags import argparse_flags as argparse
 
 
 def __parse_args():
     parser = argparse.ArgumentParser(
         description="Entry files for local debug and pypi packages")
 
     parser.add_argument("-p", "--path", default=os.getcwd(),
@@ -37,14 +37,16 @@
                        type=str, help="path of csv files as training data")
     train.add_argument("-C", "--corpus", default=None,
                        type=str, help="path of nlp corpus dir as training data")
     train.add_argument('-r', "--set_ratio", default="0.7:0.3:0", type=str,
                        help="train set:valid set:test set ratio, between 0.0 and 1.0, default is 0.7:0.3:0")
     train.add_argument("-d", "--dataset", default="",
                        type=str, help="dataset id by aiworker")
+    train.add_argument("--analysis", action='store_true',
+                       help="result analysis mode")
 
     infer = parser.add_argument_group(
         '[infer]', description='intelliw infer -p project_path -P 8080')
     infer.add_argument("-P", "--port", default=8888, type=int,
                        help="port to listen, default: 8888")
 
     parser.add_argument_group('[package_iwa/package_iwm]',
@@ -72,14 +74,16 @@
         args.task)
     return framework_args
 
 
 def __package(task: str, alg_path: str, output_path: str):
     alg_path = os.path.abspath(alg_path)
     dir_name = os.path.basename(alg_path)
+    zip_files = os.walk(alg_path)
+
     if task == 'package' or task == 'package_iwa':
         file_name = dir_name + '.iwa'
     elif task == 'package_iwm':
         file_name = dir_name + '.iwm'
     elif task == 'package_iwp':
         file_name = dir_name + '.iwp'
     else:
@@ -92,15 +96,15 @@
     if not os.path.exists(output_path):
         os.makedirs(output_path)
     # 删除存在的文件
     if os.path.exists(output):
         os.remove(output)
 
     with zipfile.ZipFile(output, 'w', zipfile.ZIP_DEFLATED) as zipf:
-        for root, _, files in os.walk(alg_path):
+        for root, _, files in zip_files:
             if root.endswith('__pycache__') or root.endswith('.git') or root.endswith('.idea'):
                 continue
 
             relative_path = root.replace(alg_path, "")
             if relative_path.startswith(os.sep):
                 relative_path = relative_path[len(os.sep):]
             output_relative_path = os.path.join(dir_name, relative_path)
@@ -184,14 +188,17 @@
 
     # set task
     config.update_by_env()
     import intelliw.interface.controller as controller
     if args.task == 'train':
         from intelliw.utils.gen_model_cfg import generate_model_config_from_algorithm_config as __generate
 
+        if args.analysis:
+            config.FRAMEWORK_MODE = config.FrameworkMode.Analysis
+
         # 数据集比例获取
         try:
             set_ratio = args.set_ratio.split(":")
             set_ratio = [float(set_ratio[i]) for i in range(3)]
         except:
             raise Exception(f"数据集比例格式错误:{args.set_ratio}, 请参考 -r 0.7:0.2:0.1")
```

## intelliw/interface/trainjob.py

```diff
@@ -1,16 +1,17 @@
 '''
 Author: Hexu
 Date: 2022-08-24 16:52:17
 LastEditors: Hexu
-LastEditTime: 2022-09-20 13:37:56
+LastEditTime: 2023-03-27 14:07:11
 FilePath: /iw-algo-fx/intelliw/interface/trainjob.py
 Description: Train entrypoint
 '''
 import traceback
+from intelliw.core.al_decorator import flame_prof_process
 from intelliw.core.trainer import Trainer
 from intelliw.core.recorder import Recorder
 from intelliw.datasets.datasets import get_dataset
 import intelliw.utils.message as message
 from intelliw.utils.logger import _get_framework_logger
 
 logger = _get_framework_logger()
@@ -19,14 +20,15 @@
 class TrainServer:
     def __init__(self, path, dataset_cfg, response_addr=None):
         self.response_addr = response_addr
         self.path = path
         self.reporter = Recorder(response_addr)
         self.dataset_cfg = dataset_cfg
 
+    @flame_prof_process
     def run(self):
         try:
             trainer = Trainer(self.path, self.response_addr)
             datasets = get_dataset(self.dataset_cfg)
             trainer.train(datasets)
         except Exception as e:
             stack_info = traceback.format_exc()
```

## intelliw/interface/validatejob.py

```diff
@@ -1,32 +1,24 @@
 #!/usr/bin/env python
 # coding: utf-8
 
 import os
 import json
-import signal
 import traceback
 from flask import Flask, views, jsonify, request
 import intelliw.utils.message as message
 from intelliw.config import config
 from intelliw.utils.util import get_json_encoder
 from intelliw.core.pipeline import Pipeline
 from intelliw.utils.logger import _get_framework_logger
 
 logger = _get_framework_logger()
-
-childs = []
 pipeline = None
 
 
-def exit_handler(signum, frame):
-    for child in childs:
-        child.terminate()
-
-
 class MainHandler(views.MethodView):
     def post(self):
         try:
             global pipeline
             r = request.json
             cfgs = r['transforms']
             data = r['data']
@@ -69,9 +61,8 @@
         pipeline = Pipeline(reporter, self.PERODIC_INTERVAL)
         pipeline.importalg(path, False)
         pipeline.reporter.report(
             message.CommonResponse(200, "validate", '', json.dumps({}, cls=get_json_encoder(), ensure_ascii=False)))
         self.app = Application(self.name)()
 
     def run(self):
-        signal.signal(signal.SIGILL, exit_handler)
         self.app.run(host='0.0.0.0', port=self.port)
```

## intelliw/utils/.DS_Store

```diff
@@ -1,14 +1,14 @@
-00000000: 0000 0001 4275 6431 0000 1000 0000 0800  ....Bud1........
-00000010: 0000 1000 0000 0025 0000 0000 0000 0000  .......%........
+00000000: 0000 0001 4275 6431 0000 1800 0000 0800  ....Bud1........
+00000010: 0000 1800 0000 100b 0000 0000 0000 0000  ................
 00000020: 0000 0000 0000 0000 0000 0000 0000 0800  ................
 00000030: 0000 0800 0000 0000 0000 0000 0000 0000  ................
-00000040: 0000 0000 0000 0002 0000 0000 0000 0000  ................
-00000050: 0000 0001 0000 1000 0000 0000 0000 0000  ................
-00000060: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00000040: 0000 0000 0000 0002 0000 0000 0000 0004  ................
+00000050: 0000 0001 0000 1000 006b 005f 0070 0072  .........k._.p.r
+00000060: 006f 0063 0000 0000 0000 0000 0000 0000  .o.c............
 00000070: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000080: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000090: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000000a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000000b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000000c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000000d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -250,126 +250,126 @@
 00000f90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001000: 0000 0000 0000 0003 0000 0000 0000 100b  ................
-00001010: 0000 0045 0000 0025 0000 0000 0000 0000  ...E...%........
-00001020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001060: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001070: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001080: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001090: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000010a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000010b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000010c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000010d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000010e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000010f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001100: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001110: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001120: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001130: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001140: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001150: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001160: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001170: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001180: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001190: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000011a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000011b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000011c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000011d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000011e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000011f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001200: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001210: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001220: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001260: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001270: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001280: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001290: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000012a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000012b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000012c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000012d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000012e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000012f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001300: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001310: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001320: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001330: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001340: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001350: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001360: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001370: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001380: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001390: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000013a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000013b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000013c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000013d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000013e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000013f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001400: 0000 0000 0000 0000 0000 0000 0000 0001  ................
-00001410: 0444 5344 4200 0000 0100 0000 0000 0000  .DSDB...........
-00001420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001430: 0100 0000 6000 0000 0000 0000 0100 0000  ....`...........
-00001440: 8000 0000 0100 0001 0000 0000 0100 0002  ................
-00001450: 0000 0000 0100 0004 0000 0000 0200 0008  ................
-00001460: 0000 0018 0000 0000 0000 0000 0100 0020  ............... 
-00001470: 0000 0000 0100 0040 0000 0000 0100 0080  .......@........
-00001480: 0000 0000 0100 0100 0000 0000 0100 0200  ................
-00001490: 0000 0000 0100 0400 0000 0000 0100 0800  ................
-000014a0: 0000 0000 0100 1000 0000 0000 0100 2000  .............. .
-000014b0: 0000 0000 0100 4000 0000 0000 0100 8000  ......@.........
-000014c0: 0000 0000 0101 0000 0000 0000 0102 0000  ................
-000014d0: 0000 0000 0104 0000 0000 0000 0108 0000  ................
-000014e0: 0000 0000 0110 0000 0000 0000 0120 0000  ............. ..
-000014f0: 0000 0000 0140 0000 0000 0000 0000 0000  .....@..........
-00001500: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001510: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001520: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001530: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001540: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001550: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001560: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001570: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001580: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000015a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000015b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000015c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000015d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000015e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000015f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001600: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001610: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001690: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000016a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000016b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000016c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000016d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000016e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-000016f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001000: 0000 0000 0000 0000 0000 0004 0000 000d  ................
+00001010: 0073 0070 0061 0072 006b 005f 0070 0072  .s.p.a.r.k._.p.r
+00001020: 006f 0063 0065 0073 0073 6277 7370 626c  .o.c.e.s.sbwspbl
+00001030: 6f62 0000 00ba 6270 6c69 7374 3030 d601  ob....bplist00..
+00001040: 0203 0405 0607 0807 080b 085d 5368 6f77  ...........]Show
+00001050: 5374 6174 7573 4261 725b 5368 6f77 546f  StatusBar[ShowTo
+00001060: 6f6c 6261 725b 5368 6f77 5461 6256 6965  olbar[ShowTabVie
+00001070: 775f 1014 436f 6e74 6169 6e65 7253 686f  w_..ContainerSho
+00001080: 7753 6964 6562 6172 5c57 696e 646f 7742  wSidebar\WindowB
+00001090: 6f75 6e64 735b 5368 6f77 5369 6465 6261  ounds[ShowSideba
+000010a0: 7208 0908 095f 101a 7b7b 2d32 3335 322c  r...._..{{-2352,
+000010b0: 2036 3531 7d2c 207b 3932 302c 2034 3336   651}, {920, 436
+000010c0: 7d7d 0908 1523 2f3b 525f 6b6c 6d6e 6f8c  }}...#/;R_klmno.
+000010d0: 0000 0000 0000 0101 0000 0000 0000 000d  ................
+000010e0: 0000 0000 0000 0000 0000 0000 0000 008d  ................
+000010f0: 0000 000d 0073 0070 0061 0072 006b 005f  .....s.p.a.r.k._
+00001100: 0070 0072 006f 0063 0065 0073 0073 6c73  .p.r.o.c.e.s.sls
+00001110: 7643 626c 6f62 0000 0316 6270 6c69 7374  vCblob....bplist
+00001120: 3030 d801 0203 0405 0607 0809 0a0b 1b56  00.............V
+00001130: 570a 5958 6963 6f6e 5369 7a65 5f10 0f73  W.YXiconSize_..s
+00001140: 686f 7749 636f 6e50 7265 7669 6577 5763  howIconPreviewWc
+00001150: 6f6c 756d 6e73 5f10 1163 616c 6375 6c61  olumns_..calcula
+00001160: 7465 416c 6c53 697a 6573 5874 6578 7453  teAllSizesXtextS
+00001170: 697a 655a 736f 7274 436f 6c75 6d6e 5f10  izeZsortColumn_.
+00001180: 1075 7365 5265 6c61 7469 7665 4461 7465  .useRelativeDate
+00001190: 735f 1012 7669 6577 4f70 7469 6f6e 7356  s_..viewOptionsV
+000011a0: 6572 7369 6f6e 2340 3000 0000 0000 0009  ersion#@0.......
+000011b0: ae0c 151d 2226 2b30 353a 3f43 484c 50d4  ...."&+05:?CHLP.
+000011c0: 0d0e 0f10 0a12 0a14 5776 6973 6962 6c65  ........Wvisible
+000011d0: 5577 6964 7468 5961 7363 656e 6469 6e67  UwidthYascending
+000011e0: 5a69 6465 6e74 6966 6965 7209 1101 7209  Zidentifier...r.
+000011f0: 546e 616d 65d4 1016 1718 191a 1b1b 5577  Tname.........Uw
+00001200: 6964 7468 5961 7363 656e 6469 6e67 5776  idthYascendingWv
+00001210: 6973 6962 6c65 5875 6269 7175 6974 7910  isibleXubiquity.
+00001220: 2308 08d4 0d0e 0f10 0a1f 1b21 0910 a208  #..........!....
+00001230: 5c64 6174 654d 6f64 6966 6965 64d4 0d0e  \dateModified...
+00001240: 0f10 1b1f 1b25 0808 5b64 6174 6543 7265  .....%..[dateCre
+00001250: 6174 6564 d40d 0e0f 100a 281b 2a09 1061  ated......(.*..a
+00001260: 0854 7369 7a65 d40d 0e0f 100a 2d0a 2f09  .Tsize......-./.
+00001270: 1073 0954 6b69 6e64 d40d 0e0f 101b 320a  .s.Tkind......2.
+00001280: 3408 1064 0955 6c61 6265 6cd4 0d0e 0f10  4..d.Ulabel.....
+00001290: 1b37 0a39 0810 4b09 5776 6572 7369 6f6e  .7.9..K.Wversion
+000012a0: d40d 0e0f 101b 3c0a 3e08 1101 2c09 5863  ......<.>...,.Xc
+000012b0: 6f6d 6d65 6e74 73d4 0d0e 0f10 1b1f 1b42  omments........B
+000012c0: 0808 5e64 6174 654c 6173 744f 7065 6e65  ..^dateLastOpene
+000012d0: 64d4 0d0e 0f10 1b45 1b47 0810 c808 5a73  d......E.G....Zs
+000012e0: 6861 7265 4f77 6e65 72d4 0d0e 0f10 1b45  hareOwner......E
+000012f0: 1b4b 0808 5f10 0f73 6861 7265 4c61 7374  .K.._..shareLast
+00001300: 4564 6974 6f72 d410 1617 184d 1f1b 1b59  Editor.....M...Y
+00001310: 6461 7465 4164 6465 6408 08d4 1816 1710  dateAdded.......
+00001320: 1b52 1b54 0810 d208 5f10 1069 6e76 6974  .R.T...._..invit
+00001330: 6174 696f 6e53 7461 7475 7308 2340 2a00  ationStatus.#@*.
+00001340: 0000 0000 0054 6e61 6d65 0914 0000 0000  .....Tname......
+00001350: 0000 0000 0000 0000 0000 0001 0008 0019  ................
+00001360: 0022 0034 003c 0050 0059 0064 0077 008c  .".4.<.P.Y.d.w..
+00001370: 0095 0096 00a5 00ae 00b6 00bc 00c6 00d1  ................
+00001380: 00d2 00d5 00d6 00db 00e4 00ea 00f4 00fc  ................
+00001390: 0105 0107 0108 0109 0112 0113 0115 0116  ................
+000013a0: 0123 012c 012d 012e 013a 0143 0144 0146  .#.,.-...:.C.D.F
+000013b0: 0147 014c 0155 0156 0158 0159 015e 0167  .G.L.U.V.X.Y.^.g
+000013c0: 0168 016a 016b 0171 017a 017b 017d 017e  .h.j.k.q.z.{.}.~
+000013d0: 0186 018f 0190 0193 0194 019d 01a6 01a7  ................
+000013e0: 01a8 01b7 01c0 01c1 01c3 01c4 01cf 01d8  ................
+000013f0: 01d9 01da 01ec 01f5 01ff 0200 0201 020a  ................
+00001400: 020b 020d 020e 0221 0222 022b 0230 0231  .......!.".+.0.1
+00001410: 0000 0000 0000 0201 0000 0000 0000 005a  ...............Z
+00001420: 0000 0000 0000 0000 0000 0000 0000 0242  ...............B
+00001430: 0000 000d 0073 0070 0061 0072 006b 005f  .....s.p.a.r.k._
+00001440: 0070 0072 006f 0063 0065 0073 0073 6c73  .p.r.o.c.e.s.sls
+00001450: 7670 626c 6f62 0000 026d 6270 6c69 7374  vpblob...mbplist
+00001460: 3030 d801 0203 0405 0607 0809 0a0b 1d45  00.............E
+00001470: 460a 4858 6963 6f6e 5369 7a65 5f10 0f73  F.HXiconSize_..s
+00001480: 686f 7749 636f 6e50 7265 7669 6577 5763  howIconPreviewWc
+00001490: 6f6c 756d 6e73 5f10 1163 616c 6375 6c61  olumns_..calcula
+000014a0: 7465 416c 6c53 697a 6573 5874 6578 7453  teAllSizesXtextS
+000014b0: 697a 655a 736f 7274 436f 6c75 6d6e 5f10  izeZsortColumn_.
+000014c0: 1075 7365 5265 6c61 7469 7665 4461 7465  .useRelativeDate
+000014d0: 735f 1012 7669 6577 4f70 7469 6f6e 7356  s_..viewOptionsV
+000014e0: 6572 7369 6f6e 2340 3000 0000 0000 0009  ersion#@0.......
+000014f0: d90c 0d0e 0f10 1112 1314 151e 2328 2d32  ............#(-2
+00001500: 363b 4058 636f 6d6d 656e 7473 556c 6162  6;@XcommentsUlab
+00001510: 656c 5776 6572 7369 6f6e 5b64 6174 6543  elWversion[dateC
+00001520: 7265 6174 6564 5473 697a 655c 6461 7465  reatedTsize\date
+00001530: 4d6f 6469 6669 6564 546b 696e 6454 6e61  ModifiedTkindTna
+00001540: 6d65 5e64 6174 654c 6173 744f 7065 6e65  me^dateLastOpene
+00001550: 64d4 1617 1819 1a1b 0a1d 5569 6e64 6578  d.........Uindex
+00001560: 5577 6964 7468 5961 7363 656e 6469 6e67  UwidthYascending
+00001570: 5776 6973 6962 6c65 1007 1101 2c09 08d4  Wvisible....,...
+00001580: 1617 1819 1f20 0a1d 1005 1064 0908 d416  ..... .....d....
+00001590: 1718 1924 250a 1d10 0610 4b09 08d4 1617  ...$%.....K.....
+000015a0: 1819 292a 1d1d 1002 10a2 0808 d416 1718  ..)*............
+000015b0: 192e 2f1d 0a10 0310 6108 09d4 1617 1819  ../.....a.......
+000015c0: 332a 1d0a 1001 0809 d416 1718 1937 380a  3*...........78.
+000015d0: 0a10 0410 7309 09d4 1617 1819 3c3d 0a0a  ....s.......<=..
+000015e0: 1000 1101 7209 09d4 1617 1819 412a 1d1d  ....r.......A*..
+000015f0: 1008 0808 0823 402a 0000 0000 0000 546e  .....#@*......Tn
+00001600: 616d 6509 1400 0000 0000 0000 0000 0000  ame.............
+00001610: 0000 0000 0100 0800 1900 2200 3400 3c00  ..........".4.<.
+00001620: 5000 5900 6400 7700 8c00 9500 9600 a900  P.Y.d.w.........
+00001630: b200 b800 c000 cc00 d100 de00 e300 e800  ................
+00001640: f701 0001 0601 0c01 1601 1e01 2001 2301  ............ .#.
+00001650: 2401 2501 2e01 3001 3201 3301 3401 3d01  $.%...0.2.3.4.=.
+00001660: 3f01 4101 4201 4301 4c01 4e01 5001 5101  ?.A.B.C.L.N.P.Q.
+00001670: 5201 5b01 5d01 5f01 6001 6101 6a01 6c01  R.[.]._.`.a.j.l.
+00001680: 6d01 6e01 7701 7901 7b01 7c01 7d01 8601  m.n.w.y.{.|.}...
+00001690: 8801 8b01 8c01 8d01 9601 9801 9901 9a01  ................
+000016a0: 9b01 a401 a901 aa00 0000 0000 0002 0100  ................
+000016b0: 0000 0000 0000 4900 0000 0000 0000 0000  ......I.........
+000016c0: 0000 0000 0001 bb00 0000 0d00 7300 7000  ............s.p.
+000016d0: 6100 7200 6b00 5f00 7000 7200 6f00 6300  a.r.k._.p.r.o.c.
+000016e0: 6500 7300 7376 5372 6e6c 6f6e 6700 0000  e.s.svSrnlong...
+000016f0: 0100 0000 0000 0000 0000 0000 0000 0000  ................
 00001700: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001710: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001720: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001730: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001740: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001750: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001760: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -378,8 +378,136 @@
 00001790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001800: 0000 0000                                ....
+00001800: 0000 0000 0000 0003 0000 0000 0000 180b  ................
+00001810: 0000 0045 0000 100b 0000 0000 0000 0000  ...E............
+00001820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001890: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000018a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000018b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000018c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000018d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000018e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000018f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001900: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001910: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001920: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001930: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001940: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001950: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001960: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001970: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001980: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000019a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000019b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000019c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000019d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000019e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+000019f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a00: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a10: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001a90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001aa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001ab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001ac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001ad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001ae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001af0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b00: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b10: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001b90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001c00: 0000 0000 0000 0000 0000 0000 0000 0001  ................
+00001c10: 0444 5344 4200 0000 0100 0000 0000 0000  .DSDB...........
+00001c20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001c30: 0200 0000 2000 0000 6000 0000 0000 0000  .... ...`.......
+00001c40: 0100 0000 8000 0000 0100 0001 0000 0000  ................
+00001c50: 0100 0002 0000 0000 0100 0004 0000 0000  ................
+00001c60: 0100 0008 0000 0000 0000 0000 0100 0020  ............... 
+00001c70: 0000 0000 0100 0040 0000 0000 0100 0080  .......@........
+00001c80: 0000 0000 0100 0100 0000 0000 0100 0200  ................
+00001c90: 0000 0000 0100 0400 0000 0000 0100 0800  ................
+00001ca0: 0000 0000 0100 1000 0000 0000 0100 2000  .............. .
+00001cb0: 0000 0000 0100 4000 0000 0000 0100 8000  ......@.........
+00001cc0: 0000 0000 0101 0000 0000 0000 0102 0000  ................
+00001cd0: 0000 0000 0104 0000 0000 0000 0108 0000  ................
+00001ce0: 0000 0000 0110 0000 0000 0000 0120 0000  ............. ..
+00001cf0: 0000 0000 0140 0000 0000 0000 0028 2d32  .....@.......(-2
+00001d00: 363b 4058 636f 6d6d 656e 7473 556c 6162  6;@XcommentsUlab
+00001d10: 656c 5776 6572 7369 6f6e 5b64 6174 6543  elWversion[dateC
+00001d20: 7265 6174 6564 5473 697a 655c 6461 7465  reatedTsize\date
+00001d30: 4d6f 6469 6669 6564 546b 696e 6454 6e61  ModifiedTkindTna
+00001d40: 6d65 5e64 6174 654c 6173 744f 7065 6e65  me^dateLastOpene
+00001d50: 64d4 1617 1819 1a1b 0a1d 5569 6e64 6578  d.........Uindex
+00001d60: 5577 6964 7468 5961 7363 656e 6469 6e67  UwidthYascending
+00001d70: 5776 6973 6962 6c65 1007 1101 2c09 08d4  Wvisible....,...
+00001d80: 1617 1819 1f20 0a1d 1005 1064 0908 d416  ..... .....d....
+00001d90: 1718 1924 250a 1d10 0610 4b09 08d4 1617  ...$%.....K.....
+00001da0: 1819 292a 1d1d 1002 10a2 0808 d416 1718  ..)*............
+00001db0: 192e 2f1d 0a10 0310 6108 09d4 1617 1819  ../.....a.......
+00001dc0: 332a 1d0a 1001 0809 d416 1718 1937 380a  3*...........78.
+00001dd0: 0a10 0410 7309 09d4 1617 1819 3c3d 0a0a  ....s.......<=..
+00001de0: 1000 1101 7209 09d4 1617 1819 412a 1d1d  ....r.......A*..
+00001df0: 1008 0808 0823 402a 0000 0000 0000 546e  .....#@*......Tn
+00001e00: 616d 6509 1400 0000 0000 0000 0000 0000  ame.............
+00001e10: 0000 0000 0100 0800 1900 2200 3400 3c00  ..........".4.<.
+00001e20: 5000 5900 6400 7700 8c00 9500 9600 a900  P.Y.d.w.........
+00001e30: b200 b800 c000 cc00 d100 de00 e300 e800  ................
+00001e40: f701 0001 0601 0c01 1601 1e01 2001 2301  ............ .#.
+00001e50: 2401 2501 2e01 3001 3201 3301 3401 3d01  $.%...0.2.3.4.=.
+00001e60: 3f01 4101 4201 4301 4c01 4e01 5001 5101  ?.A.B.C.L.N.P.Q.
+00001e70: 5201 5b01 5d01 5f01 6001 6101 6a01 6c01  R.[.]._.`.a.j.l.
+00001e80: 6d01 6e01 7701 7901 7b01 7c01 7d01 8601  m.n.w.y.{.|.}...
+00001e90: 8801 8b01 8c01 8d01 9601 9801 9901 9a01  ................
+00001ea0: 9b01 a401 a901 aa00 0000 0000 0002 0100  ................
+00001eb0: 0000 0000 0000 4900 0000 0000 0000 0000  ......I.........
+00001ec0: 0000 0000 0001 bb00 0000 0d00 7300 7000  ............s.p.
+00001ed0: 6100 7200 6b00 5f00 7000 7200 6f00 6300  a.r.k._.p.r.o.c.
+00001ee0: 6500 7300 7376 5372 6e6c 6f6e 6700 0000  e.s.svSrnlong...
+00001ef0: 0100 0000 0000 0000 0000 0000 0000 0000  ................
+00001f00: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f10: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001f90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00001ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00002000: 0000 0000                                ....
```

## intelliw/utils/check_algorithm_py.py

```diff
@@ -43,11 +43,11 @@
         if len(sig.parameters) != 2 or 'self' not in sig.parameters:
             return False, message.err_missing_load_parameter
 
     if not (hasattr(cls, 'save') and inspect.isfunction(cls.save)):
         return False, message.err_missing_save_method
     else:
         sig = signature(cls.save)
-        if len(sig.parameters) != 2 or 'self' not in sig.parameters:
+        if len(sig.parameters) < 1 or 'self' not in sig.parameters:
             return False, message.err_missing_save_parameter
 
     return True, None
```

## intelliw/utils/debug_controller.py

```diff
@@ -39,16 +39,16 @@
     '''
     DATASET_ID = None
 
     # csv
     TRAIN_CSV_PATH = None
 
     # nlp
-    TRAIN_NLP_CORPORA = None
-    iwconfig.INPUT_DATA_SOURCE_TRAIN_TYPE = 1  # 20-txt 21-csv 22-json
+    TRAIN_NLP_CORPORA = None  # 需要是文件夹
+    iwconfig.INPUT_DATA_SOURCE_TRAIN_TYPE = 22  # 20-txt 21-csv 22-json
 
     DATASET_RATIO = [0.7, 0.2, 0.1]
     iwconfig.DATA_SPLIT_MODE = 1  # 0-顺序划分 1-随机划分
 
     iwconfig.set_dataset_config(DATASET_ID, DATASET_RATIO,
                                 TRAIN_CSV_PATH, TRAIN_NLP_CORPORA)
 
@@ -145,9 +145,10 @@
     try:
         debug_train()
         # debug_infer()
         # debug_router()
         # debug_infer_http_server()
         # debug_server_available()
     except (ImportError, TypeError) as e:
-        print("\033[1;31;40m如果为debug文件错误， 请通过命令 `intelliw init_debug` 更新debug文件\033[0m")
+        print(
+            "\033[1;31;40m如果为debug文件错误， 请通过命令 `intelliw init_debug` 更新debug文件\033[0m")
         traceback.print_exc()
```

## intelliw/utils/exception.py

```diff
@@ -1,36 +1,125 @@
 '''
 Author: Hexu
 Date: 2022-03-30 11:47:31
 LastEditors: Hexu
-LastEditTime: 2022-08-31 11:18:00
+LastEditTime: 2023-03-27 14:17:57
 FilePath: /iw-algo-fx/intelliw/utils/exception.py
 Description: 错误类定义
 '''
 ####### error class #######
 
 
+import os
+
+
+class ExceptionNoStack(Exception):
+    def ignore_stack(self):
+        return True
+
+
 class PipelineException(Exception):
     pass
 
 
+class ModelLoadException(Exception):
+    def __init__(self, msg) -> None:
+        self.msg = msg
+
+    def __str__(self) -> str:
+        return '''模型加载异常\n
+        报错信息: {}\n
+        可能原因:\n
+        1 内存分配过小, 模型加载时服务崩溃\n
+        2 模型文件是否存在,加载模型的路径是否正确\n
+        3 是否补全load函数,load函数是否存在问题\n
+        4 如果使用checkpoint,检查代码是否正确加载checkpoint\n
+        '''.format(self.msg)
+
+    def ignore_stack(self):
+        return True
+
+
 class DatasetException(Exception):
-    pass
+    def ignore_stack(self):
+        return True
 
 
 class InferException(Exception):
     pass
 
 
 class FeatureProcessException(Exception):
-    pass
+    def ignore_stack(self):
+        return True
 
 
 class DataSourceDownloadException(Exception):
-    pass
+    def ignore_stack(self):
+        return True
 
 
 class LinkServerException(Exception):
     pass
 
+
 class HttpServerException(Exception):
     pass
+
+
+class CheckpointException(Exception):
+    def __str__(self) -> str:
+        return '''
+        checkpoint保存模型异常，发生错误的可能：
+            1. save()方法在save_checkpoint()方法之前调用,请在训练结束后调用save()方法保存模型
+            2. save_checkpoint()方法多次调用
+        '''
+
+    def ignore_stack(self):
+        return True
+
+
+class FileTransferDevice(Exception):
+    def __init__(self, file, transfer_type) -> None:
+        self.curkey = None
+        self.err_msg = None
+        self.transfer_type = transfer_type
+        self.__put_file__(file)
+
+    def __put_file__(self, file) -> None:
+        from intelliw.utils.storage_service import StorageService
+        from intelliw.config import config
+        import intelliw.utils.message as message
+        from intelliw.utils.global_val import gl
+
+        try:
+            if config.is_server_mode():
+                self.curkey = os.path.join(config.STORAGE_SERVICE_PATH,
+                                           config.SERVICE_ID, "data_check.json")
+                StorageService(
+                    self.curkey, config.FILE_UP_TYPE, "upload"
+                ).upload(file)
+                gl.recorder.report(message.CommonResponse(
+                    200, 'file_transfer', 'success',
+                    {'filepath': self.curkey},
+                    businessType=self.transfer_type))
+        except Exception as e:
+            import logging
+            self.err_msg = e
+            logging.error(f"上传文件错误: {e}")
+
+    def __str__(self) -> str:
+        msg = ""
+        if self.err_msg is None:
+            msg = f'''文件传输成功：
+            文件传输类型：{self.transfer_type}
+            文件详情请查看：{self.curkey}
+            '''
+        else:
+            msg = f'''文件传输错误:
+            文件传输类型：{self.transfer_type}
+            错误详情: {self.err_msg}
+        '''
+        return msg
+
+    def ignore_stack(self):
+        return True
```

## intelliw/utils/global_val.py

```diff
@@ -1,11 +1,11 @@
 '''
 Author: hexu
 Date: 2021-10-18 10:44:06
-LastEditTime: 2022-08-01 13:48:53
+LastEditTime: 2023-03-16 14:24:56
 LastEditors: Hexu
 Description: 用来存储项目中需要的全局环境变量， 减少代码的耦合
 FilePath: /iw-algo-fx/intelliw/utils/global_val.py
 '''
 import threading
 
 
@@ -19,14 +19,15 @@
         if not hasattr(GlobalVal, "_instance"):
             with GlobalVal._instance_lock:
                 if not hasattr(GlobalVal, "_instance"):
                     GlobalVal._instance = object.__new__(cls)
         return GlobalVal._instance
 
     def __init__(self):
+        self.__recorder = None
         self._global_dict = {}
 
     def set(self, key, value):
         """ 定义一个全局变量 """
         self._global_dict[key] = value
 
     def set_dict(self, _dict: dict):
@@ -47,9 +48,17 @@
     def value_is(self, key, value):
         return self._global_dict.get(key, None) == value
 
     def clear(self):
         del self._global_dict
         self._global_dict = {}
 
+    @property
+    def recorder(self):
+        return self.__recorder
+
+    @recorder.setter
+    def recorder(self, recorder):
+        self.__recorder = recorder
+
 
 gl = GlobalVal()
```

## intelliw/utils/iuap_request.py

```diff
@@ -1,12 +1,13 @@
 """
 iUAP HTTP auth protocol implementation
 """
 import time
 import math
+import ssl
 import hashlib
 import base64
 import urllib.request
 import urllib.parse
 import urllib.error
 from intelliw.config import config
 
@@ -16,14 +17,17 @@
     has_jwt_package = True
 except ImportError:
     print("[Framework Log] \033[33mIf want use authsdk, you need: pip install pyjwt\033[0m")
     has_jwt_package = False
 
 DEFAULT_TIMEOUT: float = 10.0
 
+# 全局取消证书验证
+ssl._create_default_https_context = ssl._create_unverified_context
+
 
 class Response:
     def __init__(self, status: int, body: str, error: Exception = None):
         self.status = status
         self.body = body
         self.error = error
         self.json = self._try_json()
@@ -51,29 +55,35 @@
         return 'status: {}, body: {}, error: {}'.format(self.status, self.body, self.error)
 
 
 class IuapRequestException(Exception):
     pass
 
 
-def download(url, output_path, method="GET", params=None, body=None, json=None, headers=None):
+def download(url, output_path=None, method="GET", params=None, body=None, json=None, headers=None):
     # 加签
-    token = sign(url, params)
+    token = ""
+    if has_jwt_package:
+        token = ""
+    if has_jwt_package:
+        token = sign(url, params)
+
     if headers is None:
         headers = {}
     headers['YYCtoken'] = token
 
-    resp = sign_and_request(method=method, url=url,
-                            data=body, params=params, json=json)
-    resp.raise_for_status()
-    mode = "wb"
-    if type(resp.body) == str:
-        mode = "w"
-    with open(output_path, mode) as code:
-        code.write(resp.body)
+    resp = __do_request(method=method, url=url,
+                        data=body, params=params, json=json)
+    if output_path is None:
+        return resp
+    else:
+        resp.raise_for_status()
+        mode = "w" if isinstance(resp.body, str) else "wb"
+        with open(output_path, mode) as code:
+            code.write(resp.body)
 
 
 # stream_download 流式下载文件
 def stream_download(url, output_path, method="get", params=None, body=None, json=None, headers=None):
     def report_hook(count, block_size, total_size):
         chunk_size = 1024
         speed = ((count * block_size) / (time.time() - start_time)) / \
@@ -122,50 +132,51 @@
     if headers is None:
         headers = {}
     headers['Content-type'] = 'application/json; charset=UTF-8'
     return sign_and_request(url, 'POST', headers, params, json=json, timeout=timeout)
 
 
 def put_file(url: str, headers: dict = None, params: dict = None, data: object = None,
-             timeout: float = DEFAULT_TIMEOUT) -> Response:
+             timeout: float = DEFAULT_TIMEOUT, need_auth=False) -> Response:
     if headers is None:
-        headers = {}
-        headers['Content-type'] = 'application/octet-stream'
-    return sign_and_request(url, 'PUT', headers, params, data=data, timeout=timeout)
+        headers = {'Content-Type': 'application/octet-stream'}
+    return sign_and_request(url, 'PUT', headers, params, data=data, timeout=timeout, need_auth=need_auth)
 
 
 def sign_and_request(url: str,
                      method: str = 'GET',
                      headers: dict = None,
                      params: dict = None,
                      data: bytes = None,
                      json: dict = None,
-                     sign_params: dict = None,
-                     timeout: float = DEFAULT_TIMEOUT) -> Response:
+                     timeout: float = DEFAULT_TIMEOUT, need_auth=True) -> Response:
     """
     sign and do request
 
     :param url: request url, without query
     :param method: Http request method, GET, POST...
     :param headers: request headers
     :param params: parameters will be sent as url parameters. Also used to generate signature if sign_params is None.
     :param data: request body
     :param json: Request body in Json format
     :param sign_params: params used to generate signature, if sign_params is None, signature will be generated base on params.
     :param timeout: url access timeout
     :return: response body
     """
     token = ""
-    if has_jwt_package:
-        token = sign(url, sign_params) if sign_params is not None else sign(
-            url, params)
+    if has_jwt_package and need_auth:
+        if params is None:
+            params = {}
+        params['AuthSdkServer'] = "true"
+        token = sign(url, params)
 
     if headers is None:
         headers = {}
     headers['YYCtoken'] = token
+    headers['instanceID'] = config.INSTANCE_ID
 
     return __do_request(url, method, headers, params, data, json, timeout=timeout)
 
 
 def sign(url: str, params: dict) -> str:
     """
     generate iuap signature
@@ -220,33 +231,50 @@
 def __do_request(url: str,
                  method: str = 'GET',
                  headers: dict = {},
                  params: dict = {},
                  data: bytes = None,
                  json: object = None,
                  timeout: float = DEFAULT_TIMEOUT) -> Response:
+    # data format
     if json is not None:
         import json as json_m
-        if hasattr(type(json), '__str__'):
+        try:
+            data = json_m.dumps(json).encode('utf-8')
+        except:
             data = bytes(str(json), encoding="utf-8")
-        else:
-            data = bytes(json_m.dumps(
-                json, ensure_ascii=False), encoding="utf-8")
-    if params is not None:
+
+    # params format
+    if params is not None and len(params) > 0:
         url = url + '?' + urllib.parse.urlencode(params)
-    headers['X-tenantId'] = config.TENANT_ID
+
+    # header format
+    if headers is None:
+        headers = {}
+    headers = {
+        **headers,
+        **{'X-tenantId': config.TENANT_ID,
+           'tenant_id': config.TENANT_ID,
+           'tenantId': config.TENANT_ID}
+    }
+
+    # do request
     req = urllib.request.Request(
         url=url, data=data, headers=headers, method=method)
     for i in range(1, 5):
+        body = None
         try:
-            response = urllib.request.urlopen(req, timeout=timeout)
+            # 创建未验证的上下文
+            context = ssl._create_unverified_context()
+            response = urllib.request.urlopen(
+                req, timeout=timeout, context=context)
             body = response.read()
             try:
                 body = body.decode('UTF-8')
             except UnicodeDecodeError:
                 body = body
             return Response(response.status, body)
         except urllib.error.HTTPError as e:
             if i == 4:
                 raise e
             time.sleep(i * 2)
-            print(f"[request] request retry time: {i}, url: {url}")
+            print(f"[request] request retry time: {i}, url: {url}, body: {body}, error: {e}")
```

## intelliw/utils/logger.py

```diff
@@ -1,64 +1,86 @@
 #!/usr/bin/env python
 # coding: utf-8
 from intelliw.config import config
 import threading
+import logging
 import logging.handlers
 import os
 
 framework_logger = None
 user_logger = None
 
+default_level = logging.INFO if config.is_server_mode() else logging.DEBUG
+default_format = '[%(name)s] %(asctime)s.%(msecs)03d %(levelname)s %(filename)s:%(lineno)s: %(message)s'
+default_data_format = '%Y-%m-%d %H:%M:%S'
+log_path = './logs/'
+
 
 class Logger():
     _instance_lock = threading.Lock()
 
     def __new__(cls):
         """ 单例,防止调用生成更多 """
         if not hasattr(Logger, "_instance"):
             with Logger._instance_lock:
                 if not hasattr(Logger, "_instance"):
+                    if not os.path.exists(log_path):
+                        os.makedirs(log_path)
                     Logger._instance = object.__new__(cls)
+                    Logger.__global_logger(cls)
         return Logger._instance
 
-    def __init__(self):
-        level = logging.INFO if config.is_server_mode else logging.DEBUG
+    def __global_logger(cls):
+        root_logger = logging.getLogger()
+        root_logger.setLevel(default_level)
+
+        formatter = logging.Formatter(
+            default_format, datefmt=default_data_format)
+
+        # Setup the logger to write into file
+        if os.access(log_path, os.W_OK):
+            time_file_handler = logging.handlers.TimedRotatingFileHandler(
+                os.path.join(log_path, 'iw-algo-fx.log'),
+                when='MIDNIGHT',
+                backupCount=2
+            )
+            time_file_handler.suffix = '%Y-%m-%d.log'
+            time_file_handler.setLevel(default_level)
+            time_file_handler.setFormatter(formatter)
+            root_logger.addHandler(time_file_handler)
+
+        # Setup the logger to write into stdout
+        consoleHandler = logging.StreamHandler()
+        consoleHandler.setFormatter(formatter)
+        root_logger.addHandler(consoleHandler)
 
+    def __init__(self):
         self.framework_logger = self._get_logger(
-            "Framework Log", level=level, filename="iw-algo-fx.log")
+            "Framework Log", level=default_level, filename="iw-algo-fx-framework.log")
         self.user_logger = self._get_logger(
-            "Algorithm Log", level=level, filename="iw-algo-fx-user.log")
+            "Algorithm Log", level=default_level, filename="iw-algo-fx-user.log")
 
     def _get_logger(self, logger_type, level=logging.INFO, format=None, filename=None):
         logger = logging.getLogger(logger_type)
         if logger.handlers:
             return logger
 
-        if not format:
-            format = '[%(name)s] %(asctime)s.%(msecs)03d %(levelname)s %(filename)s:%(lineno)s: %(message)s'
-
-        logging.basicConfig(level=level, format=format,
-                            datefmt='%Y-%m-%d %H:%M:%S')
-
+        format = format or default_format
         if filename is not None:
-            log_path = './logs/'
-            if not os.path.exists(log_path):
-                os.makedirs(log_path)
             if os.access(log_path, os.W_OK):
                 time_file_handler = logging.handlers.TimedRotatingFileHandler(
                     os.path.join(log_path, filename),
                     when='MIDNIGHT',
-                    interval=1,
-                    backupCount=100
+                    backupCount=15
                 )
-                format_object = logging.Formatter(
-                    format, datefmt='%Y-%m-%d %H:%M:%S')
+                formatter = logging.Formatter(
+                    format, datefmt=default_data_format)
                 time_file_handler.suffix = '%Y-%m-%d.log'
                 time_file_handler.setLevel(level)
-                time_file_handler.setFormatter(format_object)
+                time_file_handler.setFormatter(formatter)
                 logger.addHandler(time_file_handler)
         return logger
 
 
 def _get_framework_logger():
     global framework_logger
     if framework_logger is None:
```

## intelliw/utils/message.py

```diff
@@ -64,31 +64,33 @@
         return self._format()
 
 
 err_invalid_validate_request = APIResponse(500, "validate", "无效验证请求")
 
 
 class CommonResponse:
-    def __init__(self, code, typ, msg='', data=''):
+    def __init__(self, code, typ, msg='', data='', **kwargs):
         self.code = code
         self.typ = typ
         self.msg = msg
         self.data = data
+        self.other = kwargs
 
     def _format(self):
-        return {
+        keymap = {
             "id": config.INSTANCE_ID,
             "serviceId": config.SERVICE_ID,
             "msgid": get_msg_id(),
             "token": config.TOKEN,
             "code": self.code,
             "type": self.typ,
             "message": self.msg,
             "data": self.data
         }
+        return {**keymap, **self.other}
 
     def __str__(self):
         return json.dumps(self._format(), cls=get_json_encoder(), ensure_ascii=False)
 
     def __call__(self):
         return self._format()
 
@@ -117,17 +119,17 @@
 err_import_algorithm = CommonResponse(500, "importalg", "框架导入算法失败")
 err_import_model = CommonResponse(500, "importmodel", "框架导入模型失败")
 err_transform_support_type = \
     CommonResponse(500, "importmodel",
                    "当前特征工程只支持四种类型，pre-train(训练前), pre-predict(推理前), pre-train(推理后)")
 
 ok_train_finish = CommonResponse(200, 'train_finish', '', '')
-err_empty_train_data = CommonResponse(500, "train_finish", "训练错误，训练数据为空")
-err_empty_val_data = CommonResponse(500, "train_finish", "训练错误，验证数据为空")
-err_train = CommonResponse(500, "train_finish", "训练错误，算法程序错误")
+err_empty_train_data = CommonResponse(500, "train_finish", "训练错误-训练数据为空")
+err_empty_val_data = CommonResponse(500, "train_finish", "训练错误-验证数据为空")
+err_train = CommonResponse(500, "train_finish", "训练错误-算法程序错误")
 err_dataset = CommonResponse(500, "train_finish", '数据集存在问题')
 err_function_process = CommonResponse(500, "train_finish", "特征值/数据处理错误")
 
 
 err_missing_algorithm_mod = CommonResponse(
     500, "importalg", "算法模块文件 algorithm.py 无效")
 err_missing_algorithm_class = CommonResponse(
```

## intelliw/utils/storage_service.py

```diff
@@ -1,18 +1,20 @@
 '''
 Author: hexu
 Date: 2021-11-19 10:29:02
-LastEditTime: 2022-09-07 14:10:23
+LastEditTime: 2023-03-21 10:20:41
 LastEditors: Hexu
 Description: 文件存储服务
 FilePath: /iw-algo-fx/intelliw/utils/storage_service.py
 '''
 import os
+import io
+import mimetypes
 from intelliw.utils import iuap_request
-from intelliw.config.config import STORAGE_SERVICE_URL
+from intelliw.config.config import STORAGE_SERVICE_URL, INSTANCE_ID
 
 
 class StorageService:
     """
         文件传输，通过服务端获取的临时url进行文件在多个云存储（AliOss/Minio/HWObs）上的操作
         所有的请求需要进行加签
     """
@@ -27,48 +29,56 @@
                 client_type ： AliOss/Minio/HWObs
                 process_type : download/upload
             Returns:
                 self.service_url 为服务端根据client_type，process_type生成的链接， download和upload操作需要此链接
         """
         self.key = key
         self.client_type = self._get_client_type(client_type)
-        self.service_url = self.__client_init(key, self.client_type, process_type)
-
+        self.service_url = self.__client_init(
+            key, self.client_type, process_type)
+        self.content_type = mimetypes.guess_type(key)[0] or 'application/octet-stream'
+    
     def __client_init(self, key, client_type, process_type):
         """
         从服务端获取云存储操作链接
 
         下载链接一般可以直接下载的
         """
         if not STORAGE_SERVICE_URL and process_type == "download" and key.startswith("http"):
             return self.key
 
         resp = iuap_request.get(url=STORAGE_SERVICE_URL, params={
-                                "key": key, "UrlType": process_type, "clientType": client_type})
+                                "key": key, "UrlType": process_type, "clientType": client_type, 'instanceid': INSTANCE_ID})
         resp.raise_for_status
         result = resp.json
         assert result.get("status") == 1, result
         return result.get("data")
 
-    def upload(self, filepath):
+    def upload(self, file):
         """
         通过操作链接进行上传操作
             Args:
                 filepath ： 上传文件本地路径
         """
-        if os.path.exists(filepath) and os.path.isfile(filepath):
+        def _put_file(f):
+            headers = {'Content-Type': self.content_type}
+            resp = iuap_request.put_file(
+                url=self.service_url, headers=headers, data=f, need_auth=False)
+            resp.raise_for_status()
+
+        if isinstance(file, str) and os.path.exists(file) and os.path.isfile(file):
             try:
-                with open(filepath, 'rb') as f:
-                    resp = iuap_request.put_file(
-                        url=self.service_url, headers={'Content-Type': 'application/octet-stream'}, data=f)
-                    resp.raise_for_status()
+                with open(file, 'rb') as f:
+                    _put_file(f)
             except Exception as e:
                 raise Exception(f'{e}')
+        elif isinstance(file, (bytes, bytearray, io.BufferedIOBase)):
+            _put_file(file)
         else:
-            raise FileNotFoundError(f'文件 {filepath} 不存在')
+            raise FileNotFoundError('上传的文件文件不存在')
 
     def download(self, output_path, stream=False):
         """
         通过操作链接进行下载操作
             Args:
                 output_path ： 下载文件保存路径
         """
```

## intelliw/utils/util.py

```diff
@@ -165,7 +165,21 @@
     if zipfile.is_zipfile(zipfilename):
         fz = zipfile.ZipFile(zipfilename, 'r')
         for file in fz.namelist():
             fz.extract(file, unziptodir)
     else:
         raise FileExistsError('This is not zip')
     return unziptodir
+
+
+DB_NUM_TYPE = [
+    "TINYINT",
+    "SMALLINT",
+    "MEDIUMIN",
+    "INT",
+    "INTEGER",
+    "BIGINT",
+    "NUMERIC",
+    "FLOAT",
+    "DOUBLE",
+    "DECIMAL",
+]
```

## Comparing `intelliw/utils/dataprocess/engine.py` & `intelliw/utils/spark_process/engine.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 '''
 Author: Hexu
 Date: 2022-08-16 10:18:29
 LastEditors: Hexu
-LastEditTime: 2022-11-08 13:34:27
-FilePath: /iw-algo-fx/intelliw/utils/dataprocess/engine.py
+LastEditTime: 2023-03-28 16:58:10
+FilePath: /iw-algo-fx/intelliw/utils/spark_process/engine.py
 Description: data process engine
 '''
 import inspect
 import threading
 from functools import wraps
 from typing import Tuple, Union
 import numpy as np
 import pandas as pd
 from random import Random
 from intelliw.utils.exception import DatasetException, FeatureProcessException
-from intelliw.config.config import SPARK_MODE, FRAMEWORK_MODE
-
+from intelliw.config.config import SPARK_MODE, FRAMEWORK_MODE, FrameworkMode
+from intelliw.utils.logger import _get_framework_logger
+logger = _get_framework_logger()
 
 # 只有训练才需要spark
-if SPARK_MODE and FRAMEWORK_MODE in ("distributedtrain", "train"):
+if SPARK_MODE and FRAMEWORK_MODE in (FrameworkMode.DistTrain, FrameworkMode.Train):
     try:
         from .spark import Spark
         from pyspark import pandas as ps
         from pyspark.ml.feature import (Bucketizer, MaxAbsScaler, MinMaxScaler,
                                         QuantileDiscretizer, StandardScaler,
                                         VectorAssembler)
         from pyspark.pandas.series import Series as SparkSeries
@@ -80,16 +81,16 @@
             with Engine._instance_lock:
                 if not hasattr(Engine, "_instance"):
                     Engine.spark_mode = SPARK_MODE
                     if Engine.spark_mode:
                         Engine.spark_core = Spark.get_spark()
                     Engine._instance = object.__new__(cls)
                     _mode = "Spark" if Engine.spark_mode else "Normal"
-                    print(
-                        f"[Framework Log] Data Process Engine: \033[33m{_mode}\033[0m")
+                    logger.info(
+                        f"\033[33mData Process Engine: {_mode}\033[0m")
         return Engine._instance
 
     def __init__(self) -> None:
         self.pd = pd
         if Engine.spark_mode:
             self.ps = Engine.spark_core.ps
             self.ps.set_option('compute.ops_on_diff_frames', True)
```

## Comparing `intelliw/utils/dataprocess/spark.py` & `intelliw/utils/spark_process/spark.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 '''
 Author: Hexu
 Date: 2022-08-16 10:23:16
 LastEditors: Hexu
-LastEditTime: 2022-09-06 14:46:57
-FilePath: /iw-algo-fx/intelliw/utils/dataprocess/spark.py
+LastEditTime: 2023-03-28 16:50:55
+FilePath: /iw-algo-fx/intelliw/utils/spark_process/spark.py
 Description: spark function
 '''
 import os
 from intelliw.utils.exception import DatasetException
 from pyspark import SparkContext
 from pyspark import pandas as ps
 from pyspark.sql import SparkSession
@@ -24,15 +24,15 @@
     TXT = "txt"
     JSON = "json"
 
 
 class Spark(object):
     """
     Container class holding SparkContext and SparkSession instances, so that any changes
-    will be propagated across the applicatio
+    will be propagated across the application
     """
     sc = None
     spark = None
     ps = ps
 
     # config
```

## Comparing `intelliw-1.2.8.dist-info/LICENCE` & `intelliw-1.2.9.dist-info/LICENCE`

 * *Files identical despite different names*

## Comparing `intelliw-1.2.8.dist-info/METADATA` & `intelliw-1.2.9.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,31 @@
 Metadata-Version: 2.1
 Name: intelliw
-Version: 1.2.8
+Version: 1.2.9
 Summary: An easy to start Intelligent Workshop Algorithm Framework
 Home-page: http://git.yonyou.com/iuapaipaas/iw-algo-fx
 Author: yonyou
 Author-email: yonyou@yonyou.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
 Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 License-File: LICENCE
 Requires-Dist: flask (==2.0.3)
 Requires-Dist: PyYAML (==5.4.1)
 Requires-Dist: pandas (>=1.1.5)
+Requires-Dist: pycryptodome (>=3.14.1)
+Requires-Dist: absl-py (>=1.4.0)
 
 # 智能工场算法框架（Intelligent Workshop Algorithm Framework）
 
 
 > 欢迎使用用友AI工作坊脚手架，该脚手架提供了本地开发、调试、打包等基本功能，供开发者在**本地**开发调试。
 
 环境要求
```

## Comparing `intelliw-1.2.8.dist-info/RECORD` & `intelliw-1.2.9.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,83 +1,80 @@
 intelliw/.DS_Store,sha256=TutGDz113X2DKrGZocoVuWAzctpfHrMr-ILNPA1JotQ,10244
-intelliw/__init__.py,sha256=6yFUOIb1UxaHIohuHop8ow7RQ1oKSnguOLgGS_dYXQU,703
-intelliw/feature.py,sha256=vy1hdUszm6cflb_9veNoynL-07ySycK6LgMBD-Xrv3Q,435
-intelliw/prepare_env.sh,sha256=DUdQJPfluWvdkb0g-UonJH4iaaP8m2nQQBOZvb44tIo,9500
+intelliw/__init__.py,sha256=zgtXdAD1_YxjUwHiyCbzq7ywxRZOpZaKQELKg98c6cM,704
+intelliw/feature.py,sha256=m1l_YEnCLLWQpblgguFAkiwrGoRTcVt09mRqqKddz_g,607
+intelliw/prepare_env.sh,sha256=F_uxBt_3XALLiT2mqbVJ8a-rUxwH0NOtcajSX1Soxdk,9562
 intelliw/algorithms_demo/README.md,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 intelliw/algorithms_demo/algorithm.py,sha256=QHqjCiOpKVQdNB7DLS2nAZkvDSW5S1FLYBSfyr2ntvM,5402
-intelliw/algorithms_demo/algorithm.yaml,sha256=ivXpqRS6OrYkjCbjw3JAI84zqHEsoaENZ3nxotHOliM,369
+intelliw/algorithms_demo/algorithm.yaml,sha256=vqZfwbiktmJh_AYcyn9cllX99nK8-mvtQO666nqPQL0,305
 intelliw/algorithms_demo/requirements.txt,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 intelliw/config/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 intelliw/config/cfg_parser.py,sha256=g02uILyKej1bFV2iURm34IQ1K3yRFdQHcC6-mC140kk,9271
-intelliw/config/config.py,sha256=DXMiwktBfH2l8HWfXLipfksk-XbvK3J-5ne1njSiNsE,8672
+intelliw/config/config.py,sha256=4SaO-6hpmP1kKF8zOEtegKFzCubgK56wOMqo3uh-vnQ,9195
 intelliw/config/loader/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 intelliw/config/loader/schema.py,sha256=-CIPOHCmBSr_kILybYKEcSZpocSMUF71DpoocOUr30g,7455
 intelliw/config/loader/yaml_helpers.py,sha256=bdyp9r_PJSL1p0CKr46_QctygvkZjGLAe3yc-U3pfnk,2697
 intelliw/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intelliw/core/al_decorator.py,sha256=f3Rf5HgDwAQdCpslGijIDH4YHB5kAbfnrvlgmusWoQ8,7440
+intelliw/core/al_decorator.py,sha256=a0n4hlEzJc_Qk5YHnIC346MBlnWQQOnHO2-HPSlN2P8,9580
 intelliw/core/infer.py,sha256=xvwcN9IZ0rlsRWIf_drEkSlX5Wm5zz4-C72gt0wD5tI,723
 intelliw/core/linkserver.py,sha256=2sfS_psPN8PZK-0V2zyO_7NrOArI62SXLQqEfpbHsTM,9609
-intelliw/core/pipeline.py,sha256=9sqYq5JLby3WZ_8KGMz9Sc9gzNYwl8U4pM3yajTBLiY,25661
-intelliw/core/recorder.py,sha256=4mhCTgmOZ0mq9Y-LCIs74HR5ia7Ia8nkmI0QLr5v-7Q,3302
-intelliw/core/trainer.py,sha256=bycIZgS2LNFRO4-9t7qHVJRyFJHqa00p6I3uXSDuF-I,718
-intelliw/core/logs/iw-algo-fx-user.log,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intelliw/core/logs/iw-algo-fx.log,sha256=DdtXY9QI1I7DvktjS4KadNcSvbOsV7h48NGFn3u-VJA,865
+intelliw/core/pipeline.py,sha256=S5AMjLknSlSm-Dqga_ySA20dhLGtdRbg0uvNmkF3uTU,26440
+intelliw/core/recorder.py,sha256=gvnlZcC4IIICihhonXm_f6aKKyd_xkrnBzKKsDuLMG8,3398
+intelliw/core/trainer.py,sha256=T9sGvauYmdeznkMPb5zkCL0sEJjQJh7XcgLDZ8oMRTk,922
 intelliw/datasets/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intelliw/datasets/datasets.py,sha256=pYVKpIGdnk1YEKub1Awo3umuYGjOk_URwfwiS0ZMD_c,12068
-intelliw/datasets/datasource_base.py,sha256=Pe75NGuXRxFR0nq2Yt7EcadVKa2T_UShaKBrl_gUlmI,4442
-intelliw/datasets/datasource_intelliv.py,sha256=goWQENPmTh-0Az_xTRvgMxl9q-ttcf9mCPcDoxoXM4k,8280
-intelliw/datasets/datasource_iwfactorydata.py,sha256=4ctY04pjNhUcaaiFYYoqOWu1FaAb3ZlvPUtOixw1d5Q,8099
-intelliw/datasets/datasource_iwimgdata.py,sha256=2kKYGspbd_HpJhq_YRe5FFCc6sp5c3NSn0wlNJ-iOpc,14760
-intelliw/datasets/datasource_local_csv.py,sha256=m2-eTuLGInZrry_Y5N1NAo3xI94sKJu2v-S7a0QBkvE,5195
-intelliw/datasets/datasource_nlp_corpora.py,sha256=_tnu23eIMZi-la_SU1nK1t1ok5twS3ZuUKgpkjAsAII,14046
+intelliw/datasets/dataset_writer.py,sha256=qQ1MLDR5GaYmca-5Sty0pClNV-BbFUfOV0DA4CsWXck,9150
+intelliw/datasets/datasets.py,sha256=9hEiM9-ARyeDCQswaPYpoy-1hp5GYEM7RAqGUxePUqU,12269
+intelliw/datasets/datasource_base.py,sha256=GbLv582EcIa493dHmiXoXm_bWnfR-E-0xMidS4IslaI,4749
+intelliw/datasets/datasource_intelliv.py,sha256=9PufGLGYOpZ5lX69D9gEvmS4mMaMIQl4iXuOVT3XLCQ,6307
+intelliw/datasets/datasource_iwfactorydata.py,sha256=pPbMdpMBC0kVDyEK2Ibbqjg2SaH9iBkdkc9UanfQorc,7809
+intelliw/datasets/datasource_iwimgdata.py,sha256=d1GA8tLP1I6VTjSlGD8IbM_ELDECod7U7ZSFuQtxZg0,14898
+intelliw/datasets/datasource_local_csv.py,sha256=gQGJoWkatjXLpmnJfFNja_9AatgrF4TKetlZJbg52H0,5109
+intelliw/datasets/datasource_nlp_corpora.py,sha256=uObhp4s2513nxZlD_V_rVNIu7a-DF7MzhF-n2fB6sUw,14012
 intelliw/datasets/datasource_remote_csv.py,sha256=3QCjaKSeZCASQD4HxhEMDKLmiWXpOmsg8naQzXPM8zo,1986
-intelliw/datasets/spliter.py,sha256=ZgVP48d4oO7od3pZrEw_-A7FV9MrA4DI3Q8ZBDIoDr0,12687
-intelliw/docs/Q&A.md,sha256=6x3mbNssgaPdtKTKzPeEtt717Z9wCpQx0VhUa4TAxCo,10284
-intelliw/docs/README.md,sha256=Y3yRmB94up5YtopqWcOzu61SVhgs_nuFI78dOsIqbjE,3178
-intelliw/docs/instructions.md,sha256=KsygGulRNsA_KMKn_8UEUZeuvy_rdmHnoGLCKu2CmSc,42744
+intelliw/datasets/datasource_semantic.py,sha256=Q1Uv3-UMQsb3QW_M8-P9c0MqW18QDRx1Mbcv-8Wk_kk,6752
+intelliw/datasets/spliter.py,sha256=161Fyc4M3A5nrUChYCxfskg4vhlbxVyMRgNu9RdJ5S4,12667
+intelliw/docs/Q&A.md,sha256=6793NZF4lFbXzgP5aHLK5f2ZI7wjPALhtoN5GztnDCY,10278
+intelliw/docs/README.md,sha256=1c8NJ71VvMilxIHrvMlvS4MrPCxkqRXJtEwS6t2aLBc,3142
+intelliw/docs/instructions.md,sha256=G5bMD0Pq-jVmBSkZYmfQOcYD71dwYrtqk3cmq5DVlRA,52196
 intelliw/functions/__init__.py,sha256=HgRYG-ApO4hXiHu8mcNP-8IZLSImSGbwSAfQMJ1xMm4,699
 intelliw/functions/data_filtering.py,sha256=qLmf2JyVgipn4jXO3wJNVYpHMkdWv1V3towbgQ4_hzI,5512
-intelliw/functions/feature_process.py,sha256=dj0mvP6IPmBYx04encjB_CjnkSykmcn35SxzN5nXmpU,37437
+intelliw/functions/feature_process.py,sha256=q_ZF68-sUhcnEhGtu5P9LWpB_fBwBJGSYN3P8OH1Hpo,37567
 intelliw/functions/general.py,sha256=LlqalXzNAYbSl224fhKTo5RH3kdE_T0Ek88Yktj52kQ,619
 intelliw/functions/matrix.py,sha256=8lNKGWC6KaHtw9SvhtF90ptuonjq-EMWWn6PyNv3v6o,1486
 intelliw/functions/opencv.py,sha256=kXruqVOB11DTRdQIB0zZEJi5tx8aubrXrxTaW0Xa_-0,1711
 intelliw/functions/post.py,sha256=cf1c-TJ4bwNwXldAhdr9zOBLBkOT3M4-fMmJQwwbUfM,650
 intelliw/functions/select_columns.py,sha256=U4DNOnCJ3mTOudQx_1TNt2vciPu04zpN7P0RXNGJvk4,779
-intelliw/infer/conf/syslog.xml,sha256=gwoeZidu4RROHzlUi74khVYkwoyCuhpzDeBaEQiCMsQ,1383
 intelliw/interface/__init__.py,sha256=Kp7gnULYZPdO7EReW-0DKWdyV7PI-AB9QAB9B8l6Lx8,171
-intelliw/interface/apihandler.py,sha256=P_kxv1nhT3s2Otv5Mg_DakCfjhkEttKRqL285ylnPQo,5030
-intelliw/interface/apijob.py,sha256=wZAxKzFsCRNgynWpFW1hLwan0WdNdbrPHElIH3VVMA8,7484
-intelliw/interface/batchjob.py,sha256=d8GwgUfwbqGtVL6rjIb8489fKW--PvM0lu3r7EgxXEs,7687
-intelliw/interface/controller.py,sha256=JZIMSmh2pJFvyjRDwHzRXqDxZADOam0g0C2JmGwgjKA,5013
-intelliw/interface/entrypoint.py,sha256=QeKD6nb1bo_mlZGudmxJ_zg5odGmuj67P67hE8pnM30,8383
-intelliw/interface/goinferjob.py,sha256=POJ-q1vvPapRO_MOTPQ9_zl5KD39Lci0C7zMe7bh7sI,5164
-intelliw/interface/trainjob.py,sha256=kI5Bq661zq3kZe716QMBicbmYCBEfvi1Isb4xHBuHe8,1204
-intelliw/interface/validatejob.py,sha256=5bcB0G_BXhf9js4xcOSsDy_BgEEc3MXUdv9X0k5CriM,2636
-intelliw/logs/iw-algo-fx-user.log,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intelliw/logs/iw-algo-fx.log,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intelliw/utils/.DS_Store,sha256=1lFlJ5EFymdzGAUAaI30vcaaLHt3F1LwpG7xILf9jsM,6148
+intelliw/interface/apihandler.py,sha256=DdqZCv_LOINDjtx928Y62HscwEkGT6INL3LI9RCAp3k,5030
+intelliw/interface/apijob.py,sha256=vn8tcad6b1pg3TazuQZpFB7MdjLMdLn7MvvZuyZ_XHo,6998
+intelliw/interface/batchjob.py,sha256=53h_AVf0BbOEgJe-v_g3UyHapalpQatIAHvkltybYD4,7606
+intelliw/interface/controller.py,sha256=XJOOwy0GQM9NHYAqGAmpgyu5jpVE2Q1bncEnFVwUrOY,4092
+intelliw/interface/entrypoint.py,sha256=KhyUGL_Ns2WxLrEW9_llF1uFPkPcc0k7RjCl3TcHCWo,8647
+intelliw/interface/trainjob.py,sha256=xWcKhWBeWQuiOJ9d1VNovDPKsW3PcEg5429IiWY2MK4,1286
+intelliw/interface/validatejob.py,sha256=oBG9OLwQbmPL-lJSdXIME4YSKGG7oki7iBdj4ejkRSM,2463
+intelliw/utils/.DS_Store,sha256=-iDqkB4TeefLohfmGyM3ZMwomwFpXpB3tmKSEdxBv4s,8196
 intelliw/utils/__init__.py,sha256=GSxKDj3SJV5XVFtCgACfiPZmxO1ooV8XgMCpyu-1zgE,167
-intelliw/utils/check_algorithm_py.py,sha256=HWuleN1ilHVD8ADEv8ZG4yWaABlEAoAjavPV9bJvhOo,2049
+intelliw/utils/check_algorithm_py.py,sha256=eYbwT0VFsbStIJ1HATfOMP6SevaB22pPTaksRkVHw9c,2048
 intelliw/utils/crontab.py,sha256=4Z-M45gFvIea_RoL06MewCPuGRTM17jgR4fu8RD3Vh8,4103
-intelliw/utils/debug_controller.py,sha256=vXt1nnJayQ0uSnoHSWFI7JA2GMvpDWcFJt8321HpVzM,5267
-intelliw/utils/exception.py,sha256=wee-W6Ad7z3BRDGRs2cOp6APec_J34HcBZrP8wy6dzQ,550
-intelliw/utils/gen_high_performance_cfg.py,sha256=XOIjVaI-FSvGCx-Iy2yeIovTzZAo7lhmyQO7dDQSpTM,8802
+intelliw/utils/debug_controller.py,sha256=wNBOnGAjCRHpQLB6SVVW9JzLEMwDCIDc1NiHTpRxt9Q,5303
+intelliw/utils/exception.py,sha256=Yg25nXDTGFwIdpb2DC-0xp0iY7456KSMY8sjFZlaOlc,3352
 intelliw/utils/gen_model_cfg.py,sha256=NxmS1Gm1lO2liic8lAsdgPzH5-cSX7Nq-kcxI5IvlNc,2233
-intelliw/utils/global_val.py,sha256=7ND2omq10iaaP7LYqm7w_oSFRQuznl625aKQGHxYp6M,1634
-intelliw/utils/iuap_request.py,sha256=4zRgC3Uwe5pi0F7zpUtioNiRICx5ChmU8Q7Aozv-OJQ,8106
-intelliw/utils/logger.py,sha256=D7z5lFJX66XPkDuH9uXMtr8pa0UnysL8fGS3HFsXQi4,3044
-intelliw/utils/message.py,sha256=VaEu6XBgbnsJW_3nik0zovdxxv622JLpoZL7dHXDuc8,6506
-intelliw/utils/storage_service.py,sha256=10oib5XVI7QiGBIKObKNvd711oCn322ULGghgicEqXM,3410
-intelliw/utils/util.py,sha256=vh6SvslIsELu4NzHRwFjCETfLetPZXd27tCbVfD9jo0,5443
+intelliw/utils/global_val.py,sha256=OMrd2zFzMndaQDsemDkQLOxnUPiJ-5dyvb7M4axuqq0,1826
+intelliw/utils/iuap_request.py,sha256=fDvs4ZKr_EGhyau1IKHTldFYsWeovymFn65SD-cLBQo,8800
+intelliw/utils/logger.py,sha256=ClOchRKbLREgD6Ys3ayotz4TtRryYIap82fZ2AmJSMo,3982
+intelliw/utils/message.py,sha256=IAbf-F5nFwyUyWaYlhjbNsIfh9KCG56kdXDnjnWe55k,6582
+intelliw/utils/storage_service.py,sha256=-AYWT3muxLOvBnzSXuwndkH88lniMYF21dTND3Dmwrc,3765
+intelliw/utils/util.py,sha256=JB_UZ11C4ru32AEpIzSYmFf3NERjdTEEozlN4SYLt4g,5621
 intelliw/utils/database/__init__.py,sha256=O1O1dlwx3aI41RN_3R6rhp0-zwHMmsRdf-Fb0N-5_cg,466
 intelliw/utils/database/connection.py,sha256=0_-TatMn_aU8m4O2BScdODH0eymgFpyRgxCHfV6idDk,3253
 intelliw/utils/database/proxy.py,sha256=P5WOq4bwsTUDloOQss901Dij70BbduIQuM-AEeaZNN4,2023
 intelliw/utils/database/schema.py,sha256=a19X5qr3woPC_BQy-HMObgie754xQaDO8UsvTaTd2G0,2605
-intelliw/utils/dataprocess/__init__.py,sha256=VJ7OMg8K06lStMmcAyZf-BP0MiRnIVPXBhytMYEZDac,200
-intelliw/utils/dataprocess/engine.py,sha256=rvuuv_qUITKgDwYpvFoZhhnnkd2yagV83eZFFlAoQZU,25295
-intelliw/utils/dataprocess/spark.py,sha256=_e7xv6-B3VBpBIhsnh0fAbxcLPQ28mAXHuhMTTxfn9E,4417
-intelliw-1.2.8.dist-info/LICENCE,sha256=jOtLnuWt7d5Hsx6XXB2QxzrSe2sWWh3NgMfFRetluQM,35147
-intelliw-1.2.8.dist-info/METADATA,sha256=-25TXWYzVZaozn2wmFn8gWO3RKKBGubdt3ljVOLKLuA,3946
-intelliw-1.2.8.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-intelliw-1.2.8.dist-info/entry_points.txt,sha256=E_-Vq2ijv1UDQH5T8qTxFhwDQJkMAoTqOxYqagIiD80,64
-intelliw-1.2.8.dist-info/top_level.txt,sha256=dmGM8TdTr29-SSGGklu4UuJvUesyssGJcegjlhTaYLE,9
-intelliw-1.2.8.dist-info/RECORD,,
+intelliw/utils/redis/__init__.py,sha256=PP_dh6Eqc3MUU3juS_bDo99qjWU3vXZq9VDncUaChwA,336
+intelliw/utils/redis/connection.py,sha256=7XudE8hOsnsBx-Tw6ZNzplEkWjS2mveh8BbRm3qF8_I,3397
+intelliw/utils/spark_process/__init__.py,sha256=VJ7OMg8K06lStMmcAyZf-BP0MiRnIVPXBhytMYEZDac,200
+intelliw/utils/spark_process/engine.py,sha256=dkHIL0ed9EJ9K-ovx4ONwcFrrj-eq-jyxLhKGw2yUAo,25407
+intelliw/utils/spark_process/spark.py,sha256=C9hz566trwIS8iMbsnkVkDMyKwRZzHblBojOSIuoIXY,4420
+intelliw-1.2.9.dist-info/LICENCE,sha256=jOtLnuWt7d5Hsx6XXB2QxzrSe2sWWh3NgMfFRetluQM,35147
+intelliw-1.2.9.dist-info/METADATA,sha256=DReuprRQcjtU75No3xNxey3SBGVVISes2O6YUAen15E,4069
+intelliw-1.2.9.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+intelliw-1.2.9.dist-info/entry_points.txt,sha256=E_-Vq2ijv1UDQH5T8qTxFhwDQJkMAoTqOxYqagIiD80,64
+intelliw-1.2.9.dist-info/top_level.txt,sha256=dmGM8TdTr29-SSGGklu4UuJvUesyssGJcegjlhTaYLE,9
+intelliw-1.2.9.dist-info/RECORD,,
```

